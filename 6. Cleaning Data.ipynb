{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d448308a",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis and Data Cleaning\n",
    "\n",
    "- Data cleaning is the process of identifying and correcting errors and inconsistencies in data sets so that they can be used for analysis. \n",
    "- This is to understand what is happening within their businesses, deliver trustworthy analytics any user can leverage, and help their organizations operate more efficiently.\n",
    "- Create functions to automate the repetitive parts of EDA and Data Cleaning.\n",
    "- Another benefit of using functions in the EDA and Data Cleaning is to\n",
    "    - eliminate the inconsistency of results caused by accidental differences in the code.\n",
    "    \n",
    "Data Quality\n",
    "- accuracy\n",
    "- completeness\n",
    "- consistency\n",
    "- integrity\n",
    "- timeliness\n",
    "- uniformity\n",
    "- validity\n",
    "\n",
    "Data characteristics and attributes are used to measure the cleanliness and overall quality of data sets\n",
    "    \n",
    "Steps to follow:\n",
    "- Modifying the column names\n",
    "    - Make sure feature names follow certain format\n",
    "        - Lowercase\n",
    "        - Spaces represented by underscore\n",
    "- Examine data to \n",
    "    - Inspect data types, \n",
    "    - Identify null values, and \n",
    "    - Inspect the summary statistics for each column available.\n",
    "- Casting/ Data type convertsion\n",
    "- Separate Data Types\n",
    "    - Make sure numerical and categorical/ object type columns are separated for further analysis   \n",
    "- Dealing with Duplicates\n",
    "- Dealing with Missing Values\n",
    "- Dealing this Outliers\n",
    "- Dealing wih Noisy Data\n",
    "    - Data binning\n",
    "- For numerical fields, Do some Normalisation\n",
    "- For Object filds, Do some Stirng manipulation\n",
    "- Data Transformation - Column corrections\n",
    "- Data Transformation - Creating Computed fields/columns.\n",
    "- Explore Relationship with Target variable (Machine learning)\n",
    "    - Explore whether there is a relationship between our potential feature columns and our target\n",
    "        - Calculate the correlations between potential features and our target \n",
    "        - Correlations won’t be calculated for non-numeric columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988b6a35",
   "metadata": {},
   "source": [
    "# Automating Data Cleaning\n",
    "\n",
    "- To make sure that our processing steps are fairly generic and can adapt to various types of datasets.\n",
    "- Things to look at\n",
    "    - What format does my data come in? CSV, JSON, text? Or another format? \n",
    "        - How am I going to handle this format?\n",
    "    - What data types do our data features come in? \n",
    "        - Does our dataset contain categorical and/or numerical data? \n",
    "        - How do we deal with each? \n",
    "        - Do we want to one-hot encode our data, and/or perform data type transformations?\n",
    "    - Does our data contain missing values? \n",
    "        - If yes, how do we deal with them? \n",
    "        - Do we want to perform some imputation technique? \n",
    "        - Or can we safely delete the observations with missing values?\n",
    "    - Does our data contain outliers? \n",
    "        - If yes, do we apply a regularization technique, or do we leave them as they are? …and wait, \n",
    "        - what do we even consider as an “outlier”?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00675613",
   "metadata": {},
   "source": [
    "#  Rename column names\n",
    "\n",
    "- Column names that have capital letters or space in between, very often we need to change the column names and replace them with lowercase and underscore \n",
    "- To make the column names as explicit as possible such that your friends will roughly know what a column contains just by looking at the column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3c187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_col(df): \n",
    "    '''\n",
    "    AIM    -> rename column names\n",
    "    \n",
    "    INPUT  -> df\n",
    "    \n",
    "    OUTPUT -> updated df with new column names \n",
    "    ------\n",
    "    '''\n",
    "    df.rename(index=str, columns={'col_1': 'new_col_1',\n",
    "                                  'col_2': 'new_col_1'}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c7639",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns = [i.replace(' ', '_').lower() for i in train.columns] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d1a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_cleaner(df):\n",
    "    df.columns = [x.replace(\"leadingword\", '').replace('anotherword','') for x in df.columns]\n",
    "    df.fillna('', inplace = True)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff7461",
   "metadata": {},
   "source": [
    "# Data Inspection/examination\n",
    "\n",
    "Define a function, that takes my data as an input, and returns a data frame where each \n",
    "- feature in my data set is now a row and the summary statistics are columns.\n",
    "\n",
    "The function will take a data frame as an input and calculate summary statistics to reveal insights about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ames_eda(df): \n",
    "    eda_df = {}\n",
    "    eda_df['null_sum'] = df.isnull().sum()\n",
    "    eda_df['null_pct'] = df.isnull().mean()\n",
    "    eda_df['dtypes'] = df.dtypes\n",
    "    eda_df['count'] = df.count()\n",
    "    eda_df['mean'] = df.mean()\n",
    "    eda_df['median'] = df.median()\n",
    "    eda_df['min'] = df.min()\n",
    "    eda_df['max'] = df.max()\n",
    "    \n",
    "    return pd.DataFrame(eda_df)\n",
    "ames_eda(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503a792",
   "metadata": {},
   "source": [
    "### Inspect what all my column types and evaluate if there are any implications for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009b3f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d26cc",
   "metadata": {},
   "source": [
    "# Separating our features into numerical and categorical early on\n",
    "\n",
    "To see all the different object columns, the following will return a list\n",
    "\n",
    "Inspect the data dictionary for a high level explanation of what these columns represent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf505cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = airbnb.select_dtypes(include=['object'])\n",
    "num_df = airbnb.select_dtypes(exclude=['object'])\n",
    "\n",
    "def printColumnTypes(non_numeric_df, numeric_df):\n",
    "    '''separates non-numeric and numeric columns'''\n",
    "    print(\"Non-Numeric columns:\")\n",
    "    for col in non_numeric_df:\n",
    "        print(f\"{col}\")\n",
    "    print(\"\")\n",
    "    print(\"Numeric columns:\")\n",
    "    for col in numeric_df:\n",
    "        print(f\"{col}\")\n",
    "        \n",
    "printColumnTypes(cat_df, num_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfb5285",
   "metadata": {},
   "source": [
    "### Exploring the Object Columns:\n",
    "\n",
    "- Object columns may be categorical or ordinal features that:\n",
    "    - can be converted to numeric values through data cleaning, and\n",
    "    - are intuitively related to the price of a house — \n",
    "        - a house with central air would logically have a higher sale price than one without, holding all else constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea745170",
   "metadata": {},
   "source": [
    "# Data validation: \n",
    "\n",
    "### Duplicate values.\n",
    "\n",
    "the nunique() function can be used to check for duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with missing values and duplicate values\n",
    "data = {'col1': [1, 2, None, 4, 5],\n",
    "        'col2': ['A', 'B', None, 'D', 'E'],\n",
    "        'col3': ['X', 'Y', 'Z', 'X', 'Y']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Check for duplicate values\n",
    "duplicate_values = df.nunique()\n",
    "print(\"\\nDuplicate Values:\")\n",
    "print(duplicate_values)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().any()\n",
    "print(\"Missing Values:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0591784b",
   "metadata": {},
   "source": [
    "# Dealing with Duplicates\n",
    "\n",
    "- When there are identical rows in the dataset, it is duplicate data problem. \n",
    "- It can happen because of data combination mistake (same row coming from multiple sources), \n",
    "    - the user might submit his or her answer twice, etc. \n",
    "    - Ideal way to handle the issue is just to delete the copy rows.\n",
    "\n",
    "- rows have duplicate values\n",
    "- duplicated values, call duplicated().any() on your data frame, and if it’s true, use the drop_duplicates function.\n",
    "- specify columns where you want to remove duplicate values\n",
    "\n",
    "##### Check for duplicates\n",
    "- check weather there is duplicate row or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450661a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb.duplicated().any()\n",
    "\n",
    "#if true\n",
    "airbnb.drop_duplicates()\n",
    "\n",
    "#if you want to drop duplicates at specific column\n",
    "airbnb.drop('col_name', axis=1, inplace=True).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c768040",
   "metadata": {},
   "source": [
    "# Dealing with Duplicates\n",
    "\n",
    "### Removing duplicate rows: \n",
    "\n",
    "The drop_duplicates() function in pandas can be used to remove duplicate rows from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with duplicate rows\n",
    "data = {'col1': [1, 2, 3, 3, 4, 5, 5],\n",
    "        'col2': ['A', 'B', 'C', 'C', 'D', 'E', 'E']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Print the DataFrame without duplicate rows\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2923ba9e",
   "metadata": {},
   "source": [
    "# Missing Values\n",
    "\n",
    "- place of empty cells itself can tell you something useful\n",
    "    - NA values are back to back only at the tail or in the middle of the dataset. This means, \n",
    "        - there may be a technical problem during data collection .\n",
    "        - analyze data collection process for that particular sequence of samples and try to find the origin of the issue.\n",
    "        \n",
    "### Missing values\n",
    "\n",
    "The isnull() function in pandas can be used to check for missing values in a DataFrame and "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6e154",
   "metadata": {},
   "source": [
    "### Outputing columns in the dataframe that have null values (NaN) in them.\n",
    "\n",
    "- use the .info() method and check if the lengths of certain columns are less than the length of the dataframe using the len() function.\n",
    "\n",
    "- your data may have null values showing up differently as “?” or “0”. \n",
    "    - It is advisable to scan your data and if the null values are not NaN then \n",
    "        - you can use the .replace() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ba915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\Directory\\filename.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19afeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into NaN values and show columns with NaN values\n",
    "\n",
    "df.replace(\"?\", np.nan, inplace = True)\n",
    "\n",
    "# Get Boolean output with all True values being the null values.\n",
    "\n",
    "missing_data=df.isnull()\n",
    "\n",
    "missing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in missing_data.columns.values.tolist():\n",
    "    print(column)\n",
    "    print (missing_data[column].value_counts())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de822f",
   "metadata": {},
   "source": [
    "### Check missing data 2.0\n",
    "\n",
    "- If you want to check the number of missing data for each column, this is the fastest way to go with. \n",
    "- This gives you a better understanding of which columns have higher number of missing data that determine your next action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e9883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_data(df):\n",
    "    # check for any missing data in the df (display in descending order)\n",
    "    return df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba262e",
   "metadata": {},
   "source": [
    "- Call isnull() and sum() to get a count of how many null values there are in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847735b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f7fe64",
   "metadata": {},
   "source": [
    "### Checking missing values 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88488d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_cols(df):\n",
    "    '''prints out columns with its amount of missing values'''\n",
    "    total = 0\n",
    "    for col in df.columns:\n",
    "        missing_vals = df[col].isnull().sum()\n",
    "        total += missing_vals\n",
    "        if missing_vals != 0:\n",
    "            print(f\"{col} => {df[col].isnull().sum()}\")\n",
    "    \n",
    "    if total == 0:\n",
    "        print(\"no missing values left\")\n",
    "            \n",
    "missing_cols(airbnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e63693",
   "metadata": {},
   "source": [
    "##### Total and percentage of missing data in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd8d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intitial_eda_checks(df):\n",
    "    '''\n",
    "    Takes df\n",
    "    Checks nulls\n",
    "    '''\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        mask_total = df.isnull().sum().sort_values(ascending=False) \n",
    "        total = mask_total[mask_total > 0]\n",
    "\n",
    "        mask_percent = df.isnull().mean().sort_values(ascending=False) \n",
    "        percent = mask_percent[mask_percent > 0] \n",
    "\n",
    "        missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    \n",
    "        print(f'Total and Percentage of NaN:\\n {missing_data}')\n",
    "    else: \n",
    "        print('No NaN found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfbb73f",
   "metadata": {},
   "source": [
    "##### Getting a list of columns that have missing values over that threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491cce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_columns_w_many_nans(df, missing_percent):\n",
    "    '''\n",
    "    Checks which columns have over specified percentage of missing values\n",
    "    Takes df, missing percentage\n",
    "    Returns columns as a list\n",
    "    '''\n",
    "    mask_percent = df.isnull().mean()\n",
    "    series = mask_percent[mask_percent > missing_percent]\n",
    "    columns = series.index.to_list()\n",
    "    print(columns) \n",
    "    return columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8fe3b",
   "metadata": {},
   "source": [
    "##### Get the percentage of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e37f903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_missing(df):\n",
    "    '''prints out columns with missing values with its %'''\n",
    "    for col in df.columns:\n",
    "        pct = df[col].isna().mean() * 100\n",
    "        if (pct != 0):\n",
    "            print('{} => {}%'.format(col, round(pct, 2)))\n",
    "    \n",
    "perc_missing(airbnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ed603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"Amount of missing values in - \")\n",
    "for column in df.columns:\n",
    "    percentage_missing = np.mean(df[column].isna())\n",
    "    print(f'{column} : {round(percentage_missing*100)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e3c6e",
   "metadata": {},
   "source": [
    "### Heat mapping missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70803ae",
   "metadata": {},
   "source": [
    "##### Visualise Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2fb4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(airbnb.isnull(), yticklabels=False, cmap='viridis', cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f8f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isna().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e9629",
   "metadata": {},
   "source": [
    "### Output a dataframe without NaN values for a particular column\n",
    "- Useful when you want to output a dataframe with all the available data that a column has.\n",
    "    - dataframe with all customers’ information and you want to output an updated dataframe with all the available customers’ ID and\n",
    "    - remove the rows with missing customers’ ID.\n",
    "- Similar concept can be applied to time series data where the column is timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae38fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_values(df):\n",
    "    '''\n",
    "    AIM    -> remove NaN values of a particular column and output the whole dataframe\n",
    "     \n",
    "    INPUT  -> df\n",
    "    \n",
    "    OUTPUT -> updated df without NaN values for a particular column \n",
    "    ------\n",
    "    '''\n",
    "    df = df[df['col_1'].notnull()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f62f70",
   "metadata": {},
   "source": [
    "# Dealing with missing values\n",
    "\n",
    "- dropping rows/columns, you’re essentially losing information that might be useful for prediction\n",
    "    - If more than 70–80% of column is NA, you can drop the column.\n",
    "\n",
    "- imputing values will introduce bias to your data but it still might better than removing your features.\n",
    "    - If the NA values are in the column which is an optional question in the form, that column can be decoded like the user answered (1) or not answered (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb68df",
   "metadata": {},
   "source": [
    "### Technique to deal with missing values\n",
    "\n",
    "1. Drop the feature\n",
    "2. Drop the row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064e8ed",
   "metadata": {},
   "source": [
    "### Dropping \n",
    "\n",
    "##### Remove any columns that had 40% or more of its data as null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f88bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_val = df_cleaned.isna().sum()\n",
    "def na_filter(na, threshold = .4): \n",
    "#only select variables that passees the threshold\n",
    "    col_pass = []\n",
    "    for i in na.keys():\n",
    "        if na[i]/df_cleaned.shape[0]<threshold:\n",
    "            col_pass.append(i)\n",
    "    return col_passdf_cleaned = df_cleaned[na_filter(NA_val)]\n",
    "df_cleaned.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf71050",
   "metadata": {},
   "source": [
    "##### Drop the columns with too many missing values (over a certain threshold you specify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76500aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_w_many_nans(df, missing_percent):\n",
    "    '''\n",
    "    Takes df, missing percentage\n",
    "    Drops the columns whose missing value is bigger than missing percentage\n",
    "    Returns df\n",
    "    '''\n",
    "    series = view_columns_w_many_nans(df, missing_percent=missing_percent)\n",
    "    list_of_cols = series.index.to_list()\n",
    "    df.drop(columns=list_of_cols)\n",
    "    print(list_of_cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17336af",
   "metadata": {},
   "source": [
    "#####  Drop multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_multiple_col(col_names_list, df): \n",
    "    '''\n",
    "    AIM    -> Drop multiple columns based on their column names \n",
    "    \n",
    "    INPUT  -> List of column names, df\n",
    "    \n",
    "    OUTPUT -> updated df with dropped columns \n",
    "    ------\n",
    "    '''\n",
    "    df.drop(col_names_list, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91b058a",
   "metadata": {},
   "source": [
    "##### Drop the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f6b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns that are not important\n",
    "colsToDrop = ['id','host_name','last_review']\n",
    "\n",
    "airbnb.drop(colsToDrop, axis=1, inplace=True)\n",
    "\n",
    "missing_cols(airbnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afca42f",
   "metadata": {},
   "source": [
    "##### Drop the whole row\n",
    "\n",
    "##### Drop the whole column (axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop whole row with NaN\n",
    "df.dropna(subset=[\"price\"], axis=0, inplace=True)\n",
    "\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#Replacing numerical null values with the mean\n",
    "\n",
    "avg_loss = df[\"losses\"].astype(\"float\").mean(axis=0)\n",
    "\n",
    "df[\"losses\"].replace(np.nan, avg_loss, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875bc738",
   "metadata": {},
   "source": [
    "##### Drop the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with missing values in price\n",
    "airbnb['price'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039357ca",
   "metadata": {},
   "source": [
    "### Ways to Handle Missing Values\n",
    "\n",
    "- Drop missing values\n",
    "- Ignore tuples with missing values\n",
    "- Imputation etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb33563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [6, 7, 8, 9, np.nan],\n",
    "    'C': [10, np.nan, 12, 13, 14]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop missing values\n",
    "df_dropped = df.dropna()  # Drop rows with any missing values\n",
    "df_dropped_column = df.dropna(axis=1)  # Drop columns with any missing values\n",
    "\n",
    "# Ignore tuples with missing values\n",
    "df_ignore = df.dropna(how='any')  # Drop rows with any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bf6c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation\n",
    "df_imputed_mean = df.fillna(df.mean())  # Fill missing values with column mean\n",
    "df_imputed_median = df.fillna(df.median())  # Fill missing values with column median\n",
    "df_imputed_custom = df.fillna({'A': 0, 'B': 1, 'C': 2})  # Fill missing values with custom values\n",
    "\n",
    "print(df_dropped)\n",
    "print(df_dropped_column)\n",
    "print(df_ignore)\n",
    "print(df_imputed_mean)\n",
    "print(df_imputed_median)\n",
    "print(df_imputed_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40641e1f",
   "metadata": {},
   "source": [
    "### Imputing/ Replacing\n",
    "\n",
    "3. Impute the missing value\n",
    "    - mean, median, mode;\n",
    "    - kNN;\n",
    "        - common practice to predict missing values in our data with the help of various regression or classification models.\n",
    "    - zero or constant and etc.\n",
    "4. Replace it\n",
    "\n",
    "The choice of how we handle missing values will depend mostly on:\n",
    "\n",
    "- the data type (numerical or categorical) and\n",
    "- how many missing values we have relative to the number of total samples we have \n",
    "    - (deleting 1 observation out of 100k will have a different impact than deleting 1 out of 100)\n",
    "\n",
    "##### Normally distributed data\n",
    "\n",
    "- Get all the values that are within 2 standard deviations from the mean. \n",
    "- Next, fill in the missing values by generating random numbers between (mean — 2 * std) & (mean + 2 * std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9585422",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = np.random.randint(average_age - 2*std_age, average_age + 2*std_age, size = count_nan_age)\n",
    "dataframe[\"age\"][np.isnan(dataframe[\"age\"])] = rand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c19dae1",
   "metadata": {},
   "source": [
    "##### Numerical data\n",
    "- Replace NaN with the mean.\n",
    "\n",
    "##### Categorical data\n",
    "- Replace NaN with the frequency.\n",
    "\n",
    "- Replace with another function \n",
    "    - here we can use a function with np.vectorize() or the .apply() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf44f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing categorical data with the frequency\n",
    "\n",
    "df['doors'].value_counts().idmax()\n",
    "#output is 'four'\n",
    "\n",
    "df['doors'].replace(np.nan,'four',inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6770d0",
   "metadata": {},
   "source": [
    "###### Handling missing data: \n",
    "\n",
    "- The fillna() function in pandas can be used to replace missing values with a specific value or using a forward or backward fill. \n",
    "- The dropna() function can be used to remove rows or columns that contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860cae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "data = {'col1': [1, 2, None, 4, 5],\n",
    "        'col2': ['A', 'B', None, 'D', 'E']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Replace missing values with a specific value\n",
    "df_filled = df.fillna('Unknown')\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_dropped = df.dropna()\n",
    "\n",
    "# Print the DataFrames\n",
    "print(\"DataFrame with filled missing values:\")\n",
    "print(df_filled)\n",
    "print(\"\\nDataFrame with dropped missing values:\")\n",
    "print(df_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc59762",
   "metadata": {},
   "source": [
    "##### Using a python loop in pandas to fill in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95309813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_value(df, fillvalue = value):\n",
    "    for col in df.columns:\n",
    "        for i,value in enumerate(df[col].values):\n",
    "            if value == 'None' or value == ' ':\n",
    "                df[col][i] = fillvalue\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def painful_fillna(df, fillvalue = 0):\n",
    "    df2 = df.copy()\n",
    "    for col in df2.columns:\n",
    "        for i, value in enumerate(df2[col].values):\n",
    "            if np.isnan(value):\n",
    "                df2[col][i] = fillvalue\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf8b84",
   "metadata": {},
   "source": [
    "##### Replacing missing values and strings with 0 and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0d9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = fuction(x)\n",
    "\n",
    "{\n",
    "    res = vector()   # create an empty vector to store counts for each elements\n",
    "    for (i in 1:lenth(x)){  # itrate through each element\n",
    "        res[i] = if else(x[i] == \" \", 0, lenth(unlist(str.split(x[i], \"/t\")))) # if element is space return 0, ekse split string by \\t and count new strings\n",
    "    }\n",
    "    return res returned stored values\n",
    "}\n",
    "\n",
    "df(sapply(dt, function(x) ff(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14bf73a",
   "metadata": {},
   "source": [
    "##### Filling in or replacing values columnwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b25cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply['col'].astype(str).apply(lambda x: x.strip().replace('', np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d536a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(lambda x: x.str.strip().replace(' '), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2513f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col'] = df['col'].apply(lambda x: x.str.strip().replace(' '), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(lambda x: x.strip() if isnstance(x, str) else x).replace('', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd6580b",
   "metadata": {},
   "source": [
    "##### Imputing\n",
    "For imputing, there are 3 main techniques shown below.\n",
    "\n",
    "- fillna — filling in null values based on given value (mean, median, mode, or specified value)\n",
    "- bfill / ffill — stands for backward fill and forward fill (filling in missing values based on the value after or before the column.)\n",
    "- Simple Imputer — Sk-learn’s built-in function that imputes missing values (commonly used alongside a pipeline when building ML models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719357fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing price with mean\n",
    "price_mean_value = round(airbnb['price'].mean(), 2)\n",
    "airbnb['price'].fillna(price_mean_value, inplace=True)\n",
    "\n",
    "# imputing price with median\n",
    "price_median_value = round(airbnb['price'].median(), 2)\n",
    "airbnb['price'].fillna(price_median_value, inplace=True)\n",
    "\n",
    "# imputing with bfill or ffill\n",
    "airbnb['price'].bfill(inplace=True)\n",
    "airbnb['price'].ffill(inplace=True)\n",
    "\n",
    "# imputing with SimpleImputor from the sklearn library\n",
    "from sklearn.impute import SimpleImputer\n",
    "# define the imputer\n",
    "imr = SimpleImputer(missing_values=np.nan, strategy='mean') # or median\n",
    "\n",
    "airbnb[['price']] = imr.fit_transform(airbnb[['price']])\n",
    "\n",
    "# use strategy = 'most_frequent' for categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15131b3d",
   "metadata": {},
   "source": [
    "##### Replace\n",
    "To replace values, the fillna function is also used.\n",
    "\n",
    "- You define the value you want to replace in the key, and the substitute in the value — {column_name: replacement_for_NA}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532db742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace null values in reviews_per_month with 0 \n",
    "airbnb.fillna({'reviews_per_month':0}, inplace=True)\n",
    "\n",
    "missing_cols(airbnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb88d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace null values in name with 'None'\n",
    "airbnb.fillna({'name':'None'}, inplace=True)\n",
    "\n",
    "missing_cols(airbnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f02b1",
   "metadata": {},
   "source": [
    "# 6 Different Ways to Compensate for Missing Values In a cross-sectional datasets ( Time-series dataset is a different story )\n",
    "Popular strategies to statistically impute missing values in a dataset\n",
    "\n",
    "- Missing Values are often encoded as NaNs, blanks or any other placeholders.\n",
    "- algorithms such as scikit-learn estimators assume that all values are numerical and have and hold meaningful value.\n",
    "\n",
    "### Handle this problem\n",
    "- is to get rid of the observations that have missing data. \n",
    "    - risk losing data points with valuable information. \n",
    "- A better strategy would be to impute the missing values.\n",
    "    - we need to infer those missing values from the existing part of the data. \n",
    "\n",
    "### Three main types of missing data: \n",
    "- Missing completely at random (MCAR)\n",
    "- Missing at random (MAR)\n",
    "- Not missing at random (NMAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e7988",
   "metadata": {},
   "source": [
    "###### 1) Do Nothing:\n",
    "- let the algorithm handle the missing data. \n",
    "- Some algorithms can factor in the missing values and learn the best imputation values for the missing data based on the training loss reduction \n",
    "    - (ie. XGBoost)\n",
    "- Some other algorithms have the option to just ignore them \n",
    "    - (ie. LightGBM — use_missing=false).\n",
    "- other algorithms will panic and throw an error complaining about the missing values \n",
    "    - (ie. Scikit learn — LinearRegression).\n",
    "        - For such cases, handle the missing data and clean it before feeding it to the algorithm.\n",
    "        \n",
    "##### 2) Imputation Using (Mean/Median) Values:\n",
    "- calculating the mean/median of the non-missing values in a column and \n",
    "- then replacing the missing values within each column separately and independently from the others\n",
    "\n",
    "- Pros:\n",
    "    - Easy and fast.\n",
    "    - Works well with small numerical datasets.\n",
    "- Cons:\n",
    "    - Doesn’t factor the correlations between features. It only works on the column level.\n",
    "    - Will give poor results on encoded categorical features (do NOT use it on categorical features).\n",
    "    - Not very accurate.\n",
    "    - Doesn’t account for the uncertainty in the imputations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4828e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f39b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching the dataset\n",
    "import pandas as pd\n",
    "dataset = fetch_california_housing()\n",
    "train, target = pd.DataFrame(dataset.data), pd.DataFrame(dataset.target)\n",
    "train.columns = ['0','1','2','3','4','5','6','7']\n",
    "train.insert(loc=len(train.columns), column='target', value=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "198e3189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640\n",
      "20640\n"
     ]
    }
   ],
   "source": [
    "#Randomly replace 40% of the first column with NaN values\n",
    "column = train['0']\n",
    "print(column.size)\n",
    "missing_pct = int(column.size * 0.4)\n",
    "i = [random.choice(range(column.shape[0])) for _ in range(missing_pct)]\n",
    "column[i] = np.NaN\n",
    "print(column.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39e34e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the values using scikit-learn SimpleImpute Class\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_mean = SimpleImputer( strategy='mean') #for median imputation replace 'mean' with 'median'\n",
    "imp_mean.fit(train)\n",
    "imputed_train_df = imp_mean.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba32e5",
   "metadata": {},
   "source": [
    "##### 3)  Imputation Using (Most Frequent) or (Zero/Constant) Values:\n",
    "- **Most Frequent** is another statistical strategy to impute missing values\n",
    "    - It works with categorical features (strings or numerical representations)\n",
    "    - by replacing missing data with the most frequent values within each column.\n",
    "- **Zero or Constant imputation** — it replaces the missing values with either zero or any constant value you specify\n",
    "\n",
    "- Pros:\n",
    "    - Works well with categorical features.\n",
    "- Cons:\n",
    "    - It also doesn’t factor the correlations between features.\n",
    "    - It can introduce bias in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the values using scikit-learn SimpleImpute Class\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_mean = SimpleImputer( strategy='most_frequent')\n",
    "imp_mean.fit(train)\n",
    "imputed_train_df = imp_mean.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9b41c",
   "metadata": {},
   "source": [
    "##### 4) Imputation Using k-NN:\n",
    "- The k nearest neighbours is an algorithm that is used for simple classification. \n",
    "- The algorithm uses ‘feature similarity’ to predict the values of any new data points. \n",
    "    - This means that the new point is assigned a value based on how closely it resembles the points in the training set. \n",
    "    - This can be very useful in making predictions about the missing values by finding the k’s closest neighbours to the observation with missing data and then imputing them based on the non-missing values in the neighbourhood.\n",
    "        - using Impyute library which provides a simple and easy way to use KNN for imputation\n",
    "- It creates a basic mean impute then uses the resulting complete list to construct a KDTree. \n",
    "- Then, it uses the resulting KDTree to compute nearest neighbours (NN). \n",
    "- After it finds the k-NNs, it takes the weighted average of them.\n",
    "\n",
    "- Pros:\n",
    "    - Can be much more accurate than the mean, median or most frequent imputation methods (It depends on the dataset).\n",
    "- Cons:\n",
    "    - Computationally expensive. KNN works by storing the whole training dataset in memory.\n",
    "    - K-NN is quite sensitive to outliers in the data (unlike SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7cfb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from impyute.imputation.cs import fast_knn\n",
    "sys.setrecursionlimit(100000) #Increase the recursion limit of the OS\n",
    "\n",
    "# start the KNN training\n",
    "imputed_training=fast_knn(train.values, k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007cc83",
   "metadata": {},
   "source": [
    "##### 5) Imputation Using Multivariate Imputation by Chained Equation (MICE)\n",
    "\n",
    "- This type of imputation works by filling the missing data multiple times. \n",
    "- Multiple Imputations (MIs) are much better than a single imputation as it measures the uncertainty of the missing values in a better way. \n",
    "- The chained equations approach is also very flexible and can handle different variables of different data types \n",
    "    - (ie., continuous or binary) as well as \n",
    "    - complexities such as bounds or survey skip patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6f596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from impyute.imputation.cs import mice\n",
    "\n",
    "# start the MICE training\n",
    "imputed_training=mice(train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f98c6",
   "metadata": {},
   "source": [
    "###### 6) Imputation Using Deep Learning (Datawig):\n",
    "- This method works very well with categorical and non-numerical features. \n",
    "- It is a library that learns Machine Learning models using Deep Neural Networks to impute missing values in a dataframe. \n",
    "- It also supports both CPU and GPU for training.\n",
    "- Pros:\n",
    "    - Quite accurate compared to other methods.\n",
    "    - It has some functions that can handle categorical data (Feature Encoder).\n",
    "    - It supports CPUs and GPUs.\n",
    "- Cons:\n",
    "    - Single Column imputation.\n",
    "    - Can be quite slow with large datasets.\n",
    "    - You have to specify the columns that contain information about the target column that will be imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571064ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datawig\n",
    "\n",
    "df_train, df_test = datawig.utils.random_split(train)\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer = datawig.SimpleImputer(\n",
    "    input_columns=['1','2','3','4','5','6','7', 'target'], # column(s) containing information about the column we want to impute\n",
    "    output_column= '0', # the column we'd like to impute values for\n",
    "    output_path = 'imputer_model' # stores model data and metrics\n",
    "    )\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer.fit(train_df=df_train, num_epochs=50)\n",
    "\n",
    "#Impute missing values and return original dataframe with predictions\n",
    "imputed = imputer.predict(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa12d866",
   "metadata": {},
   "source": [
    "##### Other Imputation Methods:\n",
    "**Stochastic regression imputation:**\n",
    "- It is quite similar to regression imputation which tries to predict the missing values by regressing it from other related variables in the same dataset plus some random residual value.\n",
    "\n",
    "**Extrapolation and Interpolation:**\n",
    "- It tries to estimate values from other observations within the range of a discrete set of known data points.\n",
    "\n",
    "**Hot-Deck imputation:**\n",
    "- Works by randomly choosing the missing value from a set of related and similar variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ad81a4",
   "metadata": {},
   "source": [
    "### Pipeline will follow the strategy\n",
    "\n",
    "- Imputation > deletion, and \n",
    "- will support the following techniques: \n",
    "    - prediction with Linear and Logistic Regression, \n",
    "    - imputation with \n",
    "        - K-NN, \n",
    "        - mean, \n",
    "        - median and \n",
    "        - mode, as well as \n",
    "        - deletion.\n",
    "        \n",
    "##### Function\n",
    "- create a separate class for handling missing values. \n",
    "- The function handle below will handle numerical and categorical missing values in a different manner: \n",
    "    - some imputation techniques might be applicable only for numerical data, \n",
    "    - whereas some only for categorical data. \n",
    "    - Let’s look at the first part of it which handles numerical features\n",
    "    \n",
    "- **How it works**\n",
    "- checks which handling method has been chosen for numerical and categorical features. \n",
    "- The default setting is set to ‘auto’ which means that:\n",
    "    - numerical missing values \n",
    "        - will first be imputed through prediction with Linear Regression, and \n",
    "        - the remaining values will be imputed with K-NN\n",
    "    - categorical missing values \n",
    "        - will first be imputed through prediction with Logistic Regression, and \n",
    "        - the remaining values will be imputed with K-NN\n",
    "- Catagorical features:        \n",
    "    - support only imputation with \n",
    "        - Logistic Regression, \n",
    "        - K-NN and \n",
    "            - When using K-NN, \n",
    "                - we will first label encode our categorical features to integers, \n",
    "                - use these labels to predict our missing values, \n",
    "                - and finally map the labels back to their original values.\n",
    "        - mode imputation. \n",
    "    - Depending on the handling method chosen, the handle function calls the required functions from within its class to then manipulate the data with the help of various Sklearn packages: \n",
    "        - the _impute function will be in charge of K-NN, mean, median and mode imputation, \n",
    "        - _lin_regression_impute and log_regression_impute will perform imputation through prediction, \n",
    "        - the role of _delete is self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eb0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingValues:\n",
    "    # Function for handling missing values in the data\n",
    "    def handle(df, missing_num='auto', missing_categ='auto', _n_neighbors=3):\n",
    "        count_missing = df.isna().sum().sum()\n",
    "        if count_missing != 0:\n",
    "            # drop rows containing only missing values\n",
    "            df = df.dropna(how='all')\n",
    "            df.reset_index(drop=True)\n",
    "            \n",
    "            if self.missing_num:\n",
    "                # automated handling of numerical missing values\n",
    "                if missing_num == 'auto':\n",
    "                    missing_num = 'linreg'\n",
    "                    lr = LinearRegression()\n",
    "                    df = MissingValues._lin_regression_impute(self, df, lr)\n",
    "                    missing_num = 'knn'\n",
    "                    imputer = KNNImputer(n_neighbors=_n_neighbors)\n",
    "                    df = MissingValues._impute(self, df, imputer, type='num')\n",
    "                # linear regression imputation\n",
    "                elif missing_num == 'linreg':\n",
    "                    lr = LinearRegression()\n",
    "                    df = MissingValues._lin_regression_impute(self, df, lr)\n",
    "                # knn imputation\n",
    "                elif missing_num == 'knn':\n",
    "                    imputer = KNNImputer(n_neighbors=_n_neighbors)\n",
    "                    df = MissingValues._impute(self, df, imputer, type='num')\n",
    "                # mean, median or mode imputation\n",
    "                elif missing_num in ['mean', 'median', 'most_frequent']:\n",
    "                    imputer = SimpleImputer(strategy=self.missing_num)\n",
    "                    df = MissingValues._impute_missing(self, df, imputer, type='num')\n",
    "                # delete missing values\n",
    "                elif missing_num == 'delete':\n",
    "                    df = MissingValues._delete(self, df, type='num')\n",
    "                   \n",
    "            if missing_categ:\n",
    "                ...\n",
    "        else:\n",
    "            pass\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingValues:\n",
    "    def handle(df, missing_num='auto', missing_categ='auto', _n_neighbors=3):\n",
    "        ...\n",
    "    def _impute(df, imputer, type):\n",
    "        ...\n",
    "    def _lin_regression_impute(df, model):\n",
    "        ...\n",
    "    def _log_regression_impute(df, model):\n",
    "        ...\n",
    "    def _delete(df, type):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b82c4",
   "metadata": {},
   "source": [
    "# Dealing with Outliers\n",
    "\n",
    "- Outliers are extremely large or small values relative to the other points of dataset. \n",
    "    - Their existence dramatically affects mathematical models’ performance.\n",
    "- Outliers can be dangerous as they can skew your model and give you predictions that are biased and erroneous\n",
    "- Use the describe function and look at information such as maximum and mean.\n",
    "- Any data value that lies more than (1.5 * IQR) away from the Q1 and Q3 quartiles is considered an outlier.\n",
    "    - The values 1.5 x IQR (interquartile range) higher / smaller than Q3 / Q1 are called outliers. IQR is the difference between Q3 and Q1 (IQR = Q3-Q1).\n",
    "\n",
    "##### Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc18641",
   "metadata": {},
   "source": [
    "##### Or we can Manually filter out what we define as being Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0292d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned[df_cleaned['price'].between(999.99, 99999.00)]\n",
    "df_cleaned = df_cleaned[df_cleaned['year'] > 1990]\n",
    "df_cleaned = df_cleaned[df_cleaned['odometer'] < 899999.00]df_cleaned.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4d37c",
   "metadata": {},
   "source": [
    "##### Formular for getting the IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a231d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_outliers(df):\n",
    "    \n",
    "    df = df.select_dtypes(exclude = 'object')\n",
    "    \n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    return ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de95c790",
   "metadata": {},
   "source": [
    "##### Outlier detection: \n",
    "\n",
    "The zscore() function in NumPy can be used to calculate the z-score of each value in a dataset, which can be used to identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7212be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a NumPy array with values\n",
    "data = np.array([10, 15, 20, 25, 100])\n",
    "\n",
    "# Calculate the z-score for each value\n",
    "z_scores = np.abs((data - np.mean(data)) / np.std(data))\n",
    "\n",
    "# Identify outliers based on a threshold\n",
    "threshold = 2.5\n",
    "outliers = data[z_scores > threshold]\n",
    "\n",
    "# Print the outliers\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5947f03d",
   "metadata": {},
   "source": [
    "##### Treating outliers\n",
    "\n",
    "- Treat outliers is to make them equal to Q3 or Q1. \n",
    "- By using pandas and numpy libraries, the below function does this task. \n",
    "- Here, lower_upper_range function finds the range whose outside are outliers. \n",
    "- Then with numpy clip function the values are clipped to the ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96345e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_upper_range(datacolumn):\n",
    "    sorted(datacolumn)\n",
    "    Q1,Q3 = np.percentile(datacolumn , [25,75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower_range = Q1 - (1.5 * IQR)\n",
    "    upper_range = Q3 + (1.5 * IQR)\n",
    "    return lower_range,upper_range\n",
    "  \n",
    "for col in columns:  \n",
    "    lowerbound,upperbound = lower_upper_range(df[col])\n",
    "    df[col]=np.clip(df[col],a_min=lowerbound,a_max=upperbound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f40e312",
   "metadata": {},
   "source": [
    "### Visualisng  Outliers\n",
    "\n",
    "##### Numerical Data\n",
    "\n",
    "- You can plot a box-plot chart to see the \n",
    "    - Max\n",
    "    - Min\n",
    "    - Mean\n",
    "    - Median?\n",
    "    - IQR 1\n",
    "    - IQR 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f89332",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "airbnb.boxplot(column=['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b1c9f5",
   "metadata": {},
   "source": [
    "##### Catagorical Data \n",
    " \n",
    "- you can plot a bar chart to see whether a particular category to view the count of the categories.\n",
    "- Outliers in categorical data is tricky, because you have to determine whether it’s appropriate to call it an outlier based on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b3e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "airbnb['neighbourhood_group'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00690fd",
   "metadata": {},
   "source": [
    "### Pipeline will follow the strategy\n",
    "Ask ourselves: \n",
    "- when do we consider a value to be an outlier? \n",
    "- For our pipeline, we will use a commonly applied rule that says that \n",
    "    - a data point can be considered an outlier if is outside the following range:\n",
    "        - [Q1 — 1.5 * IQR ; Q3 + 1.5 * IQR]\n",
    "    - where Q1 and Q3 are the 1st and the 3rd quartiles and IQR is the interquartile range.\n",
    "    \n",
    "- There are various strategies to handle Outliers, and we will focus on the following two: \n",
    "    - winsorization and\n",
    "        - When using winsorization, we will again use our above defined range to replace outliers:\n",
    "            - values > upper bound will be replaced by the upper range value and\n",
    "            - values < lower bound will be replaced by the lower range value.\n",
    "    - deletion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee435416",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Outliers:\n",
    "    # Function that handles outliers in the data\n",
    "    def handle(df, outliers='winz'):\n",
    "        if outliers:\n",
    "            if outliers == 'winz':  \n",
    "                df = Outliers._winsorization(self, df)\n",
    "            elif ourliers == 'delete':\n",
    "                df = Outliers._delete(self, df)\n",
    "        return df     \n",
    "    def _winsorization(df):\n",
    "        ...\n",
    "    def _delete(df):\n",
    "        ...\n",
    "    def _compute_bounds(df, feature):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4036601",
   "metadata": {},
   "source": [
    "### Calculating Outliers and thier types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Create sample data\n",
    "data = {\n",
    "    'Feature1': [1, 2, 3, 4, 1000],\n",
    "    'Feature2': [5, 6, 7, 8, 2000],\n",
    "    'Context': ['A', 'A', 'A', 'A', 'B']\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4181d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Outliers\n",
    "global_outliers = []\n",
    "for feature in df.columns:\n",
    "    z_scores = (df[feature] - df[feature].mean()) / df[feature].std()\n",
    "    global_outliers.extend(df[abs(z_scores) > 3].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextual Outliers\n",
    "contextual_outliers = []\n",
    "for context in df['Context'].unique():\n",
    "    context_data = df[df['Context'] == context]\n",
    "    for feature in df.columns[:-1]:\n",
    "        z_scores = (context_data[feature] - context_data[feature].mean()) / context_data[feature].std()\n",
    "        contextual_outliers.extend(context_data[abs(z_scores) > 3].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330bf59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collective Outliers using DBSCAN\n",
    "X = df.drop('Context', axis=1)\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=2)\n",
    "dbscan.fit(X)\n",
    "collective_outliers = np.where(dbscan.labels_ == -1)[0]\n",
    "\n",
    "# Print the outliers\n",
    "print(\"Global Outliers:\", global_outliers)\n",
    "print(\"Contextual Outliers:\", contextual_outliers)\n",
    "print(\"Collective Outliers:\", collective_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d518765",
   "metadata": {},
   "source": [
    "# Noisy Data\n",
    "\n",
    "- Noise unwanted/meaningless data items, \n",
    "    - features or records which don’t help in explaining the feature itself, or the relationship between feature & target. \n",
    "- The occurrences of noisy data in data set can significantly impact prediction of any meaningful information and causes the algorithms to miss out patterns in the data. \n",
    "- Noise in data set dramatically led to decreased classification accuracy and poor prediction results. It can be — certain anomalies in features & target, irrelevant/weak features and noisy records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccfe403",
   "metadata": {},
   "source": [
    "### Data Binning\n",
    "\n",
    "- Use the most when pre-processing my data because an important aspect of data preparation is ensuring the data is meaningful and easy to understand.\n",
    "\n",
    "##### Numerical data \n",
    "\n",
    "- Data binning allows you to split continuous numerical data into bins for ease of grouping and visualization.\n",
    "- to create an evenly spaced sequence for continuous, numerical data is to use np.linspace()\n",
    "\n",
    "##### Catagorical data\n",
    "\n",
    "- Categorical data can also be binned in a way to add meaning to data which has arbitrary values that mean something else based on a business requirement specification or index. I would refer to this as data mapping or flagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(min(df[\"losses\"]), max(df[\"losses\"]), 4)\n",
    "\n",
    "group_names = ['Low', 'Medium', 'High']\n",
    "\n",
    "df['losses-binned'] = pd.cut(df['losses'], bins, labels=group_names, include_lowest=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0900bb",
   "metadata": {},
   "source": [
    "##### This new binned column can be visualized using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e1d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as plt\n",
    "from matplotlib import pyplot\n",
    "plt.pyplot.bar(group_names,df[\"losses\"].value_counts())\n",
    "\n",
    "# set x/y labels and plot title\n",
    "plt.pyplot.xlabel(\"losses\")\n",
    "plt.pyplot.ylabel(\"count\")\n",
    "plt.pyplot.title(\"losses binned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e0415",
   "metadata": {},
   "source": [
    "### Get discrete intervals from numerical values\n",
    "\n",
    "- Use to converting numerical values in a column to discrete intervals based on the range specified\n",
    "- Use pd.cut if you want to convert a continuous variable to a categorical variable.\n",
    "    - have a rating column that consists of numerical values from 1–10. \n",
    "    - What if we want to convert these rating values to certain groups within given their values and specified range? \n",
    "        - We can bin the values in discrete intervals and label them as bad, moderate, good, strong given the range specified using pd.cut.\n",
    "    - convert ages to groups of age ranges where you can categorize each age to a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d3d7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_intervals_from_values(df):\n",
    "    '''\n",
    "    AIM    -> get discrete intervals by binning values to a range\n",
    "     \n",
    "    INPUT  -> df\n",
    "    \n",
    "    OUTPUT -> updated df with discrete intervals based on numerical values \n",
    "    ------\n",
    "    '''\n",
    "    df['rating'] = pd.cut(df['rating'], \n",
    "                          bins=[-1,3,5,7,10], \n",
    "                          labels=['bad','moderate','good','strong'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6536ea",
   "metadata": {},
   "source": [
    "### Using Binning to handle Noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd323277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = {\n",
    "    'Age': [25, 30, 35, 40, 45, 50, 55, 60, 65, 70],\n",
    "    'Income': [50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000, 140000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Binning\n",
    "df['AgeBin'] = pd.cut(df['Age'], bins=[0, 30, 40, 50, 100], labels=['Young', 'Adult', 'Middle-aged', 'Senior'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af404405",
   "metadata": {},
   "source": [
    "### Using Regression to handle Noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b712c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create sample data\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Regression\n",
    "regression = LinearRegression()\n",
    "regression.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "X_new = np.array([6, 7, 8]).reshape(-1, 1)\n",
    "predictions = regression.predict(X_new)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a0e93e",
   "metadata": {},
   "source": [
    "### Using Clustering to handle Noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create sample data\n",
    "X, _ = make_blobs(n_samples=100, centers=3, random_state=0)\n",
    "\n",
    "# Clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Predict\n",
    "new_data = np.array([[0, 0], [4, 4]])\n",
    "predictions = kmeans.predict(new_data)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53ced2d",
   "metadata": {},
   "source": [
    "# Functions for Data Visualization\n",
    "- Human brain is very good at identify patterns.\n",
    "- Visualizing your dataset during the EDA process and identifying the patterns can be very beneficial.\n",
    "    - Histograms make analyzing the distribution of the data an easier task;\n",
    "    - Boxplot is great for identifying outliers; \n",
    "    - Scatter plot is very useful when it comes to checking the correlations between two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80950237",
   "metadata": {},
   "source": [
    "### Creating group plots for each features at once\n",
    "\n",
    "##### Numeric values \n",
    "- look at the distributions of columns with numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d31d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histograms_numeric_columns(df, numerical_columns):\n",
    "    '''\n",
    "    Takes df, numerical columns as list\n",
    "    Returns a group of histagrams\n",
    "    '''\n",
    "    f = pd.melt(df, value_vars=numerical_columns) \n",
    "    g = sns.FacetGrid(f, col='variable',  col_wrap=4, sharex=False, sharey=False)\n",
    "    g = g.map(sns.distplot, 'value')\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a043961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "edab.histograms_numeric_columns(df, numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6266f23",
   "metadata": {},
   "source": [
    "##### Heatmaps\n",
    "\n",
    "- To check the correlation between your dependent and independent variables.\n",
    "- heatmaps can be visually cluttered if you have too many features. \n",
    "    - One way to avoid it is to create a heatmap just for the dependent variable (target) and independent variables (features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bdb876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_numeric_w_dependent_variable(df, dependent_variable):\n",
    "    '''\n",
    "    Takes df, a dependant variable as str\n",
    "    Returns a heatmap of all independent variables' correlations with dependent variable \n",
    "    '''\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    g = sns.heatmap(df.corr()[[dependent_variable]].sort_values(by=dependent_variable), \n",
    "                    annot=True, \n",
    "                    cmap='coolwarm', \n",
    "                    vmin=-1,\n",
    "                    vmax=1) \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c682e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "edab.heatmap_numeric_w_dependent_variable(df, dependent_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41076845",
   "metadata": {},
   "source": [
    "# Changing Data Types\n",
    "\n",
    "- good practice to check data type of all columns with pandas dtype function.\n",
    "- memory usage of different data types and fasten the processes by choosing right choice of type.\n",
    "- Reading the data dictionary is very illuminating during this step.\n",
    "\n",
    "### Transform categorical features into numerical (ordinal) features\n",
    "\n",
    "##### transformer\n",
    "- that will transform each str in a list into a int, where the int is the index of that element in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8879b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_to_ordinal_transformer(categories):\n",
    "    '''\n",
    "    Returns a function that will map categories to ordinal values based on the\n",
    "    order of the list of `categories` given. Ex.\n",
    "\n",
    "    If categories is ['A', 'B', 'C'] then the transformer will map \n",
    "    'A' -> 0, 'B' -> 1, 'C' -> 2.\n",
    "    '''\n",
    "    return lambda categorical_value: categories.index(categorical_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df22c08",
   "metadata": {},
   "source": [
    "##### Second function has two parts:\n",
    "\n",
    "- first part, it takes a dictionary of the following form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_numerical_mapping = {\n",
    "    'Utilities': ['ELO', 'NoSeWa', 'NoSewr', 'AllPub'],\n",
    "    'Exter Qual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'Exter Cond': ['Po', 'Fa', 'TA', 'Gd', 'Ex']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593e984",
   "metadata": {},
   "source": [
    "- Prev function it turns the dictionary into this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e456a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = {'Utilities': <utilties_transformer>,\n",
    "                'Exter Qual': <exter_qual_transformer>,\n",
    "                'Exter Cond': <exter_cond_transfomer>}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb07319e",
   "metadata": {},
   "source": [
    "- second part of the function uses the .map() method to map each transformer function onto the dataframe. \n",
    "    - Note that a copy of the original dataframe will be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939ffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_categorical_to_numercial(df, categorical_numerical_mapping):\n",
    "    '''\n",
    "    Transforms categorical columns to numerical columns\n",
    "    Takes a df, a dictionary \n",
    "    Returns df\n",
    "    '''\n",
    "    transformers = {k: categorical_to_ordinal_transformer(v) \n",
    "                    for k, v in categorical_numerical_mapping.items()}\n",
    "    new_df = df.copy()\n",
    "    for col, transformer in transformers.items():\n",
    "        new_df[col] = new_df[col].map(transformer).astype('int64')\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90bdd2a",
   "metadata": {},
   "source": [
    "### Convert categorical variable to numerical variable 2.0\n",
    "\n",
    "- Machine learning models require variables to be in numerical format. \n",
    "- This is when we need to convert categorical variables to numerical variables before feeding them to the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8385a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cat2num(df):\n",
    "    # Convert categorical variable to numerical variable\n",
    "    num_encode = {'col_1' : {'YES':1, 'NO':0},\n",
    "                  'col_2'  : {'WON':1, 'LOSE':0, 'DRAW':0}}  \n",
    "    df.replace(num_encode, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd9fffc",
   "metadata": {},
   "source": [
    "### Pipeline will follow the strategy\n",
    "\n",
    "##### Catagorical encoding\n",
    "\n",
    "- to perform computations with categorical data, in most cases we need our data to be of a \n",
    "    - numeric \n",
    "        - type i. e. numbers, or integers. Therefore, \n",
    "- common techniques consist of \n",
    "    - one-hot encoding data, or \n",
    "        -  One-hot encoding of data represents each unique value of a feature as a binary vector\n",
    "    - label encoding data.\n",
    "        - label encoding assigns a unique integer to each value.\n",
    "        \n",
    "- There are various pros and cons for each of the methods, \n",
    "    - one-hot encoding\n",
    "        - produces a lot of additional features\n",
    "    - label encoding, \n",
    "        - the labels might be interpreted by certain algorithms as mathematically dependent: \n",
    "            - 1 apple + 1 orange = 1 banana, \n",
    "        - which is obviously a wrong interpretation of this type of categorical data.\n",
    "        \n",
    "- set the default strategy ‘auto’ to perform the encoding according to the following rules:\n",
    "    - if the feature contains < 10 unique values, it will be one-hot-encoded\n",
    "    - if the feature contains < 20 unique values, it will be label-encoded\n",
    "    - if the feature contains > 20 unique values, it will not be encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b59e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeCateg:\n",
    "    # Function for encoding of categorical features\n",
    "    # to specify columns set encode_categ to: ['auto', ['col1', col2']]\n",
    "    def handle(df, encode_categ=['auto']):\n",
    "        if encode_categ[0]:\n",
    "            # select non numeric features\n",
    "            cols_categ = set(df.columns) ^ set(df.select_dtypes(include=np.number).columns)\n",
    "            # check if all columns should be encoded\n",
    "            if len(encode_categ) == 1:\n",
    "                target_cols = cols_categ # encode ALL columns\n",
    "            else:\n",
    "                target_cols = encode_categ[1] # encode only specific columns\n",
    "            for feature in target_cols:\n",
    "                if feature in cols_categ:\n",
    "                    feature = feature # columns are column names\n",
    "                else:\n",
    "                    feature = df.columns[feature] # columns are indexes\n",
    "                try:\n",
    "                    # skip encoding of datetime features\n",
    "                    pd.to_datetime(df[feature])\n",
    "                except:\n",
    "                    try:\n",
    "                        if encode_categ[0] == 'auto':\n",
    "                            # ONEHOT encode if not more than 10 unique values to encode\n",
    "                            if df[feature].nunique() <=10:\n",
    "                                df = EncodeCateg._to_onehot(df, feature)\n",
    "                            # LABEL encode if not more than 20 unique values to encode\n",
    "                            elif df[feature].nunique() <=20:\n",
    "                                df = EncodeCateg._to_label(df, feature)\n",
    "                            # skip encoding if more than 20 unique values to encode\n",
    "                        elif encode_categ[0] == 'onehot':\n",
    "                            df = EncodeCateg._to_onehot(df, feature)\n",
    "                        elif encode_categ[0] == 'label':\n",
    "                            df = EncodeCateg._to_label(df, feature)\n",
    "                    except:\n",
    "                        pass\n",
    "        return df\n",
    "    def _to_onehot(df, feature, limit=10):\n",
    "        ...\n",
    "    def _to_label(df, feature):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caff7f1",
   "metadata": {},
   "source": [
    "- The handle function takes a list as input, whereas the features we want to manually encode can be defined by column names or indexes as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd02248",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_categ = [‘onehot’, [‘column_name’, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33811465",
   "metadata": {},
   "source": [
    "##### Data conversion: \n",
    "\n",
    "The astype() function in pandas can be used to convert columns to a specific data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with columns of different data types\n",
    "data = {'col1': [1, 2, 3],\n",
    "        'col2': ['A', 'B', 'C'],\n",
    "        'col3': [True, False, True]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert col1 to float data type\n",
    "df['col1'] = df['col1'].astype(float)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e6345",
   "metadata": {},
   "source": [
    "# Change data type to reduce memory\n",
    "\n",
    "- Changing data type is common if you want to reduce memory usage.\n",
    "    - use the astype(‘dtype’) function where you specify the dtype you want.\n",
    "- changed the data type for the host_id column from int64 to int32\n",
    "- Observe the memory before changing the data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb515eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb['host_id'] = airbnb['host_id'].astype('int32')\n",
    "airbnb.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa7597",
   "metadata": {},
   "source": [
    "### Change dtypes\n",
    "\n",
    "- When a dataset gets larger, we need to convert the dtypes in order to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6842d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtypes(col_int, col_float, df): \n",
    "    '''\n",
    "    AIM    -> Changing dtypes to save memory\n",
    "     \n",
    "    INPUT  -> List of column names (int, float), df\n",
    "    \n",
    "    OUTPUT -> updated df with smaller memory  \n",
    "    ------\n",
    "    '''\n",
    "    df[col_int] = df[col_int].astype('int32')\n",
    "    df[col_float] = df[col_float].astype('float32')\n",
    "    df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ns]\")\n",
    "    df[\"Payment\"] = df[\"Payment\"].str[1:].str.replace(\",\", \".\").astype(\"float\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8118a8",
   "metadata": {},
   "source": [
    "### Converting String column to numeric Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe1cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all numeric attribute columns, i.e. excluding \"word-type\" columns such as Nationality.\n",
    "cols = ['Overall', 'Acceleration', 'Aggression',\n",
    "       'Agility', 'Balance', 'Ball control', 'Composure', 'Crossing', 'Curve',\n",
    "       'Dribbling', 'Finishing', 'Free kick accuracy', 'GK diving',\n",
    "       'GK handling', 'GK kicking', 'GK positioning', 'GK reflexes',\n",
    "       'Heading accuracy', 'Interceptions', 'Jumping', 'Long passing',\n",
    "       'Long shots', 'Marking', 'Penalties', 'Positioning', 'Reactions',\n",
    "       'Short passing', 'Shot power', 'Sliding tackle', 'Sprint speed',\n",
    "       'Stamina', 'Standing tackle', 'Strength', 'Vision', 'Volleys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a5506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function.\n",
    "def to_float(x):    \n",
    "    \"Transforms attribute columns to type float\"\n",
    "    \n",
    "    if type(x) is int:\n",
    "        return float(x)\n",
    "    else:\n",
    "        return float(x[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use applymap() function to transform all selected columns.\n",
    "df[cols] = df[cols].applymap(to_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc42c4f1",
   "metadata": {},
   "source": [
    "### Changing int column 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274c66d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "floatage = lambda x: float(x)\n",
    "\n",
    "df[\"Age\"][df[\"Age\"] > 36].apply(floatage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45987be",
   "metadata": {},
   "source": [
    "### Convert Percentage String to Numeric\n",
    "\n",
    "- Col is displayed as percentages and treated as strings. \n",
    "- fine when presenting the table as a report but will be impossible for us to perform any meaningful \n",
    "- Mathematic operations or analysis on them as they are not numeric variables.\n",
    "- **Solution**\n",
    "    - 1st use pandas.Series.str.rstrip() method to remove the trailing ‘%’ character and then \n",
    "    - 2nd use astype(float) to convert it to numeric. \n",
    "    - use Series.str.lstrip() \n",
    "        - to remove leading characters in series and \n",
    "    - use Series.str.strip() \n",
    "        - to remove both leading and trailing characters in series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac94cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[column] = df[column].str.rstrip(\"%\").astype(float)/100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d5cd4",
   "metadata": {},
   "source": [
    "### Change the decimal places, say to 2 decimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8839b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242cee41",
   "metadata": {},
   "source": [
    "### Convert Numeric to Percentage String\n",
    "- How to do this vice versa — to convert the numeric back to the percentage string? \n",
    "- To convert it back to percentage string\n",
    "    - use python’s string format syntax '{:.2%}’.format to add the ‘%’ sign back. \n",
    "    - Then we use python’s map() function to iterate and apply the formatting to all the rows in the specific column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3872c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, column] = df[column].map('{:.2%}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77487023",
   "metadata": {},
   "source": [
    "# Data normalization: \n",
    "\n",
    "The minmax_scale() function in SciPy can be used to normalize a dataset so that all values are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f630b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "rom scipy import stats\n",
    "\n",
    "# Create a NumPy array with values\n",
    "data = np.array([10, 20, 30, 40, 50])\n",
    "\n",
    "# Normalize the data between 0 and 1\n",
    "normalized_data = stats.minmax_scale(data)\n",
    "\n",
    "# Print the normalized data\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c90a4",
   "metadata": {},
   "source": [
    "# String Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84562521",
   "metadata": {},
   "source": [
    "### Remove strings in columns\n",
    "\n",
    "- When you’d face the new line character or other weird symbols that appear in your columns of strings. \n",
    "- This could easily be dealt with using df['col_1'].replace where col_1 is one of the columns in the dataframe df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d66fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax\n",
    "\n",
    "string.replace(\"old\", \"new\", count)\n",
    "\n",
    "# Where Count is the number of values you want to replace. if not specified, all occurance will be replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe80d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"I have to continue pushing and strive to be the best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b13b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have to continue working and strive to be the best'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.replace(\"pushing\", \"working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c9558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_col_str(df):\n",
    "    # remove a portion of string in a dataframe column - col_1\n",
    "    df['col_1'].replace('\\n', '', regex=True, inplace=True)\n",
    "    \n",
    "    # remove all the characters after &# (including &#) for column - col_1\n",
    "    df['col_1'].replace(' &#.*', '', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1400291a",
   "metadata": {},
   "source": [
    "### Remove white space in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a62fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax\n",
    "\n",
    "String.strip(\"Char\")\n",
    "\n",
    "# There also the following\n",
    "# lstrip() - Remove at the begining\n",
    "# rstrip() - Remove at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3320f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "string2 = \"  n,s    I have to complete these and moor Python \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b4a3a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have to complete these and moor Pytho'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string2.strip(\" s,n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ab27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_col_white_space(df,col):\n",
    "    # remove white space at the beginning of string \n",
    "    df[col] = df[col].str.lstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f2b51c",
   "metadata": {},
   "source": [
    "### Creating two cols from a Single column\n",
    "\n",
    "##### Case\n",
    "- represent them with all upper case or lower case letters. \n",
    "- Another option is to just capitalize them (i.e. only the first letter is upper case).\n",
    "\n",
    "##### Order\n",
    "- other issue is to switch the order of the last and first names. First name and then last name is a more standard representation.\n",
    "    - prefer is to first split the values at comma\n",
    "    - take the second column (i.e. 1) and combine it with the first column (i.e. 0) with a space in between"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6a97ee",
   "metadata": {},
   "source": [
    "### Splitting string with an identified Saperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e8eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax\n",
    "\n",
    "string.split(\"Separator\", maxsplit = i)\n",
    "\n",
    "# where the sep is the string where the split will occur\n",
    "# where maxsplit indicates how many splits to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b377a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I have so much to do, its not even funny\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c21f654d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'so', 'much', 'to', 'do', 'its', 'not', 'even', 'funny']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abd97f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have so much to do', ' its not even funny']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17732bb7",
   "metadata": {},
   "source": [
    "### Concatenate two columns with strings (with condition)\n",
    "\n",
    "- When you want to combine two columns with strings conditionally. \n",
    "- For instance, you want to concatenate the 1st column with the 2nd column if the strings in the 1st column end with certain letters. \n",
    "- The ending letters can also be removed after the concatenation, depending on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8676d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_it_apart(df, col, sep):\n",
    "    df.col.str.split(sep, expand=True)\n",
    "    df[\"Name\"] = (df.col.str.split(\",\", expand=True)[1] + \" \" + \n",
    "                  df.col.str.split(\",\", expand=True)[0]).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_col_str_condition(df):\n",
    "    # concat 2 columns with strings if the last 3 letters of the first column are 'pil'\n",
    "    mask = df['col_1'].str.endswith('pil', na=False)\n",
    "    col_new = df[mask]['col_1'] + df[mask]['col_2']\n",
    "    col_new.replace('pil', ' ', regex=True, inplace=True)  # replace the 'pil' with emtpy space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5195d0eb",
   "metadata": {},
   "source": [
    "### Joining/ Concatenating Stings in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c6ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax\n",
    "\n",
    "string.join(seq)\n",
    "\n",
    "# where the string is the char that the sequence will join based on\n",
    "# where sequence is the sequence of string elements to be joined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67eabeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [\"Sip\", \"is\", \"going\", \"to\", \"be\", \"rich\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5908f8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sip is going to be rich'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e8d74a",
   "metadata": {},
   "source": [
    "### Must Know String methods\n",
    "\n",
    "- **Capitalize()** : Converts the first charactor into upper case\n",
    "- **Casefold()/lower()** : Converts a string into lowecase\n",
    "- **Count()** : Count the number of times a value occurs in a string\n",
    "- **Endswith()** : Return True if the string ends with specific value\n",
    "- **Strartswith()** : Returns True is the string starts with a specific value\n",
    "- **Index()** : Returns the postion of a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a024a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fde36f16",
   "metadata": {},
   "source": [
    "### Output a dataframe based on unique last strings in a column\n",
    "\n",
    "- Using back the dataframe with all customers’ information and adding a column of timestamp for each id.\n",
    "- Now each id is not unique and it’s repeated throughout their respective time period. \n",
    "    - For each id you want to get the last row of the id because you only care about the final customers’ information for each id.\n",
    "    - To drop all duplicated id and just keep the last row. \n",
    "        - This makes sure that we always get the final customers’ information with unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed0dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_last_str(df):\n",
    "    '''\n",
    "    AIM    -> get unique last str for a column\n",
    "     \n",
    "    INPUT  -> df\n",
    "    \n",
    "    OUTPUT -> updated df based on the unique last strings in a column \n",
    "    ------\n",
    "    '''\n",
    "    df = df[df.index.isin(df['col_1'].drop_duplicates(keep='last').index)].reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542f2cd6",
   "metadata": {},
   "source": [
    "##### Replace all values that do not contain a specific string in pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ba55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col'] = np.where(~df['col'].str.contains('land', na = False), 'other', df['col'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb71ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col'] = np.where(df['col'].str.contains('land') == True, \"other\", df['col'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf75d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col'] = np.where(df['col'].str.contains('land'), df[''], 'other')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53764f02",
   "metadata": {},
   "source": [
    "# Inconsistent data/Irrelevant features\n",
    "\n",
    "- Inconsistent data means that the unique classes of a column have different representations.\n",
    "- Inconsistent data refers to things like \n",
    "    - spelling errors in your data, \n",
    "    - column names that are not relevant to the data, the \n",
    "    - wrong data type\n",
    "    \n",
    "- there is no automation for this task, hence we need to analyze the classes manually. \n",
    "    - unique function of pandas is for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CarName'] = df['CarName'].str.split().str[0]\n",
    "print(df['CarName'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83abaa",
   "metadata": {},
   "source": [
    "- use pandas loc function to solve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['CarName'] == 'maxda', 'CarName'] = 'mazda'\n",
    "df.loc[df['CarName'] == 'Nissan', 'CarName'] = 'nissan'\n",
    "df.loc[df['CarName'] == 'porcshce', 'CarName'] = 'porsche'\n",
    "df.loc[df['CarName'] == 'toyouta', 'CarName'] = 'toyota'\n",
    "df.loc[df['CarName'] == 'vokswagen', 'CarName'] = 'volkswagen'\n",
    "df.loc[df['CarName'] == 'vw', 'CarName'] = 'volkswagen'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b36704",
   "metadata": {},
   "source": [
    "# Regex\n",
    "\n",
    "- characters in a column that need to be removed\n",
    "- to remove non-alphanumerical characters (e.g. ?, !, -, ., and so on)\n",
    "- replace function can be used in this case too because it accepts a regular expression (i.e. regex)\n",
    "- Use this If we only want alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b56b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alpha_char(df, col):\n",
    "    # to remove non-alpha + numerical characters (e.g. ?, !, -, ., and so on)\n",
    "    # to continue having only alphabetical characters\n",
    "    df.col.str.replace('[^a-zA-Z]', '').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphanumerical_char(df, col):\n",
    "    # to keep both alphabetical and numeric (i.e. alphanumerical), we need to add the numbers in our regex\n",
    "    df.col.str.replace('[^a-zA-Z0-9]', '').str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5798c8e7",
   "metadata": {},
   "source": [
    "### Finding Correctly Formatted strings stored in a specific way and contains specific characters\n",
    "\n",
    "##### Use Regular Expression (regex)\n",
    "- Helps with searching for common string patterns\n",
    "- package re\n",
    "- Includes alot of functions to deal with Regular expressions\n",
    "\n",
    "##### metacharacters\n",
    "- Control Characters of RegEx Engine and are interpreted in a special way.\n",
    "    - ^ : Checks if string begins with a specific value\n",
    "    - (period sign (.) : Matches a single char except for new line\n",
    "    - astericks sign (*) : Matches 0 or more occurances of the preceeding patterns\n",
    "    - [...] : matches any single char in brackets\n",
    "    - [^...] : match any single char not in brackets\n",
    "    - {...} : match exactly the number of occurances\n",
    "    - (...) : Used to group sub patterns\n",
    "    - (plus sign) + : matches one or more occurances\n",
    "    - | : matches either values\n",
    "    - \\ : Escape special characters. Put a splash before the char that you are unsure whether it has special meaning or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8a58d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax\n",
    "\n",
    "re.search()\n",
    "\n",
    "# Scan string for matching patterns then retunr the location where the patterns match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0d9d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a24bd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "string3 = \"Siphamandla mandindi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0fc78f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(5, 8), match='man'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"man\", string3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d23d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "string4 = \"Superman3539\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "395fa214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(8, 11), match='353'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"[0-9][0-9][0-9]\", string4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax\n",
    "\n",
    "re.findall()\n",
    "\n",
    "# returns a list containing all matches\n",
    "\n",
    "# note\n",
    "# d+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b8ddc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "string5 = \"Sip is 28, and Ayanda is 27\"\n",
    "pattern = \"\\d+\"\n",
    "res = re.findall(pattern, string5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6bc5dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['28', '27']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6ce9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax\n",
    "\n",
    "re.split()\n",
    "\n",
    "# Devides string into pieces where patterns occur and return list of the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecb23e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "string5 = \"Sip is 28, and Ayanda is 27\"\n",
    "pattern = \"^is\"\n",
    "res = re.split(pattern, string5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fa781b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sip is 28, and Ayanda is 27']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73859ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax\n",
    "\n",
    "re.sub()\n",
    "\n",
    "# returns a string where matched char are replaced with new specifies values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a59b735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wednesday is a great day'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string6 = \"today is a great day\"\n",
    "pattern = \"^to\"\n",
    "replace = \"Wednes\"\n",
    "result = re.sub(pattern, replace, string6)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f808d76",
   "metadata": {},
   "source": [
    "### Data cleaning using regular expressions: \n",
    "\n",
    "python re module can be used to perform data cleaning operations using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab97d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define a string with dirty data\n",
    "data = \"abc123def456ghi\"\n",
    "\n",
    "# Remove all numbers from the string\n",
    "clean_data = re.sub(r'\\d+', '', data)\n",
    "\n",
    "# Print the cleaned data\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e58bfb",
   "metadata": {},
   "source": [
    "### Remove rows based on regex\n",
    "\n",
    "- Choose a word as my target, and I used the function str.contains() to find the indexes that contain those rows.\n",
    "- using the drop function, and setting axis to index, I can supply the indexes I have and drop those rows.\n",
    "- Printing out the number of rows, you can see it reduced by three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118f2b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: remove rows that contain the target word\n",
    "target = '[Nn]oisy'\n",
    "\n",
    "noisy_airbnb = airbnb[airbnb['name'].str.contains(target, regex=True)]\n",
    "\n",
    "# show rows that contains the word noisy\n",
    "print(noisy_airbnb['name'])\n",
    "\n",
    "# get the index that contains the word noisy\n",
    "index_to_drop = noisy_airbnb['name'].index\n",
    "\n",
    "# print(index_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd0cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows based on index\n",
    "airbnb.drop(index_to_drop, axis='index', inplace=True)\n",
    "\n",
    "print(len(airbnb_ori))\n",
    "print(len(airbnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e498e",
   "metadata": {},
   "source": [
    "# Syntax errors\n",
    "\n",
    "### Fixing typos\n",
    "- Solution to Correcting different words but with the same meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ceebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['gender'].map({'m': 'male', 'fem.': 'female'})\n",
    "re.sub(r\"\\^m\\$\", 'Male', 'male', flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816b8156",
   "metadata": {},
   "source": [
    "### Spelling errors in categorical data\n",
    "\n",
    "- Sometimes your categorical data might have spelling errors or different capitalization that can mess up your categorization.\n",
    "- fix this by using the replace function in pandas. \n",
    "- We first give the values that are wrong, then supply the right ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6eb885",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb['neighbourhood_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8123c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_spelling = ['manhatann', 'brookln']\n",
    "\n",
    "# replace them with the wrong spelling\n",
    "airbnb.loc[random_index,'neighbourhood_group'] = wrong_spelling\n",
    "airbnb['neighbourhood_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25b48d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb['neighbourhood_group'].replace(['manhatann', 'brookln'],\n",
    "                             ['Manhattan', 'Brooklyn'], inplace=True)\n",
    "airbnb['neighbourhood_group'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad455c",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "\n",
    "### Invalid Data\n",
    "\n",
    "- values which are simply not logically correct\n",
    "- reasons leading to invalid data:\n",
    "    - Data collection errors\n",
    "        - The data engineer can type 1799 instead of 179 for height column. \n",
    "        - This kind of random mistakes can be taken as null value and imputed alongside other NAs.\n",
    "\n",
    "    - Data manipulation errors\n",
    "        - Some columns of dataset can be output of functions coded by developers. \n",
    "        - For example, a function calculates age from birthdate and answers are negative. It means the equation is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f342f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function.\n",
    "def position_type(s):\n",
    "    \n",
    "    \"\"\"\"This function converts the individual positions (abbreviations) and classifies it\n",
    "    as either a forward, midfielder, back or goalkeeper\"\"\"\n",
    "    \n",
    "    if (s[-2] == 'T') | (s[-2] == 'W'):\n",
    "        return 'Forward'\n",
    "    elif s[-2] == 'M':\n",
    "        return 'Midfielder'\n",
    "    elif s[-2] == 'B':\n",
    "        return 'Back'\n",
    "    else:\n",
    "        return 'GoalKeeper'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee0333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create position type column.\n",
    "df['Preferred Positions Type'] = df['Preferred Positions'].apply(position_type)\n",
    "\n",
    "# Look at first 5 entries.\n",
    "df['Preferred Positions Type'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dc97d2",
   "metadata": {},
   "source": [
    "### Convert from its categorical labels to numeric.\n",
    "\n",
    "- Identify the range of values that a certain feature may contain. \n",
    "- Based on the values identified, we can create a function to overwrite each value with numerical values.\n",
    "\n",
    "Function would take a pandas series as an input and convert the string to a numeric value. \n",
    "- We can then apply the function to the series of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d5975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col'].value_counts()\n",
    "\n",
    "\n",
    "def garage_qual_cleaner(cell):\n",
    "    if cell == 'Ex':\n",
    "        return 5\n",
    "    elif cell == 'Gd':\n",
    "        return 4\n",
    "    elif cell == 'TA':\n",
    "        return 3\n",
    "    elif cell == 'Fa':\n",
    "        return 2\n",
    "    elif cell == 'Po':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6eaab2",
   "metadata": {},
   "source": [
    "Alternatively, map a dictionary to overwrite values without creating a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe81cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['kitchen_qual'].map({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5478b6",
   "metadata": {},
   "source": [
    "### Create a Master funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaner(df):\n",
    "    # map numeric values onto all the quality columns using a quality dictionary\n",
    "    qual_dict = {'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4}\n",
    "    # create a list of ordinal column names \n",
    "    ordinal_col_names = [col for col in df.columns if (col[-4:] in ['qual', 'cond']) and col[:3] != 'ove'] # last section ignores \"overall quality columns which will be addressed below\n",
    "    # creating a new feature called age\n",
    "    df['age'] = df.apply(lambda row: row['yr_sold'] - max(row['year_built'], row['year_remod/add']), axis=1)\n",
    "    # dummify the date sold column \n",
    "    df['date_sold'] = df.apply(lambda row: str(row['mo_sold'])+ '-' + str(row['yr_sold']), axis=1)\n",
    "    df.loc[:,df.dtypes!= 'object'] = df.loc[:, df.dtypes != 'object'].apply(lambda col: col.fillna(col.mean()))\n",
    "    \n",
    "    # transforming columns \n",
    "    df[ordinal_col_names] = df[ordinal_col_names].applymap(lambda cell: 2 if pd.isnull(cell) else qual_dict[cell])\n",
    "    \n",
    "    return df\n",
    "# applying the function to train data\n",
    "train = clean_data(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111fadcd",
   "metadata": {},
   "source": [
    "### Data transformation: \n",
    "\n",
    "The apply() function in pandas can be used to apply a custom function to each element in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e9c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with a column\n",
    "data = {'col1': [1, 2, 3, 4, 5]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Square each value in col1 using a custom function\n",
    "df['col1_squared'] = df['col1'].apply(lambda x: x**2)\n",
    "\n",
    "# Print the transformed DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302400a1",
   "metadata": {},
   "source": [
    "### Columnwise Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e612636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_wise_trans(col_name):\n",
    "    df = pd.DataFrame()\n",
    "    for i,j in df[[col_name]].itterrows():\n",
    "        if j[col_name] != \"[]\":\n",
    "            x = j[col_name].replace('=>', ':')\n",
    "            x_1 = x.replace('\\n', '').replace('\\\\r','')\n",
    "            d = pd.json_normalize(ast.leteral_eval(x_1))\n",
    "            df = df.append( d, ingore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_wise_trans(col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49072dd6",
   "metadata": {},
   "source": [
    "### What about variables without a clear, ordered relationship? \n",
    "\n",
    "- Visualize the relationship with our target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265fe14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(35,10)) # adjust the fig size to see everything\n",
    "sns.boxplot(train['neighborhood'], train['saleprice']).set_title('Sale Price varies widely by Ames Neighborhood')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501774c4",
   "metadata": {},
   "source": [
    "- Variables that do not have take on ordinal values can converted to a numerical amount by dummifying. \n",
    "- Pandas provides a method to get dummify the variables — \n",
    "    - for each value (in this case neighborhood) a new feature will be created and the row will have a value of 0 or 1 for that column — \n",
    "        - A 1 signifying that in the original string column, a row contained the value that is now in the column name.\n",
    "- Use the pandas get dummies method to convert these to numeric values. \n",
    "- It is important that we call the ‘drop_first’ argument and set it as ‘True.’ \n",
    "- This will dummify all variables after dropping the first one. We do this because it is important that a categorical variable of K categories, or levels, usually enters a regression as a sequence of K-1 dummy variables and the dropped variable will serve as our reference category. \n",
    "- If a row has a value of 0 for all categories, we know that that observation belonged to the dropped column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa6bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df, columns= ['col'], drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45beeb4b",
   "metadata": {},
   "source": [
    "# Timestamps\n",
    "\n",
    "### Convert timestamp(from string to datetime format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_str_datetime(df): \n",
    "    '''\n",
    "    AIM    -> Convert datetime(String) to datetime(format we want)\n",
    "     \n",
    "    INPUT  -> df\n",
    "    \n",
    "    OUTPUT -> updated df with new datetime format \n",
    "    ------\n",
    "    '''\n",
    "    df.insert(loc=2, column='timestamp', value=pd.to_datetime(df.transdate, format='%Y-%m-%d %H:%M:%S.%f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f248095c",
   "metadata": {},
   "source": [
    "### Output a dataframe without NaN values for a Timestamp column\n",
    "\n",
    "- Want to output a dataframe with all the available data that a column has.\n",
    "    - Exlcude rows with NaN.\n",
    "    - Time series data where the column is timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced09120",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef remove_nan_values(df):\n",
    "    '''\n",
    "    AIM    -> remove NaN values of a particular column and output the whole dataframe\n",
    "     \n",
    "    INPUT  -> df\n",
    "    \n",
    "    OUTPUT -> updated df without NaN values for a particular column \n",
    "    ------\n",
    "    '''\n",
    "    df = df[df['col_1'].notnull()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0db62b",
   "metadata": {},
   "source": [
    "### Pipeline will follow the strategy\n",
    "\n",
    "##### Extraction of DateTime features\n",
    "\n",
    "- datetime values, like \n",
    "    - timestamps or \n",
    "    - dates, \n",
    "- want to extract these so they become easier to handle when processing or visualizing later on.\n",
    "- We can also do this in an automated fashion: \n",
    "    - we will let our pipeline search through the features and check whether one of these can be converted into the datetime type. \n",
    "    - If yes, then we can safely assume that this feature holds datetime values.\n",
    "    \n",
    "- We can define the granularity at which the datetime features are extracted, whereas the default is set to ‘s’ for seconds.\n",
    "- After the extraction, the function checks whether the entries for dates and times are valid meaning: \n",
    "    - if the extracted columns ‘Day’, ‘Month’ and ‘Year’ all contain 0’s, all three will be deleted. \n",
    "    - The same happens for ‘Hour’, ‘Minute’ and ‘Sec’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a69c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature for extracting datetime values\n",
    "def convert_datetime(df, extract_datetime='s'):\n",
    "    cols = set(df.columns) ^ set(df.select_dtypes(include=np.number).columns) \n",
    "    for feature in cols: \n",
    "        try:\n",
    "            # convert features encoded as strings to type datetime ['D','M','Y','h','m','s']\n",
    "            df[feature] = pd.to_datetime(df[feature], infer_datetime_format=True)\n",
    "            df['Day'] = pd.to_datetime(df[feature]).dt.day\n",
    "            if extract_datetime in ['M','Y','h','m','s']:\n",
    "                df['Month'] = pd.to_datetime(df[feature]).dt.month\n",
    "                if extract_datetime in ['Y','h','m','s']:\n",
    "                    df['Year'] = pd.to_datetime(df[feature]).dt.year\n",
    "                    if extract_datetime in ['h','m','s']:\n",
    "                        df['Hour'] = pd.to_datetime(df[feature]).dt.hour\n",
    "                        if extract_datetime in ['m','s']:\n",
    "                            df['Minute'] = pd.to_datetime(df[feature]).dt.minute\n",
    "                            if extract_datetime in ['s']:\n",
    "                              df['Sec'] = pd.to_datetime(df[feature]).dt.second\n",
    "            try: # check if entries for the extracted dates/times are valid, otherwise drop\n",
    "                if (df['Hour'] == 0).all() and (df['Minute'] == 0).all() and (df['Sec'] == 0).all():\n",
    "                    df.drop('Hour', inplace = True, axis =1 )\n",
    "                    df.drop('Minute', inplace = True, axis =1 )\n",
    "                    df.drop('Sec', inplace = True, axis =1 )\n",
    "                elif (df['Day'] == 0).all() and (df['Month'] == 0).all() and (df['Year'] == 0).all():\n",
    "                    df.drop('Day', inplace = True, axis =1 )\n",
    "                    df.drop('Month', inplace = True, axis =1 )\n",
    "                    df.drop('Year', inplace = True, axis =1 )  \n",
    "            except:\n",
    "                pass\n",
    "        except: # feature cannot be converted to datetime\n",
    "            pass          \n",
    "return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23af6440",
   "metadata": {},
   "source": [
    "### List comprehension\n",
    "- list comprehension is so elegant and clean that you can abstract your logic in a single line of code without having any for loops, and \n",
    "    - compute that much faster than for loops.\n",
    "- Use list comprehension if I want to get a list of values based on certain conditions to append to the existing dataframe or use that for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_comprehension(df):\n",
    "    '''\n",
    "    AIM    -> IF ELSE for the list comprehension \n",
    "    \n",
    "    INPUT  -> df \n",
    "    \n",
    "    OUTPUT -> List \n",
    "    ------\n",
    "    '''\n",
    "    compute_list = [df['col_1'][i] if df['col_1'][i] > 0 else -1 for i in range(len(df))]\n",
    "    return compute_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e5a8a",
   "metadata": {},
   "source": [
    "### Pipeline will follow the strategy\n",
    "\n",
    "##### Dataframe Polishing\n",
    "\n",
    "- some features that were originally of type integer could have been converted to floats, due to imputation techniques or other processing steps that were applied. Before outputting our final dataframe we will convert these values back to integers\n",
    "- want to round all the float features in our dataset to the same number of decimals as they had in our original input dataset. This is to avoid on one hand unnecessary trailing 0’s in the float decimals, and on the other hand make sure not to round our values more than our original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76303043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AutoClean import AutoClean\n",
    "pipeline = AutoClean(dataset)\n",
    "\n",
    "pipeline.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef219a7",
   "metadata": {},
   "source": [
    "# Speed up your Data Cleaning and Preprocessing with klib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ed5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a0638b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b7f4665",
   "metadata": {},
   "source": [
    "### Relationship between Numeric independant variable vs dependant\n",
    "-  Pandas’ corrwith() method will return a pair-wise correlation for each numeric variable with the target and ignore non-numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce941fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = train.corrwith(train['saleprice']).iloc[:-1].to_frame()\n",
    "correlations['abs'] = correlations[0].abs()\n",
    "sorted_correlations = correlations.sort_values('abs', ascending=False)[0]\n",
    "fig, ax = plt.subplots(figsize=(10,20))\n",
    "sns.heatmap(sorted_correlations.to_frame(), cmap='coolwarm', annot=True, vmin=-1, vmax=1, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e1865b",
   "metadata": {},
   "source": [
    "### Relationship between Object variables vs dependant\n",
    "- Ordinal and categorical variables such as ‘exterior condition’ and ‘central air’ intuitively would have a relationship to sale price. It is critical that we visualize this.\n",
    "- achieve this by generating a box plot that compares the values of an ordinal/categorical and visualize a relationship with sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee01c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(train['central_air'],\n",
    "        train['saleprice']).set_title('Central Air vs. Sale Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af8311",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(train['kitchen_qual'], \n",
    "            train['saleprice']).set_title('Kitchen Quality vs. Sale Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1df1a35",
   "metadata": {},
   "source": [
    "# Data manipulation\n",
    "\n",
    "### Adding new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7dadcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a DataFrame\n",
    "data = {'Name': ['John', 'Alice', 'Bob', 'Emily'],\n",
    "        'Age': [25, 30, 28, 35],\n",
    "        'City': ['New York', 'Paris', 'London', 'Tokyo']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Adding new data\n",
    "new_data = {'Name': 'Mark', 'Age': 27, 'City': 'Sydney'}\n",
    "df = df.append(new_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce941dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John</td>\n",
       "      <td>25</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alice</td>\n",
       "      <td>30</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bob</td>\n",
       "      <td>28</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emily</td>\n",
       "      <td>35</td>\n",
       "      <td>Tokyo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mark</td>\n",
       "      <td>27</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age      City\n",
       "0   John   25  New York\n",
       "1  Alice   30     Paris\n",
       "2    Bob   28    London\n",
       "3  Emily   35     Tokyo\n",
       "4   Mark   27    Sydney"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb37a7",
   "metadata": {},
   "source": [
    "### Updating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f290bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating data\n",
    "df.loc[df['Name'] == 'Alice', 'Age'] = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb75df22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John</td>\n",
       "      <td>25</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alice</td>\n",
       "      <td>31</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bob</td>\n",
       "      <td>28</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emily</td>\n",
       "      <td>35</td>\n",
       "      <td>Tokyo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mark</td>\n",
       "      <td>27</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age      City\n",
       "0   John   25  New York\n",
       "1  Alice   31     Paris\n",
       "2    Bob   28    London\n",
       "3  Emily   35     Tokyo\n",
       "4   Mark   27    Sydney"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c4858",
   "metadata": {},
   "source": [
    "### Deleting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "055660d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting data\n",
    "df = df.drop(index=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc1bf62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John</td>\n",
       "      <td>25</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alice</td>\n",
       "      <td>31</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emily</td>\n",
       "      <td>35</td>\n",
       "      <td>Tokyo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mark</td>\n",
       "      <td>27</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age      City\n",
       "0   John   25  New York\n",
       "1  Alice   31     Paris\n",
       "3  Emily   35     Tokyo\n",
       "4   Mark   27    Sydney"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0106bfeb",
   "metadata": {},
   "source": [
    "### Sorting and filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46792fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting and filtering data\n",
    "df = df.sort_values('Age', ascending=False)\n",
    "filtered_df = df[df['Age'] > 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78677751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emily</td>\n",
       "      <td>35</td>\n",
       "      <td>Tokyo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alice</td>\n",
       "      <td>31</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mark</td>\n",
       "      <td>27</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John</td>\n",
       "      <td>25</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age      City\n",
       "3  Emily   35     Tokyo\n",
       "1  Alice   31     Paris\n",
       "4   Mark   27    Sydney\n",
       "0   John   25  New York"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da55a93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emily</td>\n",
       "      <td>35</td>\n",
       "      <td>Tokyo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alice</td>\n",
       "      <td>31</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age   City\n",
       "3  Emily   35  Tokyo\n",
       "1  Alice   31  Paris"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d0be02",
   "metadata": {},
   "source": [
    "### Joining and merging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50570c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining and merging data\n",
    "data2 = {'Name': ['John', 'Alice', 'Bob'],\n",
    "         'Salary': [5000, 6000, 4500]}\n",
    "df2 = pd.DataFrame(data2)\n",
    "merged_df = pd.merge(df, df2, on='Name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc651059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Emily</td>\n",
       "      <td>35</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alice</td>\n",
       "      <td>31</td>\n",
       "      <td>Paris</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mark</td>\n",
       "      <td>27</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>John</td>\n",
       "      <td>25</td>\n",
       "      <td>New York</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age      City  Salary\n",
       "0  Emily   35     Tokyo     NaN\n",
       "1  Alice   31     Paris  6000.0\n",
       "2   Mark   27    Sydney     NaN\n",
       "3   John   25  New York  5000.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45183cf4",
   "metadata": {},
   "source": [
    "### Transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "732cd4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming data\n",
    "df['City'] = df['City'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "604b9c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emily</td>\n",
       "      <td>35</td>\n",
       "      <td>TOKYO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alice</td>\n",
       "      <td>31</td>\n",
       "      <td>PARIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mark</td>\n",
       "      <td>27</td>\n",
       "      <td>SYDNEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John</td>\n",
       "      <td>25</td>\n",
       "      <td>NEW YORK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age      City\n",
       "3  Emily   35     TOKYO\n",
       "1  Alice   31     PARIS\n",
       "4   Mark   27    SYDNEY\n",
       "0   John   25  NEW YORK"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6c1d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "df['A_squared'] = df['A'].transform(lambda x: x ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d675f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>Bins</th>\n",
       "      <th>A_squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.015811</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>Low</td>\n",
       "      <td>1.031872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.016815</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4.067541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014207</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>Low</td>\n",
       "      <td>0.000202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.053917</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>Medium</td>\n",
       "      <td>16.434243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.891070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>Medium</td>\n",
       "      <td>23.922570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A    B   C   D    Bins  A_squared\n",
       "0  1.015811  6.0  10  15     Low   1.031872\n",
       "1  2.016815  7.0  11  16  Medium   4.067541\n",
       "2  0.014207  8.0  12  17     Low   0.000202\n",
       "3  4.053917  9.0  13  18  Medium  16.434243\n",
       "4  4.891070  0.0  14  19  Medium  23.922570"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45dbe2",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc0dee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [6, 7, 8, 9, np.nan],\n",
    "    'C': [10, 11, 12, 13, 14]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Missing Data\n",
    "df.fillna(0, inplace=True)  # Fill missing values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f000e3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B   C\n",
       "0  1.0  6.0  10\n",
       "1  2.0  7.0  11\n",
       "2  0.0  8.0  12\n",
       "3  4.0  9.0  13\n",
       "4  5.0  0.0  14"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2464ceb2",
   "metadata": {},
   "source": [
    "### Noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1211d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy Data\n",
    "df['A'] = df['A'] + np.random.normal(0, 0.1, len(df))  # Add noise to column A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f35bc046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.015811</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.016815</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014207</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.053917</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.891070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A    B   C\n",
       "0  1.015811  6.0  10\n",
       "1  2.016815  7.0  11\n",
       "2  0.014207  8.0  12\n",
       "3  4.053917  9.0  13\n",
       "4  4.891070  0.0  14"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b556f13",
   "metadata": {},
   "source": [
    "### Outliers Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47e9e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers Detection\n",
    "def detect_outliers(data, threshold=3):\n",
    "    z_scores = (data - data.mean()) / data.std()\n",
    "    outliers = np.abs(z_scores) > threshold\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2272c680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-831b02f4f1a1>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['A'][outliers] = np.nan  # Replace outliers with NaN\n"
     ]
    }
   ],
   "source": [
    "outliers = detect_outliers(df['A'])\n",
    "df['A'][outliers] = np.nan  # Replace outliers with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f92e243b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.015811</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.016815</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014207</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.053917</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.891070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A    B   C\n",
       "0  1.015811  6.0  10\n",
       "1  2.016815  7.0  11\n",
       "2  0.014207  8.0  12\n",
       "3  4.053917  9.0  13\n",
       "4  4.891070  0.0  14"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37390de",
   "metadata": {},
   "source": [
    "### Join / Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e765a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join\n",
    "df2 = pd.DataFrame({'D': [15, 16, 17, 18, 19]})\n",
    "df = df.join(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19c6399c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.015811</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.016815</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014207</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.053917</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.891070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A    B   C   D\n",
       "0  1.015811  6.0  10  15\n",
       "1  2.016815  7.0  11  16\n",
       "2  0.014207  8.0  12  17\n",
       "3  4.053917  9.0  13  18\n",
       "4  4.891070  0.0  14  19"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d189d0b1",
   "metadata": {},
   "source": [
    "### Melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45b7b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt\n",
    "df_melted = pd.melt(df, id_vars=['D'], value_vars=['A', 'B', 'C'], var_name='Variable', value_name='Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34bd60fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D</th>\n",
       "      <th>Variable</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>A</td>\n",
       "      <td>1.015811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>A</td>\n",
       "      <td>2.016815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>A</td>\n",
       "      <td>0.014207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>A</td>\n",
       "      <td>4.053917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>A</td>\n",
       "      <td>4.891070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>B</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17</td>\n",
       "      <td>B</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18</td>\n",
       "      <td>B</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19</td>\n",
       "      <td>B</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>C</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>C</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17</td>\n",
       "      <td>C</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18</td>\n",
       "      <td>C</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>19</td>\n",
       "      <td>C</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     D Variable      Value\n",
       "0   15        A   1.015811\n",
       "1   16        A   2.016815\n",
       "2   17        A   0.014207\n",
       "3   18        A   4.053917\n",
       "4   19        A   4.891070\n",
       "5   15        B   6.000000\n",
       "6   16        B   7.000000\n",
       "7   17        B   8.000000\n",
       "8   18        B   9.000000\n",
       "9   19        B   0.000000\n",
       "10  15        C  10.000000\n",
       "11  16        C  11.000000\n",
       "12  17        C  12.000000\n",
       "13  18        C  13.000000\n",
       "14  19        C  14.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_melted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc097dc",
   "metadata": {},
   "source": [
    "### Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9526bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>Bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.015811</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.016815</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014207</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.053917</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.891070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A    B   C   D    Bins\n",
       "0  1.015811  6.0  10  15     Low\n",
       "1  2.016815  7.0  11  16  Medium\n",
       "2  0.014207  8.0  12  17     Low\n",
       "3  4.053917  9.0  13  18  Medium\n",
       "4  4.891070  0.0  14  19  Medium"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cut\n",
    "df['Bins'] = pd.cut(df['A'], bins=[0, 2, 5, np.inf], labels=['Low', 'Medium', 'High'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b9bd3",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87325bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "df.dropna(inplace=True)  # Remove rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91c2f882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>Bins</th>\n",
       "      <th>A_squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.015811</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>Low</td>\n",
       "      <td>1.031872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.016815</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4.067541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014207</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>Low</td>\n",
       "      <td>0.000202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.053917</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>Medium</td>\n",
       "      <td>16.434243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.891070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>Medium</td>\n",
       "      <td>23.922570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A    B   C   D    Bins  A_squared\n",
       "0  1.015811  6.0  10  15     Low   1.031872\n",
       "1  2.016815  7.0  11  16  Medium   4.067541\n",
       "2  0.014207  8.0  12  17     Low   0.000202\n",
       "3  4.053917  9.0  13  18  Medium  16.434243\n",
       "4  4.891070  0.0  14  19  Medium  23.922570"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd825a",
   "metadata": {},
   "source": [
    "### Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71a1eb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     B   C   D\n",
       "1  7.0  11  16\n",
       "2  8.0  12  17"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slicing\n",
    "df_sliced = df.iloc[1:3, 1:4]  # Select rows 1 and 2, columns 1, 2, and 3\n",
    "df_sliced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb009fc",
   "metadata": {},
   "source": [
    "### Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c336709f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Bins</th>\n",
       "      <th>Low</th>\n",
       "      <th>Medium</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.015811</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.016815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.014207</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.053917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.891070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Bins       Low    Medium\n",
       "D                       \n",
       "15    1.015811       NaN\n",
       "16         NaN  2.016815\n",
       "17    0.014207       NaN\n",
       "18         NaN  4.053917\n",
       "19         NaN  4.891070"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshaping\n",
    "df_pivoted = df.pivot(index='D', columns='Bins', values='A')\n",
    "df_pivoted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900864b6",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1331cacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>Bins</th>\n",
       "      <th>A_squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.016815</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4.067541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.053917</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>Medium</td>\n",
       "      <td>16.434243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.891070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>Medium</td>\n",
       "      <td>23.922570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A    B   C   D    Bins  A_squared\n",
       "1  2.016815  7.0  11  16  Medium   4.067541\n",
       "3  4.053917  9.0  13  18  Medium  16.434243\n",
       "4  4.891070  0.0  14  19  Medium  23.922570"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter\n",
    "df_filtered = df[df['A'] > 2]  # Filter rows where column A > 2\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe5ea38",
   "metadata": {},
   "source": [
    "### Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7acc6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>A_squared</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>0.515009</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.516037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medium</th>\n",
       "      <td>3.653934</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>17.666667</td>\n",
       "      <td>14.808118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               A         B          C          D  A_squared\n",
       "Bins                                                       \n",
       "Low     0.515009  7.000000  11.000000  16.000000   0.516037\n",
       "Medium  3.653934  5.333333  12.666667  17.666667  14.808118\n",
       "High         NaN       NaN        NaN        NaN        NaN"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by\n",
    "df_grouped = df.groupby('Bins').mean()  # Calculate the mean for each group\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a2ce9",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62be60be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "df['Bins_encoded'] = df['Bins'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4de16090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>Bins</th>\n",
       "      <th>A_squared</th>\n",
       "      <th>Bins_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.015811</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>Low</td>\n",
       "      <td>1.031872</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.016815</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4.067541</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014207</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>Low</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.053917</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>Medium</td>\n",
       "      <td>16.434243</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.891070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>Medium</td>\n",
       "      <td>23.922570</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A    B   C   D    Bins  A_squared  Bins_encoded\n",
       "0  1.015811  6.0  10  15     Low   1.031872             0\n",
       "1  2.016815  7.0  11  16  Medium   4.067541             1\n",
       "2  0.014207  8.0  12  17     Low   0.000202             0\n",
       "3  4.053917  9.0  13  18  Medium  16.434243             1\n",
       "4  4.891070  0.0  14  19  Medium  23.922570             1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb4875f",
   "metadata": {},
   "source": [
    "### Pivot and Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c65a61ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>Bins</th>\n",
       "      <th>A_squared</th>\n",
       "      <th>Bins_encoded</th>\n",
       "      <th>Low</th>\n",
       "      <th>Medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.015811</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>Low</td>\n",
       "      <td>1.031872</td>\n",
       "      <td>0</td>\n",
       "      <td>1.015811</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.016815</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4.067541</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.016815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014207</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>Low</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014207</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.053917</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>Medium</td>\n",
       "      <td>16.434243</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.053917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.891070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>Medium</td>\n",
       "      <td>23.922570</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.891070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A    B   C   D    Bins  A_squared  Bins_encoded       Low    Medium\n",
       "0  1.015811  6.0  10  15     Low   1.031872             0  1.015811       NaN\n",
       "1  2.016815  7.0  11  16  Medium   4.067541             1       NaN  2.016815\n",
       "2  0.014207  8.0  12  17     Low   0.000202             0  0.014207       NaN\n",
       "3  4.053917  9.0  13  18  Medium  16.434243             1       NaN  4.053917\n",
       "4  4.891070  0.0  14  19  Medium  23.922570             1       NaN  4.891070"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pivot and Merge\n",
    "df_pivot = df.pivot(index='D', columns='Bins', values='A')\n",
    "df_merge = pd.merge(df, df_pivot, on='D')\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54acfeee",
   "metadata": {},
   "source": [
    "### Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "285ab34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>Bins</th>\n",
       "      <th>A_squared</th>\n",
       "      <th>Bins_encoded</th>\n",
       "      <th>Low</th>\n",
       "      <th>Medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.015811</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>1.031872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.016815</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4.067541</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014207</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.053917</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Medium</td>\n",
       "      <td>16.434243</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.891070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Medium</td>\n",
       "      <td>23.922570</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.015811</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.016815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014207</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.053917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.891070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           A    B     C     D    Bins  A_squared  Bins_encoded       Low  \\\n",
       "0   1.015811  6.0  10.0  15.0     Low   1.031872           0.0       NaN   \n",
       "1   2.016815  7.0  11.0  16.0  Medium   4.067541           1.0       NaN   \n",
       "2   0.014207  8.0  12.0  17.0     Low   0.000202           0.0       NaN   \n",
       "3   4.053917  9.0  13.0  18.0  Medium  16.434243           1.0       NaN   \n",
       "4   4.891070  0.0  14.0  19.0  Medium  23.922570           1.0       NaN   \n",
       "15       NaN  NaN   NaN   NaN     NaN        NaN           NaN  1.015811   \n",
       "16       NaN  NaN   NaN   NaN     NaN        NaN           NaN       NaN   \n",
       "17       NaN  NaN   NaN   NaN     NaN        NaN           NaN  0.014207   \n",
       "18       NaN  NaN   NaN   NaN     NaN        NaN           NaN       NaN   \n",
       "19       NaN  NaN   NaN   NaN     NaN        NaN           NaN       NaN   \n",
       "\n",
       "      Medium  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  \n",
       "15       NaN  \n",
       "16  2.016815  \n",
       "17       NaN  \n",
       "18  4.053917  \n",
       "19  4.891070  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate\n",
    "df_concat = pd.concat([df, df_pivot], axis=1)\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf5e1a2",
   "metadata": {},
   "source": [
    "### MultiIndexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3e82165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>A_squared</th>\n",
       "      <th>Bins_encoded</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <th>Bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <th>Low</th>\n",
       "      <td>1.015811</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.031872</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>Medium</th>\n",
       "      <td>2.016815</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11</td>\n",
       "      <td>4.067541</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <th>Low</th>\n",
       "      <td>0.014207</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <th>Medium</th>\n",
       "      <td>4.053917</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13</td>\n",
       "      <td>16.434243</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <th>Medium</th>\n",
       "      <td>4.891070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>23.922570</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  A    B   C  A_squared  Bins_encoded\n",
       "D  Bins                                              \n",
       "15 Low     1.015811  6.0  10   1.031872             0\n",
       "16 Medium  2.016815  7.0  11   4.067541             1\n",
       "17 Low     0.014207  8.0  12   0.000202             0\n",
       "18 Medium  4.053917  9.0  13  16.434243             1\n",
       "19 Medium  4.891070  0.0  14  23.922570             1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MultiIndexing\n",
    "df_multiindexed = df.set_index(['D', 'Bins'])\n",
    "df_multiindexed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479cafe3",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a3516a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D   Bins                \n",
       "15  Low     A                1.015811\n",
       "            B                6.000000\n",
       "            C               10.000000\n",
       "            A_squared        1.031872\n",
       "            Bins_encoded     0.000000\n",
       "16  Medium  A                2.016815\n",
       "            B                7.000000\n",
       "            C               11.000000\n",
       "            A_squared        4.067541\n",
       "            Bins_encoded     1.000000\n",
       "17  Low     A                0.014207\n",
       "            B                8.000000\n",
       "            C               12.000000\n",
       "            A_squared        0.000202\n",
       "            Bins_encoded     0.000000\n",
       "18  Medium  A                4.053917\n",
       "            B                9.000000\n",
       "            C               13.000000\n",
       "            A_squared       16.434243\n",
       "            Bins_encoded     1.000000\n",
       "19  Medium  A                4.891070\n",
       "            B                0.000000\n",
       "            C               14.000000\n",
       "            A_squared       23.922570\n",
       "            Bins_encoded     1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacking\n",
    "df_stacked = df_multiindexed.stack()\n",
    "df_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984bc3ab",
   "metadata": {},
   "source": [
    "### Hierarchical indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e9a8942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>A_squared</th>\n",
       "      <th>Bins_encoded</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <th>Bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <th>Low</th>\n",
       "      <td>1.015811</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.031872</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>Medium</th>\n",
       "      <td>2.016815</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11</td>\n",
       "      <td>4.067541</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <th>Low</th>\n",
       "      <td>0.014207</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <th>Medium</th>\n",
       "      <td>4.053917</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13</td>\n",
       "      <td>16.434243</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <th>Medium</th>\n",
       "      <td>4.891070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>23.922570</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  A    B   C  A_squared  Bins_encoded\n",
       "D  Bins                                              \n",
       "15 Low     1.015811  6.0  10   1.031872             0\n",
       "16 Medium  2.016815  7.0  11   4.067541             1\n",
       "17 Low     0.014207  8.0  12   0.000202             0\n",
       "18 Medium  4.053917  9.0  13  16.434243             1\n",
       "19 Medium  4.891070  0.0  14  23.922570             1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hierarchical indexing\n",
    "df_hierarchical = df.set_index(['D', 'Bins'])\n",
    "df_hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ada828",
   "metadata": {},
   "source": [
    "### Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0847615a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>1.030018</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medium</th>\n",
       "      <td>10.961802</td>\n",
       "      <td>12.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                A          C\n",
       "Bins                        \n",
       "Low      1.030018  11.000000\n",
       "Medium  10.961802  12.666667\n",
       "High     0.000000        NaN"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate\n",
    "df_aggregated = df.groupby('Bins').agg({'A': 'sum', 'C': 'mean'})\n",
    "df_aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d94e27",
   "metadata": {},
   "source": [
    "### Summarize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aece2a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>A_squared</th>\n",
       "      <th>Bins_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.398364</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>9.091286</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.043017</td>\n",
       "      <td>3.535534</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>10.567947</td>\n",
       "      <td>0.547723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.014207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.015811</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.031872</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.016815</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>4.067541</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.053917</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>16.434243</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.891070</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>23.922570</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              A         B          C          D  A_squared  Bins_encoded\n",
       "count  5.000000  5.000000   5.000000   5.000000   5.000000      5.000000\n",
       "mean   2.398364  6.000000  12.000000  17.000000   9.091286      0.600000\n",
       "std    2.043017  3.535534   1.581139   1.581139  10.567947      0.547723\n",
       "min    0.014207  0.000000  10.000000  15.000000   0.000202      0.000000\n",
       "25%    1.015811  6.000000  11.000000  16.000000   1.031872      0.000000\n",
       "50%    2.016815  7.000000  12.000000  17.000000   4.067541      1.000000\n",
       "75%    4.053917  8.000000  13.000000  18.000000  16.434243      1.000000\n",
       "max    4.891070  9.000000  14.000000  19.000000  23.922570      1.000000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarize data\n",
    "df_summary = df.describe()\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe19b1",
   "metadata": {},
   "source": [
    "# 1.Common Data Problems\n",
    "\n",
    "- Data Type Constraints\n",
    "    - column in a given dataframe is of a certain data type by default and needs to be corrected/converted to another datatype for ease of calculation and analytics.\n",
    "    - Example:\n",
    "        - numeric value like revenue which should be an integer is stored as a string value in the data\n",
    "        - categorical variable is represented as a number and mistakenly becomes an int variable \n",
    "- Data Range Constraints\n",
    "    - Data that should fall within a range.\n",
    "    - Example:\n",
    "        - data which can only hold values between 1–5 or 1–10.\n",
    "        - Subscription date column which cannot have value as future dates.    \n",
    "- Uniqueness Constraints\n",
    "    - duplicate values can be diagnosed when we have the same exact information repeated across multiple rows, for a some or all columns in our DataFrame.\n",
    "    - Casues :\n",
    "        - Data Entry Error\n",
    "        - Join or Merge Errors\n",
    "        - Bugs and Design Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4020a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################### Data type conversion\n",
    "############################# Example 1\n",
    "\n",
    "# Get data types of columns\n",
    "sales.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07afc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sales['Revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea8eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['Revenue'] = sales['Revenue'].str.strip('$')\n",
    "sales['Revenue'] = sales['Revenue'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a83bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Example 2\n",
    "\n",
    "# marriage_status values as numbers: 0 = Never married ,1 = Married ,2 = Separated ,3 = Divorced\n",
    "\n",
    "df['marriage_status'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20333d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"marriage_status\"] = df[\"marriage_status\"].astype('category')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0604d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################### Data Range Constraints\n",
    "############################# Example 1\n",
    "\n",
    "#  movie rating data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(movies['avg_rating'])\n",
    "plt.title('Average rating of movies (1-5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b2d0c",
   "metadata": {},
   "source": [
    "Many ways exist to deal with these out of range values, \n",
    "- like imputing the rows, \n",
    "- setting them to value 5 , or \n",
    "- setting to an average rating value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82068627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Output Movies with rating > 5\n",
    "movies[movies['avg_rating'] > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1730335",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies[movies['avg_rating'] <= 5]\n",
    "\n",
    "# Drop values using .drop()\n",
    "movies.drop(movies[movies['avg_rating'] > 5].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb90861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert avg_rating > 5 to 5\n",
    "movies.loc[movies['avg_rating'] > 5, 'avg_rating'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Example 2\n",
    "\n",
    "# see the date range falling out of expected values:\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "# Output data types\n",
    "user_signups.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f00a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to date\n",
    "user_signups['subscription_date'] = pd.to_datetime(user_signups['subscription_date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599741c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter our wrong/future date values in the given dataframe\n",
    "import datetime as dt\n",
    "today_date = dt.date.today()\n",
    "user_signups[user_signups['subscription_date'] > dt.date.today()]\n",
    "# this is an old code and 'today's date' was 1/1/2020 , result has been filtered accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the values of this column which are showing future dates with today’s date using the loc function\n",
    "\n",
    "today_date = dt.date.today()\n",
    "# Drop values using filtering\n",
    "user_signups = user_signups[user_signups['subscription_date'] < today_date]\n",
    "# Drop values using .drop()\n",
    "user_signups.drop(user_signups[user_signups['subscription_date'] > today_date].index, inplace = True)\n",
    "# Drop values using filtering\n",
    "user_signups.loc[user_signups['subscription_date'] > today_date, 'subscription_date'] = today_date\n",
    "# Assert its true\n",
    "assert user_signups.subscription_date.max().date() <= today_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################# Uniqueness: How to find duplicate Values\n",
    "########################### Example 1\n",
    "\n",
    "height_weight.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe225b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding duplicates in a DataFrame by using the duplicated() method.\n",
    "# returns a Series of boolean values that are True for duplicate values, and False for non-duplicated values.\n",
    "\n",
    "duplicates = height_weight.duplicated()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as all the columns are required to have duplicate values by default, with all duplicate values being marked as True except for the first occurrence\n",
    "# To calibrate how we go about finding duplicates, we will use 2 arguments from the duplicated() method.\n",
    "### The subset argument lets us set a list of column names to check for duplication.\n",
    "###  The keep argument lets us keep the first occurrence of a duplicate value by setting it to the string first, the last occurrence of a duplicate value by setting it the string last, or keep all occurrences of duplicate values by setting it to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d96032",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['first_name','last_name','address']\n",
    "duplicates = height_weight.duplicated(subset = column_names, keep = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_weight[duplicates].sort_values(by = 'first_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f739c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating Duplicates\n",
    "# drop_duplicates() method — takes same set of arguments as duplicates method — subset and inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_weight.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01074e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "006dc6a8",
   "metadata": {},
   "source": [
    "# 2. Text and Categorical Data Problems\n",
    "\n",
    "categorical variable is a variable that can take on one of a limited, and usually fixed, number of possible values. Hence categorical data represent variables that represent predefined finite set of categories. Categorical data represent a predefined set of categories, they can’t have values that go beyond these predefined categories.\n",
    "- Examples:\n",
    "    - marriage status, \n",
    "    - household income categories, \n",
    "    - loan status \n",
    "    \n",
    "machine learning models run on categorical data are often coded as numbers.\n",
    "\n",
    "- Membership constraints\n",
    "    - inconsistencies in our categorical data\n",
    "    - reasons:\n",
    "        - due to data entry issues with free text vs dropdown fields, \n",
    "        - data parsing errors and other types of errors.\n",
    "- Categorical variables\n",
    "    - Examples:\n",
    "        - value inconsistency, \n",
    "        - the presence of too many categories that could be collapsed into one, and \n",
    "        - making sure data is of the right type.\n",
    "- Cleaning text data\n",
    "    - Examples of text data problems include:\n",
    "        - handling inconsistencies, \n",
    "        - making sure text data is of a certain length, \n",
    "        - typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3856775",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## Dealng with Inconsistencies\n",
    "####################### Example 1\n",
    "\n",
    "# Check predefined set of Catagories\n",
    "# The categories dataframe will help us systematically spot all rows with these inconsistencies.\n",
    "\n",
    "print(Catagories_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b434b54",
   "metadata": {},
   "source": [
    "##### Use joins to fix categorical inconsistencies in data, 2 main types: anti joins and inner joins.\n",
    "join DataFrames on common columns between them. \n",
    "- Anti joins, take in two DataFrames A and B, and return data from one DataFrame that is not contained in another.\n",
    "- Example, \n",
    "    - we are performing a left anti join of A and B, and are returning the columns of DataFrames A and B for values only found in A of the common column between them being joined on.\n",
    "- Inner joins, return only the data that is contained in both DataFrames. \n",
    "- Example,\n",
    "    - inner join of A and B, would return columns from both DataFrames for values only found in A and B, of the common column between them being joined on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  left anti join essentially returns all the data in study data with inconsistent blood types\n",
    "\n",
    "inconsistent_categories = set(study_data['blood_type']).difference(categories['blood_type'])\n",
    "print(inconsistent_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a22737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the row which has this inconsistent value :\n",
    "\n",
    "# Get and print rows with inconsistent categories\n",
    "inconsistent_rows = study_data['blood_type'].isin(inconsistent_categories)\n",
    "study_data[inconsistent_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53674a58",
   "metadata": {},
   "source": [
    "drop inconsistent rows and keep ones that are only consistent. We just use the tilde symbol while subsetting which returns everything except inconsistent rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6500a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inconsistent_categories = set(study_data['blood_type']).difference(categories['blood_type'])\n",
    "inconsistent_rows = study_data['blood_type'].isin(inconsistent_categories)\n",
    "inconsistent_data = study_data[inconsistent_rows]\n",
    "# Drop inconsistent categories and get consistent data only\n",
    "consistent_data = study_data[~inconsistent_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# Value Inconsistancy and collased into single catagory\n",
    "######################### Example 1\n",
    "\n",
    "# Get marriage status column\n",
    "marriage_status = demographics['marriage_status']\n",
    "marriage_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5685873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal by capitalize or lowercase the marriage_status column. \n",
    "# done with the str-dot-upper() or dot-lower() functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04383b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capitalize\n",
    "marriage_status['marriage_status'] = marriage_status['marriage_status'].str.upper()\n",
    "marriage_status['marriage_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87891e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "marriage_status['marriage_status'] = marriage_status['marriage_status'].str.lower()\n",
    "marriage_status['marriage_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Example 2\n",
    "# Leading or Trailing spaces: ‘married ‘ , ‘married’ , ‘unmarried’ , ‘ unmarried’ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29015755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get marriage status column\n",
    "marriage_status = demographics['marriage_status']\n",
    "marriage_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4168188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove leading spaces, we can use the str-dot-strip() method which when given no input, \n",
    "# strips all leading and trailing white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33b5e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip all spaces\n",
    "demographics = demographics['marriage_status'].str.strip()\n",
    "demographics['marriage_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaf3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# Collapse values in a column into categories or bins.\n",
    "# number of categories is very high, and we may need to collapse them into a smaller group by mapping few categories into a broader category .\n",
    "############################# Example 1\n",
    "\n",
    "# collapses into bins\n",
    "# using the cut() method : lets us define category cutoff ranges with the bins argument. \n",
    "# It takes in a list of cutoff points for each category, with the final one being infinity represented with np-dot-inf().\n",
    "\n",
    "# Using cut() - create category ranges and names\n",
    "ranges = [0,200000,500000,np.inf]\n",
    "group_names = ['0-200K', '200K-500K', '500K+']\n",
    "# Create income group column\n",
    "demographics['income_group'] = pd.cut(demographics['household_income'], bins=ranges, labels=group_names)\n",
    "demographics[['income_group', 'household_income']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c544c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Example 2\n",
    "\n",
    "# Mapping categories to fewer ones :\n",
    "# operating_system column is: ‘Microsoft’, ‘MacOS’, ‘IOS’, ‘Android’, ‘Linux’\n",
    "# operating_system column should become: ‘DesktopOS’, ‘MobileOS’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aeb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping dictionary and replace\n",
    "mapping = {'Microsoft':'DesktopOS', 'MacOS':'DesktopOS', 'Linux':'DesktopOS',\n",
    "'IOS':'MobileOS', 'Android':'MobileOS'}\n",
    "devices['operating_system'] = devices['operating_system'].replace(mapping)\n",
    "devices['operating_system'].unique() # to check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d9fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################### Cleaning text Data\n",
    "####################### Example 1\n",
    "\n",
    "# Use cases\n",
    "# feed these phone numbers into an automated call system\n",
    "# create a report discussing the distribution of users by area code\n",
    "\n",
    "# What we expect :\n",
    "### 1. Phone numbers are aligned to begin with 00\n",
    "### 2. Any number below the 10 digit value is replaced with NaN to represent a missing value, and\n",
    "### 3. All the dashes have been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578c6da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"+\" with \"00\"\n",
    "phones[\"Phone number\"] = phones[\"Phone number\"].str.replace(\"+\",\"00\") \n",
    "phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e84950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"-\" with nothing\n",
    "phones[\"Phone number\"] = phones[\"Phone number\"].str.replace(\"-\",\"\")\n",
    "phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace phone numbers with lower than 10 digits to NaN\n",
    "digits = phones['Phone number'].str.len() \n",
    "phones.loc[digits <10,\"Phone number\"] = np.nan \n",
    "phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f9c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################### Regular Expressions\n",
    "####################### Example 2\n",
    "\n",
    "# Column contains a range of symbols from plus signs, dashes, parenthesis.\n",
    "# Regular expressions give us the ability to search for any pattern in text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace letters with nothing\n",
    "phones['Phone number'] = phones['Phone number'].str.replace(r'\\D+', '')\n",
    "phones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ce46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16d23ca4",
   "metadata": {},
   "source": [
    "# 3. Advanced Data Problems\n",
    "- Uniformity\n",
    "- Cross field validation\n",
    "- Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5525fc1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48c89ec2",
   "metadata": {},
   "source": [
    "# 4. Record Linkage\n",
    "- String comparison\n",
    "- Generating pairs\n",
    "- Linking Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece199f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
