{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52bc25f7",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf07849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to compute True Positives, True Negatives, False Positives and False Negatives\n",
    "\n",
    "def true_positive(y_true, y_pred):\n",
    "    tp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 1:\n",
    "            tp += 1\n",
    "    return tp\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    tn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 0:\n",
    "            tn += 1        \n",
    "    return tn\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    fp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 1:\n",
    "            fp += 1       \n",
    "    return fp\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    fn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 0:\n",
    "            fn += 1        \n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea735dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "TP = np.diag(cnf_matrix)\n",
    "TN = cnf_matrix.sum() - (FP + FN + TP)FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy for each class\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf443ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation for table metrics:\n",
    "import sklearn.metrics\n",
    "import mathdef matrix_metrix(real_values,pred_values,beta):\n",
    "CM = confusion_matrix(real_values,pred_values)\n",
    "TN = CM[0][0]\n",
    "FN = CM[1][0] \n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "Population = TN+FN+TP+FP\n",
    "Prevalence = round( (TP+FP) / Population,2)\n",
    "Accuracy   = round( (TP+TN) / Population,4)\n",
    "Precision  = round( TP / (TP+FP),4 )\n",
    "NPV        = round( TN / (TN+FN),4 )\n",
    "FDR        = round( FP / (TP+FP),4 )\n",
    "FOR        = round( FN / (TN+FN),4 ) \n",
    "check_Pos  = Precision + FDR\n",
    "check_Neg  = NPV + FOR\n",
    "Recall     = round( TP / (TP+FN),4 )\n",
    "FPR        = round( FP / (TN+FP),4 )\n",
    "FNR        = round( FN / (TP+FN),4 )\n",
    "TNR        = round( TN / (TN+FP),4 ) \n",
    "check_Pos2 = Recall + FNR\n",
    "check_Neg2 = FPR + TNR\n",
    "LRPos      = round( Recall/FPR,4 ) \n",
    "LRNeg      = round( FNR / TNR ,4 )\n",
    "DOR        = round( LRPos/LRNeg)\n",
    "F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)\n",
    "FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "BM         = Recall+TNR-1\n",
    "MK         = Precision+NPV-1   \n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos','check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','F1','FBeta','MCC','BM','MK'],     \n",
    "                        'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,F1,FBeta,MCC,BM,MK]})   \n",
    "\n",
    "return (mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Implementation\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplotfpr, tpr, thresholds = roc_curve(real_values, prob_values)\n",
    "\n",
    "auc = roc_auc_score(real_values, prob_values)\n",
    "print('AUC: %.3f' % auc)pyplot.plot(fpr, tpr, linestyle='--', label='Roc curve')\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()pyplot.show()\n",
    "\n",
    "# Precision-recall implementation\n",
    "\n",
    "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(real_values,prob_values)pyplot.plot(recall, precision, linestyle='--', label='Precision versus Recall')\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "pyplot.legend()pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba51b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for get many metrics directly from sklearn\n",
    "\n",
    "def sk_metrix(real_values,pred_values,beta):\n",
    "Accuracy = round( sklearn.metrics.accuracy_score(real_values,pred_values) ,4)\n",
    "Precision= round( sklearn.metrics.precision_score(real_values,pred_values),4 )\n",
    "Recall   = round( sklearn.metrics.recall_score(real_values,pred_values),4 )   \n",
    "F1       = round ( sklearn.metrics.f1_score(real_values,pred_values),4)\n",
    "FBeta    = round ( sklearn.metrics.fbeta_score(real_values,pred_values,beta) ,4)\n",
    "MCC      = round ( sklearn.metrics.matthews_corrcoef(real_values,pred_values)  ,4)   \n",
    "Hamming  = round ( sklearn.metrics.hamming_loss(real_values,pred_values) ,4)   \n",
    "Jaccard  = round ( sklearn.metrics.jaccard_score(real_values,pred_values) ,4)   \n",
    "Prec_Avg = round ( sklearn.metrics.average_precision_score(real_values,pred_values) ,4)   \n",
    "Accu_Avg = round ( sklearn.metrics.balanced_accuracy_score(real_values,pred_values) ,4)   \n",
    "\n",
    "mat_met = pd.DataFrame({\n",
    "'Metric': ['Accuracy','Precision','Recall','F1','FBeta','MCC','Hamming','Jaccard','Precision_Avg','Accuracy_Avg'],\n",
    "'Value': [Accuracy,Precision,Recall,F1,FBeta,MCC,Hamming,Jaccard,Prec_Avg,Accu_Avg]})   \n",
    "\n",
    "return (mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics For Multi-class Classification\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate accuracy\n",
    "    -> param y_true: list of true values\n",
    "    -> param y_pred: list of predicted values\n",
    "    -> return: accuracy score\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "# Intitializing variable to store count of correctly predicted classes\n",
    "    correct_predictions = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == yp:\n",
    "            correct_predictions += 1\n",
    "    #returns accuracy\n",
    "    return correct_predictions / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eeb9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of macro-averaged precision\n",
    "\n",
    "def macro_precision(y_true, y_pred):\n",
    "\n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize precision to 0\n",
    "    precision = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(y_true.unique()):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false positive for current class\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute precision for current class\n",
    "        temp_precision = tp / (tp + fp + 1e-6)\n",
    "        # keep adding precision for all classes\n",
    "        precision += temp_precision\n",
    "        \n",
    "    # calculate and return average precision over all classes\n",
    "    precision /= num_classes\n",
    "    \n",
    "    return precision\n",
    "\n",
    "print(f\"Macro-averaged Precision score : {macro_precision(y_test, y_pred) }\")\n",
    "\n",
    "# implement marco-averaged precision using sklearn\n",
    "macro_averaged_precision = metrics.precision_score(y_test, y_pred, average = 'macro')\n",
    "print(f\"Macro-Averaged Precision score using sklearn library : {macro_averaged_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of micro-averaged precision\n",
    "\n",
    "def micro_precision(y_true, y_pred):\n",
    "\n",
    "\n",
    "    # find the number of classes \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in y_true.unique():\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate true positive for current class\n",
    "        # and update overall tp\n",
    "        tp += true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate false positive for current class\n",
    "        # and update overall tp\n",
    "        fp += false_positive(temp_true, temp_pred)\n",
    "        \n",
    "    # calculate and return overall precision\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision\n",
    "\n",
    "print(f\"Micro-averaged Precision score : {micro_precision(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "#  implement mirco-averaged precision using sklearn\n",
    "micro_averaged_precision = metrics.precision_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged Precision score using sklearn library : {micro_averaged_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ed0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of macro-averaged recall\n",
    "\n",
    "def macro_recall(y_true, y_pred):\n",
    "\n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize recall to 0\n",
    "    recall = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(y_true.unique()):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false negative for current class\n",
    "        fn = false_negative(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute recall for current class\n",
    "        temp_recall = tp / (tp + fn + 1e-6)\n",
    "        \n",
    "        # keep adding recall for all classes\n",
    "        recall += temp_recall\n",
    "        \n",
    "    # calculate and return average recall over all classes\n",
    "    recall /= num_classes\n",
    "    \n",
    "    return recall\n",
    "\n",
    "print(f\"Macro-averaged recall score : {macro_recall(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "# implement macro-averaged recall using sklearn\n",
    "\n",
    "macro_averaged_recall = metrics.recall_score(y_test, y_pred, average = 'macro')\n",
    "print(f\"Macro-averaged recall score using sklearn : {macro_averaged_recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd95acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of micro-averaged recall\n",
    "\n",
    "def micro_recall(y_true, y_pred):\n",
    "\n",
    "\n",
    "    # find the number of classes \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in y_true.unique():\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate true positive for current class\n",
    "        # and update overall tp\n",
    "        tp += true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate false negative for current class\n",
    "        # and update overall tp\n",
    "        fn += false_negative(temp_true, temp_pred)\n",
    "        \n",
    "    # calculate and return overall recall\n",
    "    recall = tp / (tp + fn)\n",
    "    return recall\n",
    "\n",
    "print(f\"Micro-averaged recall score : {micro_recall(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "#  implement micro-averaged recall using sklearn\n",
    "\n",
    "micro_averaged_recall = metrics.recall_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged recall score using sklearn library : {micro_averaged_recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d50779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of macro-averaged f1 score\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "\n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize f1 to 0\n",
    "    f1 = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(y_true.unique()):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false negative for current class\n",
    "        fn = false_negative(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false positive for current class\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute recall for current class\n",
    "        temp_recall = tp / (tp + fn + 1e-6)\n",
    "        \n",
    "        # compute precision for current class\n",
    "        temp_precision = tp / (tp + fp + 1e-6)\n",
    "        \n",
    "        \n",
    "        temp_f1 = 2 * temp_precision * temp_recall / (temp_precision + temp_recall + 1e-6)\n",
    "        \n",
    "        # keep adding f1 score for all classes\n",
    "        f1 += temp_f1\n",
    "        \n",
    "    # calculate and return average f1 score over all classes\n",
    "    f1 /= num_classes\n",
    "    \n",
    "    return f1\n",
    "\n",
    "\n",
    "print(f\"Macro-averaged f1 score : {macro_f1(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "# implement macro-averaged F1 score using sklearn\n",
    "\n",
    "macro_averaged_f1 = metrics.f1_score(y_test, y_pred, average = 'macro')\n",
    "print(f\"Macro-Averaged F1 score using sklearn library : {macro_averaged_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of micro-averaged fi score\n",
    "\n",
    "def micro_f1(y_true, y_pred):\n",
    "\n",
    "\n",
    "    #micro-averaged precision score\n",
    "    P = micro_precision(y_true, y_pred)\n",
    "\n",
    "    #micro-averaged recall score\n",
    "    R = micro_recall(y_true, y_pred)\n",
    "\n",
    "    #micro averaged f1 score\n",
    "    f1 = 2*P*R / (P + R)    \n",
    "\n",
    "    return f1\n",
    "\n",
    "print(f\"Micro-averaged recall score : {micro_f1(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "# implement micro-averaged F1 score using sklearn\n",
    "\n",
    "micro_averaged_f1 = metrics.f1_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged F1 score using sklearn library : {micro_averaged_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe51cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC AUCurve Computation\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def roc_auc_score_multiclass(actual_class, pred_class, average = \"macro\"):\n",
    "    \n",
    "    #creating a set of all the unique classes using the actual class list\n",
    "    unique_class = set(actual_class)\n",
    "    roc_auc_dict = {}\n",
    "    for per_class in unique_class:\n",
    "        \n",
    "        #creating a list of all the classes except the current class \n",
    "        other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "        #marking the current class as 1 and all other classes as 0\n",
    "        new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "        new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "\n",
    "        #using the sklearn metrics method to calculate the roc_auc_score\n",
    "        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n",
    "        roc_auc_dict[per_class] = roc_auc\n",
    "\n",
    "    return roc_auc_dict\n",
    "\n",
    "roc_auc_dict = roc_auc_score_multiclass(y_test, y_pred)\n",
    "roc_auc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC implementation: \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from itertools import cycle\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Load the iris data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target# Binarize the output\n",
    "y_bin = label_binarize(y, classes=[0, 1, 2])\n",
    "n_classes = y_bin.shape[1]# We split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size= 0.5, random_state=0)\n",
    "\n",
    "\n",
    "# We define the model as an SVC in OneVsRestClassifier setting.\n",
    "# this means that the model will be used for class 1 vs class 2, \n",
    "# class 2vs class 3 and class 1 vs class 3. \n",
    "# So, we have 3 cases at #the end and within each case, the bias will be varied in order to \n",
    "# Get the ROC curve of the given case - 3 ROC curves as output.\n",
    "\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=0))\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "# Plotting and estimation of FPR, TPR\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "colors = cycle(['blue', 'red', 'green'])\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=1.5, label='ROC curve of class {0} (area = {1:0.2f})' ''.format(i+1, roc_auc[i]))\n",
    "    plt.plot([0, 1], [0, 1], 'k-', lw=1.5)\n",
    "    plt.xlim([-0.05, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic for multi-class data')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69585b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
