{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb638b74",
   "metadata": {},
   "source": [
    "# Statisitcs and Research methods\n",
    "\n",
    "## Understanding Statistical Models vs. Machine Learning Models\n",
    "\n",
    "It's essential first to understand the distinctions between statistical models and machine learning models, as they serve different purposes, assumptions, and interpretative depth.\n",
    "\n",
    "- Statistical Models: \n",
    "    - These are rooted in traditional statistics and \n",
    "        - focus on relationships between variables through predefined equations. \n",
    "    - Statistical models aim to understand the underlying data-generating process, focusing on hypothesis testing and inference. \n",
    "    - These models often rely on strong assumptions like:\n",
    "        - linearity, \n",
    "        - normality, and \n",
    "        - homoscedasticity \n",
    "        - and are **interpretable**, making it easier to understand the impact of individual variables.\n",
    "\n",
    "- Machine Learning Models: \n",
    "    - These prioritize **predictive** power over interpretability. \n",
    "    - They are designed to automatically learn patterns and relationships within data, often with minimal assumptions. \n",
    "    - Machine learning models can handle complex and high-dimensional data but may lack transparency about how individual features affect the outcome, especially in “black box” models like neural networks or ensemble methods.\n",
    "\n",
    "\n",
    "## Choosing the Right Statistical Model\n",
    "\n",
    "The type of statistical model you use depends on your data and problem:\n",
    "\n",
    "- Linear Regression: For predicting a **continuous target variable** based on one or more predictors.\n",
    "- Logistic Regression: For predicting a **binary outcomes**, often used in classification problems.\n",
    "- ANOVA (Analysis of Variance): For comparing means across multiple groups.\n",
    "- Time Series Models: For data that’s ordered by time (e.g., ARIMA, SARIMA).\n",
    "- Survival Analysis: For time-to-event data, such as customer churn timing.\n",
    "- Multivariate Analysis: For understanding interactions across multiple variables (e.g., MANOVA, PCA).\n",
    "\n",
    "## Preprocessing the Data\n",
    "Prepare your data by cleaning and preprocessing it:\n",
    "\n",
    "- Missing Values: Decide whether to impute or drop missing values.\n",
    "- Outliers: Identify and consider handling outliers, especially in regression.\n",
    "- Data Transformation: Transform non-normal variables if required (e.g., using log transformations).\n",
    "- Feature Scaling: For some models, standardizing or normalizing data is essential.\n",
    "\n",
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "EDA is essential to understand: \n",
    "- patterns,\n",
    "    - visualizations\n",
    "- distributions,\n",
    "    - summary statistics\n",
    "- relationships\n",
    "    - correlation matrices\n",
    "    \n",
    "This is to identify relevant features and spot potential issues like multicollinearity.\n",
    "\n",
    "## Building the Statistical Model\n",
    "\n",
    "- **Statsmodels** provides \n",
    "    - coefficients, \n",
    "    - p-values, and \n",
    "    - confidence intervals for each variable, \n",
    "        - enabling hypothesis testing on whether each predictor significantly affects the outcome.\n",
    "\n",
    "## Evaluating Model Performance\n",
    "Regression Metrics: \n",
    "- Use R-squared, \n",
    "- Adjusted R-squared, \n",
    "- RMSE, and \n",
    "- MAE to evaluate regression models.\n",
    "\n",
    "Classification Metrics: \n",
    "- Use confusion matrix, \n",
    "- accuracy, \n",
    "- precision, \n",
    "- recall, and \n",
    "- AUC-ROC.\n",
    "\n",
    "Residual Analysis: \n",
    "- Residual plots help assess assumptions\n",
    "    - homoscedasticity, \n",
    "    - normality of residuals).\n",
    "\n",
    "## Model Interpretation\n",
    "Statistical models are highly interpretable. \n",
    "- In linear regression, each coefficient represents the expected change in the dependent variable for a one-unit change in the predictor, holding all else constant.\n",
    "\n",
    "Confidence Intervals: \n",
    "- Look at 95% CI for each coefficient; if it does not contain zero, it suggests the predictor has a statistically significant effect.\n",
    "\n",
    "P-Values: \n",
    "- A p-value below a threshold (usually 0.05) indicates that the predictor significantly affects the outcome.\n",
    "\n",
    "## Validating Assumptions\n",
    "- Linearity: Check scatter plots of residuals.\n",
    "- Normality of Residuals: Use a Q-Q plot to verify.\n",
    "- No Multicollinearity: Variance inflation factor (VIF) helps detect multicollinearity.\n",
    "- Homoscedasticity: Plot residuals vs. fitted values.\n",
    "\n",
    "## Reporting and Communicating Results\n",
    "Present your findings by focusing on:\n",
    "\n",
    "- Key Coefficients: Explain which predictors significantly affect the outcome.\n",
    "- Model Fit: Interpret R-squared values (e.g., explaining how much variance in the target variable is explained).\n",
    "- Real-World Implications: Describe how insights from the model can impact business decisions.\n",
    "\n",
    "# Approach to statistical modeling\n",
    "\n",
    "Each model type has specific \n",
    "- applications, \n",
    "- strengths, and \n",
    "- limitations, \n",
    "\n",
    "Understand when and how to use them.\n",
    "\n",
    "### Step 1: Define Objectives and Hypotheses\n",
    "\n",
    "Identify the Problem and Objectives: \n",
    "- Clearly define the goal.\n",
    "    - Are you trying to predict, classify, find patterns, or estimate relationships? \n",
    "    - Setting objectives helps in choosing the right model.\n",
    "\n",
    "- Formulate Hypotheses: \n",
    "    - Based on the problem, develop hypotheses. \n",
    "        - For instance, in a sales prediction problem, you may hypothesize that `certain features like advertising spend, time of year, and economic indicators affect sales.`\n",
    "\n",
    "### Step 2: Data Collection and Preprocessing\n",
    "Data Collection: \n",
    "- Gather historical data related to the problem. \n",
    "\n",
    "Data Cleaning: \n",
    "- Handle missing values, remove duplicates, and ensure consistency.\n",
    "\n",
    "Feature Engineering: \n",
    "- Create new features if necessary. \n",
    "- This could involve \n",
    "    - transformations, \n",
    "    - encoding categorical variables, or \n",
    "    - creating interaction terms.\n",
    "\n",
    "Data Splitting: \n",
    "- Split the data into training and testing sets. Typically, an 80-20 or 70-30 split is used.\n",
    "\n",
    "### Step 3: Select the Type of Statistical Model\n",
    "Statistical models can be broadly categorized as:\n",
    "\n",
    "- **Descriptive Models**: Summarize data patterns.\n",
    "- **Inferential Models**: Help make inferences about the population.\n",
    "- **Predictive Models**: Used to predict future outcomes based on historical data.\n",
    "- **Prescriptive Models**: Suggest actions based on predictions.\n",
    "\n",
    "Let's go through common types of statistical models and their applications.\n",
    "\n",
    "\n",
    "# Regression Analysis\n",
    " \n",
    "Regression Analysis is a statistical method to analyze the relationship between a dependent variable and one or more independent variables.\n",
    "\n",
    "### Three types of regression analysis\n",
    "\n",
    "##### Real-world examples\n",
    "- Simple linear regression\n",
    "    - A real estate agent wants to determine the relationship between the size of a house (in square feet) and its selling price. They can use simple linear regression to predict the selling price of a house based on its size.\n",
    "    \n",
    "-  Multiple Linear Regression / Multivariate Linear Regression\n",
    "    - A car manufacturer wants to predict the fuel efficiency of their vehicles based on various independent variables such as engine size, horsepower, and weight.\n",
    "    \n",
    "- Logistic regression\n",
    "    - A bank wants to predict whether a customer will default on their loan based on their credit score, income, and other factors. By using logistic regression, the bank can estimate the probability of default and take appropriate measures to minimize their risk.\n",
    "\n",
    "## 1. Linear Regression\n",
    "\n",
    "Linear Regression is a supervised learning algorithm used to model the relationship between a dependent variable (outcome) and one or more independent variables (predictors)\n",
    "\n",
    "Linear Regression predicts a continuous target variable (e.g., the number of readmissions) by minimizing the residual sum of squares between observed and predicted values.\n",
    "\n",
    "What It Means: \n",
    "- Linear regression models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. It assumes a straight-line relationship. \n",
    "\n",
    "- It is employed to establish a link between a dependant variable and a single independent variable. \n",
    "    - A linear equation defines the relationship, with the \n",
    "        - slope and \n",
    "        - intercept \n",
    "    - of the line representing the effect of the independent variable on the dependant variable.\n",
    "        - An independent variable is the variable that is controlled in a scientific experiment to test the effects on the dependent variable.\n",
    "        - A dependent variable is the variable being measured in a scientific experiment.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each coefficient represents how much the dependent variable (outcome) changes when the predictor variable changes by one unit, keeping all else constant.\n",
    "\n",
    "**Assumptions of Linear Regression**\n",
    "- Linearity (Linear Relationship): The relationship between the predictors and the outcome is linear.\n",
    "- Independence of Errors: Residuals (errors) are independent of each other.\n",
    "- Normality of Errors: Residuals are normally distributed.\n",
    "- Multivariate Normality\n",
    "- No or Little Multicollinearity\n",
    "- No or Little Autocorrelation\n",
    "- Homoscedasticity: Variance of residuals is constant across all levels of predictors.\n",
    "\n",
    "Performance Measures:\n",
    "- R-squared: Indicates the proportion of the variance in the dependent variable explained by the independent variables. \n",
    "    - Values closer to 1 indicate a better fit.\n",
    "- Mean Squared Error (MSE): The average squared difference between observed and predicted values; lower values are better.\n",
    "\n",
    "Lay Explanation: \n",
    "- Think of linear regression like drawing a best-fit line through a scatterplot of data points, aiming to predict outcomes based on relationships in the data.\n",
    "- Finds a relationship between independent and dependent variables by finding a “best-fitted line” that has minimal distance from all the data points.\n",
    "\n",
    "Use Case: \n",
    "- When there is a linear relationship between the target and predictor variables.\n",
    "\n",
    "### Mathematics or Linear Regression\n",
    "\n",
    "- it is using the least square method finds a linear equation that minimizes the sum of squared residuals (SSR).\n",
    "- Cost Function:\n",
    "\n",
    "$ J(\\theta) = \\frac{1}{2m}\\sum^{m}_{i=1}(h_{\\theta}(x^{(i)})- y^{(i)})^{2}$\n",
    "\n",
    "Model Equation:\n",
    "$ 𝑦=𝛽_{0}+𝛽_{1}𝑥_{1}+…+𝛽_{𝑛}𝑥_{𝑛}+ 𝜖 $\n",
    "\n",
    "where:\n",
    "- $y$ = dependent variable\n",
    "- $𝛽_{0}$ = Y intercept\n",
    "- $𝛽_{1}$ = Slope coefficient\n",
    "- $𝑥_{1}$ = independent variable\n",
    "- $𝜖 $ = error term\n",
    "\n",
    "**What is Cost Function ?**\n",
    "\n",
    "A cost function, also referred to as a: \n",
    "- loss function : Used when we refer to the error for a single training example. \n",
    "- objective function : Used to refer to an average of the loss functions over an entire training dataset.\n",
    "It quantifies the difference between predicted and actual values, serving as a metric to evaluate the performance of a model.\n",
    "\n",
    "Objective \n",
    "- is to minimize the cost function, indicating better alignment between predicted and observed outcomes.\n",
    "- Guides the model towards optimal predictions by measuring its accuracy against the training data.\n",
    "\n",
    "**Why to use a Cost function**\n",
    "\n",
    "Cost function helps us reach the optimal solution. \n",
    "- How: It takes both predicted outputs by the model and actual outputs and calculates how much wrong the model was in its prediction.\n",
    "    - It basically measures the discrepancy between the model’s predictions and the true values it is attempting to predict. \n",
    "    - This variance is depicted as a lone numerical figure, enabling us to measure the model’s **precision**.\n",
    "- The cost function is the technique of evaluating “the performance of our algorithm/model”.\n",
    "\n",
    "Classifiers have very high accuracy but one solution (Classifier) is the best because it does not misclassify any point.\n",
    "- Reason why it classifies all the points perfectly is that the:\n",
    "    - line is almost exactly in between the two (n) groups, and not closer to any one of the groups.\n",
    "\n",
    "Explanation of the function of a cost function:\n",
    "\n",
    "- Error calculation: It determines the difference between the predicted outputs (what the model predicts as the answer) and the actual outputs (the true values we possess for the data).\n",
    "- Gives one value: This simplifies comparing the model’s performance on various datasets or training rounds.\n",
    "- Improving Guides: The objective is to reduce the cost function. \n",
    "    - How: Through modifying the internal parameters of the model such as weights and biases, we can aim to minimize the total error and enhance the accuracy / precision of the model.\n",
    "\n",
    "**Types of Cost function in machine learning**\n",
    "\n",
    "Its use cases depend on whether it is a regression problem or classification problem.\n",
    "- Regression cost Function\n",
    "- Binary Classification cost Functions\n",
    "- Multi-class Classification cost Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8576db",
   "metadata": {},
   "source": [
    "### Problem Context: Predicting Hospital Readmission Rates\n",
    "The aim to reduce hospital readmission rates. \n",
    "- High readmission rates can strain resources and negatively impact patient outcomes.\n",
    "- The goal is to predict the number of readmissions within 30 days of discharge for a particular condition, such as \n",
    "    - diabetes, based on \n",
    "        - patient demographic, \n",
    "        - clinical data, and \n",
    "        - treatment data.\n",
    "\n",
    "**Step 1. Define the Problem**\n",
    "\n",
    "We want to predict the number of readmissions ($𝑌$) using features ($𝑋$) such as:\n",
    "- Patient age\n",
    "- Length of hospital stay\n",
    "- Severity of condition\n",
    "- Medication adherence rate\n",
    "- Comorbidities (e.g., hypertension, kidney disease)\n",
    "- Number of follow-up visits scheduled\n",
    "\n",
    "**Step 2. Collect and Prepare Data**\n",
    "\n",
    "- Data Collection: Gather historical patient data from the hospital's database.\n",
    "- Understand the \n",
    "    - model description\n",
    "    - causality and \n",
    "    - directionality\n",
    "- Check the data\n",
    "    - categorical data, \n",
    "    - missing data and \n",
    "    - outliers\n",
    "- Data Cleaning: \n",
    "    - Dummy variable takes only the value 0 or 1 to indicate the effect for categorical variables.\n",
    "    - Handle missing values, \n",
    "    - remove duplicates, and \n",
    "    - correct errors.\n",
    "    - Outlier is a data point that differs significantly from other observations. \n",
    "        - use standard deviation method and \n",
    "        - interquartile range (IQR) method.\n",
    "- Feature Engineering: \n",
    "    - Encode categorical variables (e.g., age group), \n",
    "    - scale continuous variables (e.g., length of stay), and \n",
    "    - create interaction terms if necessary.\n",
    "\n",
    "**Step 3. Conduct a Simple Analysis**\n",
    "- Check the **effect** comparing between \n",
    "    - Dependent variable to independent variable and \n",
    "    - Independent variable to independent variable\n",
    "- Check the correlation.\n",
    "    - Use scatter plots\n",
    "- Check Multicollinearity \n",
    "    - This occurs when more than two independent variables are highly correlated. \n",
    "    - Use Variance Inflation Factor (VIF) \n",
    "        - if VIF > 5 there is highly correlated and \n",
    "        - if VIF > 10 there is certainly multicollinearity among the variables.\n",
    "- Interaction Term imply a change in the slope from one value to another value.\n",
    "\n",
    "**Step 4. Formulate the Model (From Scratch)**\n",
    "- y in this equation stands for the predicted value, \n",
    "- x means the independent variable and \n",
    "- m & b are the **coefficients** we need to optimize in order to fit the regression line to our data.\n",
    "\n",
    "Calculating coefficient of the equation:\n",
    "- To calculate the coefficients we need the formula for \n",
    "\n",
    "Covariance \n",
    "\n",
    "$Cov (X,Y) = \\frac{\\sum (X_{i}- X)(Y_{j} - Y)}{n}$\n",
    "\n",
    "Variance\n",
    "\n",
    "$var(x) = \\frac{\\sum^{n}_{i} (x_i -\\mu)^2}{N}$\n",
    "\n",
    "- To calculate the coefficient m\n",
    "    - m = cov(x, y) / var(x)\n",
    "    - b = mean(y) — m * mean(x)\n",
    "\n",
    "**Functions to calculate the Mean, Covariance, and Variance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c74157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean \n",
    "def get_mean(arr):\n",
    "    return np.sum(arr)/len(arr)\n",
    "\n",
    "# variance\n",
    "def get_variance(arr, mean):\n",
    "    return np.sum((arr-mean)**2)\n",
    "\n",
    "# covariance\n",
    "def get_covariance(arr_x, mean_x, arr_y, mean_y):\n",
    "    final_arr = (arr_x - mean_x)*(arr_y - mean_y)\n",
    "    return np.sum(final_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8932f",
   "metadata": {},
   "source": [
    "**Fuction to calculate the coefficients and the Linear Regression Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients \n",
    "# m = cov(x, y) / var(x)\n",
    "# b = y - m*x\n",
    "\n",
    "def get_coefficients(x, y):\n",
    "    x_mean = get_mean(x)\n",
    "    y_mean = get_mean(y)\n",
    "    m = get_covariance(x, x_mean, y, y_mean)/get_variance(x, x_mean)\n",
    "    b = y_mean - x_mean*m\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e179ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression \n",
    "# Train and Test\n",
    "# Train Split 80 % Test Split 20 %\n",
    "def linear_regression(x_train, y_train, x_test, y_test):\n",
    "    prediction = []\n",
    "    m, b = get_coefficients(x_train, y_train)\n",
    "    for x in x_test:\n",
    "        y = m*x + b\n",
    "        prediction.append(y)\n",
    "    \n",
    "    r2 = r2_score(prediction, y_test)\n",
    "    mse = mean_squared_error(prediction, y_test)\n",
    "    print(\"The R2 score of the model is: \", r2)\n",
    "    print(\"The MSE score of the model is: \", mse)\n",
    "    return prediction\n",
    "\n",
    "prediction = linear_regression(x[:80], y[:80], x[80:], y[80:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ec0333",
   "metadata": {},
   "source": [
    "**Visualize the regression line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb4088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reg_line(x, y):\n",
    "    # Calculate predictions for x ranging from 1 to 100\n",
    "    prediction = []\n",
    "    m, c = get_coefficients(x, y)\n",
    "    for x0 in range(1,100):\n",
    "        yhat = m*x0 + c\n",
    "        prediction.append(yhat)\n",
    "    \n",
    "    # Scatter plot without regression line\n",
    "    fig = plt.figure(figsize=(20,7))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.scatterplot(x=x, y=y)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Scatter Plot between X and Y')\n",
    "    \n",
    "    # Scatter plot with regression line\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.scatterplot(x=x, y=y, color = 'blue')\n",
    "    sns.lineplot(x = [i for i in range(1, 100)], y = prediction, color='red')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Regression Plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5bc7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression plot form seaborn\n",
    "# regplot is basically the combination of the scatter plot and the line plot\n",
    "sns.regplot(x, y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title(\"Regression Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aeea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reg_line(x, y):\n",
    "    # Calculate predictions for x ranging from 1 to 100\n",
    "    prediction = []\n",
    "    m, c = get_coefficients(x, y)\n",
    "    for x0 in range(1,100):\n",
    "        yhat = m*x0 + c\n",
    "        prediction.append(yhat)\n",
    "    \n",
    "    # Scatter plot without regression line\n",
    "    fig = plt.figure(figsize=(20,7))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.scatterplot(x=x, y=y)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Scatter Plot between X and Y')\n",
    "    \n",
    "    # Scatter plot with regression line\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.scatterplot(x=x, y=y, color = 'blue')\n",
    "    sns.lineplot(x = [i for i in range(1, 100)], y = prediction, color='red')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Regression Plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8247ed70",
   "metadata": {},
   "source": [
    "**Step 4. Formulate the model and Fit the Model (using library)**\n",
    "\n",
    "- Split the Data: Divide data into training and testing sets (e.g., 80% training, 20% testing).\n",
    "- Train the Model: Use a library like sklearn in Python to fit the regression model on the training data.\n",
    "- Evaluate the Model: Check metrics such as $𝑅^2$ (explained variance) and RMSE (Root Mean Squared Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Example\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create the dataset\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
    "y = np.array([2, 4, 5, 7, 8, 10, 11, 13, 14, 16])\n",
    "\n",
    "# Create the linear regression model\n",
    "model = LinearRegression().fit(X, y)\n",
    "\n",
    "# Get the slope and intercept of the line\n",
    "slope = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "# Plot the data points and the regression line\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, slope*X + intercept, color='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc4414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Example dataset\n",
    "X = data[['age', 'length_of_stay', 'severity', 'medication_adherence', 'comorbidities']]\n",
    "y = data['readmissions']\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse}, R^2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f3e50",
   "metadata": {},
   "source": [
    "**Regression cost functions: Regression model evaluation metrics**\n",
    "\n",
    "**loss function** is for a single training example. It is also sometimes called an error function. \n",
    "\n",
    "**cost function**, on the other hand, is the average loss over the entire training dataset. \n",
    "\n",
    "**Steps for Loss Functions**\n",
    "1. Define the predictor function f(X), and identify the parameters to find.\n",
    "2. Determine the loss for each training example.\n",
    "3. Derive the expression for the Cost Function, representing the average loss across all examples.\n",
    "4. Compute the gradient of the Cost Function concerning each unknown parameter.\n",
    "5. Select the learning rate and execute the weight update rule for a fixed number of iterations.\n",
    "\n",
    "These steps guide the optimization process, aiding in the determination of optimal model parameters.\n",
    "\n",
    "Regression model we generally use to evaluate the prediction error rates and model performance in regression analysis.\n",
    "\n",
    "- **R-squared (Coefficient of determination)** represents the coefficient of how well the values fit compared to the original values. The value from 0 to 1 interpreted as percentages. The higher the value is, the better the model is.\n",
    "- **MSE (Mean Squared Error)** represents the difference between the original and predicted values extracted by squared the average difference over the data set.\n",
    "- **RMSE (Root Mean Squared Error)** is the error rate by the square root of MSE.\n",
    "- **MAE (Mean absolute error)** represents the difference between the original and predicted values extracted by averaged the absolute difference over the data set.\n",
    "\n",
    "1. Mean Error (ME)\n",
    "- The error for each training data is calculated and then the mean value of all these errors is derived.\n",
    "- Errors can be both negative and positive. So they can cancel each other out during summation giving zero mean error for the model.\n",
    "- Not a recommended cost function but it does lay the foundation for other cost functions of regression models.\n",
    "\n",
    "2. Mean Squared Error (MSE)\n",
    "- known as L2 loss.\n",
    "- Here a square of the difference between the actual and predicted value is calculated to avoid any possibility of negative error(drawback cause).\n",
    "- It is measured as the average of the sum of squared differences between predictions and actual observations.\n",
    "- Since each error is squared, it helps to penalize even small deviations in prediction when compared to MAE. \n",
    "    - But if our dataset has outliers that contribute to larger prediction errors, then squaring this error further will magnify the error many times more and also lead to higher MSE error.\n",
    "    - MSE loss function penalizes the model for making large errors by squaring them. Squaring a large quantity makes it even larger\n",
    "        - it is less robust to outliers\n",
    "        - not to be used if our data is prone to many outliers.\n",
    "\n",
    "Graphically\n",
    "- It is a positive quadratic function (of the form $ax^2 + bx + c$ where $a > 0$)\n",
    "- A quadratic function only has a global minimum. \n",
    "    - Since there are no local minima, we will never get stuck in one. \n",
    "- Hence, it is always guaranteed that Gradient Descent will converge (if it converges at all) to the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ff6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_MSE(m, b, X, Y, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    for i in range(N):\n",
    "        # Calculate partial derivatives\n",
    "        # -2x(y - (mx + b))\n",
    "        m_deriv += -2*X[i] * (Y[i] - (m*X[i] + b))\n",
    "\n",
    "        # -2(y - (mx + b))\n",
    "        b_deriv += -2*(Y[i] - (m*X[i] + b))\n",
    "\n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfde74f",
   "metadata": {},
   "source": [
    "3. Mean Absolute Error (MAE)\n",
    "- known as L1 Loss.\n",
    "- Absolute Error for each training example is the distance between the predicted and the actual values, irrespective of the sign.\n",
    "    - it is the absolute difference between the actual and predicted values.\n",
    "- Here an absolute difference between the actual and predicted value is calculated to avoid any possibility of negative error.\n",
    "- It is measured as the average of the sum of absolute differences between predictions and actual observations.\n",
    "    - It is robust to outliers thus it will give better results even when our dataset has noise or outliers.\n",
    "    - MAE cost is more robust to outliers as compared to MSE\n",
    "-  The cost is the Mean of these Absolute Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34787f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_MAE(m, b, X, Y, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    for i in range(N):\n",
    "        # Calculate partial derivatives\n",
    "        # -x(y - (mx + b)) / |mx + b|\n",
    "        m_deriv += - X[i] * (Y[i] - (m*X[i] + b)) / abs(Y[i] - (m*X[i] + b))\n",
    "\n",
    "        # -(y - (mx + b)) / |mx + b|\n",
    "        b_deriv += -(Y[i] - (m*X[i] + b)) / abs(Y[i] - (m*X[i] + b))\n",
    "\n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ab3d2",
   "metadata": {},
   "source": [
    "4. Huber Loss\n",
    "\n",
    "- The Huber loss combines the best properties of MSE and MAE.\n",
    "- It is quadratic for smaller errors and is linear otherwise (and similarly for its gradient). \n",
    "- It is identified by its delta parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b94332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_Huber(m, b, X, Y, delta, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    for i in range(N):\n",
    "        # derivative of quadratic for small values and of linear for large values\n",
    "        if abs(Y[i] - m*X[i] - b) <= delta:\n",
    "          m_deriv += -X[i] * (Y[i] - (m*X[i] + b))\n",
    "          b_deriv += - (Y[i] - (m*X[i] + b))\n",
    "        else:\n",
    "          m_deriv += delta * X[i] * ((m*X[i] + b) - Y[i]) / abs((m*X[i] + b) - Y[i])\n",
    "          b_deriv += delta * ((m*X[i] + b) - Y[i]) / abs((m*X[i] + b) - Y[i])\n",
    "    \n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ddd25a",
   "metadata": {},
   "source": [
    "**Step 5: Interpret the Results**\n",
    "\n",
    "Residual Analysis:\n",
    "- Check normal distribution and normality for the residuals.\n",
    "- Homoscedasticity describes a situation in which error term is the same across all values of the independent variables. \n",
    "    - means that the residuals are equal across the regression line.\n",
    "\n",
    "Interpretation of Regression Output\n",
    "- R-Squared : is a statistical measure of fit that indicates how much variation of a dependent variable is explained by the independent variables. \n",
    "    - Higher R-Squared value represents smaller differences between the observed data and fitted values.\n",
    "- P-value\n",
    "\n",
    "Interpret the Regression Equation\n",
    "- The coefficients ($𝛽$) indicate the magnitude and direction of the relationship between each predictor and readmissions.\n",
    "    - Example: A coefficient of -0.5 for medication_adherence means that for every 1% increase in medication adherence, readmissions decrease by 0.5.\n",
    "- The intercept ($𝛽_0$) represents the expected number of readmissions when all predictors are zero.\n",
    "\n",
    "**Optimization technique/Strategy**\n",
    "\n",
    "We will use Gradient Descent as an optimization strategy to find the regression line.\n",
    "- Weight Update Rule\n",
    "\n",
    "NB: Perform optimization on the training data and check its performance on a new validation data.\n",
    "\n",
    "What is gradient descent?\n",
    "- lay man: \n",
    "    - It is a way of checking the ground near you and observe where the land tends to descend.\n",
    "    - It gives an idea in what direction you should take your steps.\n",
    "    - It helps models find the optimal set of parameters by iteratively adjusting them in the opposite direction of the gradient, aiming to find the optimal set of parameters.\n",
    "\n",
    "Mathematical terms:\n",
    "- find out the best parameters ($θ_1$) and ($θ_2$) for our learning algorithm.\n",
    "\n",
    "Cost space is how our algorithm would perform when we choose a particular value for a parameter.\n",
    "\n",
    "Cost Function is a function that measures the performance of a model for any given data. Cost Function quantifies the error between predicted values and expected values and presents it in the form of a single real number.\n",
    "\n",
    "1. Make a hypothesis with initial parameters\n",
    "- Hypothesis: $h_θ(x) = θ_0 + θ_1 x$\n",
    "- Parameters: $θ_o, θ_1$\n",
    "2. Calculate the Cost function\n",
    "- Cost Function: $J(θ_o, θ_1) = \\frac{1}{2m}\\sum^{m}_{i = 1} (h_θ (x^{(i)}) - y^{i})^2$\n",
    "3. The goal is to reduce the cost function, we modify the parameters by using the Gradient descent algorithm over the given data.\n",
    "- Goal: $minimize_{θ_o, θ_1} J(θ_o, θ_1)$\n",
    "\n",
    "Gradient descent aims to find the parameters that minimize this discrepancy and improve the model’s performance.\n",
    "\n",
    "The algorithm operates by calculating the gradient of the cost function, \n",
    "    - which indicates the direction and magnitude of the steepest ascent. \n",
    "\n",
    "However, since the goal is to minimize the cost function, gradient descent moves in the opposite direction of the gradient, \n",
    "    - known as the negative gradient direction.\n",
    "\n",
    "Iteratively updating the model’s parameters in the negative gradient direction, gradient descent gradually converges towards the optimal set of parameters that yields the lowest cost.\n",
    "\n",
    "- Hyperparameter: learning rate, determines the step size taken in each iteration, influencing the speed and stability of convergence.\n",
    "\n",
    "Gradient descent can be applied to:\n",
    "    - linear regression, \n",
    "    - logistic regression, \n",
    "    - neural networks, and \n",
    "    - support vector machines.\n",
    "\n",
    "**Definition**: Gradient descent is an iterative optimization algorithm for finding the local minimum of a function.\n",
    "\n",
    "To find the local minimum of a function using gradient descent, we must take steps proportional to the negative of the gradient (move away from the gradient) of the function at the current point.\n",
    "- If we take steps proportional to the positive of the gradient (moving towards the gradient), we will approach a local maximum of the function, and the procedure is called Gradient Ascent.\n",
    "\n",
    "The goal of the gradient descent algorithm is to minimize the given function (say, cost function)\n",
    "- it performs two steps iteratively:\n",
    "1. Compute the gradient (slope), the first-order derivative of the function at that point\n",
    "2. Make a step (move) in the direction opposite to the gradient. The opposite direction of the slope increases from the current point by alpha times the gradient at that point.\n",
    "\n",
    "This code creates a function called gradient_descent, which requires the training data, learning rate, and number of iterations as parameters.\n",
    "\n",
    "Steps :\n",
    "1. Sets weights and bias to arbitrary values during initialization.\n",
    "2. Executes a set number of iterations for loops.\n",
    "3. Computes the estimated y values by utilizing the existing weights and bias.\n",
    "4. Calculates the discrepancy between expected and real y values.\n",
    "5. Determines the changes in the cost function based on weights and bias.\n",
    "6. Adjusts the weights and bias by incorporating the gradients and learning rate.\n",
    "7. Outputs the acquired weights and bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad28b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(X, y, learning_rate, num_iters):\n",
    "  \"\"\"\n",
    "  Performs gradient descent to find optimal weights and bias for linear regression.\n",
    "\n",
    "  Args:\n",
    "      X: A numpy array of shape (m, n) representing the training data features.\n",
    "      y: A numpy array of shape (m,) representing the training data target values.\n",
    "      learning_rate: The learning rate to control the step size during updates.\n",
    "      num_iters: The number of iterations to perform gradient descent.\n",
    "\n",
    "  Returns:\n",
    "      A tuple containing the learned weights and bias.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize weights and bias with random values\n",
    "  m, n = X.shape\n",
    "  weights = np.random.rand(n)\n",
    "  bias = 0\n",
    "\n",
    "  # Loop for the number of iterations\n",
    "  for i in range(num_iters):\n",
    "    # Predict y values using current weights and bias\n",
    "    y_predicted = np.dot(X, weights) + bias\n",
    "\n",
    "    # Calculate the error\n",
    "    error = y - y_predicted\n",
    "\n",
    "    # Calculate gradients for weights and bias\n",
    "    weights_gradient = -2/m * np.dot(X.T, error)\n",
    "    bias_gradient = -2/m * np.sum(error)\n",
    "\n",
    "    # Update weights and bias using learning rate\n",
    "    weights -= learning_rate * weights_gradient\n",
    "    bias -= learning_rate * bias_gradient\n",
    "\n",
    "  return weights, bias\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[1, 1], [2, 2], [3, 3]])\n",
    "y = np.array([2, 4, 5])\n",
    "learning_rate = 0.01\n",
    "num_iters = 100\n",
    "\n",
    "weights, bias = gradient_descent(X, y, learning_rate, num_iters)\n",
    "\n",
    "print(\"Learned weights:\", weights)\n",
    "print(\"Learned bias:\", bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af34d913",
   "metadata": {},
   "source": [
    "How Does Gradient Descent Work?\n",
    "1. The algorithm optimizes to minimize the model’s cost function.\n",
    "2. The cost function measures how well the model fits the training data and defines the difference between the predicted and actual values.\n",
    "3. The cost function’s gradient is the derivative with respect to the model’s parameters and points in the direction of the steepest ascent.\n",
    "4. The algorithm starts with an initial set of parameters and updates them in small steps to minimize the cost function.\n",
    "5. In each iteration of the algorithm, it computes the gradient of the cost function with respect to each parameter.\n",
    "6. The gradient tells us the direction of the steepest ascent, and by moving in the opposite direction, we can find the direction of the steepest descent.\n",
    "7. The learning rate controls the step size, which determines how quickly the algorithm moves towards the minimum.\n",
    "8. The process is repeated until the cost function converges to a minimum. Therefore indicating that the model has reached the optimal set of parameters.\n",
    "9. Different variations of gradient descent include batch gradient descent, stochastic gradient descent, and mini-batch gradient descent, each with advantages and limitations.\n",
    "10. Efficient implementation of gradient descent is essential for performing well in machine learning tasks. The choice of the learning rate and the number of iterations can significantly impact the algorithm’s performance.\n",
    "\n",
    "On the basis of differentiation techniques \n",
    "- Gradient descent requires Calculation of gradient by differentiation of cost function. We can either use first order differentiation or second order differentiation.\n",
    "    - First order Differentiation\n",
    "    - Second order Differentiation.\n",
    "\n",
    "**Types of Gradient Descent**\n",
    "\n",
    "Classified by two methods mainly:\n",
    "- On the basis of data ingestion: choice of gradient descent algorithm depends on the problem at hand and the size of the dataset.\n",
    "    - Full Batch Gradient Descent Algorithm:\n",
    "        - Batch gradient descent,\n",
    "            - also known as vanilla gradient descent, \n",
    "        - full batch gradient descent algorithms, you use whole data at once to compute the gradient.\n",
    "            - It updates the model’s parameters using the gradient of the entire training set.\n",
    "        - It calculates the average gradient of the cost function for all the training examples and updates the parameters in the opposite direction.\n",
    "            - calculates the error for each example within the training dataset.\n",
    "            - The model is not changed until every training sample has been assessed. \n",
    "                - The entire procedure is referred to as a **cycle and a training epoch**.\n",
    "        - Batch gradient descent guarantees convergence to the global minimum but can be computationally expensive and slow for large datasets.\n",
    "            - Batch gradient descent is suitable for small datasets.\n",
    "            - Its computational efficiency, which produces a stable error gradient and a stable convergence.\n",
    "        - Drawbacks are that the stable error gradient can sometimes result in a state of convergence that isn’t the best the model can achieve. \n",
    "            - It also requires the entire training dataset to be in memory and available to the algorithm.\n",
    "\n",
    "Advantages\n",
    "- Fewer model updates mean that this variant of the steepest descent method is more computationally efficient than the stochastic gradient descent method.\n",
    "- Reducing the update frequency provides a more stable error gradient and a more stable convergence for some problems.\n",
    "- Separating forecast error calculations and model updates provides a parallel processing-based algorithm implementation.\n",
    "\n",
    "Disadvantages\n",
    "- A more stable error gradient can cause the model to prematurely converge to a suboptimal set of parameters.\n",
    "- End-of-training epoch updates require the additional complexity of accumulating prediction errors across all training examples.\n",
    "- The batch gradient descent method typically requires the entire training dataset in memory and is implemented for use in the algorithm.\n",
    "- Large datasets can result in very slow model updates or training speeds.\n",
    "- Slow and require more computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ec0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDRegressor:\n",
    "    \n",
    "    def __init__(self,learning_rate=0.01,epochs=100):\n",
    "        \n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def fit(self,X_train,y_train):\n",
    "        # init your coefs\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            # update all the coef and the intercept\n",
    "            y_hat = np.dot(X_train,self.coef_) + self.intercept_\n",
    "            #print(\"Shape of y_hat\",y_hat.shape)\n",
    "            intercept_der = -2 * np.mean(y_train - y_hat)\n",
    "            self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "            \n",
    "            coef_der = -2 * np.dot((y_train - y_hat),X_train)/X_train.shape[0]\n",
    "            self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "        \n",
    "        print(self.intercept_,self.coef_)\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c381ad",
   "metadata": {},
   "source": [
    "- Stochastic Gradient Descent Algorithm\n",
    "    - stochastic you take a sample while computing the gradient.\n",
    "        - It randomly selects a training dataset example, \n",
    "            - changes the parameters for each training sample one at a time for each training example in the dataset.\n",
    "                - The regular updates give us a fairly accurate idea of the rate of improvement. (benefit)\n",
    "        - computes the gradient of the cost function for that example, \n",
    "        - and updates the parameters in the opposite direction.\n",
    "    - stochastic gradient descent algorithm is more suitable for large datasets.\n",
    "    - It is computationally efficient and can converge faster than batch gradient descent. It can be noisy (produce noisy gradients), cause the error rate to fluctuate rather than gradually go down and may not converge to the global minimum.\n",
    "\n",
    "Advantages\n",
    "- You can instantly see your model’s performance and improvement rates with frequent updates.\n",
    "- This variant of the steepest descent method is probably the easiest to understand and implement, especially for beginners.\n",
    "- Increasing the frequency of model updates will allow you to learn more about some issues faster.\n",
    "- The noisy update process allows the model to avoid local minima (e.g., premature convergence).\n",
    "- Faster and require less computational power.\n",
    "- Suitable for the larger dataset.\n",
    "\n",
    "Disadvantages\n",
    "- Frequent model updates are more computationally intensive than other steepest descent configurations, and it takes considerable time to train the model with large datasets.\n",
    "- Frequent updates can result in noisy gradient signals. This can result in model parameters and cause errors to fly around (more variance across the training epoch).\n",
    "- A noisy learning process along the error gradient can also make it difficult for the algorithm to commit to the model’s minimum error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c4540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "clf.fit(X, y)\n",
    "SGDClassifier(max_iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b03ce9",
   "metadata": {},
   "source": [
    "Mini-batch Gradient Descent\n",
    "- Mini-batch is a good compromise between the two and is often used in practice.\n",
    "- updates the model’s parameters using the gradient of a small batch size of the training dataset, known as a mini-batch. \n",
    "- It calculates the average gradient of the cost function for the mini-batch and updates the parameters in the opposite direction.\n",
    "- It is the most commonly used method in practice because combines the ideas of batch gradient descent with SGD.\n",
    "        - strikes a balance between batch gradient descent’s effectiveness and stochastic gradient descent’s durability.\n",
    "- It is computationally efficient and less noisy than stochastic gradient descent while still being able to converge to a good solution.\n",
    "- Mini-batch sizes typically range from 50 to 256.\n",
    "\n",
    "Advantages\n",
    "- The model is updated more frequently than the stack gradient descent method, allowing for more robust convergence and avoiding local minima.\n",
    "- Batch updates provide a more computationally efficient process than stochastic gradient descent.\n",
    "- Batch processing allows for both the efficiency of not having all the training data in memory and implementing the algorithm.\n",
    "\n",
    "Disadvantages\n",
    "- Mini-batch requires additional hyperparameters “mini-batch size” to be set for the learning algorithm.\n",
    "- Error information should be accumulated over a mini-batch of training samples, such as batch gradient descent.\n",
    "- it will generate complex functions.\n",
    "\n",
    "Configure Mini-Batch Gradient Descent:\n",
    "\n",
    "- The mini-batch steepest descent method is a variant of the steepest descent method recommended for most applications, intense learning.\n",
    "- Mini-batch sizes, commonly called “batch sizes” for brevity, are often tailored to some aspect of the computing architecture in which the implementation is running. \n",
    "        - For example, a power of 2 that matches the memory requirements of the GPU or CPU hardware, such as 32, 64, 128, and 256.\n",
    "- The stack size is a slider for the learning process.\n",
    "- Smaller values ​​allow the learning process to converge quickly at the expense of noise in the training process. Larger values ​​result in a learning - process that slowly converges to an accurate estimate of the error gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8eaf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBGDRegressor:\n",
    "    \n",
    "    def __init__(self,batch_size,learning_rate=0.01,epochs=100):\n",
    "        \n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def fit(self,X_train,y_train):\n",
    "        # init your coefs\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            for j in range(int(X_train.shape[0]/self.batch_size)):\n",
    "                \n",
    "                idx = random.sample(range(X_train.shape[0]),self.batch_size)\n",
    "                \n",
    "                y_hat = np.dot(X_train[idx],self.coef_) + self.intercept_\n",
    "                #print(\"Shape of y_hat\",y_hat.shape)\n",
    "                intercept_der = -2 * np.mean(y_train[idx] - y_hat)\n",
    "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "\n",
    "                coef_der = -2 * np.dot((y_train[idx] - y_hat),X_train[idx])\n",
    "                self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "        \n",
    "        print(self.intercept_,self.coef_)\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c2ddb9",
   "metadata": {},
   "source": [
    "**Step 6: Use the Model for Decision-Making**\n",
    "\n",
    "Understanding which factors significantly influence readmissions,\n",
    "\n",
    "To do this, you need a systematic approach grounded in exploratory analysis, statistical rigor, and effective communication\n",
    "\n",
    "1. Thinking Approach: Identifying Significant Factors\n",
    "- Define the Business Objective\n",
    "    - Objective: Identify key drivers of hospital readmissions (to improve patient care and optimize resource allocation)\n",
    "    - Questions to Answer:\n",
    "        - What are the strongest predictors of readmissions?\n",
    "        - Which predictors can be influenced through policy or operational changes?\n",
    "        - How much can readmissions be reduced if certain factors are addressed?\n",
    "\n",
    "- Perform Exploratory Data Analysis (EDA)\n",
    "    - Inspect Data Distributions: Use histograms and boxplots to understand the spread of variables.\n",
    "    - Check Relationships:\n",
    "        - Pairwise correlations for numerical variables (e.g., length_of_stay vs. readmissions).\n",
    "        - Grouped summaries for categorical variables (e.g., readmissions across age groups).\n",
    "        - Example Insights:\n",
    "            - Patients with longer stays might have higher readmission risks.\n",
    "            - Non-adherence to medication might strongly correlate with readmissions.\n",
    "\n",
    "- Statistical Hypothesis Testing\n",
    "    - Use statistical tests to confirm relationships:\n",
    "        - T-tests for differences in means (e.g., medication adherence between high and low readmission groups).\n",
    "        - Chi-square tests for independence between categorical variables (e.g., age group vs. readmission rates).\n",
    "\n",
    "Example 1: Statistical Hypothesis Testing for Medication Adherence\n",
    "- Objective: Determine if medication adherence significantly differs between patients who are readmitted and those who are not.\n",
    "- Approach: Two-Sample t-Test\n",
    "- Hypotheses: \n",
    "    - $𝐻_0$ : The mean adherence rate is the same for both groups (readmitted and not readmitted).\n",
    "    - $𝐻_𝑎$ : The mean adherence rate differs between the groups.\n",
    "\n",
    "- Steps:\n",
    "    - Prepare the Data:\n",
    "    - Split patients into two groups: \"Readmitted\" and \"Not Readmitted.\"\n",
    "    - Collect medication adherence rates for each group.\n",
    "\n",
    "- Check Assumptions:\n",
    "    - Normality: Use a Shapiro-Wilk or Kolmogorov-Smirnov test to check if adherence rates are normally distributed.\n",
    "    - Equal Variance: Use Levene’s test or Bartlett’s test.\n",
    "\n",
    "- Perform the t-Test:\n",
    "    - If variances are equal, use a standard t-test. If not, use Welch’s t-test.\n",
    "\n",
    "- Interpret Results: \n",
    "    - If $𝑝 < 0.05$, reject $𝐻_0$\n",
    "    - Conclude that adherence rates differ significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21143b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Example data\n",
    "adherence_readmitted = [0.7, 0.65, 0.6, 0.75, 0.8]  # Adherence rates for readmitted\n",
    "adherence_not_readmitted = [0.9, 0.85, 0.88, 0.92, 0.89]  # Adherence rates for not readmitted\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = ttest_ind(adherence_readmitted, adherence_not_readmitted, equal_var=False)\n",
    "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3ddb9d",
   "metadata": {},
   "source": [
    "Example 2: Statistical Hypothesis Testing for Age Group vs. Readmission Rates\n",
    "- Objective: Test if age group (categorical variable) is independent of readmission status.\n",
    "- Approach: Chi-Square Test of Independence\n",
    "- Hypotheses:\n",
    "    - $𝐻_0$ : Age group is independent of readmission status.\n",
    "    - $𝐻_𝑎$ : Age group and readmission status are dependent.\n",
    "\n",
    "- Steps:\n",
    "    - Create a Contingency Table:\n",
    "        - Rows: Age groups (e.g., <40, 40–60, >60).\n",
    "        - Columns: Readmission status (e.g., Yes, No).\n",
    "\n",
    "- Perform the Chi-Square Test:\n",
    "\n",
    "- Interpret Results:\n",
    "    - If $ 𝑝< 0.05$, reject $𝐻_0$​\n",
    "    - Conclude that age group influences readmission rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aecd0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Contingency table\n",
    "table = np.array([[50, 200], [70, 230], [100, 300]])\n",
    "\n",
    "# Perform Chi-Square Test\n",
    "chi2, p_value, dof, expected = chi2_contingency(table)\n",
    "print(f\"Chi2 Statistic: {chi2}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d538a",
   "metadata": {},
   "source": [
    "Example 3: Statistical Hypothesis Testing for Length of Stay (LOS)\n",
    "- Objective: Compare Average LOS for Readmitted vs. Not Readmitted Patients\n",
    "- Approach: Two-Sample t-Test\n",
    "    - $𝐻_0$ : The mean LOS is the same for readmitted and non-readmitted patients.\n",
    "    - $𝐻_𝑎$ : The mean LOS differs.\n",
    "- Steps:\n",
    "    - Prepare the Data:\n",
    "    - Split patients into two groups: \"Readmitted\" and \"Not Readmitted.\"\n",
    "    - Collect medication Length of stay for each group.\n",
    "\n",
    "- Check Assumptions:\n",
    "    - Normality: Use a Shapiro-Wilk or Kolmogorov-Smirnov test to check if Lengths of stay are normally distributed.\n",
    "    - Equal Variance: Use Levene’s test or Bartlett’s test.\n",
    "\n",
    "- Perform the t-Test:\n",
    "    - If variances are equal, use a standard t-test. If not, use Welch’s t-test.\n",
    "\n",
    "- Interpret Results: \n",
    "    - If $𝑝 < 0.05$, reject $𝐻_0$\n",
    "    - Conclude that adherence rates differ significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a39723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe8fb84d",
   "metadata": {},
   "source": [
    "Example 4: Relationship Between LOS and Readmission Rate\n",
    "- Approach: ANOVA (Analysis of Variance)\n",
    "- Objective: Check if LOS groups (<3 days, 3–7 days, >7 days) have significantly different readmission rates.\n",
    "- Hypotheses: \n",
    "    - $𝐻_0$ : The mean readmission rate is the same across all LOS groups.\n",
    "    - $𝐻_𝑎$ : At least one group differs.\n",
    "- Steps:\n",
    "    - Group the Data:\n",
    "        - Divide LOS into groups.\n",
    "        - Calculate readmission rates for each group.\n",
    "- Perform ANOVA:\n",
    "- Interpret Results:\n",
    "    - If $𝑝 < 0.05$\n",
    "    - reject $𝐻_0$\n",
    "    - Conclude that LOS impacts readmission rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab649501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Example data\n",
    "readmission_short = [0.1, 0.12, 0.08, 0.15]  # Readmission rates for <3 days\n",
    "readmission_medium = [0.2, 0.22, 0.25, 0.18]  # Readmission rates for 3–7 days\n",
    "readmission_long = [0.35, 0.4, 0.38, 0.42]  # Readmission rates for >7 days\n",
    "\n",
    "# Perform ANOVA\n",
    "f_stat, p_value = f_oneway(readmission_short, readmission_medium, readmission_long)\n",
    "print(f\"F-statistic: {f_stat}, P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be03fb0",
   "metadata": {},
   "source": [
    "\n",
    "- Build and Interpret a Regression Model\n",
    "    - Fit the Linear Regression model to identify significant predictors:\n",
    "    - Check p-values of coefficients: Variables with p-values below a chosen threshold (e.g., 0.05) are statistically significant.\n",
    "    - Evaluate effect size: Large coefficients indicate strong influence on the target.\n",
    "    - Test for interaction effects, such as how length_of_stay and severity jointly influence readmissions.\n",
    "\n",
    "- Refine the Model\n",
    "    - Handle multicollinearity: Use Variance Inflation Factor (VIF) to remove or combine highly correlated predictors.\n",
    "    - Validate the model: Perform cross-validation to ensure robustness.\n",
    "\n",
    "This will help the institute to:\n",
    "- Improve medication adherence programs for high-risk patients.\n",
    "- Extend hospital stays for patients with severe conditions if needed.\n",
    "- Schedule follow-up visits more effectively to minimize readmission risks.\n",
    "\n",
    "Example 2: Predicting Readmissions Based on LOS\n",
    "- Approach: Linear Regression\n",
    "- Objective: Use regression to predict readmissions based on LOS and other predictors.\n",
    "\n",
    "##### Linear Regression Helps Solve This Problem\n",
    "- Quantifies Relationships: Identifies and quantifies the factors contributing to readmissions.\n",
    "- Predicts Outcomes: Provides actionable predictions to guide healthcare interventions.\n",
    "- Allocates Resources: Helps prioritize patients who need more attention post-discharge.\n",
    "- Supports Policy Changes: Enables data-driven policy improvements in patient care."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03949464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Example data\n",
    "X = [2, 4, 6, 8, 10]  # LOS\n",
    "y = [0, 1, 0, 1, 1]  # Readmission (0 = No, 1 = Yes)\n",
    "\n",
    "# Add constant for intercept\n",
    "X = sm.add_constant(X)\n",
    "model = sm.Logit(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d99cd9",
   "metadata": {},
   "source": [
    "2. Presenting Findings to Senior Management and Board\n",
    "- Tailor Communication to the Audience\n",
    "    - Senior management: Focus on actionable insights, resource implications, and patient care improvements.\n",
    "    - Board of directors: Emphasize high-level trends, financial impacts, and alignment with strategic goals.\n",
    "\n",
    "- Structure of Presentation\n",
    "    - Introduction\n",
    "        - Start with the context: \"Readmission rates are a critical indicator of hospital performance and patient care quality.\"\n",
    "        - Summarize the objective: \"This study identifies key factors driving readmissions and proposes targeted interventions.\"\n",
    "\n",
    "    - Key Findings\n",
    "        - Use visuals like \n",
    "            - bar charts, \n",
    "            - scatter plots, and \n",
    "            - regression coefficient tables:\n",
    "                - Example: \"Medication adherence has the strongest inverse relationship with readmissions. A 10% increase in adherence reduces readmissions by 5%.\"\n",
    "            - Highlight statistical significance:\n",
    "                - \"Length of stay and severity are significant at p < 0.05, confirming their importance.\"\n",
    "    \n",
    "    - Implications\n",
    "        - Show real-world impact: \"Addressing non-adherence could prevent ~300 readmissions annually, saving $1.2M in costs.\"\n",
    "        - Prioritize recommendations: \"Focus on medication adherence programs, especially for older patients with comorbidities.\"\n",
    "\n",
    "    - Actionable Recommendations\n",
    "        - Immediate Steps:\n",
    "            - Develop a post-discharge follow-up protocol for high-risk groups.\n",
    "            - Launch an adherence monitoring program.\n",
    "        - Future Research:\n",
    "            - Investigate additional factors like social determinants of health.\n",
    "\n",
    "    - Conclusion\n",
    "        - Reinforce value: \"By addressing these factors, we can improve patient outcomes, meet regulatory benchmarks, and reduce financial strain.\"\n",
    "\n",
    "- Tools for Communication\n",
    "    - Visual Dashboards: Create dashboards showing predicted readmissions, trends over time, and \"what-if\" scenarios.\n",
    "    - Executive Summaries: Provide concise summaries with high-impact visuals and key takeaways.\n",
    "    - Financial Impact Models: Quantify cost savings or ROI of proposed interventions.\n",
    "\n",
    "3. Example Insights and Visualizations\n",
    "Insight Example: Medication Adherence\n",
    "    - Insight: \"Medication adherence has a strong negative correlation with readmissions ($𝑅=−0.65$)\n",
    "        - A 10% increase in adherence is associated with a 5% reduction in readmissions.\"\n",
    "\n",
    "Visualization:\n",
    "    - A bar chart comparing adherence rates and average readmissions.\n",
    "    - Regression coefficient chart showing the magnitude of influence.\n",
    "\n",
    "Insight Example: Length of Stay\n",
    "    - Insight: \"Patients with hospital stays >7 days are 2x more likely to be readmitted within 30 days.\"\n",
    "\n",
    "Visualization:\n",
    "    - Scatter plot: length_of_stay vs. readmissions.\n",
    "    - Box plot: Readmission rates by length-of-stay categories.\n",
    "\n",
    "4. Implementation Plan\n",
    "Once the board approves, focus on operationalizing findings:\n",
    "\n",
    "- Deploy targeted interventions for high-risk patients.\n",
    "- Set KPIs to monitor the effectiveness of changes.\n",
    "- Continuously refine the model based on new data.\n",
    "\n",
    "##### Set KPIs to monitor the effectiveness of changes\n",
    "\n",
    "**KPI 1: 30-Day Readmission Rate**\n",
    "- Definition: Percentage of patients readmitted to the hospital within 30 days of discharge.\n",
    "- Why Important: This is the primary metric to assess whether interventions are reducing readmissions.\n",
    "- Formula: $Readmission Rate = \\frac{Number of patients readmitted within 30 days}{Total number of discharged patients} × 100$\n",
    "- Target: A reduction in the readmission rate over time indicates success.\n",
    "\n",
    "**KPI 2: Medication Adherence Rate**\n",
    "- Definition: Percentage of patients adhering to their prescribed medications post-discharge.\n",
    "- Why Important: Non-adherence is a leading cause of readmissions. Monitoring this ensures interventions like counseling and follow-ups are effective\n",
    "- Formula: $Medication Adherence Rate = \\frac{Number of patients adhering to medications}{Total number of patients} × 100$\n",
    "- Target: An increase in adherence correlates with better outcomes and fewer readmissions.\n",
    "\n",
    "**KPI 3: Follow-Up Appointment Compliance**\n",
    "- Definition: Percentage of discharged patients attending follow-up appointments within the recommended time frame.\n",
    "- Why Important: Follow-up visits can identify issues early and prevent readmissions.\n",
    "- Formula: $Compliance Rate= \\frac{Number of scheduled follow-ups}{Number of attended follow-ups} × 100$\n",
    "- Target: High compliance indicates improved patient engagement.\n",
    "\n",
    "**KPI 4: Average Length of Stay (LOS)**\n",
    "- Definition: Average number of days patients spend in the hospital.\n",
    "- Why Important: Shorter stays can indicate efficiency but might increase readmissions if patients are discharged prematurely.\n",
    "- Formula: $LOS= \\frac{Number of discharges}{Total inpatient days}$\n",
    "​- Target: Maintain an optimal LOS that balances cost and readmission prevention.\n",
    "\n",
    "**KPI 5: Percentage of High-Risk Patients Identified**\n",
    "- Definition: Proportion of discharged patients flagged as high-risk for readmission and targeted for interventions.\n",
    "- Why Important: Monitoring ensures that predictive models and risk stratification tools are working effectively.\n",
    "- Formula:$High-Risk Patients Identified = \\frac{Total number of discharged patients}{Number of flagged high-risk patients} × 100$\n",
    "- Target: Increase the identification rate while reducing actual readmissions.\n",
    "\n",
    "##### Presenting KPIs to Stakeholders\n",
    "\n",
    "**Visual Presentation**\n",
    "\n",
    "Use dashboards and visualizations:\n",
    "- Bar charts to compare readmission rates before and after interventions.\n",
    "- Line graphs showing trends over time for medication adherence and follow-up compliance.\n",
    "- Heatmaps for condition-specific readmission trends.\n",
    "\n",
    "Narrative\n",
    "- Highlight success: \"We reduced the 30-day readmission rate from 18% to 12%, saving $500,000 annually.\"\n",
    "- Focus on actionable insights: \"Medication adherence programs have been effective, with a 15% increase in adherence leading to a 5% drop in readmissions.\"\n",
    "\n",
    "Recommendations\n",
    "- Continue monitoring these KPIs for sustained improvements.\n",
    "- Scale successful interventions to other patient groups or hospitals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f51a4",
   "metadata": {},
   "source": [
    "## 2. Multiple Linear Regression:\n",
    "\n",
    "What it means:\n",
    "- It is used when two or more independent variables influence the dependant variable. \n",
    "\n",
    "- A linear equation defines the relationship, with the \n",
    "    - coefficients of the independent variables \n",
    "    \n",
    "- representing the effect of each variable on the dependant variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb634e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data = pd.read_csv('data.csv') # read data from csv file\n",
    "X = data[['Independent_Var_1', 'Independent_Var_2', 'Independent_Var_3']] # select independent variables\n",
    "Y = data['Dependent_Var'] # select dependent variable\n",
    "\n",
    "# Add a constant to the independent variable set\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = sm.Logit(Y, X).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6228e",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Logistic Regression\n",
    "\n",
    "Logistic regression is a statistical model used for binary classification tasks.\n",
    "- the outcome variable is categorical with two possible values (e.g., 1/0, Yes/No, Positive/Negative). \n",
    "\n",
    "It predicts the probability of an event occurring, transforming the linear combination of predictors through a logistic function (sigmoid function) to ensure the predicted probabilities lie between 0 and 1.\n",
    "\n",
    "What It Means: \n",
    "- Logistic regression estimates the probability of a binary outcome (e.g., yes/no, success/failure) based on predictor variables. \n",
    "    - It uses a logistic function to map predictions to probabilities between 0 and 1.\n",
    "\n",
    "- It is a statistical technique for investigating the relationship between a binary dependent variable (outcome) and one or more independent variables (predictors). \n",
    "\n",
    "- The goal of logistic regression is to find the best-fitting model to describe the relationship between the dependent variable and the independent variables and then use that model to predict the outcome variable.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- The model outputs probabilities that can be converted to binary outcomes. \n",
    "- Coefficients show how each predictor variable influences the likelihood of the outcome.\n",
    "\n",
    "Performance Measures:\n",
    "- Accuracy: Proportion of correct predictions.\n",
    "- AUC-ROC: Measures the model's ability to distinguish between classes; values closer to 1 indicate a better model.\n",
    "\n",
    "Lay Explanation: \n",
    "- Logistic regression is like a yes-or-no decision helper. It estimates the chances of an event happening (e.g., a customer buying a product) based on known factors.\n",
    "- It tries to find the best-fitted curve for the data\n",
    "\n",
    "Use Case: Used for binary classification (e.g., churn prediction, fraud detection).\n",
    "\n",
    "Model Equation: \n",
    "$ 𝑃(𝑦=1)= \\frac{1}{1+𝑒^{−(𝛽_{0}+𝛽_{1}𝑥_{1}+…+𝛽_{𝑛}𝑥_{𝑛}})}$\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Objective:\n",
    "- The medical institute, we want to identify the likelihood of patients being readmitted within 30 days of discharge based on patient \n",
    "    - demographics, \n",
    "    - medical history, \n",
    "    - length of stay (LOS), and \n",
    "    - clinical metrics such as blood pressure, \n",
    "    - blood glucose levels, and \n",
    "    - medication adherence.\n",
    "\n",
    "Why Logistic Regression?\n",
    "\n",
    "Logistic regression is ideal for this problem because:\n",
    "- Binary Outcome: The target variable is binary: Readmitted (1) or Not Readmitted (0).\n",
    "- Interpretability: It provides coefficients (log odds) that indicate how changes in predictors affect the likelihood of the event (readmission).\n",
    "- Insights: It helps identify the significant factors influencing readmissions.\n",
    "\n",
    "**Key Assumptions of Logistic Regression**\n",
    "- Binary Outcome: The dependent variable is binary.\n",
    "- Independence of Observations: Observations are independent of each other.\n",
    "- Linearity of Log-Odds: There is a linear relationship between the log-odds of the outcome and the independent variables.\n",
    "- No Multicollinearity: Independent variables are not highly correlated.\n",
    "- Large Sample Size: Logistic regression performs well with larger datasets.\n",
    "\n",
    "**Step 1: Define the Problem**\n",
    "- Target Variable: Readmission within 30 days (1 = Yes, 0 = No).\n",
    "- Predictors:\n",
    "    - Patient Demographics: Age, gender, insurance status.\n",
    "    - Clinical Metrics: Blood glucose levels, blood pressure, medication adherence.\n",
    "    - Hospital Metrics: Length of Stay (LOS), number of previous visits.\n",
    "\n",
    "**Step 2: Collect and Prepare Data**\n",
    "- Gather historical patient data and ensure it's clean and consistent.\n",
    "    - Check for Missing Data:\n",
    "    - Impute missing values for predictors like glucose levels using median or mean.\n",
    "    - Standardize Continuous Variables:\n",
    "    - Standardize LOS, glucose levels, and blood pressure for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed07b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    'age': [45, 60, 50, 40, 70],\n",
    "    'los': [3, 7, 4, 2, 10],\n",
    "    'glucose': [150, 200, 180, 140, 220],\n",
    "    'med_adherence': [0.8, 0.6, 0.75, 0.9, 0.5],\n",
    "    'readmitted': [1, 1, 0, 0, 1]\n",
    "})\n",
    "\n",
    "# Features and target\n",
    "X = data[['age', 'los', 'glucose', 'med_adherence']]\n",
    "y = data['readmitted']\n",
    "\n",
    "# Add constant for intercept\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8ef54",
   "metadata": {},
   "source": [
    "**Step 3: Exploratory Data Analysis**\n",
    "- Univariate Analysis: Examine distributions of continuous variables.\n",
    "- Bivariate Analysis: Analyze relationships between predictors and the target variable.\n",
    "- Correlation Matrix: Identify multicollinearity among predictors.\n",
    "\n",
    "**Step 4: Perform Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ccb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3daca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa4f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.Logit(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train = data[:800]\n",
    "test = data[800:]\n",
    "\n",
    "# Define the independent variables\n",
    "X_train = train[['age', 'gender', 'income']]\n",
    "X_test = test[['age', 'gender', 'income']]\n",
    "\n",
    "# Define the dependent variable\n",
    "y_train = train['buy_product']\n",
    "y_test = test['buy_product']\n",
    "\n",
    "# Fit the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the outcomes for the test data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa885e4",
   "metadata": {},
   "source": [
    "**Step 5: Interpret Coefficients and Evaluate the Model**\n",
    "\n",
    "- Log Odds: Each coefficient represents the change in log odds of readmission for a unit increase in the predictor.\n",
    "- Odds Ratios: Use np.exp(model.params) to convert coefficients to odds ratios.\n",
    "\n",
    "1. Accuracy\n",
    "2. Confusion Matrix\n",
    "3. ROC Curve and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X) > 0.5\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc749e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a4e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y, model.predict(X))\n",
    "auc = roc_auc_score(y, model.predict(X))\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda6f67",
   "metadata": {},
   "source": [
    "**Understanding Factors Significantly Influencing Readmission**\n",
    "\n",
    "1. Use p-values from the logistic regression summary:\n",
    "- Predictors with $𝑝< 0.05$ are statistically significant.\n",
    "2. Assess the odds ratios:\n",
    "- For example, if the odds ratio for LOS is 2.0, each additional day in the hospital doubles the odds of readmission.\n",
    "3. Visualize relationships:\n",
    "- Plot odds ratios for key predictors to present to stakeholders.\n",
    "\n",
    "**Statistical Hypothesis Testing**\n",
    "\n",
    "Example 1: Relationship Between LOS and Readmission\n",
    "- Hypotheses:\n",
    "    - $𝐻_0$: LOS has no effect on readmission.\n",
    "    - $𝐻_𝑎$: LOS has a significant effect on readmission.\n",
    "- Approach: Perform a logistic regression test and check the p-value for LOS.\n",
    "\n",
    "Example 2: Age Group vs. Readmission\n",
    "- Hypotheses:\n",
    "    - $𝐻_0$: Age group is independent of readmission.\n",
    "    - $𝐻_𝑎$: Age group and readmission are dependent.\n",
    "- Approach: Use a Chi-Square test of independence (see previous example).\n",
    "\n",
    "**Actionable Insights**\n",
    "- Highlight key factors significantly influencing readmission (e.g., LOS, medication adherence).\n",
    "- Use odds ratios to explain how much each factor increases or decreases the likelihood of readmission.\n",
    "- Present findings visually (e.g., bar charts for odds ratios, ROC curves for model performance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b8436",
   "metadata": {},
   "source": [
    "##### 3. Generalized Linear Models (GLMs)\n",
    "What It Means: \n",
    "- GLMs extend linear regression by allowing different types of data distributions\n",
    "    - Poisson for count data. \n",
    "- It models the mean of the outcome variable based on a link function.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- The coefficients explain how each predictor affects the mean outcome, given the distribution.\n",
    "\n",
    "Performance Measures:\n",
    "- Deviance: Measures how well the model fits compared to a perfect model; lower values are better.\n",
    "\n",
    "Lay Explanation: \n",
    "- GLMs are like flexible versions of linear regression that can handle different data types (like counts or binary data), giving predictions that respect the data’s nature.\n",
    "\n",
    "Use Case: \n",
    "- Extends linear regression for non-normal distributions (e.g., Poisson regression for count data).\n",
    "\n",
    "Model Types: \n",
    "- Poisson regression, \n",
    "- Binomial regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48888f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "poisson_model = sm.GLM(y_train, X_train, family=sm.families.Poisson()).fit()\n",
    "predictions = poisson_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af5a09e",
   "metadata": {},
   "source": [
    "##### 4. Time Series Models (e.g., ARIMA)\n",
    "What It Means: \n",
    "- Time series models account for:\n",
    "    - trends, \n",
    "    - seasonality, and \n",
    "    - temporal dependencies in data collected over time, often used for forecasting future values.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each prediction is based on patterns in past data points, accounting for recent trends and cycles.\n",
    "\n",
    "Performance Measures:\n",
    "- Mean Absolute Percentage Error (MAPE): Shows the average prediction error in percentage terms.\n",
    "- Root Mean Squared Error (RMSE): Measures the prediction accuracy; lower values mean better predictions.\n",
    "\n",
    "Lay Explanation: \n",
    "- Time series models are like weather forecasts—they predict future values based on past patterns, like trends and cycles.\n",
    "\n",
    "Use Case: \n",
    "- Forecasting for data with a temporal component (e.g., sales data, stock prices).\n",
    "\n",
    "Model Types: \n",
    "- ARIMA, \n",
    "- SARIMA, \n",
    "- Exponential Smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce38a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "model = ARIMA(time_series_data, order=(1,1,1))\n",
    "model_fit = model.fit()\n",
    "predictions = model_fit.forecast(steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3ef82",
   "metadata": {},
   "source": [
    "##### 5. Decision Trees and Random Forests\n",
    "What It Means: \n",
    "- Decision trees split data based on conditions, creating branches that lead to a prediction. \n",
    "- Random forests use multiple trees to improve accuracy and reduce overfitting.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each \"branch\" shows how different conditions affect the outcome, \n",
    "- and random forests average the results of many trees for robust predictions.\n",
    "\n",
    "Performance Measures:\n",
    "- Accuracy: Proportion of correctly classified samples.\n",
    "- Gini Index / Entropy: Used to measure the purity of the splits; lower values are better.\n",
    "\n",
    "Lay Explanation: \n",
    "- Decision trees are like flowcharts that guide predictions based on conditions. \n",
    "- Random forests combine many trees to make stronger, more reliable decisions.\n",
    "\n",
    "Use Case: \n",
    "- For classification or regression problems with non-linear relationships and high dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "tree_model = DecisionTreeClassifier()\n",
    "tree_model.fit(X_train, y_train)\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "predictions_tree = tree_model.predict(X_test)\n",
    "predictions_rf = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f382cd",
   "metadata": {},
   "source": [
    "##### 6. Support Vector Machines (SVM)\n",
    "What It Means: \n",
    "- SVMs classify data by finding the best “boundary” (hyperplane) that separates classes with the widest possible margin.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Data points on either side of the boundary belong to different classes, with \"support vectors\" helping to define the boundary.\n",
    "\n",
    "Performance Measures:\n",
    "- Accuracy: Proportion of correct classifications.\n",
    "- Precision and Recall: Used when classes are imbalanced; precision is the correctness of positive predictions, and recall measures coverage.\n",
    "\n",
    "Lay Explanation: \n",
    "- SVMs are like drawing a line to separate different groups, ensuring the groups are as distinct as possible with the help of a few key points.\n",
    "\n",
    "Use Case: \n",
    "- Used for classification and regression in high-dimensional spaces, often for non-linearly separable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe25b8cd",
   "metadata": {},
   "source": [
    "##### 7. Clustering Models (e.g., K-Means)\n",
    "What It Means: \n",
    "- Clustering groups similar data points together without predefined labels, often used for segmenting customers or finding patterns.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each cluster represents a natural grouping in the data, with data points in the same cluster sharing similar characteristics.\n",
    "\n",
    "Performance Measures:\n",
    "- Silhouette Score: Measures how well each point fits within its cluster; values closer to 1 indicate better-defined clusters.\n",
    "- Within-Cluster Sum of Squares (WCSS): Measures the compactness of clusters; lower values are better.\n",
    "\n",
    "Lay Explanation: \n",
    "- Clustering is like sorting items into bins based on similarity, helping us identify groups in our data.\n",
    "\n",
    "Use Case: \n",
    "- To group similar observations without predefined labels.\n",
    "\n",
    "Model Types: \n",
    "- K-Means, \n",
    "- Hierarchical Clustering, \n",
    "- DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3cfc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "clusters = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea1408",
   "metadata": {},
   "source": [
    "##### 8. Principal Component Analysis (PCA)\n",
    "What It Means: \n",
    "- PCA reduces the number of variables in the data by finding combinations of variables that capture the most information (variance).\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each \"principal component\" explains a percentage of the total variance, helping simplify the data without losing much information.\n",
    "\n",
    "Performance Measures:\n",
    "- Explained Variance Ratio: Shows how much information each principal component holds; higher is better.\n",
    "\n",
    "Lay Explanation: \n",
    "- PCA is like summarizing a book by keeping only the most important points, making data easier to work with without losing key insights.\n",
    "\n",
    "Use Case: \n",
    "- Dimensionality reduction while retaining the most critical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23bf1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6d396",
   "metadata": {},
   "source": [
    "##### 9. Bayesian Models\n",
    "What It Means: \n",
    "- Bayesian models incorporate prior knowledge or beliefs with the data to update the probability of outcomes as new evidence is available.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each output is a probability distribution reflecting both prior knowledge and the new data, offering a range of likely outcomes.\n",
    "\n",
    "Performance Measures:\n",
    "- Log-Likelihood: Measures how well the model explains the data; higher values indicate better fit.\n",
    "\n",
    "Lay Explanation: \n",
    "- Bayesian models are like revising a guess based on new evidence—updating beliefs as we get more information.\n",
    "\n",
    "Use Case: \n",
    "- To incorporate prior knowledge and quantify uncertainty.\n",
    "\n",
    "Model Types: \n",
    "- Bayesian Linear Regression, \n",
    "- Bayesian Networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6500107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "\n",
    "with pm.Model() as model:\n",
    "    alpha = pm.Normal('alpha', mu=0, sigma=1)\n",
    "    beta = pm.Normal('beta', mu=0, sigma=1, shape=len(X_train.columns))\n",
    "    epsilon = pm.HalfNormal('epsilon', sigma=1)\n",
    "    mu = alpha + pm.math.dot(X_train, beta)\n",
    "    y_pred = pm.Normal('y_pred', mu=mu, sigma=epsilon, observed=y_train)\n",
    "    trace = pm.sample(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac011c6c",
   "metadata": {},
   "source": [
    "##### 10. Survival Analysis (e.g., Cox Proportional Hazards)\n",
    "What It Means: \n",
    "- Survival analysis predicts the time until an event occurs, such as customer churn or equipment failure.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each output shows the likelihood of the event happening over time, considering various risk factors.\n",
    "\n",
    "Performance Measures:\n",
    "- Concordance Index (C-Index): Measures the model’s ability to correctly rank predictions; values closer to 1 indicate better performance.\n",
    "\n",
    "Lay Explanation: \n",
    "Survival analysis is like tracking how long something will last, based on factors that might speed it up or slow it down.\n",
    "\n",
    "Use Case: \n",
    "- For time-to-event data, such as time until a customer churns or equipment fails.\n",
    "\n",
    "Model Types: \n",
    "- Kaplan-Meier estimator, Cox Proportional Hazards Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995ebf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(data, 'time', event_col='event')\n",
    "cph.predict_survival_function(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc25f7",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf07849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to compute True Positives, True Negatives, False Positives and False Negatives\n",
    "\n",
    "def true_positive(y_true, y_pred):\n",
    "    tp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 1:\n",
    "            tp += 1\n",
    "    return tp\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    tn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 0:\n",
    "            tn += 1        \n",
    "    return tn\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    fp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 1:\n",
    "            fp += 1       \n",
    "    return fp\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    fn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 0:\n",
    "            fn += 1        \n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea735dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "TP = np.diag(cnf_matrix)\n",
    "TN = cnf_matrix.sum() - (FP + FN + TP)FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy for each class\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf443ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation for table metrics:\n",
    "import sklearn.metrics\n",
    "import mathdef matrix_metrix(real_values,pred_values,beta):\n",
    "CM = confusion_matrix(real_values,pred_values)\n",
    "TN = CM[0][0]\n",
    "FN = CM[1][0] \n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "Population = TN+FN+TP+FP\n",
    "Prevalence = round( (TP+FP) / Population,2)\n",
    "Accuracy   = round( (TP+TN) / Population,4)\n",
    "Precision  = round( TP / (TP+FP),4 )\n",
    "NPV        = round( TN / (TN+FN),4 )\n",
    "FDR        = round( FP / (TP+FP),4 )\n",
    "FOR        = round( FN / (TN+FN),4 ) \n",
    "check_Pos  = Precision + FDR\n",
    "check_Neg  = NPV + FOR\n",
    "Recall     = round( TP / (TP+FN),4 )\n",
    "FPR        = round( FP / (TN+FP),4 )\n",
    "FNR        = round( FN / (TP+FN),4 )\n",
    "TNR        = round( TN / (TN+FP),4 ) \n",
    "check_Pos2 = Recall + FNR\n",
    "check_Neg2 = FPR + TNR\n",
    "LRPos      = round( Recall/FPR,4 ) \n",
    "LRNeg      = round( FNR / TNR ,4 )\n",
    "DOR        = round( LRPos/LRNeg)\n",
    "F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)\n",
    "FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "BM         = Recall+TNR-1\n",
    "MK         = Precision+NPV-1   \n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos','check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','F1','FBeta','MCC','BM','MK'],     \n",
    "                        'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,F1,FBeta,MCC,BM,MK]})   \n",
    "\n",
    "return (mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Implementation\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplotfpr, tpr, thresholds = roc_curve(real_values, prob_values)\n",
    "\n",
    "auc = roc_auc_score(real_values, prob_values)\n",
    "print('AUC: %.3f' % auc)pyplot.plot(fpr, tpr, linestyle='--', label='Roc curve')\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()pyplot.show()\n",
    "\n",
    "# Precision-recall implementation\n",
    "\n",
    "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(real_values,prob_values)pyplot.plot(recall, precision, linestyle='--', label='Precision versus Recall')\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "pyplot.legend()pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba51b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for get many metrics directly from sklearn\n",
    "\n",
    "def sk_metrix(real_values,pred_values,beta):\n",
    "Accuracy = round( sklearn.metrics.accuracy_score(real_values,pred_values) ,4)\n",
    "Precision= round( sklearn.metrics.precision_score(real_values,pred_values),4 )\n",
    "Recall   = round( sklearn.metrics.recall_score(real_values,pred_values),4 )   \n",
    "F1       = round ( sklearn.metrics.f1_score(real_values,pred_values),4)\n",
    "FBeta    = round ( sklearn.metrics.fbeta_score(real_values,pred_values,beta) ,4)\n",
    "MCC      = round ( sklearn.metrics.matthews_corrcoef(real_values,pred_values)  ,4)   \n",
    "Hamming  = round ( sklearn.metrics.hamming_loss(real_values,pred_values) ,4)   \n",
    "Jaccard  = round ( sklearn.metrics.jaccard_score(real_values,pred_values) ,4)   \n",
    "Prec_Avg = round ( sklearn.metrics.average_precision_score(real_values,pred_values) ,4)   \n",
    "Accu_Avg = round ( sklearn.metrics.balanced_accuracy_score(real_values,pred_values) ,4)   \n",
    "\n",
    "mat_met = pd.DataFrame({\n",
    "'Metric': ['Accuracy','Precision','Recall','F1','FBeta','MCC','Hamming','Jaccard','Precision_Avg','Accuracy_Avg'],\n",
    "'Value': [Accuracy,Precision,Recall,F1,FBeta,MCC,Hamming,Jaccard,Prec_Avg,Accu_Avg]})   \n",
    "\n",
    "return (mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics For Multi-class Classification\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate accuracy\n",
    "    -> param y_true: list of true values\n",
    "    -> param y_pred: list of predicted values\n",
    "    -> return: accuracy score\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "# Intitializing variable to store count of correctly predicted classes\n",
    "    correct_predictions = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == yp:\n",
    "            correct_predictions += 1\n",
    "    #returns accuracy\n",
    "    return correct_predictions / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eeb9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of macro-averaged precision\n",
    "\n",
    "def macro_precision(y_true, y_pred):\n",
    "\n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize precision to 0\n",
    "    precision = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(y_true.unique()):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false positive for current class\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute precision for current class\n",
    "        temp_precision = tp / (tp + fp + 1e-6)\n",
    "        # keep adding precision for all classes\n",
    "        precision += temp_precision\n",
    "        \n",
    "    # calculate and return average precision over all classes\n",
    "    precision /= num_classes\n",
    "    \n",
    "    return precision\n",
    "\n",
    "print(f\"Macro-averaged Precision score : {macro_precision(y_test, y_pred) }\")\n",
    "\n",
    "# implement marco-averaged precision using sklearn\n",
    "macro_averaged_precision = metrics.precision_score(y_test, y_pred, average = 'macro')\n",
    "print(f\"Macro-Averaged Precision score using sklearn library : {macro_averaged_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of micro-averaged precision\n",
    "\n",
    "def micro_precision(y_true, y_pred):\n",
    "\n",
    "\n",
    "    # find the number of classes \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in y_true.unique():\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate true positive for current class\n",
    "        # and update overall tp\n",
    "        tp += true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate false positive for current class\n",
    "        # and update overall tp\n",
    "        fp += false_positive(temp_true, temp_pred)\n",
    "        \n",
    "    # calculate and return overall precision\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision\n",
    "\n",
    "print(f\"Micro-averaged Precision score : {micro_precision(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "#  implement mirco-averaged precision using sklearn\n",
    "micro_averaged_precision = metrics.precision_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged Precision score using sklearn library : {micro_averaged_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ed0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of macro-averaged recall\n",
    "\n",
    "def macro_recall(y_true, y_pred):\n",
    "\n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize recall to 0\n",
    "    recall = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(y_true.unique()):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false negative for current class\n",
    "        fn = false_negative(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute recall for current class\n",
    "        temp_recall = tp / (tp + fn + 1e-6)\n",
    "        \n",
    "        # keep adding recall for all classes\n",
    "        recall += temp_recall\n",
    "        \n",
    "    # calculate and return average recall over all classes\n",
    "    recall /= num_classes\n",
    "    \n",
    "    return recall\n",
    "\n",
    "print(f\"Macro-averaged recall score : {macro_recall(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "# implement macro-averaged recall using sklearn\n",
    "\n",
    "macro_averaged_recall = metrics.recall_score(y_test, y_pred, average = 'macro')\n",
    "print(f\"Macro-averaged recall score using sklearn : {macro_averaged_recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd95acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of micro-averaged recall\n",
    "\n",
    "def micro_recall(y_true, y_pred):\n",
    "\n",
    "\n",
    "    # find the number of classes \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in y_true.unique():\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate true positive for current class\n",
    "        # and update overall tp\n",
    "        tp += true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate false negative for current class\n",
    "        # and update overall tp\n",
    "        fn += false_negative(temp_true, temp_pred)\n",
    "        \n",
    "    # calculate and return overall recall\n",
    "    recall = tp / (tp + fn)\n",
    "    return recall\n",
    "\n",
    "print(f\"Micro-averaged recall score : {micro_recall(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "#  implement micro-averaged recall using sklearn\n",
    "\n",
    "micro_averaged_recall = metrics.recall_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged recall score using sklearn library : {micro_averaged_recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d50779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of macro-averaged f1 score\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "\n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize f1 to 0\n",
    "    f1 = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(y_true.unique()):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false negative for current class\n",
    "        fn = false_negative(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false positive for current class\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute recall for current class\n",
    "        temp_recall = tp / (tp + fn + 1e-6)\n",
    "        \n",
    "        # compute precision for current class\n",
    "        temp_precision = tp / (tp + fp + 1e-6)\n",
    "        \n",
    "        \n",
    "        temp_f1 = 2 * temp_precision * temp_recall / (temp_precision + temp_recall + 1e-6)\n",
    "        \n",
    "        # keep adding f1 score for all classes\n",
    "        f1 += temp_f1\n",
    "        \n",
    "    # calculate and return average f1 score over all classes\n",
    "    f1 /= num_classes\n",
    "    \n",
    "    return f1\n",
    "\n",
    "\n",
    "print(f\"Macro-averaged f1 score : {macro_f1(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "# implement macro-averaged F1 score using sklearn\n",
    "\n",
    "macro_averaged_f1 = metrics.f1_score(y_test, y_pred, average = 'macro')\n",
    "print(f\"Macro-Averaged F1 score using sklearn library : {macro_averaged_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of micro-averaged fi score\n",
    "\n",
    "def micro_f1(y_true, y_pred):\n",
    "\n",
    "\n",
    "    #micro-averaged precision score\n",
    "    P = micro_precision(y_true, y_pred)\n",
    "\n",
    "    #micro-averaged recall score\n",
    "    R = micro_recall(y_true, y_pred)\n",
    "\n",
    "    #micro averaged f1 score\n",
    "    f1 = 2*P*R / (P + R)    \n",
    "\n",
    "    return f1\n",
    "\n",
    "print(f\"Micro-averaged recall score : {micro_f1(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "# implement micro-averaged F1 score using sklearn\n",
    "\n",
    "micro_averaged_f1 = metrics.f1_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged F1 score using sklearn library : {micro_averaged_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe51cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC AUCurve Computation\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def roc_auc_score_multiclass(actual_class, pred_class, average = \"macro\"):\n",
    "    \n",
    "    #creating a set of all the unique classes using the actual class list\n",
    "    unique_class = set(actual_class)\n",
    "    roc_auc_dict = {}\n",
    "    for per_class in unique_class:\n",
    "        \n",
    "        #creating a list of all the classes except the current class \n",
    "        other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "        #marking the current class as 1 and all other classes as 0\n",
    "        new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "        new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "\n",
    "        #using the sklearn metrics method to calculate the roc_auc_score\n",
    "        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n",
    "        roc_auc_dict[per_class] = roc_auc\n",
    "\n",
    "    return roc_auc_dict\n",
    "\n",
    "roc_auc_dict = roc_auc_score_multiclass(y_test, y_pred)\n",
    "roc_auc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC implementation: \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from itertools import cycle\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Load the iris data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target# Binarize the output\n",
    "y_bin = label_binarize(y, classes=[0, 1, 2])\n",
    "n_classes = y_bin.shape[1]# We split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size= 0.5, random_state=0)\n",
    "\n",
    "\n",
    "# We define the model as an SVC in OneVsRestClassifier setting.\n",
    "# this means that the model will be used for class 1 vs class 2, \n",
    "# class 2vs class 3 and class 1 vs class 3. \n",
    "# So, we have 3 cases at #the end and within each case, the bias will be varied in order to \n",
    "# Get the ROC curve of the given case - 3 ROC curves as output.\n",
    "\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=0))\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "# Plotting and estimation of FPR, TPR\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "colors = cycle(['blue', 'red', 'green'])\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=1.5, label='ROC curve of class {0} (area = {1:0.2f})' ''.format(i+1, roc_auc[i]))\n",
    "    plt.plot([0, 1], [0, 1], 'k-', lw=1.5)\n",
    "    plt.xlim([-0.05, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic for multi-class data')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69585b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
