{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb638b74",
   "metadata": {},
   "source": [
    "# Statisitcs and Research methods\n",
    "\n",
    "## Understanding Statistical Models vs. Machine Learning Models\n",
    "\n",
    "It's essential first to understand the distinctions between statistical models and machine learning models, as they serve different purposes, assumptions, and interpretative depth.\n",
    "\n",
    "- Statistical Models: \n",
    "    - These are rooted in traditional statistics and \n",
    "        - focus on relationships between variables through predefined equations. \n",
    "    - Statistical models aim to understand the underlying data-generating process, focusing on hypothesis testing and inference. \n",
    "    - These models often rely on strong assumptions like:\n",
    "        - linearity, \n",
    "        - normality, and \n",
    "        - homoscedasticity \n",
    "        - and are **interpretable**, making it easier to understand the impact of individual variables.\n",
    "\n",
    "- Machine Learning Models: \n",
    "    - These prioritize **predictive** power over interpretability. \n",
    "    - They are designed to automatically learn patterns and relationships within data, often with minimal assumptions. \n",
    "    - Machine learning models can handle complex and high-dimensional data but may lack transparency about how individual features affect the outcome, especially in “black box” models like neural networks or ensemble methods.\n",
    "\n",
    "\n",
    "## Choosing the Right Statistical Model\n",
    "\n",
    "The type of statistical model you use depends on your data and problem:\n",
    "\n",
    "- Linear Regression: For predicting a **continuous target variable** based on one or more predictors.\n",
    "- Logistic Regression: For predicting a **binary outcomes**, often used in classification problems.\n",
    "- ANOVA (Analysis of Variance): For comparing means across multiple groups.\n",
    "- Time Series Models: For data that’s ordered by time (e.g., ARIMA, SARIMA).\n",
    "- Survival Analysis: For time-to-event data, such as customer churn timing.\n",
    "- Multivariate Analysis: For understanding interactions across multiple variables (e.g., MANOVA, PCA).\n",
    "\n",
    "## Preprocessing the Data\n",
    "Prepare your data by cleaning and preprocessing it:\n",
    "\n",
    "- Missing Values: Decide whether to impute or drop missing values.\n",
    "- Outliers: Identify and consider handling outliers, especially in regression.\n",
    "- Data Transformation: Transform non-normal variables if required (e.g., using log transformations).\n",
    "- Feature Scaling: For some models, standardizing or normalizing data is essential.\n",
    "\n",
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "EDA is essential to understand: \n",
    "- patterns,\n",
    "    - visualizations\n",
    "- distributions,\n",
    "    - summary statistics\n",
    "- relationships\n",
    "    - correlation matrices\n",
    "    \n",
    "This is to identify relevant features and spot potential issues like multicollinearity.\n",
    "\n",
    "## Building the Statistical Model\n",
    "\n",
    "- **Statsmodels** provides \n",
    "    - coefficients, \n",
    "    - p-values, and \n",
    "    - confidence intervals for each variable, \n",
    "        - enabling hypothesis testing on whether each predictor significantly affects the outcome.\n",
    "\n",
    "## Evaluating Model Performance\n",
    "Regression Metrics: \n",
    "- Use R-squared, \n",
    "- Adjusted R-squared, \n",
    "- RMSE, and \n",
    "- MAE to evaluate regression models.\n",
    "\n",
    "Classification Metrics: \n",
    "- Use confusion matrix, \n",
    "- accuracy, \n",
    "- precision, \n",
    "- recall, and \n",
    "- AUC-ROC.\n",
    "\n",
    "Residual Analysis: \n",
    "- Residual plots help assess assumptions\n",
    "    - homoscedasticity, \n",
    "    - normality of residuals).\n",
    "\n",
    "## Model Interpretation\n",
    "Statistical models are highly interpretable. \n",
    "- In linear regression, each coefficient represents the expected change in the dependent variable for a one-unit change in the predictor, holding all else constant.\n",
    "\n",
    "Confidence Intervals: \n",
    "- Look at 95% CI for each coefficient; if it does not contain zero, it suggests the predictor has a statistically significant effect.\n",
    "\n",
    "P-Values: \n",
    "- A p-value below a threshold (usually 0.05) indicates that the predictor significantly affects the outcome.\n",
    "\n",
    "## Validating Assumptions\n",
    "- Linearity: Check scatter plots of residuals.\n",
    "- Normality of Residuals: Use a Q-Q plot to verify.\n",
    "- No Multicollinearity: Variance inflation factor (VIF) helps detect multicollinearity.\n",
    "- Homoscedasticity: Plot residuals vs. fitted values.\n",
    "\n",
    "## Reporting and Communicating Results\n",
    "Present your findings by focusing on:\n",
    "\n",
    "- Key Coefficients: Explain which predictors significantly affect the outcome.\n",
    "- Model Fit: Interpret R-squared values (e.g., explaining how much variance in the target variable is explained).\n",
    "- Real-World Implications: Describe how insights from the model can impact business decisions.\n",
    "\n",
    "# Approach to statistical modeling\n",
    "\n",
    "Each model type has specific \n",
    "- applications, \n",
    "- strengths, and \n",
    "- limitations, \n",
    "\n",
    "Understand when and how to use them.\n",
    "\n",
    "### Step 1: Define Objectives and Hypotheses\n",
    "\n",
    "Identify the Problem and Objectives: \n",
    "- Clearly define the goal.\n",
    "    - Are you trying to predict, classify, find patterns, or estimate relationships? \n",
    "    - Setting objectives helps in choosing the right model.\n",
    "\n",
    "- Formulate Hypotheses: \n",
    "    - Based on the problem, develop hypotheses. \n",
    "        - For instance, in a sales prediction problem, you may hypothesize that `certain features like advertising spend, time of year, and economic indicators affect sales.`\n",
    "\n",
    "### Step 2: Data Collection and Preprocessing\n",
    "Data Collection: \n",
    "- Gather historical data related to the problem. \n",
    "\n",
    "Data Cleaning: \n",
    "- Handle missing values, remove duplicates, and ensure consistency.\n",
    "\n",
    "Feature Engineering: \n",
    "- Create new features if necessary. \n",
    "- This could involve \n",
    "    - transformations, \n",
    "    - encoding categorical variables, or \n",
    "    - creating interaction terms.\n",
    "\n",
    "Data Splitting: \n",
    "- Split the data into training and testing sets. Typically, an 80-20 or 70-30 split is used.\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "### Why is EDA important?\n",
    "\n",
    "Exploratory Data Analysis (EDA) helps us to understand our data without making any assumptions. EDA is a vital component before we continue with the modelling phase as it provides context and guidance on the course of action to take when developing the appropriate model. It will also assist in interpreting the results correctly. Without doing EDA you will not understand your data fully.\n",
    "\n",
    "\n",
    "### The different types of EDA\n",
    "\n",
    "EDA are generally classified in two ways:\n",
    "\n",
    "    1) Non-graphical or Graphical\n",
    "    2) Univariate or Multivariate\n",
    "    \n",
    "<div align=\"left\" style=\"width: 600px; text-align: left;\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/f860f39251c523eda779dea0140316ccbefdd8e0/eda_map.jpg?raw=True\"\n",
    "     alt=\"EDA Diagram\"\n",
    "     style=\"padding-bottom=0.5em\"\n",
    "     width=600px/>\n",
    "</div>\n",
    "\n",
    "\n",
    "#### Non-graphical EDA\n",
    "Involves calculations of summary/descriptive statistics. \n",
    "\n",
    "#### Graphical EDA\n",
    "This type of analysis will contain data visualisations.\n",
    "\n",
    "#### Univariate Analysis \n",
    "This is performed on one variable at a time as the prefix 'uni' indicates. \n",
    "\n",
    "#### Multivariate Analysis \n",
    "This type of analysis explores the relationship between two or more variables. \n",
    "When only comparing two variables it is known as **bivariate analysis** as indicated by the prefix 'bi'.\n",
    "\n",
    "Read a more detailed explanation <a href=\"https://www.stat.cmu.edu/~hseltman/309/Book/chapter4.pdf\">here</a>.\n",
    "\n",
    "### 1. Basic Analysis\n",
    "\n",
    "For a practical example, we will be looking at the Medical Claims Data. Using these four commands, we will perform a basic analysis:\n",
    "\n",
    "    - df.head()\n",
    "    - df.shape\n",
    "    - df.info()\n",
    "        - feature (variable) is categorical the Dtype is object and if it is a numerical variable the Dtype is an int64 or float64. \n",
    "        - This command also shows us that out of the 1338 none of the features contain any null values.\n",
    "    - df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/regression_sprint/claims_data.csv')\n",
    "\n",
    "# Looking at the top five rows of our data\n",
    "df.head()\n",
    "\n",
    "# shape command shows us that we have x rows of data and y features.\n",
    "df.shape\n",
    "\n",
    "#  confirms our categorical and numerical features.\n",
    "df.info()\n",
    "\n",
    "# Null values for each feature can also be checked by using the following command\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83721513",
   "metadata": {},
   "source": [
    "# Population and Sample\n",
    "\n",
    "**Population**\n",
    "- Population is a collection of all data points of interest.\n",
    "    - eg: Total number of employees in the organization is known as population\n",
    "- **Parameter**\n",
    "    - numbers that are obtained when working with a population\n",
    "        - eg: total number of employees working in an organization and After completion of our survey, we arrive at number – 20000. \n",
    "    \n",
    "**Sample**\n",
    "- Sample is a subset of the population.\n",
    "    - eg: Total number of employees in a project is known as a sample.\n",
    "- **Statistic**\n",
    "    - numbers that are obtained when working with a sample\n",
    "        - eg: count the total number of employees working on a particular project. After completion of our survey, we arrive at number – 20.\n",
    "\n",
    "What to chose between Population and Sample?\n",
    "\n",
    "The real-life case scenarios, we always deal with sample data. \n",
    "- The reason behind this is that a sample is easy to collect and easier to compute than the population. \n",
    "- Based on the result that we obtained for a sample, we can then use predictive analytics to make predictions about the entire population.\n",
    "\n",
    "# The Measure of Central tendency\n",
    "\n",
    "The concept of central tendency is based on the below fact –\n",
    "- “Provided with a larger number of observations of similar type, most of the observations seems to cluster around central position when represented as a graph”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98438ff3",
   "metadata": {},
   "source": [
    "# Univariate Analysis: Non-Graphical\n",
    "\n",
    "The first univariate analysis will be non-graphical. This is where we will be looking at the **descriptive statistics** of each feature. \n",
    "\n",
    "## Continous/Numeric Feature\n",
    "\n",
    "##### **Descriptive Statistics**\n",
    "\n",
    "We can get the descriptive statistics of each **numerical feature** by using the following command:\n",
    "\n",
    "    - df.describe()\n",
    "\n",
    "This command will provide the \n",
    "- mean, \n",
    "    - also known as the arithmetic mean \n",
    "    - is the statistical average of all data points in question.\n",
    "- standard deviation and\n",
    "- The five number summary of each numerical feature.\n",
    "    - Minimum, \n",
    "    - Lower Quartile (Q1) = 25%,\n",
    "    - Median (Q2) = 50%, \n",
    "        - middlemost data point in the dataset when arranged in ascending or descending order.\n",
    "        - Higher resistance to outlier as compared to mean\n",
    "        - Median with even number of data points = average of the middle two numbers.\n",
    "        - Median with an odd number of data points = middlemost observation.\n",
    "    - Upper Quartile (Q3) = 75%, \n",
    "    - Maximum is also used for creating the box plot.\n",
    "        - exposes **Outlier**: is a data point that is significantly different from the rest of the data points in consideration.\n",
    "\n",
    "Individual statistical measures can also be calculated by using the following commands:\n",
    "\n",
    "    - df.count()\n",
    "    - df.mean()\n",
    "    - df.std()\n",
    "    - df.min()\n",
    "    - df.quantile([0.25, 0.5, 0.75], axis = 0)\n",
    "    - df.median()\n",
    "    - df.max()\n",
    "\n",
    "The three measures for central tendency are the:\n",
    "- mode\n",
    "    - Mode is basically the value that appears the most in the dataset. \n",
    "- mean and \n",
    "- median**. \n",
    "\n",
    "The command to determine the mode is:\n",
    "\n",
    "    - df.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed976189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "\n",
    "# statistics of a specific feature\n",
    "df.age.describe()\n",
    "df['age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f3953",
   "metadata": {},
   "source": [
    "##### **Dispersion of Data**\n",
    "\n",
    "Dispersion of data used to understands the distribution of data.\n",
    "- Helps to understand the variation of data and provides a piece of information about the distribution data.\n",
    "\n",
    "These include: \n",
    "- Range,\n",
    "     - measure by subtracting the lowest value from the massive Number. \n",
    "          - The wide range indicates high variability,\n",
    "          - The small range specifies low variability in the distribution.\n",
    "     - Range = Highest_value  – Lowest_value\n",
    "          - range can be influence by outliers\n",
    "- Interquartile Range (IQR),\n",
    "     - IQR is a range (the boundary between the first and second quartile) and Q3 (the boundary between the third and fourth quartile).\n",
    "     - IQR is preferred over a range as, like a range, IQR does not influence by outliers. \n",
    "     - IQR is used to measure variability by splitting a data set into four equal quartiles.\n",
    "          - IQR uses a box plot to find the outliers.\n",
    "               - Formula to find outliers: [Q1 – 1.5 * IQR, Q3 + 1.5 * IQR]\n",
    "- Variance, \n",
    "     - Variance measures how far each number in the dataset from the mean.\n",
    "\n",
    "Population variance\n",
    "$$\\sigma^2 = \\frac{\\sum (x_i - \\mu)^2}{n}$$\n",
    "sample variance\n",
    "$$ s^2 = \\frac{\\sum (x_i - \\bar{x})^2}{n - 1}$$  \n",
    "\n",
    "- Standard Deviation\n",
    "     - Standard deviation is a squared root of the variance to get original values. \n",
    "     - Low standard deviation indicates data points close to mean.\n",
    "         -  68 % of values lie within 1 standard deviation.\n",
    "         - 95 % of values lies within 2 standard deviation.\n",
    "         - 99.7 % of values lie within 3 standard deviation.\n",
    "\n",
    "Population std\n",
    "$$\\sigma = \\sqrt{\\frac{1}{N}\\sum (x_i - \\mu)^2}$$\n",
    "sample std\n",
    "$$ s = \\sqrt{\\frac{1}{n - 1}\\sum (x_i - \\bar{x})^2}$$\n",
    "\n",
    "##### Standard deviation and Mean Absolute deviation (Why SD is more reliable than MAD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00270bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d3f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8a37bde",
   "metadata": {},
   "source": [
    "# Univariate Analysis: Graphical\n",
    "\n",
    "Objective:\n",
    "- Trends and Patterns of data\n",
    "- Frequency\n",
    "- Distribution of the variables\n",
    "- Relationship that may exist between different variables\n",
    "\n",
    "You can look at the **distribution** of any numerical feature by using the following plots:\n",
    "- Scatter plot\n",
    "- histogram\n",
    "- density plot\n",
    "- box plot\n",
    "- violin plot\n",
    "    \n",
    "For a categorical feature we will use a:\n",
    "- bar plot\n",
    "\n",
    "## Continous/Numerical variable\n",
    "\n",
    "### Uni-variate summary plots :\n",
    "These plots give a more concise description of the location, dispersion, and distribution of a variable than an enumerative plot. \n",
    "- Summarizing every individual data value in a plot isn’t feasible, but it efficiently represents the entire dataset,\n",
    "\n",
    "#### Histogram and Density Plot\n",
    "\n",
    "For displaying a histogram and density plot we will be using the Matplotlib library and create a list of all numerical features to visualise these features at the same time.\n",
    "\n",
    " both the histogram and density plot display the same information. The density plot can be considered a smoothed version of the histogram and does not depend on the size of bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbdb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['age', 'bmi', 'steps', 'children', 'claim_amount'] # create a list of all numerical features\n",
    "df[features].hist(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[features].plot(kind='density', subplots=True, layout=(3, 2), sharex=False, figsize=(10, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bca4f0a",
   "metadata": {},
   "source": [
    "#### Box Plot and Violin Plot\n",
    "\n",
    "For the Box Plot and Violin Plot, we will use the seaborn library and only select one feature instead of all the numerical features. We can visualise all numerical features simultaneously, but as the range of values for each feature is different, it will not create a useful visualisation. Standardisation or normalisation can be applied to a feature to adjust the range, but we will not apply it in this notebook. Further reading on standardisation and normalisation can be done <a href=\"https://medium.com/@dataakkadian/standardization-vs-normalization-da7a3a308c64\">here</a>.\n",
    "\n",
    "The `bmi` feature will be used.\n",
    "\n",
    "Although both the box plot and violin plot display the distribution of the data, the boxplot provides certain statistics that are useful. \n",
    "\n",
    "The five vertical lines in the boxplot provide the information of the five number summary and the dots on the right hand side of the graph is a display of outliers. The violin plot focuses more on a smoothed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544a3fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='bmi', data=df)\n",
    "\n",
    "sns.set(rc={'figure.figsize':(9,9)})\n",
    "sns.boxplot(x = 'var', y = 'value', data = pd.melt(dfm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8885be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x='bmi', data=df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8647e096",
   "metadata": {},
   "source": [
    "### Univariate enumerative Plots\n",
    "\n",
    "#### Scatter plot\n",
    "\n",
    "Plots different observations/values of the same variable corresponding to the index/observation number.\n",
    "- plot the variable\n",
    "- against the corresponding observation number stored as the index of the data frame (df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36309ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.index, df['var1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6205931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x= df.index , y= ['var'], hue = df['variety'])\n",
    "\n",
    "# In seaborn, the ‘hue’ parameter, an interesting feature, determines which column in the data frame to use for color encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b30c5b",
   "metadata": {},
   "source": [
    "#### Line plot\n",
    "A line plot visualizes data by connecting the data points via line segments. \n",
    "- It resembles a scatter plot but differs by ordering the measurement points (usually by their x-axis value) and connecting them with straight line segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize': (7,7)})\n",
    "sns.set(font_scale= 1.5)\n",
    "\n",
    "fig = sns.lineplot(x = df.index, y= df['var2'], markevery = 1, marker = 'd', data = df, hue = df[variety])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5be95",
   "metadata": {},
   "source": [
    "#### Strip plot and Swarm Plot :\n",
    "- The strip plot is similar to a scatter plot.\n",
    "    - helps to plot the distribution of variables for each category as individual data points.\n",
    "- The swarm-plot, similar to a strip-plot, provides a visualization technique for univariate data to view the spread of values in a continuous variable.\n",
    "    - The only difference between the strip-plot and the swarm-plot is that the swarm-plot spreads out the data points of the variable automatically to avoid overlap and hence provides a better visual overview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0e25ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(y=df['var1'])\n",
    "sns.stripplot(x= df['variety',y=df['var1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a7ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = 'figure.figsize': (5,5))\n",
    "sns.swarmplot(x = df['var'])\n",
    "sns.swarmplot(x = df['variety'], y = df['var'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf932ae",
   "metadata": {},
   "source": [
    "### Catagorical Data\n",
    "\n",
    "#### Bar Plot\n",
    "\n",
    "For the categorical features, we can create a **bar plot** to display the frequency distribution. \n",
    "\n",
    "plot on a two-dimensional axis. \n",
    "- One axis is the category axis indicating the category, while the \n",
    "- second axis is the value axis that shows the numeric value of that category, indicated by the length of the bar.\n",
    "\n",
    "We'll generate a bar plot of the `children` feature, where each bar represents a unique number of children from the data, and the height represents how many times that number of children occurred. This can be done by using seaborn's `countplot`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bbe8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['var'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'children', data = df, palette=\"hls\")\n",
    "plt.title(\"Distribution of Children\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17982bc",
   "metadata": {},
   "source": [
    "##### Pie Chart:\n",
    "Shows the numerical proportion occupied by each category\n",
    "-  pass the array of values to the ‘labels’ parameter to add labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e58776",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df['var'].value_counts(), labels= ['cat1', 'cat2', 'cat3'], shadow= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6dd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df['var'].value_counts(), startangle= 90, autopct='%.3f', labels= ['cat1', 'cat2', 'cat3'], shadow= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db98a8b6",
   "metadata": {},
   "source": [
    "# Normal Distribution\n",
    "\n",
    "Examples like: Birth weight, the IQ Score, and stock price return often form a bell-shaped curve.\n",
    "\n",
    "Normal Distribution becomes essential for data scientists is the Central Limit Theorem\n",
    "- theorem explains the magic of mathematics and is the foundation for hypothesis testing techniques.\n",
    "\n",
    "### Properties of Normal Distribution\n",
    "- Bell-shaped curve\n",
    "    - curve is symmetric around the Mean\n",
    "    - Mean, Median, and Mode are all the same.\n",
    "- Normal Distribution is symmetric, which means its tails on one side are the mirror image of the other side\n",
    "- also call a Gaussian Distribution\n",
    "- simplify the Normal Distribution’s Probability Density by using only two parameters\n",
    "    - $\\mu$\n",
    "    - $\\sigma^2$\n",
    "- Normal distribution retains the normal shape throughout, unlike other probability distributions that change their properties after a transformation. \n",
    "\n",
    "For a Normal Distribution:\n",
    "- Product of two Normal Distribution results into a Normal Distribution\n",
    "- The Sum of two Normal Distributions is a Normal Distribution\n",
    "- Convolution of two Normal Distribution is also a Normal Distribution\n",
    "- Fourier Transformation of a Normal Distribution is also Normal\n",
    "\n",
    "Empirical Rule for Normal Distribution\n",
    "- According to the Empirical Rule for Normal Distribution:\n",
    "    - 68.27% of data lies within 1 standard deviation of the mean\n",
    "    - 95.45% of data lies within 2 standard deviations of the mean\n",
    "    - 99.73% of data lies within 3 standard deviations of the mean\n",
    "-  almost all the data lies within 3 standard deviations. \n",
    "\n",
    "This rule enables us to check for Outliers and is very helpful when determining the normality of any distribution.\n",
    "\n",
    "### Standard Normal Distribution\n",
    "Standard Normal Distribution is a special case of Normal Distribution when\n",
    "- $\\mu$ = 0\n",
    "- $\\sigma$ = 1\n",
    "\n",
    "Convert Normal Distribution into Standard Normal distribution with\n",
    "$$ Z = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "Example: Comparing Maths mark with History mark of 2 students\n",
    "- who ever get the higher z-score performed better.\n",
    "\n",
    "### Skewed Distribution\n",
    "\n",
    "When data points cluster on one side more than the other. These types of distributions are called Skewed Distributions.\n",
    "\n",
    "##### **kurtosis** and **skew**. \n",
    "\n",
    "Both kurtosis and skew are important statistical terms to be familiar with in data science. Kurtosis is the measure of outliers present in the data. **High kurtosis (>3)** indicates a large number of outliers and **low kurtosis (<3)** a lack of outliers.  Skew will indicate how symmetrical your data is. Below is a table that explains the range of values with regards to skew.\n",
    "\n",
    "Left skewed distribution\n",
    "- Mode > Median > Mean.\n",
    "\n",
    "Right Skewed Distribution\n",
    "- Mode < Median < Mean\n",
    "\n",
    "\n",
    "|   Skew Value (x)  |       Description of Data      |\n",
    "|:-------------------|:---------------:|\n",
    "| -0.5 < x < 0.5              |Fairly Symmetrical |\n",
    "| -1 < x < -0.5 | Moderate Negative Skew  | \n",
    "| 0.5 < x < 1             | Moderate Positive Skew  | \n",
    "|       x < -1     |High Negative Skew  | \n",
    "|       x > 1  |High Positve Skew | \n",
    "\n",
    "<div align=\"left\" style=\"width: 500px; font-size: 80%; text-align: left; margin: 0 auto\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/f3aeedd2c056ddd233301c7186063618c1041140/regression_analysis_notebook/skew.jpg?raw=True\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: left; padding-bottom=0.5em\"\n",
    "     width=500px/>\n",
    "     For a more detailed explanation on skew and kurtosis read <a href=\"https://codeburst.io/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa\">here</a>.\n",
    "</div>\n",
    "\n",
    "\n",
    "The commands used to determine the skewness of data are:\n",
    "\n",
    "    - df.skew()\n",
    "\n",
    "### Check the **Normality** of a Distribution\n",
    "- Histogram\n",
    "- KDE Plots\n",
    "- Q_Q Plots\n",
    "- Skewness\n",
    "- Kurtosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f58c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()\n",
    "\n",
    "# Closer to 0 implies fairly symmetrical.\n",
    "# Above 0.3 implies  moderately skewed in a positive direction.\n",
    "# Above 1 implies highly skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63643b2",
   "metadata": {},
   "source": [
    "### Kertosis\n",
    "\n",
    "Check for Normality is Kurtosis. \n",
    "\n",
    "Kurtosis gives the information regarding tailedness which basically indicates the data distribution along the tails.\n",
    "- For the symmetric type of distribution, the Kurtosis value will be close to Zero. We call such types of distributions as Mesokurtic distribution. \n",
    "    - Its tails are similar to Gaussian Distribution.\n",
    "\n",
    "- If there are extreme values present in the data, then it means that more data points will lie along with the tails. In such cases, the value of K will be greater than zero.\n",
    "    - Here, Tail will be fatter and will have longer distribution. We call such types of distributions as Leptokurtic Distribution.\n",
    "        - As we can clearly see here, the tails are fatter and denser as compared to Gaussian Distribution:\n",
    "\n",
    "- If there is a low presence of extreme values compared to Normal Distribution, then lesser data points will lie along the tail.\n",
    "    - The Kurtosis value will be less than zero. We call such types of distributions as Platykurtic Distribution. \n",
    "        - It will have a thinner tail and a shorter distribution in comparison to Normal distribution.\n",
    "\n",
    "The commands used to determine the kurtosis of data are:\n",
    "\n",
    "    - df.kurtosis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicates a lack of outliers for all features.\n",
    "df.kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb4e974",
   "metadata": {},
   "source": [
    "### Transform features into Normal/Gaussian Distribution\n",
    "- Models such as Linear Regression, Logistic Regression, Artificial Neural Networks assume that features are normally distributed\n",
    "- They perform much better if the features provided to them during modeling are normally distributed.\n",
    "\n",
    "**What do we do when data provided to us does not necessarily follow a normal distribution?**\n",
    "\n",
    "### Gaussian Distribution\n",
    "\n",
    "In probability theory, a normal (or Gaussian) distribution is a type of continuous probability distribution for a real-valued random variable.\n",
    "- general form of its probability density function is\n",
    "$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}$$\n",
    "\n",
    "Samples of the Gaussian Distribution follow a bell-shaped curve and lies around the mean. \n",
    "- The mean, median, and mode of Gaussian Distribution are the same.\n",
    "\n",
    "Steps:\n",
    "1. Check if a variable is following Normal Distribution (see above)\n",
    "- Checking the distribution of variables using a Q-Q plot\n",
    "    - Q-Q plot is a scatterplot created by plotting two sets of quantiles against one another. If both sets of quantiles came from the same distribution, we should see the points forming a roughly straight line.\n",
    "        -  if the data falls in a straight line then the variable follows normal distribution otherwise not.\n",
    "\n",
    "Example: if variable is highly positively skewed\n",
    "- plot the Q-Q plot for the variable and check.\n",
    "\n",
    "If data points of the feature are not falling on a straight line. This implies that it does not follow a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccbdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import scipy.stats as stats\n",
    "import pylab\n",
    "\n",
    "stats.probplot(cp.price,plot=pylab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82346226",
   "metadata": {},
   "source": [
    "##### Function in python which will take data and feature name as inputs and return the KDE plot and Q-Q plot of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf451ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return plots for the feature\n",
    "def normality(data,feature):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.kdeplot(data[feature])\n",
    "    plt.subplot(1,2,2)\n",
    "    stats.probplot(data[feature],plot=pylab)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1b178",
   "metadata": {},
   "source": [
    "### Performing the transformations\n",
    "\n",
    "##### **Logarithmic Transformation**\n",
    "Convert to its log value i.e log(Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd003720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing logarithmic transformation on the feature\n",
    "cp['price_log']=np.log(cp['price'])\n",
    "# plotting to check the transformation\n",
    "normality(cp,'price_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf561f",
   "metadata": {},
   "source": [
    "##### **Reciprocal Transformation**\n",
    "This will inverse values of Price i.e1/Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf75ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp['price_reciprocal']=1/cp.price\n",
    "normality(cp,'price_reciprocal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9f1ff9",
   "metadata": {},
   "source": [
    "##### **Square Root Transformation**\n",
    "\n",
    "This transformation will take the square root of the Price column i.e sqrt(Price)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f7d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp['price_sqroot']=np.sqrt(cp.price)\n",
    "normality(cp,'price_sqroot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf02f1",
   "metadata": {},
   "source": [
    "##### **Exponential Transformation**\n",
    "\n",
    "The exponential value of the Price variable will be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481728d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp['price_exponential']=cp.price**(1/1.2)\n",
    "normality(cp,'price_exponential')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac8769",
   "metadata": {},
   "source": [
    "##### **Box-Cox Transformation**\n",
    "\n",
    "$$ y_i^{(\\lambda)} = \\{ {\\frac{y_i^{(\\lambda)} - 1}{\\lambda} \\text{if} \\lambda \\neq 0, \\\\ \\ln(y_i) \\text{if} \\lambda = 0,}$$\n",
    "\n",
    "where:\n",
    "- y is the response variable and \n",
    "- λ is the transformation parameter. \n",
    "    - λ value varies from -5 to 5. \n",
    "\n",
    "During the transformation, all values of λ are considered and the optimal/best value for the variable is selected. \n",
    "- log(y) is only applied when λ=0.\n",
    "\n",
    "Box cox is more logic-based and involves the λ variable which is chosen as per the best skewness for the data so Box cox will be a better transformation to go with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660127d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp['price_Boxcox'],parameters=stats.boxcox(cp['price'])\n",
    "normality(cp,'price_Boxcox')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dcf821",
   "metadata": {},
   "source": [
    "# Types Of Probability Distribution Function in Univeriate Analysis\n",
    "\n",
    "Probability Distribution Function (PDF) is a mathematical way of showing how likely different outcomes are in a random event. \n",
    "- It gives probabilities to each possible result, and \n",
    "- Adding up all the probabilities, the total is always 1. \n",
    "The PDF helps us understand the chances of different outcomes in a random experiment.\n",
    "\n",
    "### Distribution Function\n",
    "- is a mathematical expression that describes the probability of different possible outcomes for an experiment.\n",
    "- denoted as Variable ~ Type (Characteristics)\n",
    "\n",
    "Data Types\n",
    "- We have Qualitative and Quantitative data. \n",
    "    - Quantitative data, we have \n",
    "        - Continuous data types/ random variables. \n",
    "            - Continuous data measures and can take any number of values within a given finite or infinite range.\n",
    "            - Continuous data represented in decimal format.\n",
    "            - Example: \n",
    "                - person’s height, \n",
    "                - Time, \n",
    "                - distance,\n",
    "        - Discrete data types.\n",
    "            - Discrete data is counted and can take only a limited number of values.\n",
    "            - Discrete data is represented as Whole number.\n",
    "            - Example:\n",
    "                - number of students in a class, \n",
    "                - number of workers in a company\n",
    "\n",
    "### Types of distribution functions\n",
    "\n",
    "|   Discrete distributions   |      Continuous distributions     |\n",
    "|:-------------------|:---------------:|\n",
    "|  Uniform distribution | Normal distribution |\n",
    "| Binomial distribution | Standard Normal distribution  | \n",
    "| Bernoulli distribution  | Student’s T distribution  | \n",
    "| Poisson distribution  | Chi-squared  distribution  |\n",
    "\n",
    "#### **Probability Density Function (PDF):**\n",
    "- Statistical term that describes the probability distribution of a **continuous** random variable.\n",
    "- Probability associate with a single value is always Zero.\n",
    "\n",
    "$$F(X) = P(a \\leq x \\leq b) = \\int^{b}_{a} f(x)dx \\geq 0$$\n",
    "\n",
    "#### **Probability Mass Function (PMF):**\n",
    "- Statistical term that describes the probability distribution of a **discrete** random variable.\n",
    "\n",
    "$$p(x) = P(X=x)$$\n",
    "\n",
    "Where:\n",
    "- probability of x = the probability X = one specific x\n",
    "\n",
    "#### **Cumulative Distribution Function (CDF):**\n",
    "- It is another method to describe the distribution of a random variable (either continuous or discrete).\n",
    "\n",
    "$$ F_X (x) = P(X \\leq x)$$\n",
    "\n",
    "Where:\n",
    "- F_X (x) = function of X\n",
    "- X = real value variable\n",
    "- P = probability that X will have a value less then or equal to x\n",
    "\n",
    "### Discrete Distribution\n",
    "\n",
    "##### **1. Discrete Uniform distribution**\n",
    "- Denoted as X ~ U (a, b)\n",
    "- where X is a discrete random variable that follows uniform distribution ranging from a to b.\n",
    "- Uniform distribution is when all the possible events are equally likely.\n",
    "- Example:\n",
    "    - Experiment of rolling a dice\n",
    "    - six possible events X = {1, 2, 3, 4, 5, 6} each having a probability of P(X) = 1/6.\n",
    "\n",
    "Formula for PMF, CDF of Uniform distribution function:\n",
    "\n",
    "|   Term   |     Fromula     |\n",
    "|:-------------------|:---------------:|\n",
    "|  Support | $K \\in {a, a + 1, ..., b-1, b}$ |\n",
    "| PMF | $\\frac{1}{n}$  | \n",
    "| CDF | $\\frac{[k] - a + 1}{n}$  |\n",
    "| Mean | $\\frac{(a + b)}{2}$  | \n",
    "| Variance | $\\frac{(n^2 - 1)}{12}$  |\n",
    "\n",
    "Case Study: Lottery Number Simulation\n",
    "\n",
    "A lottery system allows participants to pick a number between 1 and 6, inclusive, where each number has an equal chance of being selected. \n",
    "- This setup represents a discrete uniform distribution.\n",
    "\n",
    "PMF:\n",
    "- Since each outcome is equally likely, the probability for each number from 1 to 6 will be $\\frac{1}{6} ≈ 0.1667$.\n",
    "\n",
    "CDF:\n",
    "- The cumulative probabilities for the outcomes [1, 2, 3, 4, 5, 6] will increase incrementally as: [0.1667, 0.3334, 0.5001, 0.6668, 0.8335, 1.0]\n",
    "\n",
    "Mean:\n",
    "- For a discrete uniform distribution:\n",
    "\n",
    "$$ Mean = \\frac{Low + High}{2}$$\n",
    "$$ = \\frac{1 + 6}{2}$$\n",
    "$$ = 3.5 $$\n",
    "\n",
    "Variance:\n",
    "- For a discrete uniform distribution:\n",
    "\n",
    "$$ Variance = \\frac{(High - Low + 1)^2 - 1}{12}$$\n",
    "$$ = \\frac{(6 -1 + 1)^2}{12}$$\n",
    "$$ = \\frac{35}{12} $$\n",
    "$$ ≈ 2.92 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc94acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint\n",
    "\n",
    "# 1. Define the parameters of the discrete uniform distribution\n",
    "low, high = 1, 6  # Numbers range from 1 to 6, inclusive\n",
    "\n",
    "# 2. Simulate the discrete uniform distribution\n",
    "n_samples = 10000\n",
    "samples = np.random.randint(low, high + 1, size=n_samples)\n",
    "\n",
    "# 3. Calculate the PMF\n",
    "pmf = [1 / (high - low + 1)] * (high - low + 1)  # Since it's uniform, all probabilities are equal\n",
    "outcomes = np.arange(low, high + 1)\n",
    "\n",
    "# 4. Calculate the CDF\n",
    "cdf = np.cumsum(pmf)\n",
    "\n",
    "# 5. Mean and Variance\n",
    "mean = np.mean(samples)\n",
    "variance = np.var(samples)\n",
    "\n",
    "# 6. Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# PMF Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(outcomes, pmf, color='skyblue', alpha=0.7)\n",
    "plt.title(\"PMF of Discrete Uniform Distribution\")\n",
    "plt.xlabel(\"Outcomes\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xticks(outcomes)\n",
    "\n",
    "# CDF Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.step(outcomes, cdf, where='post', color='orange', label=\"CDF\")\n",
    "plt.title(\"CDF of Discrete Uniform Distribution\")\n",
    "plt.xlabel(\"Outcomes\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.xticks(outcomes)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Print results\n",
    "print(\"PMF:\", pmf)\n",
    "print(\"CDF:\", cdf)\n",
    "print(f\"Simulated Mean: {mean:.2f}\")\n",
    "print(f\"Simulated Variance: {variance:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea177ec7",
   "metadata": {},
   "source": [
    "##### **2. Binomial distribution**\n",
    "- Denoted as X ~ B(n, p).\n",
    "- where X is a discrete random variable that follows Binomial distribution with parameters n, p.\n",
    "    - n is the no. of trials,\n",
    "    - p is the success probability for each trial.\n",
    "- Probability distribution of the number of successes in ‘n’ independent experiments sequence.\n",
    "    - Binomial event suggests the no. of times a specific outcome can be expected.\n",
    "- The two outcomes of a Binomial trial could be \n",
    "    - Success is denoted as 1, and the probability associated with it is p.\n",
    "    - Failure is denoted as 0, and the probability associate with it is q = 1-p.\n",
    "- Example: \n",
    "    - Success/Failure, \n",
    "    - Pass/Fail/, \n",
    "    - Win/Lose,\n",
    "\n",
    "\n",
    "|   Term   |     Fromula     |\n",
    "|:-------------------|:---------------:|\n",
    "| PMF | $ \\left(^{n}_{k}\\right) p^k q^{n - k}$  | \n",
    "| CDF | $I_q ( n - k, 1 + k)$  |\n",
    "| Mean | $n \\times p$  | \n",
    "| Variance | $ n \\times p \\times q$  |\n",
    "\n",
    "Case Study: Quality Control in Manufacturing\n",
    "- A manufacturing plant produces light bulbs. We inspect a batch of 10 bulbs. Each light bulb has a: \n",
    "    - 90% probability of passing quality control (success) and a \n",
    "    - 10% probability of failing (failure). \n",
    "\n",
    "PMF: \n",
    "- The PMF provides the probability of having exactly 𝑘 successes in n trials:\n",
    "- For example, P(X=9) represents the probability that 9 out of 10 light bulbs pass quality control.\n",
    "\n",
    "$$ \\left(^{n}_{k}\\right) p^k q^{n - k}$$\n",
    "\n",
    "CDF: \n",
    "- The CDF provides the cumulative probability of having up to k successes:\n",
    "$$ P(X \\leq k) = \\sum P(X = i) $$\n",
    "\n",
    "\n",
    "Mean: For a Binomial distribution:\n",
    "$$ Mean = n \\times p$$\n",
    "$$ = 10 \\times 0.9 $$\n",
    "$$ =0 $$\n",
    "\n",
    "Variance: For a Binomial distribution:\n",
    "\n",
    "$$ Variance= n \\times p \\times (1−p) $$\n",
    "$$ =10⋅0.9⋅0.1 $$\n",
    "$$ =0.9 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "\n",
    "# 1. Define the parameters of the Binomial distribution\n",
    "n = 10  # Number of trials (light bulbs in the batch)\n",
    "p = 0.9  # Probability of success (passing quality control)\n",
    "\n",
    "# 2. Simulate the Binomial distribution\n",
    "n_samples = 10000\n",
    "samples = np.random.binomial(n, p, size=n_samples)\n",
    "\n",
    "# 3. Calculate the PMF\n",
    "x = np.arange(0, n + 1)  # Possible outcomes: 0 to n successes\n",
    "pmf = binom.pmf(x, n, p)\n",
    "\n",
    "# 4. Calculate the CDF\n",
    "cdf = binom.cdf(x, n, p)\n",
    "\n",
    "# 5. Mean and Variance\n",
    "mean = n * p  # Mean of a Binomial distribution\n",
    "variance = n * p * (1 - p)  # Variance of a Binomial distribution\n",
    "\n",
    "# 6. Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# PMF Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(x, pmf, color='skyblue', alpha=0.7, label='PMF')\n",
    "plt.title(\"PMF of Binomial Distribution\")\n",
    "plt.xlabel(\"Number of Successes\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend()\n",
    "\n",
    "# CDF Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.step(x, cdf, where='post', color='orange', label='CDF')\n",
    "plt.title(\"CDF of Binomial Distribution\")\n",
    "plt.xlabel(\"Number of Successes\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Print results\n",
    "print(\"PMF:\", pmf)\n",
    "print(\"CDF:\", cdf)\n",
    "print(f\"Theoretical Mean: {mean:.2f}\")\n",
    "print(f\"Theoretical Variance: {variance:.2f}\")\n",
    "print(f\"Simulated Mean: {np.mean(samples):.2f}\")\n",
    "print(f\"Simulated Variance: {np.var(samples):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4ee37e",
   "metadata": {},
   "source": [
    "##### **3. Bernoulli distribution**\n",
    "- denoted as X ~ Bern(p).\n",
    "- Where X is a discrete random variable that follows Bernoulli distribution with parameter p.\n",
    "    - Where p is the probability of the success.\n",
    "- Bernoulli is a Binomial experiment with a single trial.\n",
    "    - Bernoulli’s event suggests which outcome can be expected for a single trial.\n",
    "- Example: tossing a fair. The two possible outcomes are \n",
    "    - Heads, Tails. \n",
    "    - The probability (p) associated with each of them is 1/2.\n",
    "- Example: In an unfair coin\n",
    "    - Heads can have a probability of p = 0.8, then the probability of tail q = 1-p = 1-0.8 = 0.2\n",
    "\n",
    "|   Term   |     Fromula     |\n",
    "|:-------------------|:---------------:|\n",
    "| PMF | $ \\{ q = 1- p \\text{  if  } k = 0 \\\\ \\{ p \\text{  if  } k = 1 \\\\ p^k (1 - p)^{1 - k}$  | \n",
    "| CDF | $\\{ 0 = 1- p \\text{  if  } k < 0 \\\\ \\{ 1 - p \\text{  if  } 0 \\leq k < 1 \\\\ \\{ 0 = 1- p \\text{  if  } k \\geq  1$  |\n",
    "| Mean | $ p$  | \n",
    "| Variance | $ p( 1 - p) = p \\times q$  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d4abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# 1. Define the parameters of the Bernoulli distribution\n",
    "p = 0.5  # Probability of success (Heads)\n",
    "\n",
    "# 2. Simulate the Bernoulli distribution\n",
    "n_samples = 10000\n",
    "samples = np.random.binomial(1, p, size=n_samples)  # Equivalent to Bernoulli\n",
    "\n",
    "# 3. Calculate the PMF\n",
    "x = [0, 1]  # Possible outcomes: 0 (Tails), 1 (Heads)\n",
    "pmf = bernoulli.pmf(x, p)\n",
    "\n",
    "# 4. Calculate the CDF\n",
    "cdf = bernoulli.cdf(x, p)\n",
    "\n",
    "# 5. Mean and Variance\n",
    "mean = p  # Mean of a Bernoulli distribution\n",
    "variance = p * (1 - p)  # Variance of a Bernoulli distribution\n",
    "\n",
    "# 6. Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# PMF Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(x, pmf, color='skyblue', alpha=0.7, label='PMF')\n",
    "plt.title(\"PMF of Bernoulli Distribution\")\n",
    "plt.xlabel(\"Outcomes (0: Tails, 1: Heads)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xticks(x)\n",
    "plt.legend()\n",
    "\n",
    "# CDF Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.step(x, cdf, where='post', color='orange', label='CDF')\n",
    "plt.title(\"CDF of Bernoulli Distribution\")\n",
    "plt.xlabel(\"Outcomes (0: Tails, 1: Heads)\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.xticks(x)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Print results\n",
    "print(\"PMF:\", pmf)\n",
    "print(\"CDF:\", cdf)\n",
    "print(f\"Theoretical Mean: {mean:.2f}\")\n",
    "print(f\"Theoretical Variance: {variance:.2f}\")\n",
    "print(f\"Simulated Mean: {np.mean(samples):.2f}\")\n",
    "print(f\"Simulated Variance: {np.var(samples):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3717be0d",
   "metadata": {},
   "source": [
    "##### **4. Poisson Distribution**\n",
    "- Denoted as X ~ Po(λ). \n",
    "- Where X is a discrete random variable that follows Poisson Distribution with parameter λ.\n",
    "    - Where λ is the expected rate of occurrences.\n",
    "- It expresses the probability of a given number of events occurring in a fixed time interval.\n",
    "- Examples: \n",
    "    - The number of diners at a restaurant on a given day.\n",
    "    - Calls per hour at a call centre.\n",
    "\n",
    "|   Term   |     Fromula     |\n",
    "|:-------------------|:---------------:|\n",
    "| PMF | $ \\frac{\\lambda^k e^{-\\lambda}}{k!} $ | \n",
    "| CDF | $ e^{-\\lambda} \\sum^{[k]}_{i = 0} \\frac{\\lambda^i}{i!}$  |\n",
    "| Mean | $ \\lambda $  | \n",
    "| Variance | $ \\lambda $  |\n",
    "\n",
    "Case Study: Website Traffic\n",
    "- A website receives an average of λ=3 inquiries per minute. \n",
    "- The number of inquiries in any given minute can be modeled using a Poisson distribution.\n",
    "\n",
    "PMF:\n",
    "-  for λ=3 and k=2:\n",
    "\n",
    "$$ P(X = k ) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$$\n",
    "$$ P(X = 2 ) = \\frac{3^2 e^{-3}}{2!}$$\n",
    "$$ 0.224$$\n",
    "\n",
    "CDF: \n",
    "\n",
    "Mean:\n",
    "- Mean=λ=3\n",
    "\n",
    "Variance:\n",
    "- Variance=λ=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1738b0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# 1. Define the parameter of the Poisson distribution\n",
    "lam = 3  # Average rate (inquiries per minute)\n",
    "\n",
    "# 2. Simulate the Poisson distribution\n",
    "n_samples = 10000\n",
    "samples = np.random.poisson(lam, size=n_samples)\n",
    "\n",
    "# 3. Calculate the PMF\n",
    "x = np.arange(0, 15)  # Possible outcomes (0 to 14 inquiries)\n",
    "pmf = poisson.pmf(x, lam)\n",
    "\n",
    "# 4. Calculate the CDF\n",
    "cdf = poisson.cdf(x, lam)\n",
    "\n",
    "# 5. Mean and Variance\n",
    "mean = lam  # Mean of a Poisson distribution\n",
    "variance = lam  # Variance of a Poisson distribution\n",
    "\n",
    "# 6. Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# PMF Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(x, pmf, color='skyblue', alpha=0.7, label='PMF')\n",
    "plt.title(\"PMF of Poisson Distribution\")\n",
    "plt.xlabel(\"Number of Inquiries\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend()\n",
    "\n",
    "# CDF Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.step(x, cdf, where='post', color='orange', label='CDF')\n",
    "plt.title(\"CDF of Poisson Distribution\")\n",
    "plt.xlabel(\"Number of Inquiries\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Print results\n",
    "print(\"PMF:\", pmf)\n",
    "print(\"CDF:\", cdf)\n",
    "print(f\"Theoretical Mean: {mean:.2f}\")\n",
    "print(f\"Theoretical Variance: {variance:.2f}\")\n",
    "print(f\"Simulated Mean: {np.mean(samples):.2f}\")\n",
    "print(f\"Simulated Variance: {np.var(samples):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b304420",
   "metadata": {},
   "source": [
    "### Continuous Distributions\n",
    "\n",
    "##### **1. Normal or Gaussian Distribution**\n",
    "- denoted as $X ~ N (μ, σ^2)$. \n",
    "- Where  X is a continuous random variable that follows a Normal distribution with parameters μ, σ2.\n",
    "    - μ is the mean. \n",
    "    - $σ^2$ is the variance.\n",
    "- describes the probability of a continuous random variable that takes real values.\n",
    "- Examples:\n",
    "    - Heights of people, \n",
    "    - exam scores of students, \n",
    "    - IQ Scores,\n",
    "- Normal distribution follows the 68-95-99.7 rule (empirical rule). \n",
    "    - 68% of data lies in the first standard deviation range, \n",
    "    - 95% of data lies in the second standard deviation range, and \n",
    "    - 99.7% of data lies in the third standard deviation range.\n",
    "\n",
    "Properties of Normal distribution:\n",
    "- The random variable takes values from -∞ to +∞\n",
    "- The probability associate with any single value is Zero.\n",
    "- looks like a bell curve and is symmetric about x=μ. \n",
    "    - 50% of data lies on the left-hand side and \n",
    "    - 50% of the data lies on the right-hand side.\n",
    "- The area under the curve (AUC) = 1\n",
    "- All the measures of central tendency coincide i.e., mean = median = mode\n",
    "\n",
    "|   Term   |     Fromula     |\n",
    "|:-------------------|:---------------:|\n",
    "| PDF | $ \\frac{1}{\\sigma \\sqrt{2\\pi}} e{-\\frac{1}{2}(\\frac{x = \\mu}{\\sigma})^2} $ | \n",
    "| CDF | $ \\frac{1}{2} [ 1 = erf(\\frac{x = \\mu}{\\sigma\\sqrt{2}})$  |\n",
    "| Mean | $ \\mu $  | \n",
    "| Variance | $ \\sigma^2 $  |\n",
    "\n",
    "Case Study: Human Heights\n",
    "- Assume the heights of adults in a population are normally distributed with:\n",
    "    - μ=170 cm (average height).\n",
    "    - σ=10 cm (standard deviation).\n",
    "\n",
    "PDF:\n",
    "- μ=170, \n",
    "- σ=10, and\n",
    "- x=180:\n",
    "\n",
    "$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e{-\\frac{1}{2}(\\frac{x = \\mu}{\\sigma})^2}$$ \n",
    "\n",
    "CDF:\n",
    "- For x = 180, P(X≤180)≈0.841.\n",
    "\n",
    "Mean:\n",
    "- Mean=μ=170,\n",
    "\n",
    "Variance: \n",
    "- Variance= $σ^2$=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ffc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# 1. Define the parameters of the Normal distribution\n",
    "mu = 170  # Mean (average height in cm)\n",
    "sigma = 10  # Standard deviation (spread of height in cm)\n",
    "\n",
    "# 2. Simulate the Normal distribution\n",
    "n_samples = 10000\n",
    "samples = np.random.normal(mu, sigma, size=n_samples)\n",
    "\n",
    "# 3. Calculate the PDF\n",
    "x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 1000)  # Range of values (±4σ)\n",
    "pdf = norm.pdf(x, mu, sigma)\n",
    "\n",
    "# 4. Calculate the CDF\n",
    "cdf = norm.cdf(x, mu, sigma)\n",
    "\n",
    "# 5. Mean and Variance\n",
    "mean = mu  # Mean of a Normal distribution\n",
    "variance = sigma ** 2  # Variance of a Normal distribution\n",
    "\n",
    "# 6. Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# PDF Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, pdf, color='skyblue', label='PDF')\n",
    "plt.title(\"PDF of Normal Distribution\")\n",
    "plt.xlabel(\"Height (cm)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "\n",
    "# CDF Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, cdf, color='orange', label='CDF')\n",
    "plt.title(\"CDF of Normal Distribution\")\n",
    "plt.xlabel(\"Height (cm)\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Print results\n",
    "print(f\"Theoretical Mean: {mean:.2f}\")\n",
    "print(f\"Theoretical Variance: {variance:.2f}\")\n",
    "print(f\"Simulated Mean: {np.mean(samples):.2f}\")\n",
    "print(f\"Simulated Variance: {np.var(samples):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c575b0",
   "metadata": {},
   "source": [
    "# Multivariate Analysis: Non-Graphical \n",
    "\n",
    "### Continuous - Continuous\n",
    "\n",
    "##### **Covariance**\n",
    "\n",
    "Statistical tool that helps to quantify the total variance of random variables from their expected value(Mean).\n",
    "- it is a measure of the linear relationship between two random variables. \n",
    "- It can take any positive and negative values.\n",
    "    - Positive Covariance: \n",
    "        - It indicates that two variables tend to move in the same direction, which means that if we increase the value of one variable other variable value will also increase.\n",
    "    - Zero Covariance: \n",
    "        - It indicates that there is no linear relationship between them.\n",
    "    - Negative Covariance: \n",
    "        - It indicates that two variables tend to move in the opposite direction, which means that if we increase the value of one variable other variable value will decrease and vice versa.\n",
    "\n",
    "Formula:\n",
    "\n",
    "Limitations of Covariance\n",
    " -Covariance magnitude does not signify the strength of their relationship, so what only matters is the sign, whether it is positive or negative which tells the relationship.\n",
    "- If we convert or scale the measurements of the variable X and Y, then Cov(X’, Y’) ≠ Cov(X, Y) should not happen.\n",
    "- Covariance does not capture the non-linear relationship between two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d4dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau, pointbiserialr\n",
    "\n",
    "# Step 1: Simulate Two Random Variables\n",
    "np.random.seed(42)  # For reproducibility\n",
    "n = 100  # Number of samples\n",
    "\n",
    "# Variable X: Continuous Random Variable\n",
    "X = np.random.normal(loc=50, scale=10, size=n)  # Mean=50, Std Dev=10\n",
    "\n",
    "# Variable Y: Continuous Random Variable\n",
    "Y = 0.5 * X + np.random.normal(loc=0, scale=5, size=n)  # Linear relationship with noise\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "data = pd.DataFrame({'X': X, 'Y': Y})\n",
    "\n",
    "# Step 2: Covariance Calculation\n",
    "# Covariance measures how two variables vary together.\n",
    "cov_matrix = np.cov(X, Y)\n",
    "covariance = cov_matrix[0, 1]\n",
    "\n",
    "print(f\"Covariance between X and Y: {covariance:.4f}\")\n",
    "\n",
    "# Step 3: Pearson Correlation Coefficient\n",
    "# Measures linear correlation between X and Y.\n",
    "pearson_corr, pearson_p_value = pearsonr(X, Y)\n",
    "print(f\"Pearson Correlation Coefficient: {pearson_corr:.4f}, p-value: {pearson_p_value:.4f}\")\n",
    "\n",
    "# Step 4: Spearman's Rank Correlation\n",
    "# Measures monotonic relationship between variables.\n",
    "spearman_corr, spearman_p_value = spearmanr(X, Y)\n",
    "print(f\"Spearman's Rank Correlation: {spearman_corr:.4f}, p-value: {spearman_p_value:.4f}\")\n",
    "\n",
    "# Step 5: Kendall's Tau Rank Correlation\n",
    "# Measures ordinal association between variables.\n",
    "kendall_corr, kendall_p_value = kendalltau(X, Y)\n",
    "print(f\"Kendall's Tau Correlation: {kendall_corr:.4f}, p-value: {kendall_p_value:.4f}\")\n",
    "\n",
    "# Step 6: Point Biserial Correlation\n",
    "# Requires one continuous variable and one binary variable.\n",
    "# Simulate a binary variable from X.\n",
    "Z = (X > np.median(X)).astype(int)  # Binary variable based on X's median\n",
    "point_biserial_corr, point_biserial_p_value = pointbiserialr(Z, Y)\n",
    "print(f\"Point Biserial Correlation: {point_biserial_corr:.4f}, p-value: {point_biserial_p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b003ca2",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "For this analysis, we can **determine the relationship between any two numerical features** by calculating the **correlation coefficient**. \n",
    "- Correlation is a measure of the degree to which two variables change together, if at all. \n",
    "    - If two features have a strong positive correlation, it means that if the value of one feature increases, the value of the other feature also increases. \n",
    "    - There are three different correlation measures:\n",
    "        - Pearson correlation \n",
    "        - Spearman rank correlation\n",
    "        - Kendall correlation\n",
    "\n",
    "For this lesson, we will focus on the **Pearson correlation**. The Pearson correlation measures the linear relationship between features and assumes that the features are normally distributed. Below is a table that explains how to interpret the Pearson correlation measure:\n",
    "\n",
    "|   Pearson Correlation Coefficient (r)  |       Description of Relationship     |\n",
    "|:-------------------|:---------------:|\n",
    "|  r = -1              |Perfect Negative Correlation |\n",
    "| -1 < r < -0.8 | Strong Negative Correlation  | \n",
    "| - 0.8 < r < -0.5             | Moderate Negative Correlation  | \n",
    "|       - 0.5 < r < 0     |Weak Negative Correlation  | \n",
    "|       r = 0  |No Linear Correlation | \n",
    "| 0 < r < 0.5 | Weak Positive Correlation  | \n",
    "| 0.5 < r < 0.8             | Moderate Positive Correlation  | \n",
    "|       0.8 < r < 1     |Strong Positive Correlation  | \n",
    "|       r = 1  |Perfect Positive Correlation | \n",
    "\n",
    "\n",
    "<div align=\"left\" style=\"width: 800px; text-align: left;\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/f3aeedd2c056ddd233301c7186063618c1041140/regression_analysis_notebook/pearson_corr.jpg?raw=True\"\n",
    "     alt=\"Pearson Correlation\"\n",
    "     style=\"padding-bottom=0.5em\"\n",
    "     width=800px/>\n",
    "</div>\n",
    "\n",
    "For a more detailed explanation of correlations, read <a href=\"https://medium.com/fintechexplained/did-you-know-the-importance-of-finding-correlations-in-data-science-1fa3943debc2#:~:text=Correlation%20is%20a%20statistical%20measure,to%20forecast%20our%20target%20variable.&text=It%20means%20that%20when%20the,variable(s)%20also%20increases.\">here</a>.\n",
    "\n",
    "The command we will use to determine the correlation between features is:\n",
    "\n",
    "    - df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a15749",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1483a",
   "metadata": {},
   "source": [
    "# Multivariate Analysis: Graphical\n",
    "\n",
    "For the multivariate graphical analysis the following visualisations will be considered:\n",
    "\n",
    "    - Heatmap\n",
    "    - Scatter Plot\n",
    "    - Pair Plot\n",
    "    - Joint Plot\n",
    "    - Bubble Plot\n",
    "    \n",
    "#### Heatmap\n",
    "\n",
    "The relationship between features can also be displayed graphically using a **heatmap**. The Seaborn library will be used for this basic heatmap visualisation. \n",
    "\n",
    "To see how different heatmap variations can be created, read <a href=\"https://medium.com/@szabo.bibor/how-to-create-a-seaborn-correlation-heatmap-in-python-834c0686b88e\">here</a>.\n",
    "\n",
    "The correlation coefficient value will be displayed on the heatmap using the `vmin` and `vmax` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bcf0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6d815",
   "metadata": {},
   "source": [
    "#### Scatter Plot\n",
    "\n",
    "A Scatter plot is used to visualise the relationship between two different features and is most likely the primary multivariate graphical method. For this exercise, we will create a scatter plot to determine if there is a relationship between `bmi` and `age`. The parameter `hue` is set to the feature `insurance_claim`, colouring the points according to whether or not a claim was submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d5bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='age',y='bmi',hue='insurance_claim', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c43d2",
   "metadata": {},
   "source": [
    "#### Pair Plot\n",
    "\n",
    "A pair plot can be used to visualise the relationships between all the numerical features at the same time. \n",
    "\n",
    "The `hue` is once again set to the feature `insurance_claim` to indicate which data points submitted an insurance claim and which didn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afc8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.pairplot(df, hue=\"insurance_claim\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd213a",
   "metadata": {},
   "source": [
    "#### Joint Plot\n",
    "\n",
    "The joint plot can be used to provide univariate and multivariate analyses at the same time. The central part of the plot will be a scatter plot comparing two different features. The top and right visualisations will display the distribution of each feature as a histogram. \n",
    "\n",
    "For this joint plot, we will once again compare `age` and `bmi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5370b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x = 'age', y = 'bmi', data = df)\n",
    "\n",
    "# including the hue as insurance_claim\n",
    "sns.jointplot(x = 'age', y = 'bmi', data = df, hue='insurance_claim')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd4b7a1",
   "metadata": {},
   "source": [
    "#### Bubble Plots\n",
    "\n",
    "A bubble plot is a variation of a scatter plot. Bubbles vary in size, dependent on another feature in the data. The same applies to the colour of the bubbles; which can be set to vary with the values of another feature. This way, we can visualise up to four dimensions/features at the same time.\n",
    "\n",
    "For this bubble plot, `bmi` and `claim_amount` will be plotted on the x-axis and y-axis, respectively. The colours of the bubbles will vary based on whether the observation is a `smoker` or not, and lastly, the size of the bubbles will vary based on the number of `children` the observation has. We will create this bubble plot by using `seaborn`’s scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(x=\"bmi\", \n",
    "                y=\"claim_amount\",\n",
    "                size=\"children\",\n",
    "                sizes=(20,100),\n",
    "                alpha=0.8,\n",
    "                hue=\"smoker\",\n",
    "                data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bcc8a9",
   "metadata": {},
   "source": [
    "## Splitting the Data\n",
    "### Two-Way Split\n",
    "\n",
    "When fitting a machine learning model to some data, we ultimately intend to use that model to make predictions/forecasts on real-world data. \n",
    "- Real-world data is unseen - it doesn't exist in the dataset we have at our disposal - so in order to validate our model (check how well it performs), we need to test it on unseen data too.\n",
    "- Gathering unseen data is not as simple as collecting it from outside the window and exposing it to the model: any new data would need to be \n",
    "    - cleaned, \n",
    "    - wrangled and \n",
    "    - annotated just like the data in our dataset.\n",
    "- The next best thing, then, is to simulate some unseen data, which we can do using the existing dataset by splitting it into two sets:\n",
    "    - One for training the model; and\n",
    "    - A second for testing it.\n",
    "   \n",
    "We fit a model using the training data, and then assess its accuracy using the test set.\n",
    "- use 80% of the data for training and \n",
    "    - the training set will contain 80% of the rows, or data points,\n",
    "- keep 20% aside for testing. \n",
    "    - and the remaining 20% of rows will be in the test set.\n",
    "These rows are selected at random, to ensure that the mix of data in the train set is as close as possible to the mix in the test set.\n",
    "\n",
    "### Three-Way Split\n",
    "\n",
    "Many academic works on machine learning talk about splitting the dataset into three distinct parts: \n",
    "- `train`, \n",
    "    - training set is used to fit the model to the observations.\n",
    "- `validation,` and\n",
    "    -  during the model tuning process where hyperparameters are tweaked and decisions on the dataset is made, the validation set is used to test the performance of the model.\n",
    "- `test` sets. \n",
    "    - Once the model designer is satisfied with the performance of the model on the validation set, the previously unseen test set is brought out and used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "#### Caveats for using a validation set\n",
    "\n",
    "On small datasets, it may not be feasible to include a validation set for the following reasons, both of which should be intuitive:\n",
    "\n",
    "- The model may need every possible data point to adequately determine model values;\n",
    "- For small enough test sets, the uncertainty of the test set can be considerably large to the point where different test sets may produce very different results.\n",
    "\n",
    "Clearly, further splitting the training data into training and validation sets would remove precious observations for the training process.\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "In the case that the designer does not desire to use a validation set, or there is simply not enough data, \n",
    "- a technique known as cross validation may be used. \n",
    "A common version of cross validation is known as K-fold cross validation: \n",
    "- during the training process, some proportion of the training data, say 10%, is held back, and effectively used as a validation set while the model parameters are calcuated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import seaborn as sns\n",
    "\n",
    "# Import the split function from sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into the response, y, and features, X\n",
    "y = df['ZAR/USD']\n",
    "X = df.drop('ZAR/USD', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26658ff9",
   "metadata": {},
   "source": [
    "Understand the four parameters to hand to the splitting function.\n",
    "\n",
    "- `X` contains the features on which we will be training the model. In this case: just `exports`;\n",
    "- `y` is the response variable, that which we are trying to predict. In this case: `exchange rate`;\n",
    "- `test_size` is a value between 0 and 1: the proportion of our dataset that we want to be used as test data. Typically 0.2 (20%);\n",
    "- `random_state` is an arbitrary value which, when set, ensures that the _random_ nature in which rows are picked to be in the test set is the same each time the split is carried out. In other words, the rows are picked at random, but we can ensure these random picks are repeatable by using the same value here. This makes it easier to assess model performance across iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02311b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Call the train_test_split function:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd317a",
   "metadata": {},
   "source": [
    "Plotting the data points in each of the training and testing sets in different colours, we should be able to see that we have a similar spread of data in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1299f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.scatter(X_train, y_train, color='green', label='Training')  # plot the training data in green\n",
    "plt.scatter(X_test, y_test, color='darkblue', label='Testing')  # plot the testing data in blue\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d204818a",
   "metadata": {},
   "source": [
    "## Advanced plotting\n",
    "Let's try and create something a little more visually appealing than the two plots above.\n",
    "​\n",
    "- We'll plot both dependent data series on the same graph;\n",
    "- We'll assign two separate y-axes: one for each series;\n",
    "- We'll display a legend near the top of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc('mathtext', default='regular')\n",
    "# Create blank figure\n",
    "fig = plt.figure()\n",
    "\n",
    "# Split figure to allow two sets of y axes\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot the first line on its axis\n",
    "ax.plot(np.arange(len(df.Y)), df.Y, '-', label = 'ZAR/USD', color='orange')\n",
    "\n",
    "# Create second y axis and plot second line\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(np.arange(len(df.X)), df.X, '-', label = 'Exports (ZAR)')\n",
    "\n",
    "# Add legends for each axis\n",
    "ax.legend(loc=2)\n",
    "ax2.legend(loc=9)\n",
    "\n",
    "ax.grid()\n",
    "\n",
    "# Set labels of axes\n",
    "ax.set_xlabel(\"Months\")\n",
    "ax.set_ylabel(\"ZAR/USD\")\n",
    "ax2.set_ylabel(\"Exports (ZAR, millions)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda9b41d",
   "metadata": {},
   "source": [
    "### Step 3: Select the Type of Statistical Model\n",
    "Statistical models can be broadly categorized as:\n",
    "\n",
    "- **Descriptive Models**: Summarize data patterns.\n",
    "- **Inferential Models**: Help make inferences about the population.\n",
    "- **Predictive Models**: Used to predict future outcomes based on historical data.\n",
    "- **Prescriptive Models**: Suggest actions based on predictions.\n",
    "\n",
    "Let's go through common types of statistical models and their applications.\n",
    "\n",
    "# Regression Analysis\n",
    " \n",
    "Regression Analysis is a statistical method to analyze the relationship between a dependent variable and one or more independent variables.\n",
    "\n",
    "Use regression analysis for one of two purposes: \n",
    "- predict the value of the dependent variable when you know the independent variables or \n",
    "- predict the effect of an independent variable on the dependent variable.\n",
    "\n",
    "### Three types of regression analysis\n",
    "\n",
    "- **Simple linear regression**\n",
    "    - Assumes a linear connection between a dependent variable (Y) and an independent variable (X).\n",
    "    - linear regression model can be simple \n",
    "        - with only one dependent and one independent variable.\n",
    "    - A real estate agent wants to determine the relationship between the size of a house (in square feet) and its selling price. They can use simple linear regression to predict the selling price of a house based on its size.\n",
    "    \n",
    "-  **Multiple Linear Regression / Multivariate Linear Regression**\n",
    "    - Assumes a linear connection between a dependent variable (Y) and an independent variable (X).\n",
    "    - linear regression model can be complex \n",
    "        - with numerous dependent and independent variables\n",
    "        - with one dependent variable and more than one independent variable.\n",
    "    - A car manufacturer wants to predict the fuel efficiency of their vehicles based on various independent variables such as engine size, horsepower, and weight.\n",
    "    \n",
    "- **Logistic regression**\n",
    "    - Used When the dependent variable is discrete.\n",
    "        - the target variable can take on only one of two values, \n",
    "    - The sigmoid curve represents its connection to the independent variable, and probability has a value between 0 and 1.\n",
    "    - A bank wants to predict whether a customer will default on their loan based on their credit score, income, and other factors. By using logistic regression, the bank can estimate the probability of default and take appropriate measures to minimize their risk.\n",
    "\n",
    "- **Polynomial Regression**\n",
    "    - Represents a non-linear relationship between dependent and independent variables. \n",
    "    - This technique is a variant of the multiple linear regression model, but the best fit line is curved rather than straight.\n",
    "\n",
    "- **Ridge Regression**\n",
    "    - Applied when the independent variables are highly correlated.\n",
    "        - When data exhibits multicollinearity\n",
    "    - While least squares estimates are unbiased in multicollinearity, their variances are significant enough to cause the observed value to diverge from the actual value. \n",
    "    - Ridge regression reduces standard errors by biassing the regression estimates.\n",
    "    - The lambda (λ) variable in the ridge regression equation resolves the multicollinearity problem.\n",
    "\n",
    "- **Lasso Regression**\n",
    "    - Lasso regression (Least Absolute Shrinkage and Selection Operator) technique penalizes the absolute magnitude of the regression coefficient. \n",
    "    - The lasso regression technique employs variable selection, which leads to the shrinkage of coefficient values to absolute zero.\n",
    "\n",
    "- **Quantile Regression**\n",
    "    - The quantile regression approach is a subset of the linear regression technique. \n",
    "    - Statisticians and econometricians employ quantile regression when linear regression requirements are not met or when the data contains outliers.\n",
    "\n",
    "- **Bayesian Linear Regression**\n",
    "    - Machine learning utilizes Bayesian linear regression, a form of regression analysis, to calculate the values of regression coefficients using Bayes’ theorem. \n",
    "    - Rather than determining the least-squares, this technique determines the features’ posterior distribution.\n",
    "    - The approach outperforms ordinary linear regression in terms of stability. \n",
    "\n",
    "- **Principal Components Regression**\n",
    "    - Multicollinear regression data is often evaluated using the principle components regression approach. \n",
    "    - The significant components regression approach, like ridge regression, reduces standard errors by biassing the regression estimates. \n",
    "    - First, principal component analysis (PCA) modifies the training data, and then the resulting transformed samples train the regressors.\n",
    "\n",
    "- **Partial Least Squares Regression**\n",
    "    - The partial least squares regression technique is a fast and efficient covariance-based regression analysis technique. \n",
    "    - It is advantageous for regression problems with many independent variables with a high probability of multicollinearity between the variables. \n",
    "    - The method reduces the number of variables to a manageable number of predictors, then uses them in regression.\n",
    "\n",
    "- **Elastic Net Regression**\n",
    "    - Elastic net regression combines ridge and lasso regression techniques that are particularly useful when dealing with strongly correlated data. \n",
    "    - It regularizes regression models by utilizing the penalties associated with the ridge and lasso regression methods.\n",
    "\n",
    "\n",
    "### Complete Workflow for Regression Modeling\n",
    "Steps of a regression modeling process, covering:\n",
    "- Exploratory Data Analysis (EDA), \n",
    "- assumption checking, \n",
    "- data transformations, \n",
    "- model fitting, and \n",
    "- interpretation.\n",
    "\n",
    "**Step 1: Problem Definition and Data Understanding**\n",
    "\n",
    "1. Define the Problem:\n",
    "- Identify the dependent (response) variable and independent (predictor) variables.\n",
    "- Clarify objectives\n",
    "    - prediction, \n",
    "    - inference,\n",
    "    - explanation.\n",
    "2. Understand the Data:\n",
    "- Review the dataset's structure, variable types, and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0069f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "def missing_values_table(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns\n",
    "\n",
    "train.head()\n",
    "train.info()\n",
    "train.shape\n",
    "\n",
    "missing_values_table(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37425f26",
   "metadata": {},
   "source": [
    "**Step 2: Exploratory Data Analysis (EDA)**\n",
    "\n",
    "1. Summary Statistics:\n",
    "Compute \n",
    "- mean, \n",
    "- median, \n",
    "- standard deviation, and \n",
    "- correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1eef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())\n",
    "print(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea9c95c",
   "metadata": {},
   "source": [
    "2. Visualization:\n",
    "- Histogram for distributions.\n",
    "- Scatter plots for relationships.\n",
    "- Box plots for detecting outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.pairplot(df, diag_kind=\"kde\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f3dfc1",
   "metadata": {},
   "source": [
    "3. Check Multicollinearity:\n",
    "\n",
    "- Compute the Variance Inflation Factor (VIF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13675dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X = df[[\"SquareFootage\", \"Bedrooms\", \"LocationIndex\"]]\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Features\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845874c3",
   "metadata": {},
   "source": [
    "**Step 3: Preprocessing and Transformations**\n",
    "\n",
    "1. Handle Missing Data:\n",
    "- Impute missing values or drop rows/columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50699106",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea404802",
   "metadata": {},
   "source": [
    "2. Encode Categorical Variables:\n",
    "- Use one-hot encoding or label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c677c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=[\"Location\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a60f544",
   "metadata": {},
   "source": [
    "3. Feature Scaling:\n",
    "- Standardize or normalize numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe495bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[[\"SquareFootage\", \"Bedrooms\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ada11e",
   "metadata": {},
   "source": [
    "4. Transform Non-linear Relationships:\n",
    "- Apply log, Box-Cox, or square root transformations for skewed variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08eeddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "df[\"LogPrice\"] = np.log(df[\"Price\"])\n",
    "df[\"BoxCoxPrice\"], _ = boxcox(df[\"Price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01549fc3",
   "metadata": {},
   "source": [
    "**Step 4: Model Fitting and Assumption Checking**\n",
    "1. Fit the Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = sm.add_constant(df[[\"SquareFootage\", \"Bedrooms\"]])  # Add intercept\n",
    "Y = df[\"Price\"]\n",
    "model = sm.OLS(Y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860b0ea4",
   "metadata": {},
   "source": [
    "2. Check Model Assumptions:\n",
    "\n",
    "(a) Linearity: \n",
    "\n",
    "Residuals vs. Fitted Plot: Look for randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec39770",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(model.fittedvalues, model.resid)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.title(\"Residuals vs. Fitted\")\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809e320",
   "metadata": {},
   "source": [
    "(b) Normality of Residuals:\n",
    "\n",
    "Use a histogram and Q-Q plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41cb69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "stats.probplot(model.resid, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c4b1d",
   "metadata": {},
   "source": [
    "(c) Homoscedasticity:\n",
    "\n",
    "Breusch-Pagan test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a426d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "bp_test = het_breuschpagan(model.resid, X)\n",
    "print(f\"p-value: {bp_test[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8593c4",
   "metadata": {},
   "source": [
    "(d) Multicollinearity:\n",
    "\n",
    "Variance Inflation Factor (VIF) as shown earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa55a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X = df[[\"SquareFootage\", \"Bedrooms\", \"LocationIndex\"]]\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Features\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02246b",
   "metadata": {},
   "source": [
    "**Step 5: Address Issues and Refine the Model**\n",
    "\n",
    "1. Linearity:\n",
    "\n",
    "Transform variables if residual plots show non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc058419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SquareFootage_Sq\"] = df[\"SquareFootage\"] ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eadd8c",
   "metadata": {},
   "source": [
    "2. Non-Normal Residuals:\n",
    "\n",
    "Apply transformations to the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b372eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"LogPrice\"] = np.log(df[\"Price\"])\n",
    "model_log = sm.OLS(df[\"LogPrice\"], X).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce33f90",
   "metadata": {},
   "source": [
    "3. Heteroscedasticity:\n",
    "\n",
    "Use Weighted Least Squares (WLS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e853e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 1 / (model.resid ** 2)\n",
    "model_wls = sm.WLS(Y, X, weights=weights).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c67352",
   "metadata": {},
   "source": [
    "4. Multicollinearity:\n",
    "- Drop or combine highly correlated variables.\n",
    "- Use PCA, Ridge, or Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a65cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31127c0d",
   "metadata": {},
   "source": [
    "**Step 6: Evaluate Model Performance**\n",
    "- Metrics: \n",
    "    - $𝑅^2$\n",
    "    - Adjusted $𝑅^2$\n",
    "    - RMSE, \n",
    "    - MAE.\n",
    "\n",
    "- Residual Plots:\n",
    "    - Confirm residuals are normally distributed and homoscedastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad0522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(Y, model.predict(X)))\n",
    "mae = mean_absolute_error(Y, model.predict(X))\n",
    "print(f\"RMSE: {rmse}, MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1f7fc7",
   "metadata": {},
   "source": [
    "**Step 7: Interpretation and Communication**\n",
    "\n",
    "Coefficient Interpretation:\n",
    "- For each predictor, interpret its coefficient in terms of the dependent variable.\n",
    "\n",
    "Confidence Intervals:\n",
    "- Report 95% confidence intervals for coefficients.\n",
    "\n",
    "Visualize Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bacc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.regplot(x=\"SquareFootage\", y=\"Price\", data=df, line_kws={\"color\": \"red\"})\n",
    "plt.title(\"Regression Line: Square Footage vs. Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b548c4",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear Regression is a supervised learning algorithm used to model the relationship between a dependent variable (outcome) and one or more independent variables (predictors).\n",
    "- Predicts the relationship between two variables by assuming they have a straight-line connection. \n",
    "\n",
    "Linear Regression predicts a continuous target variable (e.g., the number of readmissions) by minimizing the residual sum of squares between observed and predicted values.\n",
    "- It finds the best line that minimizes the differences between predicted and actual values.\n",
    "\n",
    "## 1. Simple Linear Regression\n",
    "\n",
    "In a simple linear regression, there is \n",
    "- one independent variable and \n",
    "- one dependent variable. \n",
    "\n",
    "The model estimates the slope and intercept of the line of best fit, which represents the relationship between the variables. \n",
    "- The slope represents the change in the dependent variable for each unit change in the independent variable, while \n",
    "- The intercept represents the predicted value of the dependent variable when the independent variable is zero.\n",
    "\n",
    "What It Means: \n",
    "- Linear regression models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. \n",
    "- It assumes a straight-line relationship. \n",
    "- It shows the linear relationship between the independent(predictor) variable i.e. X-axis and the dependent (output) variable i.e. Y-axis, \n",
    "    - called linear regression.\n",
    "\n",
    "- It is employed to establish a link between a dependant variable and a single independent variable. \n",
    "    - A linear equation defines the relationship, with the \n",
    "        - slope and \n",
    "        - intercept \n",
    "    - of the line representing the effect of the independent variable on the dependant variable.\n",
    "        - An independent variable is the variable that is controlled in a scientific experiment to test the effects on the dependent variable.\n",
    "        - A dependent variable is the variable being measured in a scientific experiment.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each coefficient represents how much the dependent variable (outcome) changes when the predictor variable changes by one unit, keeping all else constant.\n",
    "\n",
    "**Assumptions of Linear Regression**\n",
    "\n",
    "Regression is a parametric approach, which means that it makes assumptions about the data\n",
    "\n",
    "For successful regression analysis, it’s essential to validate the following assumptions.\n",
    "\n",
    "- Linearity (Linear Relationship): The relationship between the predictors and the outcome is linear.\n",
    "    - Plot dependent variable and independent variable(s) and see linear relationship.\n",
    "- Independence of Errors: Residuals (errors) are independent of each other.\n",
    "    - The error terms should not be dependent on one another (like in time-series data wherein the next value is dependent on the previous one). \n",
    "    - There should be no correlation between the residual terms.\n",
    "    - The absence of this phenomenon is known as Autocorrelation.\n",
    "- No or Little Autocorrelation\n",
    "- Normality of Errors: Residuals are normally distributed.\n",
    "    - The mean of residuals should follow a normal distribution with a mean equal to zero or close to zero. \n",
    "    - This is done to check whether the selected line is the line of best fit or not. \n",
    "    - If the error terms are non-normally distributed, suggests that there are a few unusual data points that must be studied closely to make a better model.\n",
    "- Multivariate Normality\n",
    "- No or Little Multicollinearity\n",
    "- Homoscedasticity: Variance of residuals is constant across all levels of predictors.\n",
    "    - The error terms must have constant variance. \n",
    "    - The presence of non-constant variance in the error terms is referred to as Heteroscedasticity. \n",
    "\n",
    "Performance Measures:\n",
    "- R-squared: Indicates the proportion of the variance in the dependent variable explained by the independent variables. \n",
    "    - Values closer to 1 indicate a better fit.\n",
    "- Mean Squared Error (MSE): The average squared difference between observed and predicted values; lower values are better.\n",
    "\n",
    "Lay Explanation: \n",
    "- Think of linear regression like drawing a best-fit line through a scatterplot of data points, aiming to predict outcomes based on relationships in the data.\n",
    "- Finds a relationship between independent and dependent variables by finding a “best-fitted line” that has minimal distance from all the data points.\n",
    "- The algorithm explains the linear relationship between the dependent(output) variable y and the independent(predictor) variable X using a straight line\n",
    "\n",
    "Use Case: \n",
    "- When there is a linear relationship between the target and predictor variables.\n",
    "\n",
    "### Mathematics or Linear Regression\n",
    "\n",
    "- it is using the least square method finds a linear equation that minimizes the sum of squared residuals (SSR).\n",
    "- Cost Function:\n",
    "\n",
    "$ J(\\theta) = \\frac{1}{2m}\\sum^{m}_{i=1}(h_{\\theta}(x^{(i)})- y^{(i)})^{2}$\n",
    "\n",
    "Model Equation:\n",
    "$ 𝑦=𝛽_{0}+𝛽_{1}𝑥_{1}+…+𝛽_{𝑛}𝑥_{𝑛}+ 𝜖 $\n",
    "\n",
    "where:\n",
    "- $y$ = dependent variable\n",
    "- $𝛽_{0}$ = Y intercept / constant\n",
    "- $𝛽_{1}$ = Slope coefficient / intercept\n",
    "- $𝑥_{1}$ = independent variable\n",
    "- $𝜖 $ = error term\n",
    "\n",
    "**What is Cost Function ?**\n",
    "\n",
    "The goal of the linear regression algorithm is to get the best values for $𝛽_{0}+𝛽_{1}$ to find the **best-fit line**.\n",
    "- is a line that has the least error which means the error between predicted values and actual values should be minimum.\n",
    "\n",
    "A cost function, also referred to as a: \n",
    "- loss function : Used when we refer to the error for a single training example. \n",
    "- objective function : Used to refer to an average of the loss functions over an entire training dataset.\n",
    "It quantifies the difference between predicted and actual values, serving as a metric to evaluate the performance of a model.\n",
    "\n",
    "Objective \n",
    "- is to minimize the cost function, indicating better alignment between predicted and observed outcomes.\n",
    "- Guides the model towards optimal predictions by measuring its accuracy against the training data.\n",
    "\n",
    "AKA - Random Error (Residuals)\n",
    "- the difference between the observed value of the dependent variable($y_{i}$) and the predicted value(predicted) is called the residuals.\n",
    "    - $𝜖_{i}$ =  $y_{predicted}  –  y_{i}$\n",
    "\n",
    "where $𝑦_{predicted} = 𝛽_{0}+𝛽_{1}𝑥_{1}+…+𝛽_{𝑛}𝑥_{𝑛}+ 𝜖 $\n",
    "\n",
    "**Why to use a Cost function**\n",
    "\n",
    "Cost function helps us reach the optimal solution / work out the optimal values for $𝛽_{0}+𝛽_{1}$ . \n",
    "- How: It takes both predicted outputs by the model and actual outputs and calculates how much wrong the model was in its prediction.\n",
    "    - It basically measures the discrepancy between the model’s predictions and the true values it is attempting to predict. \n",
    "    - This variance is depicted as a lone numerical figure, enabling us to measure the model’s **precision**.\n",
    "- The cost function is the technique of evaluating “the performance of our algorithm/model”.\n",
    "\n",
    "Classifiers have very high accuracy but one solution (Classifier) is the best because it does not misclassify any point.\n",
    "- Reason why it classifies all the points perfectly is that the:\n",
    "    - line is almost exactly in between the two (n) groups, and not closer to any one of the groups.\n",
    "\n",
    "Explanation of the function of a cost function:\n",
    "\n",
    "- Error calculation: It determines the difference between the predicted outputs (what the model predicts as the answer) and the actual outputs (the true values we possess for the data).\n",
    "- Gives one value: This simplifies comparing the model’s performance on various datasets or training rounds.\n",
    "- Improving Guides: The objective is to reduce the cost function. \n",
    "    - How: Through modifying the internal parameters of the model such as weights and biases, we can aim to minimize the total error and enhance the accuracy / precision of the model.\n",
    "\n",
    "**Types of Cost function in machine learning**\n",
    "\n",
    "Its use cases depend on whether it is a regression problem or classification problem.\n",
    "- Regression cost Function\n",
    "- Binary Classification cost Functions\n",
    "- Multi-class Classification cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8576db",
   "metadata": {},
   "source": [
    "### Problem Context: Predicting Hospital Readmission Rates\n",
    "The aim to reduce hospital readmission rates. \n",
    "- High readmission rates can strain resources and negatively impact patient outcomes.\n",
    "- The goal is to predict the number of readmissions within 30 days of discharge for a particular condition, such as \n",
    "    - diabetes, based on \n",
    "        - patient demographic, \n",
    "        - clinical data, and \n",
    "        - treatment data.\n",
    "\n",
    "**Step 1. Define the Problem**\n",
    "\n",
    "We want to predict the number of readmissions ($𝑌$) using features ($𝑋$) such as:\n",
    "- Patient age\n",
    "- Length of hospital stay\n",
    "- Severity of condition\n",
    "- Medication adherence rate\n",
    "- Comorbidities (e.g., hypertension, kidney disease)\n",
    "- Number of follow-up visits scheduled\n",
    "\n",
    "**Step 2. Collect and Prepare Data**\n",
    "\n",
    "- Data Collection: Gather historical patient data from the hospital's database.\n",
    "- Understand the \n",
    "    - model description\n",
    "    - causality and \n",
    "    - directionality\n",
    "- Check the data\n",
    "    - categorical data, \n",
    "    - missing data and \n",
    "    - outliers\n",
    "- Data Cleaning: \n",
    "    - Dummy variable takes only the value 0 or 1 to indicate the effect for categorical variables.\n",
    "    - Handle missing values, \n",
    "    - remove duplicates, and \n",
    "    - correct errors.\n",
    "    - Outlier is a data point that differs significantly from other observations. \n",
    "        - use standard deviation method and \n",
    "        - interquartile range (IQR) method.\n",
    "- Feature Engineering: \n",
    "    - Encode categorical variables (e.g., age group), \n",
    "    - scale continuous variables (e.g., length of stay), and \n",
    "    - create interaction terms if necessary.\n",
    "\n",
    "**Step 3. Conduct a Simple Analysis**\n",
    "- Check the **effect** comparing between \n",
    "    - Dependent variable to independent variable and \n",
    "    - Independent variable to independent variable\n",
    "- Check the correlation.\n",
    "    - Use scatter plots\n",
    "- Check Multicollinearity \n",
    "    - This occurs when more than two independent variables are highly correlated. \n",
    "    - Use Variance Inflation Factor (VIF) \n",
    "        - if VIF > 5 there is highly correlated and \n",
    "        - if VIF > 10 there is certainly multicollinearity among the variables.\n",
    "- Interaction Term imply a change in the slope from one value to another value.\n",
    "\n",
    "`Show the relationship between the two variables using a scatter plot.`\n",
    "- We have our Y, our X, and time (months), but we're just trying to model ZAR/USD as a *function* of Exports. \n",
    "    - To see if we can see that there possibly exists a linear relationship between the two variables: Value of Exports and ZAR/USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0754546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['X'], df['Y'])\n",
    "plt.ylabel(\"ZAR/USD\")\n",
    "plt.xlabel(\"Value of Exports (ZAR, millions)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56d3e0",
   "metadata": {},
   "source": [
    "**Step 4. Formulate the Model (From Scratch)**\n",
    "- y in this equation stands for the predicted value, \n",
    "- x means the independent variable and \n",
    "- m & b are the **coefficients** we need to optimize in order to fit the regression line to our data.\n",
    "\n",
    "#### Finding the Best Fit Line\n",
    "Let's say we have estimated some values for $a$ and $b$. We could plug in all of our values of X to find the corresponding values of Y. These *new* values of Y could be compared to the *actual* values of Y to assess the fit of the line. This becomes tedious as the number of data points increases.\n",
    "   \n",
    "Looking at the data, we can make a guess at the values of the slope and intercept of the line. We'll use a rough estimate of the slope as $\\frac{rise}{run} = \\frac{16}{80000} = 0.0002$. For the intercept, we'll just take a guess and call it $-3$.   \n",
    "   \n",
    "Let's plot a line with values of $a = -3$, and $b = 0.0002$:   \n",
    "   \n",
    "First, we will need to generate some values of y using the following formula:\n",
    "   \n",
    "$$\\hat{y}_i = a + bx_i$$   \n",
    "\n",
    "\n",
    "\n",
    "Calculating coefficient of the equation:\n",
    "- To calculate the coefficients we need the formula for \n",
    "\n",
    "Covariance \n",
    "\n",
    "$Cov (X,Y) = \\frac{\\sum (X_{i}- X)(Y_{j} - Y)}{n}$\n",
    "\n",
    "Variance\n",
    "\n",
    "$var(x) = \\frac{\\sum^{n}_{i} (x_i -\\mu)^2}{N}$\n",
    "\n",
    "- To calculate the coefficient m\n",
    "    - m = cov(x, y) / var(x)\n",
    "    - b = mean(y) — m * mean(x)\n",
    "\n",
    "**Functions to calculate the Mean, Covariance, and Variance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c74157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean \n",
    "def get_mean(arr):\n",
    "    return np.sum(arr)/len(arr)\n",
    "\n",
    "# variance\n",
    "def get_variance(arr, mean):\n",
    "    return np.sum((arr-mean)**2)\n",
    "\n",
    "# covariance\n",
    "def get_covariance(arr_x, mean_x, arr_y, mean_y):\n",
    "    final_arr = (arr_x - mean_x)*(arr_y - mean_y)\n",
    "    return np.sum(final_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8932f",
   "metadata": {},
   "source": [
    "**Fuction to calculate the coefficients and the Linear Regression Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients \n",
    "# m = cov(x, y) / var(x)\n",
    "# b = y - m*x\n",
    "\n",
    "def get_coefficients(x, y):\n",
    "    x_mean = get_mean(x)\n",
    "    y_mean = get_mean(y)\n",
    "    m = get_covariance(x, x_mean, y, y_mean)/get_variance(x, x_mean)\n",
    "    b = y_mean - x_mean*m\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e179ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression \n",
    "# Train and Test\n",
    "# Train Split 80 % Test Split 20 %\n",
    "def linear_regression(x_train, y_train, x_test, y_test):\n",
    "    prediction = []\n",
    "    m, b = get_coefficients(x_train, y_train)\n",
    "    for x in x_test:\n",
    "        y = m*x + b\n",
    "        prediction.append(y)\n",
    "    \n",
    "    r2 = r2_score(prediction, y_test)\n",
    "    mse = mean_squared_error(prediction, y_test)\n",
    "    print(\"The R2 score of the model is: \", r2)\n",
    "    print(\"The MSE score of the model is: \", mse)\n",
    "    return prediction\n",
    "\n",
    "prediction = linear_regression(x[:80], y[:80], x[80:], y[80:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate values of y from a list of x, \n",
    "# Given parameters a and b\n",
    "\n",
    "def gen_y(x_list, a, b):\n",
    "    y_gen = []\n",
    "    for x_i in x_list:\n",
    "        y_i = a + b*x_i\n",
    "        y_gen.append(y_i)\n",
    "    \n",
    "    return(y_gen)\n",
    "\n",
    "# Generate the values by invoking the 'gen_y' function\n",
    "y_gen = gen_y(df.X, -3, 0.0002)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(df.X, df.Y)  # Plot the original data\n",
    "plt.plot(df.X, y_gen, color='red')  # Plot the line connecting the generated y-values\n",
    "plt.ylabel(\"ZAR/USD\")\n",
    "plt.xlabel(\"Value of Exports (ZAR, millions)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ec0333",
   "metadata": {},
   "source": [
    "**Visualize the regression line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb4088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reg_line(x, y):\n",
    "    # Calculate predictions for x ranging from 1 to 100\n",
    "    prediction = []\n",
    "    m, c = get_coefficients(x, y)\n",
    "    for x0 in range(1,100):\n",
    "        yhat = m*x0 + c\n",
    "        prediction.append(yhat)\n",
    "    \n",
    "    # Scatter plot without regression line\n",
    "    fig = plt.figure(figsize=(20,7))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.scatterplot(x=x, y=y)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Scatter Plot between X and Y')\n",
    "    \n",
    "    # Scatter plot with regression line\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.scatterplot(x=x, y=y, color = 'blue')\n",
    "    sns.lineplot(x = [i for i in range(1, 100)], y = prediction, color='red')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Regression Plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5bc7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression plot form seaborn\n",
    "# regplot is basically the combination of the scatter plot and the line plot\n",
    "sns.regplot(x, y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title(\"Regression Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aeea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reg_line(x, y):\n",
    "    # Calculate predictions for x ranging from 1 to 100\n",
    "    prediction = []\n",
    "    m, c = get_coefficients(x, y)\n",
    "    for x0 in range(1,100):\n",
    "        yhat = m*x0 + c\n",
    "        prediction.append(yhat)\n",
    "    \n",
    "    # Scatter plot without regression line\n",
    "    fig = plt.figure(figsize=(20,7))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.scatterplot(x=x, y=y)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Scatter Plot between X and Y')\n",
    "    \n",
    "    # Scatter plot with regression line\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.scatterplot(x=x, y=y, color = 'blue')\n",
    "    sns.lineplot(x = [i for i in range(1, 100)], y = prediction, color='red')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Regression Plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8247ed70",
   "metadata": {},
   "source": [
    "**Step 4. Formulate the model and Fit the Model (using library)**\n",
    "\n",
    "- Split the Data: Divide data into training and testing sets (e.g., 80% training, 20% testing).\n",
    "- Train the Model: Use a library like sklearn in Python to fit the regression model on the training data.\n",
    "- Evaluate the Model: Check metrics such as $𝑅^2$ (explained variance) and RMSE (Root Mean Squared Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Example\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create the dataset\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
    "y = np.array([2, 4, 5, 7, 8, 10, 11, 13, 14, 16])\n",
    "\n",
    "# Create the linear regression model\n",
    "model = LinearRegression().fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc758991",
   "metadata": {},
   "source": [
    "##### Calculate the Regression Coefficients\n",
    "\n",
    "Use the formulas for $𝛽_1$ (slope) and $𝛽_0$ (intercept):\n",
    "\n",
    "$𝛽_1 = \\frac{\\sum (x_{i}- \\bar{x})(y_{j} - \\bar{y})}{\\sum (x_{i}- \\bar{x})^2}$\n",
    "\n",
    "$𝛽_0 = \\bar{y} - 𝛽_1 \\bar{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d5d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of x and y from scratch \n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "\n",
    "# Calculating beta1 (slope)\n",
    "numerator = np.sum((x - x_mean) * (y - y_mean))\n",
    "denominator = np.sum((x - x_mean) ** 2)\n",
    "beta1 = numerator / denominator\n",
    "\n",
    "# Calculating beta0 (intercept)\n",
    "beta0 = y_mean - beta1 * x_mean\n",
    "\n",
    "print(f\"Beta0 (Intercept): {beta0:.3f}\")\n",
    "print(f\"Beta1 (Slope): {beta1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the slope and intercept of the line\n",
    "slope = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "# Plot the data points and the regression line\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, slope*X + intercept, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc4414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Example dataset\n",
    "X = data[['age', 'length_of_stay', 'severity', 'medication_adherence', 'comorbidities']]\n",
    "y = data['readmissions']\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse}, R^2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7183956e",
   "metadata": {},
   "source": [
    "**Let's check the calculted fit of the line** by measuring how far the true y-values of each point are from their corresponding y-value on the line.   \n",
    "   \n",
    "We'll use the equation below to calculate the error of each generated value of y:   \n",
    "   \n",
    "$$e_i = y_i - \\hat{y}_i$$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf61017",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.array(df.Y - y_gen)\n",
    "np.round(errors, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6a59c8",
   "metadata": {},
   "source": [
    "In addition to having some very large errors, we can also see that most of the errors are positive numbers. Ideally, we want our errors to be evenly distributed either side of zero - we want our line to best fit the data, i.e. no bias.\n",
    "   \n",
    "We can measure the overall error of the fit by calculating the **Residual Sum of Squares**:\n",
    "   \n",
    "$$RSS = \\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "##### Residual Sum of Squares (RSS)\n",
    "Definition: The Residual Sum of Squares (RSS) measures the discrepancy between the actual data points and the estimated values predicted by a regression model. It is calculated as the sum of the squared differences between actual ($𝑦_𝑖$) and predicted ($\\hat{y}_𝑖 $) values.\n",
    "\n",
    "The RSS finds the difference between the y-value of each data point and our estimated line (which may be either negative or positive), squares the difference, and then adds all the differences up. In other words, it's the sum of the squares of all the errors we calculated before.\n",
    "\n",
    "Here:\n",
    "\n",
    "- $𝑦_𝑖$ = Actual value of the dependent variable for observation 𝑖.\n",
    "- $\\hat{y}_𝑖 = 𝛽_0 + 𝛽_1 𝑥_𝑖$ , where:\n",
    "    - $𝛽_0$ is the intercept.\n",
    "    - $𝛽_1$ is the slope of the regression line.\n",
    "    - $𝑥_𝑖$ is the value of the independent variable for observation 𝑖.\n",
    "\n",
    "Substituting $\\hat{y}_𝑖$:\n",
    "\n",
    "$$RSS = \\sum_{i=1}^n(y_i-(𝛽_0 + 𝛽_1 𝑥_𝑖))^2$$\n",
    "\n",
    "The RSS quantifies the \"unexplained variance\" by the model.\n",
    "\n",
    "In a simple linear regression, minimizing RSS is equivalent to finding the best-fit line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e28d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual sum of squares from scratch\n",
    "rss = np.sum((y - y_pred) ** 2)\n",
    "print(f\"Residual Sum of Squares (RSS): {rss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb521f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Residual sum of squares:\", (errors ** 2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784bd8a",
   "metadata": {},
   "source": [
    "## Least Squares Method\n",
    "Least Squares is another method that allows us to find the line of best fit while enforcing the constraint of minimising the residuals. More specifically, the **Least Squares Criterion** states that the sum of the squares of the residuals should be minimized, i.e.   \n",
    "$$Q = \\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "The formulae for the intercept, $a$, and the slope, $b$, are determined by minimizing the equation for the sum of the squared prediction errors:   \n",
    "$$Q = \\sum_{i=1}^n(y_i-(a+bx_i))^2$$\n",
    "\n",
    "Optimal values for $a$ and $b$ are found by differentiating $Q$ with respect to $a$ and $b$, setting both equal to 0 and then solving for $a$ and $b$.   \n",
    "   \n",
    "We won't go into the [derivation process](http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_10.pdf) here, but the equations for $a$ and $b$ are:   \n",
    "   \n",
    "$$b = \\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$$   \n",
    "   \n",
    "and:   \n",
    "   \n",
    "$$a = \\bar{y} - b\\bar{x}$$\n",
    "\n",
    "where:\n",
    "- $ x_i$ Values of the independent variable.\n",
    "- $ y_i$ Values of the dependent variable.\n",
    "- $\\bar{y}$ are the mean values of $y$.\n",
    "- $\\bar{x}$ are the mean values of $x$ in our dataset, respectively.\n",
    "\n",
    "### Interpreting least-squares coefficients\n",
    "\n",
    "Interpreting the least-squares coefficients provides insights into the relationship between the independent variable (x) and the dependent variable (y) in a simple linear regression.\n",
    "\n",
    "#### The Slope ($𝛽_1$)\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- If $𝛽_1 > 0$: y increases as x increases (positive relationship).\n",
    "- If $𝛽_1 < 0$: y decreases as x increases (negative relationship).\n",
    "- If $𝛽_1 = 0$: No linear relationship exists between x and y.\n",
    "\n",
    "If $𝛽_1 = 0.28$ this means that for every one-unit increase in x, y is expected to increase by 0.28 units.\n",
    "\n",
    "Key Considerations:\n",
    "\n",
    "- The magnitude of $𝛽_1$ indicates the strength of the effect.\n",
    "- The direction (+/-) indicates the nature of the relationship.\n",
    "\n",
    "#### The Intercept ($𝛽_0$)\n",
    "\n",
    "Definition:\n",
    "The intercept ($𝛽_0$) represents the predicted value of the dependent variable (y) when the independent variable (x) is zero.\n",
    "\n",
    "Interpretation:\n",
    "- The intercept gives a baseline value of y when x=0.\n",
    "- It is meaningful only if 𝑥=0 is within the range of observed data. \n",
    "    - If not, the intercept might be extrapolated and have limited interpretive value.\n",
    "\n",
    "Limitations\n",
    "- Causation vs. Correlation: The coefficients indicate relationships, not causation, unless you have a well-controlled experimental design.\n",
    "- Range of x: The interpretation of $𝛽_0$ and $𝛽_1$ applies only within the range of observed x-values.\n",
    "- Other Factors: The model assumes that other variables do not influence 𝑦, which might not be the case in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add85037",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.X.values\n",
    "Y = df.Y.values\n",
    "\n",
    "# Calculate x bar, y bar\n",
    "x_bar = np.mean(X)\n",
    "y_bar = np.mean(Y)\n",
    "\n",
    "# Calculate slope\n",
    "b = sum( (X-x_bar)*(Y-y_bar) ) / sum( (X-x_bar)**2 )\n",
    "\n",
    "# Calculate intercept\n",
    "a = y_bar - b*x_bar\n",
    "\n",
    "print(\"Slope = \" + str(b))\n",
    "print(\"Intercept = \" + str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function we created earlier:\n",
    "# it generates y-values for given x-values based on parameters a, b\n",
    "y_gen2 = gen_y(df.X, a, b)\n",
    "\n",
    "plt.scatter(df.X, df.Y)\n",
    "plt.plot(df.X, y_gen2, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7483a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors2 = np.array(y_gen2 - df.Y)\n",
    "print(np.round(errors2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce02e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Residual sum of squares:\", (errors2 ** 2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13429034",
   "metadata": {},
   "source": [
    "Here we can see our RSS has improved from ~867 down to ~321.  \n",
    "Furthermore, if we calculate the sum of the errors we find that the value is close to 0.\n",
    "\n",
    "----\n",
    "Intuitively, this should make sense as it is an indication that the sum of the positive errors is equal to the sum of the negative errors. The line fits in the 'middle' of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round off to 11 decimal places\n",
    "np.round(errors2.sum(),11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8712bc8a",
   "metadata": {},
   "source": [
    "##### Recognise the Standard error of a statistic\n",
    "\n",
    "The standard error (SE) of a statistic in linear regression quantifies the variability of the estimated coefficients ($𝛽_0$ and $𝛽_1$) and other regression outputs. \n",
    "- It measures how much the coefficient estimates are expected to vary from sample to sample due to random noise in the data.\n",
    "\n",
    "**Standard Error of the Regression Coefficients**\n",
    "\n",
    "For a coefficient $𝛽_𝑗$ , the standard error (𝑆𝐸_𝛽_𝑗) is calculated as:\n",
    "\n",
    "$$𝑆𝐸_𝛽 = \\sqrt{\\frac{ \\sigma{^2}}{\\sum{}(x_i-\\bar{x})^2}}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\sigma{^2}$: The variance of the residuals, often estimated as the mean squared error (MSE):\n",
    "$$\\sigma{^2} = \\frac{RSS}{n−2}$$\n",
    "- n is the number of observations.\n",
    "\n",
    "$ \\sum{}(𝑥_𝑖 − \\bar{𝑥})^2$ : The total variation in the independent variable 𝑥\n",
    "\n",
    "**Standard Error of the Regression**\n",
    "\n",
    "The standard error of the regression (also called the residual standard error, $𝑅𝑆𝐸$ measures the average distance that the observed values fall from the regression line.\n",
    "\n",
    "$$RSE = \\frac{RSS}{n−2}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- RSS: Residual Sum of Squares.\n",
    "- $𝑛 − 2$ : Degrees of freedom for simple linear regression ($𝑛 − 𝑘 − 1$)\n",
    "    - with 𝑘 = 1 predictor.\n",
    "\n",
    "##### Role of Standard Errors in Linear Regression\n",
    "\n",
    "Done to evaluate the reliability and precision of your regression model.\n",
    "\n",
    "1.  Coefficient Standard Errors ($𝑆𝐸_{𝛽_0}$ and $𝑆𝐸_{𝛽_1}$\n",
    "- These are used to:\n",
    "    - Quantify Precision: Smaller standard errors indicate more precise estimates of the coefficients. \n",
    "    - Construct Confidence Intervals: The confidence interval for $𝛽_𝑗$ is:\n",
    "\n",
    "$$𝛽_𝑗 \\pm t \\cdot 𝑆𝐸_{𝛽_j}$$\n",
    "\n",
    "where 𝑡 is the critical value from the t-distribution for the desired confidence level.\n",
    " \n",
    "​- Perform Hypothesis Tests: To test if $𝛽_𝑗$ = 0, we calculate:\n",
    "\n",
    "$$t = \\frac{𝛽_𝑗}{SE_{𝛽_𝑗}}$$\n",
    "\n",
    "Compare t to the critical t-value to determine significance.\n",
    "\n",
    "2. Residual Standard Error (𝑅𝑆𝐸) \n",
    "- Indicates the average error in predictions.\n",
    "- Provides a baseline for assessing the fit of the model (smaller 𝑅𝑆𝐸 implies a better fit).\n",
    " \n",
    "**Intepretation**\n",
    "\n",
    "Residual Standard Error (RSE):\n",
    "- On average, the observed y-values deviate from the predicted  y-values by 0.147 units.\n",
    "\n",
    "Standard Error of Slope ($SE_{β_1}$):\n",
    "- The variability in the estimated slope is 0.065. This is used to assess the precision of $𝛽_1.\n",
    "\n",
    "Confidence in Coefficients:\n",
    "- Smaller standard errors indicate more confidence in the coefficient estimates.\n",
    "- Standard errors also allow hypothesis testing to determine if a predictor has a statistically significant impact on y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c4e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Define data\n",
    "x = np.array([1, 2, 3, 4, 5])  # Independent variable\n",
    "y = np.array([2.2, 2.8, 3.6, 4.5, 5.1])  # Dependent variable\n",
    "\n",
    "# Step 2: Calculate coefficients\n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "beta1 = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n",
    "beta0 = y_mean - beta1 * x_mean\n",
    "\n",
    "# Step 3: Calculate residuals and RSS\n",
    "y_pred = beta0 + beta1 * x  # Predicted values\n",
    "residuals = y - y_pred  # Residuals\n",
    "RSS = np.sum(residuals ** 2)  # Residual Sum of Squares\n",
    "\n",
    "# Step 4: Calculate standard error of the regression (RSE)\n",
    "n = len(x)  # Number of observations\n",
    "RSE = np.sqrt(RSS / (n - 2))  # Residual Standard Error\n",
    "\n",
    "# Step 5: Calculate standard error of the slope (SE_beta1)\n",
    "SE_beta1 = RSE / np.sqrt(np.sum((x - x_mean) ** 2))\n",
    "\n",
    "# Step 6: Print results\n",
    "print(f\"Residual Standard Error (RSE): {RSE:.3f}\")\n",
    "print(f\"Standard Error of Slope (SE_beta1): {SE_beta1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91173ea",
   "metadata": {},
   "source": [
    "### Applications of Standard Errors\n",
    "\n",
    "The standard errors (SEs) in linear regression are used to assess the precision and reliability of the estimated coefficients and model predictions. They serve as the foundation for key inferential techniques like \n",
    "- confidence intervals, \n",
    "- hypothesis testing, and \n",
    "- evaluating the overall fit of the regression model.\n",
    "\n",
    "1. Constructing Confidence Intervals\n",
    "\n",
    "Confidence intervals provide a range of plausible values for the regression coefficients.\n",
    "\n",
    "$$𝛽_𝑗 \\pm t \\cdot 𝑆𝐸_{𝛽_j}$$\n",
    "\n",
    "where:\n",
    "- $𝛽_𝑗$: Estimated coefficient.\n",
    "- $𝑆𝐸_{𝛽_j}$: Standard error of the coefficient.\n",
    "- t: Critical value from the t-distribution based on the desired confidence level and degrees of freedom ($𝑛 − 𝑘 − 1$).\n",
    "\n",
    "Interpretation: If the confidence interval for a coefficient does not include 0, it indicates that the predictor variable has a statistically significant relationship with the dependent variable at the given confidence level.\n",
    "\n",
    "**Calculate the 95% confidence interval for a regression coefficient, such as slope($𝛽_1$)**\n",
    "\n",
    "Use the following formula:\n",
    "\n",
    "Confidence Interval = $$𝛽_1 \\pm t \\cdot 𝑆𝐸_{𝛽_1}$$\n",
    "\n",
    "Steps to Calculate the 95% Confidence Interval\n",
    "\n",
    "1. Estimate the Slope Coefficient ($𝛽_1$)\n",
    "\n",
    "$$𝛽_1 = \\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$$\n",
    "\n",
    "2. Calculate the Standard Error of the Slope ($𝛽_1$)\n",
    "\n",
    "- The standard error of the slope is:\n",
    "\n",
    "$$𝑆𝐸_{𝛽_1} = \\frac{ RSE}{\\sqrt{\\sum{}(x_i-\\bar{x})^2}}$$\n",
    "\n",
    "- RSE (Residual Standard Error) is:\n",
    "\n",
    "$$RSE = \\frac{RSS}{n−2}$$\n",
    "where RSS = $\\sum{} (y_i-\\hat{y_1})^2$\n",
    "\n",
    "3. Find the Critical t-value ($𝑡_{critical}$):\n",
    "\n",
    "Use the t-distribution with 𝑛 − 2 degrees of freedom to find the critical value for the 95% confidence level ($𝑡_{critical}$).\n",
    "\n",
    "4. Apply the Confidence Interval Formula:\n",
    "\n",
    "- Combine the values:\n",
    "    - Confidence Interval = $$𝛽_1 \\pm 𝑡_{critical} \\cdot 𝑆𝐸_{𝛽_1}$$\n",
    "\n",
    "_______\n",
    "\n",
    "2. Hypothesis Testing\n",
    "\n",
    "Hypothesis testing in Linear Regression\n",
    "- Once you have fitted a straight line on the data, you need to ask, \n",
    "    - “Is this straight line a significant fit for the data?” Or \n",
    "    - “Is the beta coefficient explain the variance in the data plotted?” \n",
    "- Here comes the idea of hypothesis testing on the beta coefficient:\n",
    "\n",
    "$H_0 : B_1  = 0$\n",
    "    \n",
    "$H_A : B_1  ≠ 0$\n",
    "\n",
    "Interpret the Regression Equation\n",
    "- The coefficients ($𝛽$) indicate the magnitude and direction of the relationship between each predictor and readmissions.\n",
    "    - Example: A coefficient of -0.5 for medication_adherence means that for every 1% increase in medication adherence, readmissions decrease by 0.5.\n",
    "- The intercept ($𝛽_0$) represents the expected number of readmissions when all predictors are zero.\n",
    "\n",
    "Assessing the Model Fit\n",
    "- Other parameters to assess a model are:\n",
    "    - t statistic: It is used to determine the p-value and hence, helps in determining whether the coefficient is significant or not\n",
    "    - F statistic: It is used to assess whether the overall model fit is significant or not. \n",
    "        - the higher the value of the F-statistic, the more significant a model turns out to be.\n",
    "\n",
    "To determine whether a predictor variable has a significant impact on the dependent variable, use hypothesis testing.\n",
    "- Null Hypothesis ($𝐻_0): 𝛽_𝑗 =0$ (the predictor has no effect on response(𝑦) varaible).\n",
    "- Alternative Hypothesis $(𝐻_𝑎): 𝛽_𝑗 ≠ 0$ (the predictor has an effect/ there is a relationship).\n",
    "- t-statistic:\n",
    "\n",
    "**How to Calculate the t-statistic in Linear regression**\n",
    "The t-statistic in linear regression measures how many standard errors the estimated coefficient is away from zero. \n",
    "- It is used for hypothesis testing to determine if a predictor variable is statistically significant.\n",
    "\n",
    "The formula to calculate the t-statistic for a coefficient\n",
    "\n",
    "$$t = \\frac{𝛽_𝑗}{SE_{𝛽_𝑗}}$$\n",
    "\n",
    "Where:\n",
    "- $𝛽_𝑗$: Estimated coefficient (e.g., slope or intercept).\n",
    "- $SE_{𝛽_𝑗}$: Standard error of the estimated coefficient.\n",
    "\n",
    "If the t-statistic is large in magnitude, it indicates that $𝛽_j (or β_1 in this case) is far from zero, suggesting the predictor has a significant effect on the dependent variable.\n",
    "\n",
    "- P-Value: Compare the computed t-value to the critical value from the t-distribution, or calculate the p-value:\n",
    "    - If $𝑝 < 𝛼 (e.g., 0.05)$, reject $𝐻_0$ and conclude the predictor is statistically significant.\n",
    "_________\n",
    "\n",
    "3. Evaluating Model Fit\n",
    "\n",
    "The standard error of the regression (Residual Standard Error, 𝑅𝑆𝐸) assesses the accuracy of the model's predictions.\n",
    "\n",
    "$$RSE = \\sqrt{\\frac{RSS}{n−k-1}}$$\n",
    "\n",
    "**Degrees of Freedom**\n",
    "The t-statistic follows a t-distribution with $𝑛 − 𝑘 − 1 degrees of freedom,\n",
    "\n",
    "where:\n",
    "- RSS: Residual sum of squares.\n",
    "- n: Number of observations.\n",
    "- k: Number of predictors (excluding the intercept).\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- A smaller 𝑅𝑆𝐸 indicates better model fit.\n",
    "- Used as a baseline to evaluate other models.\n",
    "\n",
    "________\n",
    "\n",
    "4. Comparing Predictors\n",
    "\n",
    "Standard errors help compare the relative importance of different predictors by normalizing their coefficient estimates.\n",
    "- Predictors with smaller $𝑆𝐸{𝛽_𝑗}$ have more stable effects on 𝑦.\n",
    "- Variables with larger $𝑆𝐸{𝛽_𝑗}$ might need further investigation (e.g., multicollinearity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cbf5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Applications of Standard Errors\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Step 1: Define data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2.2, 2.8, 3.6, 4.5, 5.1])\n",
    "\n",
    "# Step 2: Calculate coefficients\n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "beta1 = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n",
    "beta0 = y_mean - beta1 * x_mean\n",
    "\n",
    "# Step 3: Calculate residuals and RSS\n",
    "y_pred = beta0 + beta1 * x\n",
    "residuals = y - y_pred\n",
    "RSS = np.sum(residuals ** 2)\n",
    "\n",
    "# Step 4: Calculate RSE\n",
    "n = len(x)\n",
    "RSE = np.sqrt(RSS / (n - 2))\n",
    "\n",
    "# Step 5: Calculate standard error of the slope (SE_beta1)\n",
    "SE_beta1 = RSE / np.sqrt(np.sum((x - x_mean) ** 2))\n",
    "\n",
    "# Step 6: Hypothesis Testing and Confidence Interval\n",
    "t_stat = beta1 / SE_beta1  # t-statistic\n",
    "p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-2))  # Two-tailed test\n",
    "\n",
    "# Confidence Interval for beta1\n",
    "t_critical = stats.t.ppf(0.975, df=n-2)  # 95% confidence level\n",
    "conf_interval = (beta1 - t_critical * SE_beta1, beta1 + t_critical * SE_beta1)\n",
    "\n",
    "# Step 7: Print results\n",
    "print(f\"Coefficient (beta1): {beta1:.3f}\")\n",
    "print(f\"Standard Error (SE_beta1): {SE_beta1:.3f}\")\n",
    "print(f\"t-Statistic: {t_stat:.3f}\")\n",
    "print(f\"p-Value: {p_value:.5f}\")\n",
    "print(f\"95% Confidence Interval for beta1: {conf_interval}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb0c96b",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "- Coefficient (𝛽_1): The slope is 0.7\n",
    "    - 0.7, indicating that y increases by 0.7 units for every one-unit increase in 𝑥 \n",
    "- Standard Error (𝑆𝐸_{𝛽_1): The slope estimate has a variability of 0.094, indicating precision.\n",
    "- t-Statistic and p-Value: The large t-statistic and small p-value indicate that 𝛽_1 is statistically significant.\n",
    "- Confidence Interval: We are 95% confident that the true value of 𝛽_1 lies between  0.467 and 0.933.\n",
    "\n",
    "Summary\n",
    "\n",
    "- Confidence Intervals: Quantify the uncertainty around coefficient estimates.\n",
    "- Hypothesis Testing: Assess the statistical significance of predictors.\n",
    "- Model Diagnostics: Evaluate and compare models using 𝑅𝑆𝐸.\n",
    "- Decision-Making: Use SEs to identify reliable predictors and improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d238e8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ calculate the 95% confidence interval for a regression coefficient, such as 𝛽1 (slope), you use the following formula:\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "# Step 1: Define data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2.2, 2.8, 3.6, 4.5, 5.1])\n",
    "\n",
    "# Step 2: Calculate the slope (beta1) and intercept (beta0)\n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "beta1 = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n",
    "beta0 = y_mean - beta1 * x_mean\n",
    "\n",
    "# Step 3: Calculate residuals, RSS, and RSE\n",
    "y_pred = beta0 + beta1 * x\n",
    "residuals = y - y_pred\n",
    "RSS = np.sum(residuals ** 2)\n",
    "n = len(x)\n",
    "RSE = np.sqrt(RSS / (n - 2))\n",
    "\n",
    "# Step 4: Calculate standard error of the slope (SE_beta1)\n",
    "SE_beta1 = RSE / np.sqrt(np.sum((x - x_mean) ** 2))\n",
    "\n",
    "# Step 5: Determine t-critical value for 95% confidence interval\n",
    "alpha = 0.05  # 95% confidence level\n",
    "df = n - 2  # Degrees of freedom\n",
    "t_critical = t.ppf(1 - alpha/2, df)\n",
    "\n",
    "# Step 6: Calculate confidence interval\n",
    "lower_bound = beta1 - t_critical * SE_beta1\n",
    "upper_bound = beta1 + t_critical * SE_beta1\n",
    "\n",
    "# Step 7: Print results\n",
    "print(f\"Slope (beta1): {beta1:.3f}\")\n",
    "print(f\"Standard Error (SE_beta1): {SE_beta1:.3f}\")\n",
    "print(f\"t-Critical: {t_critical:.3f}\")\n",
    "print(f\"95% Confidence Interval for beta1: ({lower_bound:.3f}, {upper_bound:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700d1d6",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "- The 95% confidence interval for $𝛽_1$ is (0.467, 0.933).\n",
    "- This means we are 95% confident that the true slope ($𝛽_1$) lies within this range.\n",
    "- Since the interval does not include 0, it indicates that the relationship between x and 𝑦 is statistically significant at the 5% significance level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d549922",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### calculate the t-statistic for a simple linear regression with one predictor (x) and one response variable (y).\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "# Step 1: Define data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2.2, 2.8, 3.6, 4.5, 5.1])\n",
    "\n",
    "# Step 2: Calculate coefficients\n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "beta1 = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n",
    "beta0 = y_mean - beta1 * x_mean\n",
    "\n",
    "# Step 3: Calculate residuals, RSS, and RSE\n",
    "y_pred = beta0 + beta1 * x\n",
    "residuals = y - y_pred\n",
    "RSS = np.sum(residuals ** 2)\n",
    "n = len(x)\n",
    "RSE = np.sqrt(RSS / (n - 2))\n",
    "\n",
    "# Step 4: Calculate standard error of the slope (SE_beta1)\n",
    "SE_beta1 = RSE / np.sqrt(np.sum((x - x_mean) ** 2))\n",
    "\n",
    "# Step 5: Calculate the t-statistic\n",
    "t_statistic = beta1 / SE_beta1\n",
    "\n",
    "# Step 6: Calculate p-value (two-tailed test)\n",
    "df = n - 2  # Degrees of freedom\n",
    "p_value = 2 * (1 - t.cdf(abs(t_statistic), df))\n",
    "\n",
    "# Step 7: Print results\n",
    "print(f\"Slope (beta1): {beta1:.3f}\")\n",
    "print(f\"Standard Error (SE_beta1): {SE_beta1:.3f}\")\n",
    "print(f\"t-Statistic: {t_statistic:.3f}\")\n",
    "print(f\"p-Value: {p_value:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80bdf33",
   "metadata": {},
   "source": [
    "##### Interpretation\n",
    "t-Statistic:\n",
    "- The t-statistic for the slope is 7.435, indicating the estimated coefficient is significantly far from zero.\n",
    "\n",
    "p-Value:\n",
    "- The small p-value (0.00231) suggests strong evidence against the null hypothesis (𝛽_1 = 0).\n",
    "- The predictor (x) is statistically significant at a 5% significance level.\n",
    "\n",
    "By comparing the t-statistic to critical t-values or using the p-value, you can conclude whether the predictor is significantly associated with the response variable.\n",
    "\n",
    "### Explaining the rules for rejecting the null hypothesis using p-values\n",
    "\n",
    "1. What is a p-value?\n",
    "\n",
    "The p-value is the probability of observing the data (or something more extreme) if the null hypothesis ($𝐻_0$) is true.\n",
    "- A low p-value indicates that the observed result is unlikely under the assumption of the null hypothesis.\n",
    "\n",
    "2. Decision Rule for Rejecting the Null Hypothesis\n",
    "\n",
    "The decision rule depends on the significance level (𝛼), which is the threshold for rejecting. \n",
    "- Common choices for α are 0.05 (5%) or 0.01 (1%).\n",
    "\n",
    "If p-value ≤ 𝛼: Reject the null hypothesis ($𝐻_0$).\n",
    "- The result is statistically significant.\n",
    "- There is strong evidence against the null hypothesis.\n",
    "\n",
    "If p-value > α: Fail to reject the null hypothesis ($𝐻_0$).\n",
    "- The result is not statistically significant.\n",
    "- There isn’t enough evidence to conclude that the null hypothesis is false.\n",
    "\n",
    "3. Interpretation Guidelines\n",
    "\n",
    "Small p-value (≤𝛼):\n",
    "- The observed effect is unlikely due to chance alone.\n",
    "Example: \n",
    "- p=0.03 suggests that there’s only a 3% chance of observing your data if $𝐻_0$ were true.\n",
    "\n",
    "Large p-value (>α):\n",
    "- The observed effect could plausibly occur due to chance.\n",
    "Example: \n",
    "- p=0.10 suggests that there’s a 10% chance of observing your data if $𝐻_0$ were true.\n",
    "\n",
    "Common Misinterpretations to Avoid\n",
    "\n",
    "1. The p-value is not the probability that $𝐻_0$ is true.\n",
    "- It reflects the likelihood of observing the data assuming $𝐻_0$ is true.\n",
    "2. Failing to reject $𝐻_0$ does not mean $𝐻_0$ is true.\n",
    "- It only means there isn’t enough evidence to conclude otherwise.\n",
    "3. A small p-value does not indicate a large effect size.\n",
    "- Statistical significance doesn’t always mean practical significance.\n",
    "\n",
    "Summary\n",
    "\n",
    "- The choice of significance level (α) determines the threshold for rejecting $𝐻_0$\n",
    "​- The p-value provides a way to quantify the strength of evidence against $𝐻_0$.\n",
    "- Always report both the p-value and 𝛼 for transparency in hypothesis testing.\n",
    "\n",
    "##### Practical Example\n",
    "Suppose you are testing whether a new marketing strategy improves sales:\n",
    "\n",
    "Null Hypothesis ($𝐻_0$): The new marketing strategy has no effect on sales (β=0).\n",
    "\n",
    "Alternative Hypothesis ($𝐻_a$): The new marketing strategy increases sales (β>0).\n",
    "\n",
    "If your analysis gives a p-value of 0.03, and you have set α=0.05:\n",
    "- Since p-value (0.03) < α (0.05), you reject ($𝐻_0$).\n",
    "- You conclude that the new marketing strategy likely increases sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f3e50",
   "metadata": {},
   "source": [
    "### Regression cost functions: Regression model evaluation metrics\n",
    "\n",
    "**loss function** is for a single training example. It is also sometimes called an error function. \n",
    "\n",
    "**cost function**, on the other hand, is the average loss over the entire training dataset. \n",
    "\n",
    "**Steps for Loss Functions**\n",
    "1. Define the predictor function f(X), and identify the parameters to find.\n",
    "2. Determine the loss for each training example.\n",
    "3. Derive the expression for the Cost Function, representing the average loss across all examples.\n",
    "4. Compute the gradient of the Cost Function concerning each unknown parameter.\n",
    "5. Select the learning rate and execute the weight update rule for a fixed number of iterations.\n",
    "\n",
    "These steps guide the optimization process, aiding in the determination of optimal model parameters.\n",
    "\n",
    "Regression model we generally use to evaluate the prediction error rates and model performance in regression analysis.\n",
    "\n",
    "1. **R-squared (Coefficient of determination)** \n",
    "- Indicates the proportion of variance in the dependent variable explained by the independent variables. \n",
    "- It represents the coefficient of how well the values fit compared to the original values. \n",
    "- It helps answer the question: \"How well does my model explain the variability in the dependent variable?\"\n",
    "- The value from 0 to 1 interpreted as percentages. \n",
    "    - where: \n",
    "        - $𝑅^2$ = 1 (close to 1): Perfect fit (all variability in y is explained by X).\n",
    "            - The model explains a large proportion of the variability in the data.\n",
    "            - A large proportion of the variance in the dependent variable is explained by the independent variables.\n",
    "            - Example: If $𝑅^2$ = 0.85\n",
    "                - Then 85% of the variability in y is explained by X. The remaining 15% is due to unexplained variability (e.g., noise, unobserved variables).\n",
    "        - $𝑅^2$ = 0 (close to 0): No relationship (the model does not explain any variability in y).\n",
    "            - The model fails to explain much of the variability or A small proportion of the variance is explained.\n",
    "            - Example: If $𝑅^2$ = 0.1\n",
    "                - only 10% of the variability is explained by the model. This suggests either:\n",
    "                    - The model lacks important predictors.\n",
    "                    - The relationship between X and y may not be linear.\n",
    "                    - There is high variability in y that cannot be captured effectively.\n",
    "    - The higher the value is, the better the model is / model fits the data better, but it does not necessarily mean the model is accurate in predictions. and does not imply causation or that the model is the best predictor.\n",
    "\n",
    "R-squared statistic is calculated as:\n",
    "\n",
    "$$𝑅^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$$\n",
    "\n",
    "Where:\n",
    "- $SS_{res}$ (Residual Sum of Squares): Sum of squared differences between actual and predicted values.\n",
    "\n",
    "$$SS_{res} = \\sum^{n}_{i = 1} (y_i - \\hat{y_i})^2$$\n",
    "\n",
    "- $SS_{tot}$ (Total Sum of Squares): Sum of squared differences between actual values and their mean.\n",
    "\n",
    "$$SS_{res} = \\sum^{n}_{i = 1} (y_i - \\bar{y_i})^2$$\n",
    "\n",
    "### Important Caveats\n",
    "1. Overfitting in Complex Models\n",
    "- High $𝑅^2$ may result from overfitting, especially in models with many predictors.\n",
    "2. Does Not Imply Causation\n",
    "- High $𝑅^2$ shows correlation, not causation. For example, a model may explain variability due to spurious relationships.\n",
    "3. Limited Applicability to Prediction\n",
    "- High $𝑅^2$ doesn’t guarantee that the model predicts new data well (check with metrics like RMSE on test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8171e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Step 1: Create synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10  # Independent variable\n",
    "y = 3 * X.squeeze() + 7 + np.random.randn(100) * 3  # Dependent variable with noise\n",
    "\n",
    "# Step 2: Fit a simple linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Step 3: Calculate R-squared manually\n",
    "y_mean = np.mean(y)\n",
    "SS_res = np.sum((y - y_pred) ** 2)  # Residual Sum of Squares\n",
    "SS_tot = np.sum((y - y_mean) ** 2)  # Total Sum of Squares\n",
    "R_squared = 1 - (SS_res / SS_tot)\n",
    "\n",
    "print(f\"Manual R-squared: {R_squared:.4f}\")\n",
    "\n",
    "# Step 4: Calculate R-squared using sklearn\n",
    "R_squared_sklearn = r2_score(y, y_pred)\n",
    "print(f\"Sklearn R-squared: {R_squared_sklearn:.4f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(X, y, label=\"Actual Data\", alpha=0.7)\n",
    "plt.plot(X, y_pred, color=\"red\", label=\"Fitted Line\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title(f\"Linear Regression (R-squared: {R_squared:.4f})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7df5a7",
   "metadata": {},
   "source": [
    "2. **Adjusted R-squared**\n",
    "- Adjusted version of $𝑅^2$ that accounts for the number of predictors in the model.\n",
    "\n",
    "$$𝑅^2_{adj} = 1 - \\frac{(1 - 𝑅^2)(n -1)}{n - p - 1}$$\n",
    "\n",
    "where:\n",
    "- n is the number of observations, and\n",
    "- p is the number of predictors.\n",
    "\n",
    "The Significance of R-squared is:\n",
    "    - if $R^2$ = 1 : Best-fit Line\n",
    "    - if $R^2$ = 0.5 : still some errors\n",
    "    - if $R^2$ = 0.05 : not performing well\n",
    "\n",
    "Usage: Useful when comparing models with a different number of predictors\n",
    "\n",
    "_____\n",
    "\n",
    "3. Mean Error (ME)\n",
    "- The error for each training data is calculated and then the mean value of all these errors is derived.\n",
    "- Errors can be both negative and positive. So they can cancel each other out during summation giving zero mean error for the model.\n",
    "- Not a recommended cost function but it does lay the foundation for other cost functions of regression models.\n",
    "\n",
    "Residual Analysis\n",
    "\n",
    "- Residuals: Differences between actual and predicted values ($𝑦_𝑖 − \\hat{𝑦_𝑖}$).\n",
    "- Analysis: Residual plots help diagnose issues like non-linearity, heteroscedasticity, and independence of errors.\n",
    "\n",
    "_____\n",
    "\n",
    "4. **MSE (Mean Squared Error)**\n",
    "- known as L2 loss.\n",
    "- represents the difference between the original and predicted values extracted by squared the average difference over the data set.\n",
    "- Here a square of the difference between the actual and predicted value is calculated to avoid any possibility of negative error(drawback cause).\n",
    "- It is measured as the average of the sum of squared differences between predictions and actual observations.\n",
    "$$MSE = \\frac{1}{n} \\sum^{n}_{i = 1} (y_i - \\hat{y_i})^2$$\n",
    "- Since each error is squared, it helps to penalize even small deviations in prediction when compared to MAE. \n",
    "    - But if our dataset has outliers that contribute to larger prediction errors, then squaring this error further will magnify the error many times more and also lead to higher MSE error.\n",
    "    - MSE loss function penalizes the model for making large errors by squaring them. Squaring a large quantity makes it even larger\n",
    "        - it is less robust to outliers\n",
    "        - not to be used if our data is prone to many outliers.\n",
    "\n",
    "Usage: Penalizes larger errors more heavily than smaller ones.\n",
    "\n",
    "Graphically\n",
    "- It is a positive quadratic function (of the form $ax^2 + bx + c$ where $a > 0$)\n",
    "- A quadratic function only has a global minimum. \n",
    "    - Since there are no local minima, we will never get stuck in one. \n",
    "- Hence, it is always guaranteed that Gradient Descent will converge (if it converges at all) to the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ff6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_MSE(m, b, X, Y, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    for i in range(N):\n",
    "        # Calculate partial derivatives\n",
    "        # -2x(y - (mx + b))\n",
    "        m_deriv += -2*X[i] * (Y[i] - (m*X[i] + b))\n",
    "\n",
    "        # -2(y - (mx + b))\n",
    "        b_deriv += -2*(Y[i] - (m*X[i] + b))\n",
    "\n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfde74f",
   "metadata": {},
   "source": [
    "**Advantages of R-squared**:\n",
    "\n",
    "- Interpretability: $𝑅^2$ is unitless and ranges between 0 and 1, making it easy to understand and compare across datasets.\n",
    "- Proportional Explanation: Quantifies the proportion of variance explained by the model, offering insights into model effectiveness.\n",
    "- Model Comparison: Useful for comparing the explanatory power of different models or regression equations.\n",
    "\n",
    "**Disadvantages of RSE Compared to R-squared**:\n",
    "\n",
    "- RSE depends on the scale of the dependent variable, making it hard to compare across datasets with different units.\n",
    "- RSE alone does not provide information on how much variance the model explains.\n",
    "\n",
    "_______\n",
    "\n",
    "5. **RMSE (Root Mean Squared Error)** \n",
    "- is the error rate by the square root of MSE.\n",
    "\n",
    "$$RMSE = \\sqrt{MSE}$$\n",
    "\n",
    "Usage: Commonly used because it is in the same units as the dependent variable and emphasizes larger errors.\n",
    "\n",
    "______\n",
    "\n",
    "6. **MAE (Mean absolute error)**\n",
    "- known as L1 Loss.\n",
    "- represents the difference between the original and predicted values extracted by averaged the absolute difference over the data set.\n",
    "- It is the average of the absolute differences between predicted and actual values.\n",
    "- Absolute Error for each training example is the distance between the predicted and the actual values, irrespective of the sign.\n",
    "    - it is the absolute difference between the actual and predicted values.\n",
    "- Here an absolute difference between the actual and predicted value is calculated to avoid any possibility of negative error.\n",
    "- It is measured as the average of the sum of absolute differences between predictions and actual observations.\n",
    "    - It is robust to outliers thus it will give better results even when our dataset has noise or outliers.\n",
    "    - MAE cost is more robust to outliers as compared to MSE\n",
    "-  The cost is the Mean of these Absolute Errors\n",
    "\n",
    "$$MAE = \\frac{1}{n} \\sum^{n}_{i = 1} |y_i - \\hat{y_i}|$$\n",
    "\n",
    "Usage: Provides an easily interpretable measure of error in the same units as the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34787f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_MAE(m, b, X, Y, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    for i in range(N):\n",
    "        # Calculate partial derivatives\n",
    "        # -x(y - (mx + b)) / |mx + b|\n",
    "        m_deriv += - X[i] * (Y[i] - (m*X[i] + b)) / abs(Y[i] - (m*X[i] + b))\n",
    "\n",
    "        # -(y - (mx + b)) / |mx + b|\n",
    "        b_deriv += -(Y[i] - (m*X[i] + b)) / abs(Y[i] - (m*X[i] + b))\n",
    "\n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ab3d2",
   "metadata": {},
   "source": [
    "7. **Mean Absolute Percentage Error (MAPE)**\n",
    "\n",
    "Definition: The mean of the absolute percentage differences between predicted and actual values.\n",
    "\n",
    "$$MAPE = \\frac{1}{n} \\sum^{n}_{i = 1} |\\frac{y_i - \\hat{y_i}}{y_i}| \\times 100$$\n",
    "\n",
    "Usage: Expresses error as a percentage, making it easier to interpret across datasets\n",
    "\n",
    "___________\n",
    "\n",
    "8. Huber Loss\n",
    "\n",
    "- The Huber loss combines the best properties of MSE and MAE.\n",
    "- It is quadratic for smaller errors and is linear otherwise (and similarly for its gradient). \n",
    "- It is identified by its delta parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b94332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_Huber(m, b, X, Y, delta, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    for i in range(N):\n",
    "        # derivative of quadratic for small values and of linear for large values\n",
    "        if abs(Y[i] - m*X[i] - b) <= delta:\n",
    "          m_deriv += -X[i] * (Y[i] - (m*X[i] + b))\n",
    "          b_deriv += - (Y[i] - (m*X[i] + b))\n",
    "        else:\n",
    "          m_deriv += delta * X[i] * ((m*X[i] + b) - Y[i]) / abs((m*X[i] + b) - Y[i])\n",
    "          b_deriv += delta * ((m*X[i] + b) - Y[i]) / abs((m*X[i] + b) - Y[i])\n",
    "    \n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ddd25a",
   "metadata": {},
   "source": [
    "##### Choosing the Right Measure\n",
    "- Use R-squared or Adjusted R-squared to evaluate the proportion of variance explained.\n",
    "- Use MAE, MSE, or RMSE for measuring prediction accuracy in units of the target variable.\n",
    "- Use MAPE for interpreting errors as percentages.\n",
    "\n",
    "**Step 5: Interpret the Results**\n",
    "\n",
    "Residual Analysis:\n",
    "- Check normal distribution and normality for the residuals.\n",
    "- Homoscedasticity describes a situation in which error term is the same across all values of the independent variables. \n",
    "    - means that the residuals are equal across the regression line.\n",
    "\n",
    "Interpretation of Regression Output\n",
    "- R-Squared : is a statistical measure of fit that indicates how much variation of a dependent variable is explained by the independent variables. \n",
    "    - Higher R-Squared value represents smaller differences between the observed data and fitted values.\n",
    "\n",
    "**Optimization technique/Strategy**\n",
    "\n",
    "We will use Gradient Descent as an optimization strategy to find the regression line.\n",
    "- Weight Update Rule\n",
    "\n",
    "NB: Perform optimization on the training data and check its performance on a new validation data.\n",
    "\n",
    "**Gradient Descent for Linear Regression**\n",
    "\n",
    "What is gradient descent?\n",
    "- lay man: \n",
    "    - It is a way of checking the ground near you and observe where the land tends to descend.\n",
    "    - It gives an idea in what direction you should take your steps.\n",
    "    - It helps models find the optimal set of parameters by iteratively adjusting them in the opposite direction of the gradient, aiming to find the optimal set of parameters.\n",
    "\n",
    "Mathematical terms:\n",
    "- find out the best parameters ($θ_1$) and ($θ_2$) for our learning algorithm.\n",
    "\n",
    "Cost space is how our algorithm would perform when we choose a particular value for a parameter.\n",
    "\n",
    "Cost Function is a function that measures the performance of a model for any given data. Cost Function quantifies the error between predicted values and expected values and presents it in the form of a single real number.\n",
    "\n",
    "1. Make a hypothesis with initial parameters\n",
    "- Hypothesis: $h_θ(x) = θ_0 + θ_1 x$\n",
    "- Parameters: $θ_o, θ_1$\n",
    "2. Calculate the Cost function\n",
    "- Cost Function: $J(θ_o, θ_1) = \\frac{1}{2m}\\sum^{m}_{i = 1} (h_θ (x^{(i)}) - y^{i})^2$\n",
    "3. The goal is to reduce the cost function, we modify the parameters by using the Gradient descent algorithm over the given data.\n",
    "- Goal: $minimize_{θ_o, θ_1} J(θ_o, θ_1)$\n",
    "\n",
    "**Gradient descent**\n",
    "\n",
    "- one of the optimization algorithms that optimize the cost function (objective function) to reach the optimal minimal solution.\n",
    "- aims to find the parameters that minimize this discrepancy and improve the model’s performance.\n",
    "    - Need to reduce the cost function (MSE) for all data points. \n",
    "    - This is done by updating the values of the slope coefficient and the constant coefficient iteratively until we get an optimal solution for the linear function.\n",
    "\n",
    "The algorithm operates by calculating the gradient of the cost function, \n",
    "- which indicates the direction and magnitude of the steepest ascent. \n",
    "\n",
    "However, since the goal is to minimize the cost function, gradient descent moves in the opposite direction of the gradient, \n",
    "- known as the negative gradient direction.\n",
    "\n",
    "Iteratively updating the model’s parameters in the negative gradient direction, gradient descent gradually converges towards the optimal set of parameters that yields the lowest cost.\n",
    "\n",
    "- Hyperparameter: learning rate, determines the step size taken in each iteration, influencing the speed and stability of convergence.\n",
    "\n",
    "Gradient descent can be applied to:\n",
    "- linear regression, \n",
    "- logistic regression, \n",
    "- neural networks, and \n",
    "- support vector machines.\n",
    "\n",
    "**Definition**: Gradient descent is an iterative optimization algorithm for finding the local minimum of a function.\n",
    "\n",
    "To find the local minimum of a function using gradient descent, we must take steps proportional to the negative of the gradient (move away from the gradient) of the function at the current point.\n",
    "- If we take steps proportional to the positive of the gradient (moving towards the gradient), we will approach a local maximum of the function, and the procedure is called Gradient Ascent.\n",
    "\n",
    "The goal of the gradient descent algorithm is to minimize the given function (say, cost function)\n",
    "- it performs two steps iteratively:\n",
    "1. Compute the gradient (slope), the first-order derivative of the function at that point\n",
    "2. Make a step (move) in the direction opposite to the gradient. The opposite direction of the slope increases from the current point by alpha times the gradient at that point.\n",
    "- number of steps you’re taking can be considered as the learning rate, and this decides how fast the algorithm converges to the minima.\n",
    "\n",
    "This code creates a function called gradient_descent, which requires the training data, learning rate, and number of iterations as parameters.\n",
    "\n",
    "Steps :\n",
    "1. Sets weights and bias to arbitrary values during initialization.\n",
    "2. Executes a set number of iterations for loops.\n",
    "3. Computes the estimated y values by utilizing the existing weights and bias.\n",
    "4. Calculates the discrepancy between expected and real y values.\n",
    "5. Determines the changes in the cost function based on weights and bias.\n",
    "6. Adjusts the weights and bias by incorporating the gradients and learning rate.\n",
    "7. Outputs the acquired weights and bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad28b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(X, y, learning_rate, num_iters):\n",
    "  \"\"\"\n",
    "  Performs gradient descent to find optimal weights and bias for linear regression.\n",
    "\n",
    "  Args:\n",
    "      X: A numpy array of shape (m, n) representing the training data features.\n",
    "      y: A numpy array of shape (m,) representing the training data target values.\n",
    "      learning_rate: The learning rate to control the step size during updates.\n",
    "      num_iters: The number of iterations to perform gradient descent.\n",
    "\n",
    "  Returns:\n",
    "      A tuple containing the learned weights and bias.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize weights and bias with random values\n",
    "  m, n = X.shape\n",
    "  weights = np.random.rand(n)\n",
    "  bias = 0\n",
    "\n",
    "  # Loop for the number of iterations\n",
    "  for i in range(num_iters):\n",
    "    # Predict y values using current weights and bias\n",
    "    y_predicted = np.dot(X, weights) + bias\n",
    "\n",
    "    # Calculate the error\n",
    "    error = y - y_predicted\n",
    "\n",
    "    # Calculate gradients for weights and bias\n",
    "    weights_gradient = -2/m * np.dot(X.T, error)\n",
    "    bias_gradient = -2/m * np.sum(error)\n",
    "\n",
    "    # Update weights and bias using learning rate\n",
    "    weights -= learning_rate * weights_gradient\n",
    "    bias -= learning_rate * bias_gradient\n",
    "\n",
    "  return weights, bias\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[1, 1], [2, 2], [3, 3]])\n",
    "y = np.array([2, 4, 5])\n",
    "learning_rate = 0.01\n",
    "num_iters = 100\n",
    "\n",
    "weights, bias = gradient_descent(X, y, learning_rate, num_iters)\n",
    "\n",
    "print(\"Learned weights:\", weights)\n",
    "print(\"Learned bias:\", bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af34d913",
   "metadata": {},
   "source": [
    "How Does Gradient Descent Work?\n",
    "1. The algorithm optimizes to minimize the model’s cost function.\n",
    "2. The cost function measures how well the model fits the training data and defines the difference between the predicted and actual values.\n",
    "3. The cost function’s gradient is the derivative with respect to the model’s parameters and points in the direction of the steepest ascent.\n",
    "4. The algorithm starts with an initial set of parameters and updates them in small steps to minimize the cost function.\n",
    "5. In each iteration of the algorithm, it computes the gradient of the cost function with respect to each parameter.\n",
    "6. The gradient tells us the direction of the steepest ascent, and by moving in the opposite direction, we can find the direction of the steepest descent.\n",
    "7. The learning rate controls the step size, which determines how quickly the algorithm moves towards the minimum.\n",
    "8. The process is repeated until the cost function converges to a minimum. Therefore indicating that the model has reached the optimal set of parameters.\n",
    "9. Different variations of gradient descent include batch gradient descent, stochastic gradient descent, and mini-batch gradient descent, each with advantages and limitations.\n",
    "10. Efficient implementation of gradient descent is essential for performing well in machine learning tasks. The choice of the learning rate and the number of iterations can significantly impact the algorithm’s performance.\n",
    "\n",
    "On the basis of differentiation techniques \n",
    "- Gradient descent requires Calculation of gradient by differentiation of cost function. We can either use first order differentiation or second order differentiation.\n",
    "    - First order Differentiation\n",
    "    - Second order Differentiation.\n",
    "\n",
    "To update B 0 and B 1, we take gradients from the cost function. To find these gradients, we take partial derivatives for $B_0$ and $B_1$.\n",
    "\n",
    "$J = \\frac{1}{n} \\sum^{n}_{i = 1} (𝛽_{0}+𝛽_{1} . x_i - y_i)^2$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial 𝛽_{0}} = \\frac{2}{n} \\sum^{n}_{i = 1} (𝛽_{0}+𝛽_{1} . x_i - y_i)$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial 𝛽_{1}} = \\frac{2}{n} \\sum^{n}_{i = 1} (𝛽_{0}+𝛽_{1} . x_i - y_i) . x_i$\n",
    "\n",
    "$𝛽_{0} = 𝛽_{0} - \\alpha . \\frac{2}{n} \\sum^{n}_{i = 1} ( y_{pred} - y_{i}) $\n",
    "\n",
    "$𝛽_{1} = 𝛽_{1} - \\alpha . \\frac{2}{n} \\sum^{n}_{i = 1} ( y_{pred} - y_{i}) . x_i $\n",
    "\n",
    "Where: \n",
    "- The partial derivates are the gradients, and they are used to update the values of $B_0$ and $B_1$. \n",
    "- Alpha is the learning rate.\n",
    "\n",
    "**Types of Gradient Descent**\n",
    "\n",
    "Classified by two methods mainly:\n",
    "- On the basis of data ingestion: choice of gradient descent algorithm depends on the problem at hand and the size of the dataset.\n",
    "\n",
    "**Full Batch Gradient Descent Algorithm**:\n",
    "- Batch gradient descent,\n",
    "    - also known as vanilla gradient descent, \n",
    "- full batch gradient descent algorithms, you use whole data at once to compute the gradient.\n",
    "    - It updates the model’s parameters using the gradient of the entire training set.\n",
    "- It calculates the average gradient of the cost function for all the training examples and updates the parameters in the opposite direction.\n",
    "    - calculates the error for each example within the training dataset.\n",
    "    - The model is not changed until every training sample has been assessed. \n",
    "        - The entire procedure is referred to as a **cycle and a training epoch**.\n",
    "- Batch gradient descent guarantees convergence to the global minimum but can be computationally expensive and slow for large datasets.\n",
    "    - Batch gradient descent is suitable for small datasets.\n",
    "    - Its computational efficiency, which produces a stable error gradient and a stable convergence.\n",
    "- Drawbacks are that the stable error gradient can sometimes result in a state of convergence that isn’t the best the model can achieve. \n",
    "    - It also requires the entire training dataset to be in memory and available to the algorithm.\n",
    "\n",
    "Advantages\n",
    "- Fewer model updates mean that this variant of the steepest descent method is more computationally efficient than the stochastic gradient descent method.\n",
    "- Reducing the update frequency provides a more stable error gradient and a more stable convergence for some problems.\n",
    "- Separating forecast error calculations and model updates provides a parallel processing-based algorithm implementation.\n",
    "\n",
    "Disadvantages\n",
    "- A more stable error gradient can cause the model to prematurely converge to a suboptimal set of parameters.\n",
    "- End-of-training epoch updates require the additional complexity of accumulating prediction errors across all training examples.\n",
    "- The batch gradient descent method typically requires the entire training dataset in memory and is implemented for use in the algorithm.\n",
    "- Large datasets can result in very slow model updates or training speeds.\n",
    "- Slow and require more computational power.\n",
    "\n",
    "#### Variants\n",
    "\n",
    "##### Vanilla Gradient Descent, \n",
    "\n",
    "Vanilla means pure / without any adulteration.\n",
    "- simplest form of gradient descent technique\n",
    "    - main feature is that we take small steps in the direction of the minima by taking gradient of the cost function.\n",
    "\n",
    "Pseudocode Vanilla Gradient Descent\n",
    "\n",
    "$ update = learning rate * gradient of parameters$\n",
    "\n",
    "$ parameters = parameters - update$\n",
    "\n",
    "- make an update to the parameters by taking gradient of the parameters. \n",
    "- And multiplying it by a learning rate, which is essentially a constant number suggesting how fast we want to go the minimum. 4\n",
    "**Learning rate** is a hyper-parameter and should be treated with care when choosing its value.\n",
    "\n",
    "##### Gradient Descent with Momentum\n",
    "\n",
    "Tweaks the above algorithm in such a way that we pay heed to the prior step before taking the next step.\n",
    "\n",
    "Pseudocode Gradient Descent with Momentum\n",
    "\n",
    "$ update = learning_rate * gradient$ \n",
    "\n",
    "$ velocity = previous_update * momentum$ \n",
    "\n",
    "$ parameter = parameter + velocity – update$ \n",
    "\n",
    "Introduces Velocity, which considers the previous update and a constant which is called momentum.\n",
    "\n",
    "##### ADAGRAD\n",
    "\n",
    "ADAGRAD uses adaptive technique for learning rate updation. In this algorithm, on the basis of how the gradient has been changing for all the previous iterations we try to change the learning rate.\n",
    "\n",
    "Pseudocode ADAGRAD\n",
    "\n",
    "$ grad_component = previous_grad_component + (gradient * gradient)$ \n",
    "\n",
    "$ rate_change = square_root(grad_component) + epsilon$\n",
    "\n",
    "$ adapted_learning_rate = learning_rate * rate_change$\n",
    "\n",
    "$update = adapted_learning_rate * gradient$\n",
    "\n",
    "$parameter = parameter – update$\n",
    "\n",
    "where:\n",
    "-  epsilon is a constant which is used to keep rate of change of learning rate in check.\n",
    "\n",
    "##### ADAM\n",
    "\n",
    "ADAM is one more adaptive technique which builds on adagrad and further reduces it downside.\n",
    "- consider this as momentum + ADAGRAD.\n",
    "\n",
    "Pseudocode.\n",
    "\n",
    "$ adapted_gradient = previous_gradient + ((gradient – previous_gradient) * (1 – beta1))$\n",
    "\n",
    "$ gradient_component = (gradient_change – previous_learning_rate)$\n",
    "\n",
    "$ adapted_learning_rate =  previous_learning_rate + (gradient_component * (1 – beta2))$\n",
    "\n",
    "$ update = adapted_learning_rate * adapted_gradient$\n",
    "\n",
    "$ parameter = parameter – update$\n",
    "\n",
    "where:\n",
    "- beta1 and beta2 are constants to keep changes in gradient and learning rate in check\n",
    "\n",
    "There are also second order differentiation method like **l-BFGS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ec0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDRegressor:\n",
    "    \n",
    "    def __init__(self,learning_rate=0.01,epochs=100):\n",
    "        \n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def fit(self,X_train,y_train):\n",
    "        # init your coefs\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            # update all the coef and the intercept\n",
    "            y_hat = np.dot(X_train,self.coef_) + self.intercept_\n",
    "            #print(\"Shape of y_hat\",y_hat.shape)\n",
    "            intercept_der = -2 * np.mean(y_train - y_hat)\n",
    "            self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "            \n",
    "            coef_der = -2 * np.dot((y_train - y_hat),X_train)/X_train.shape[0]\n",
    "            self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "        \n",
    "        print(self.intercept_,self.coef_)\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c381ad",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent Algorithm**\n",
    "- stochastic you take a sample while computing the gradient.\n",
    "    - It randomly selects a training dataset example, \n",
    "        - changes the parameters for each training sample one at a time for each training example in the dataset.\n",
    "            - The regular updates give us a fairly accurate idea of the rate of improvement. (benefit)\n",
    "    - computes the gradient of the cost function for that example, \n",
    "    - and updates the parameters in the opposite direction.\n",
    "- stochastic gradient descent algorithm is more suitable for large datasets.\n",
    "- It is computationally efficient and can converge faster than batch gradient descent. It can be noisy (produce noisy gradients), cause the error rate to fluctuate rather than gradually go down and may not converge to the global minimum.\n",
    "\n",
    "Advantages\n",
    "- You can instantly see your model’s performance and improvement rates with frequent updates.\n",
    "- This variant of the steepest descent method is probably the easiest to understand and implement, especially for beginners.\n",
    "- Increasing the frequency of model updates will allow you to learn more about some issues faster.\n",
    "- The noisy update process allows the model to avoid local minima (e.g., premature convergence).\n",
    "- Faster and require less computational power.\n",
    "- Suitable for the larger dataset.\n",
    "\n",
    "Disadvantages\n",
    "- Frequent model updates are more computationally intensive than other steepest descent configurations, and it takes considerable time to train the model with large datasets.\n",
    "- Frequent updates can result in noisy gradient signals. This can result in model parameters and cause errors to fly around (more variance across the training epoch).\n",
    "- A noisy learning process along the error gradient can also make it difficult for the algorithm to commit to the model’s minimum error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c4540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "clf.fit(X, y)\n",
    "SGDClassifier(max_iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b03ce9",
   "metadata": {},
   "source": [
    "**Mini-batch Gradient Descent**\n",
    "- Mini-batch is a good compromise between the two and is often used in practice.\n",
    "- updates the model’s parameters using the gradient of a small batch size of the training dataset, known as a mini-batch. \n",
    "- It calculates the average gradient of the cost function for the mini-batch and updates the parameters in the opposite direction.\n",
    "- It is the most commonly used method in practice because combines the ideas of batch gradient descent with SGD.\n",
    "        - strikes a balance between batch gradient descent’s effectiveness and stochastic gradient descent’s durability.\n",
    "- It is computationally efficient and less noisy than stochastic gradient descent while still being able to converge to a good solution.\n",
    "- Mini-batch sizes typically range from 50 to 256.\n",
    "\n",
    "Advantages\n",
    "- The model is updated more frequently than the stack gradient descent method, allowing for more robust convergence and avoiding local minima.\n",
    "- Batch updates provide a more computationally efficient process than stochastic gradient descent.\n",
    "- Batch processing allows for both the efficiency of not having all the training data in memory and implementing the algorithm.\n",
    "\n",
    "Disadvantages\n",
    "- Mini-batch requires additional hyperparameters “mini-batch size” to be set for the learning algorithm.\n",
    "- Error information should be accumulated over a mini-batch of training samples, such as batch gradient descent.\n",
    "- it will generate complex functions.\n",
    "\n",
    "Configure Mini-Batch Gradient Descent:\n",
    "\n",
    "- The mini-batch steepest descent method is a variant of the steepest descent method recommended for most applications, intense learning.\n",
    "- Mini-batch sizes, commonly called “batch sizes” for brevity, are often tailored to some aspect of the computing architecture in which the implementation is running. \n",
    "        - For example, a power of 2 that matches the memory requirements of the GPU or CPU hardware, such as 32, 64, 128, and 256.\n",
    "- The stack size is a slider for the learning process.\n",
    "- Smaller values ​​allow the learning process to converge quickly at the expense of noise in the training process. Larger values ​​result in a learning - process that slowly converges to an accurate estimate of the error gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8eaf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBGDRegressor:\n",
    "    \n",
    "    def __init__(self,batch_size,learning_rate=0.01,epochs=100):\n",
    "        \n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def fit(self,X_train,y_train):\n",
    "        # init your coefs\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            for j in range(int(X_train.shape[0]/self.batch_size)):\n",
    "                \n",
    "                idx = random.sample(range(X_train.shape[0]),self.batch_size)\n",
    "                \n",
    "                y_hat = np.dot(X_train[idx],self.coef_) + self.intercept_\n",
    "                #print(\"Shape of y_hat\",y_hat.shape)\n",
    "                intercept_der = -2 * np.mean(y_train[idx] - y_hat)\n",
    "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "\n",
    "                coef_der = -2 * np.dot((y_train[idx] - y_hat),X_train[idx])\n",
    "                self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "        \n",
    "        print(self.intercept_,self.coef_)\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c2ddb9",
   "metadata": {},
   "source": [
    "**Step 6: Use the Model for Decision-Making**\n",
    "\n",
    "Understanding which factors significantly influence readmissions,\n",
    "\n",
    "To do this, you need a systematic approach grounded in exploratory analysis, statistical rigor, and effective communication\n",
    "\n",
    "1. Thinking Approach: Identifying Significant Factors\n",
    "- Define the Business Objective\n",
    "    - Objective: Identify key drivers of hospital readmissions (to improve patient care and optimize resource allocation)\n",
    "    - Questions to Answer:\n",
    "        - What are the strongest predictors of readmissions?\n",
    "        - Which predictors can be influenced through policy or operational changes?\n",
    "        - How much can readmissions be reduced if certain factors are addressed?\n",
    "\n",
    "- Perform Exploratory Data Analysis (EDA)\n",
    "    - Inspect Data Distributions: Use histograms and boxplots to understand the spread of variables.\n",
    "    - Check Relationships:\n",
    "        - Pairwise correlations for numerical variables (e.g., length_of_stay vs. readmissions).\n",
    "        - Grouped summaries for categorical variables (e.g., readmissions across age groups).\n",
    "        - Example Insights:\n",
    "            - Patients with longer stays might have higher readmission risks.\n",
    "            - Non-adherence to medication might strongly correlate with readmissions.\n",
    "\n",
    "- Statistical Hypothesis Testing\n",
    "    - Use statistical tests to confirm relationships:\n",
    "        - T-tests for differences in means (e.g., medication adherence between high and low readmission groups).\n",
    "        - Chi-square tests for independence between categorical variables (e.g., age group vs. readmission rates).\n",
    "\n",
    "Example 1: Statistical Hypothesis Testing for Medication Adherence\n",
    "- Objective: Determine if medication adherence significantly differs between patients who are readmitted and those who are not.\n",
    "- Approach: Two-Sample t-Test\n",
    "- Hypotheses: \n",
    "    - $𝐻_0$ : The mean adherence rate is the same for both groups (readmitted and not readmitted).\n",
    "    - $𝐻_𝑎$ : The mean adherence rate differs between the groups.\n",
    "\n",
    "- Steps:\n",
    "    - Prepare the Data:\n",
    "    - Split patients into two groups: \"Readmitted\" and \"Not Readmitted.\"\n",
    "    - Collect medication adherence rates for each group.\n",
    "\n",
    "- Check Assumptions:\n",
    "    - Normality: Use a Shapiro-Wilk or Kolmogorov-Smirnov test to check if adherence rates are normally distributed.\n",
    "    - Equal Variance: Use Levene’s test or Bartlett’s test.\n",
    "\n",
    "- Perform the t-Test:\n",
    "    - If variances are equal, use a standard t-test. If not, use Welch’s t-test.\n",
    "\n",
    "- Interpret Results: \n",
    "    - If $𝑝 < 0.05$, reject $𝐻_0$\n",
    "    - Conclude that adherence rates differ significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21143b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Example data\n",
    "adherence_readmitted = [0.7, 0.65, 0.6, 0.75, 0.8]  # Adherence rates for readmitted\n",
    "adherence_not_readmitted = [0.9, 0.85, 0.88, 0.92, 0.89]  # Adherence rates for not readmitted\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = ttest_ind(adherence_readmitted, adherence_not_readmitted, equal_var=False)\n",
    "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3ddb9d",
   "metadata": {},
   "source": [
    "Example 2: Statistical Hypothesis Testing for Age Group vs. Readmission Rates\n",
    "- Objective: Test if age group (categorical variable) is independent of readmission status.\n",
    "- Approach: Chi-Square Test of Independence\n",
    "- Hypotheses:\n",
    "    - $𝐻_0$ : Age group is independent of readmission status.\n",
    "    - $𝐻_𝑎$ : Age group and readmission status are dependent.\n",
    "\n",
    "- Steps:\n",
    "    - Create a Contingency Table:\n",
    "        - Rows: Age groups (e.g., <40, 40–60, >60).\n",
    "        - Columns: Readmission status (e.g., Yes, No).\n",
    "\n",
    "- Perform the Chi-Square Test:\n",
    "\n",
    "- Interpret Results:\n",
    "    - If $ 𝑝< 0.05$, reject $𝐻_0$​\n",
    "    - Conclude that age group influences readmission rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aecd0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Contingency table\n",
    "table = np.array([[50, 200], [70, 230], [100, 300]])\n",
    "\n",
    "# Perform Chi-Square Test\n",
    "chi2, p_value, dof, expected = chi2_contingency(table)\n",
    "print(f\"Chi2 Statistic: {chi2}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d538a",
   "metadata": {},
   "source": [
    "Example 3: Statistical Hypothesis Testing for Length of Stay (LOS)\n",
    "- Objective: Compare Average LOS for Readmitted vs. Not Readmitted Patients\n",
    "- Approach: Two-Sample t-Test\n",
    "    - $𝐻_0$ : The mean LOS is the same for readmitted and non-readmitted patients.\n",
    "    - $𝐻_𝑎$ : The mean LOS differs.\n",
    "- Steps:\n",
    "    - Prepare the Data:\n",
    "    - Split patients into two groups: \"Readmitted\" and \"Not Readmitted.\"\n",
    "    - Collect medication Length of stay for each group.\n",
    "\n",
    "- Check Assumptions:\n",
    "    - Normality: Use a Shapiro-Wilk or Kolmogorov-Smirnov test to check if Lengths of stay are normally distributed.\n",
    "    - Equal Variance: Use Levene’s test or Bartlett’s test.\n",
    "\n",
    "- Perform the t-Test:\n",
    "    - If variances are equal, use a standard t-test. If not, use Welch’s t-test.\n",
    "\n",
    "- Interpret Results: \n",
    "    - If $𝑝 < 0.05$, reject $𝐻_0$\n",
    "    - Conclude that adherence rates differ significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a39723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe8fb84d",
   "metadata": {},
   "source": [
    "Example 4: Relationship Between LOS and Readmission Rate\n",
    "- Approach: ANOVA (Analysis of Variance)\n",
    "- Objective: Check if LOS groups (<3 days, 3–7 days, >7 days) have significantly different readmission rates.\n",
    "- Hypotheses: \n",
    "    - $𝐻_0$ : The mean readmission rate is the same across all LOS groups.\n",
    "    - $𝐻_𝑎$ : At least one group differs.\n",
    "- Steps:\n",
    "    - Group the Data:\n",
    "        - Divide LOS into groups.\n",
    "        - Calculate readmission rates for each group.\n",
    "- Perform ANOVA:\n",
    "- Interpret Results:\n",
    "    - If $𝑝 < 0.05$\n",
    "    - reject $𝐻_0$\n",
    "    - Conclude that LOS impacts readmission rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab649501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Example data\n",
    "readmission_short = [0.1, 0.12, 0.08, 0.15]  # Readmission rates for <3 days\n",
    "readmission_medium = [0.2, 0.22, 0.25, 0.18]  # Readmission rates for 3–7 days\n",
    "readmission_long = [0.35, 0.4, 0.38, 0.42]  # Readmission rates for >7 days\n",
    "\n",
    "# Perform ANOVA\n",
    "f_stat, p_value = f_oneway(readmission_short, readmission_medium, readmission_long)\n",
    "print(f\"F-statistic: {f_stat}, P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be03fb0",
   "metadata": {},
   "source": [
    "\n",
    "- Build and Interpret a Regression Model\n",
    "    - Fit the Linear Regression model to identify significant predictors:\n",
    "    - Check p-values of coefficients: Variables with p-values below a chosen threshold (e.g., 0.05) are statistically significant.\n",
    "    - Evaluate effect size: Large coefficients indicate strong influence on the target.\n",
    "    - Test for interaction effects, such as how length_of_stay and severity jointly influence readmissions.\n",
    "\n",
    "- Refine the Model\n",
    "    - Handle multicollinearity: Use Variance Inflation Factor (VIF) to remove or combine highly correlated predictors.\n",
    "    - Validate the model: Perform cross-validation to ensure robustness.\n",
    "\n",
    "This will help the institute to:\n",
    "- Improve medication adherence programs for high-risk patients.\n",
    "- Extend hospital stays for patients with severe conditions if needed.\n",
    "- Schedule follow-up visits more effectively to minimize readmission risks.\n",
    "\n",
    "Example 2: Predicting Readmissions Based on LOS\n",
    "- Approach: Linear Regression\n",
    "- Objective: Use regression to predict readmissions based on LOS and other predictors.\n",
    "\n",
    "##### Linear Regression Helps Solve This Problem\n",
    "- Quantifies Relationships: Identifies and quantifies the factors contributing to readmissions.\n",
    "- Predicts Outcomes: Provides actionable predictions to guide healthcare interventions.\n",
    "- Allocates Resources: Helps prioritize patients who need more attention post-discharge.\n",
    "- Supports Policy Changes: Enables data-driven policy improvements in patient care."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03949464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Example data\n",
    "X = [2, 4, 6, 8, 10]  # LOS\n",
    "y = [0, 1, 0, 1, 1]  # Readmission (0 = No, 1 = Yes)\n",
    "\n",
    "# Add constant for intercept\n",
    "X = sm.add_constant(X)\n",
    "model = sm.Logit(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d99cd9",
   "metadata": {},
   "source": [
    "2. Presenting Findings to Senior Management and Board\n",
    "- Tailor Communication to the Audience\n",
    "    - Senior management: Focus on actionable insights, resource implications, and patient care improvements.\n",
    "    - Board of directors: Emphasize high-level trends, financial impacts, and alignment with strategic goals.\n",
    "\n",
    "- Structure of Presentation\n",
    "    - Introduction\n",
    "        - Start with the context: \"Readmission rates are a critical indicator of hospital performance and patient care quality.\"\n",
    "        - Summarize the objective: \"This study identifies key factors driving readmissions and proposes targeted interventions.\"\n",
    "\n",
    "    - Key Findings\n",
    "        - Use visuals like \n",
    "            - bar charts, \n",
    "            - scatter plots, and \n",
    "            - regression coefficient tables:\n",
    "                - Example: \"Medication adherence has the strongest inverse relationship with readmissions. A 10% increase in adherence reduces readmissions by 5%.\"\n",
    "            - Highlight statistical significance:\n",
    "                - \"Length of stay and severity are significant at p < 0.05, confirming their importance.\"\n",
    "    \n",
    "    - Implications\n",
    "        - Show real-world impact: \"Addressing non-adherence could prevent ~300 readmissions annually, saving $1.2M in costs.\"\n",
    "        - Prioritize recommendations: \"Focus on medication adherence programs, especially for older patients with comorbidities.\"\n",
    "\n",
    "    - Actionable Recommendations\n",
    "        - Immediate Steps:\n",
    "            - Develop a post-discharge follow-up protocol for high-risk groups.\n",
    "            - Launch an adherence monitoring program.\n",
    "        - Future Research:\n",
    "            - Investigate additional factors like social determinants of health.\n",
    "\n",
    "    - Conclusion\n",
    "        - Reinforce value: \"By addressing these factors, we can improve patient outcomes, meet regulatory benchmarks, and reduce financial strain.\"\n",
    "\n",
    "- Tools for Communication\n",
    "    - Visual Dashboards: Create dashboards showing predicted readmissions, trends over time, and \"what-if\" scenarios.\n",
    "    - Executive Summaries: Provide concise summaries with high-impact visuals and key takeaways.\n",
    "    - Financial Impact Models: Quantify cost savings or ROI of proposed interventions.\n",
    "\n",
    "3. Example Insights and Visualizations\n",
    "Insight Example: Medication Adherence\n",
    "    - Insight: \"Medication adherence has a strong negative correlation with readmissions ($𝑅=−0.65$)\n",
    "        - A 10% increase in adherence is associated with a 5% reduction in readmissions.\"\n",
    "\n",
    "Visualization:\n",
    "    - A bar chart comparing adherence rates and average readmissions.\n",
    "    - Regression coefficient chart showing the magnitude of influence.\n",
    "\n",
    "Insight Example: Length of Stay\n",
    "    - Insight: \"Patients with hospital stays >7 days are 2x more likely to be readmitted within 30 days.\"\n",
    "\n",
    "Visualization:\n",
    "    - Scatter plot: length_of_stay vs. readmissions.\n",
    "    - Box plot: Readmission rates by length-of-stay categories.\n",
    "\n",
    "4. Implementation Plan\n",
    "Once the board approves, focus on operationalizing findings:\n",
    "\n",
    "- Deploy targeted interventions for high-risk patients.\n",
    "- Set KPIs to monitor the effectiveness of changes.\n",
    "- Continuously refine the model based on new data.\n",
    "\n",
    "##### Set KPIs to monitor the effectiveness of changes\n",
    "\n",
    "**KPI 1: 30-Day Readmission Rate**\n",
    "- Definition: Percentage of patients readmitted to the hospital within 30 days of discharge.\n",
    "- Why Important: This is the primary metric to assess whether interventions are reducing readmissions.\n",
    "- Formula: $Readmission Rate = \\frac{Number of patients readmitted within 30 days}{Total number of discharged patients} × 100$\n",
    "- Target: A reduction in the readmission rate over time indicates success.\n",
    "\n",
    "**KPI 2: Medication Adherence Rate**\n",
    "- Definition: Percentage of patients adhering to their prescribed medications post-discharge.\n",
    "- Why Important: Non-adherence is a leading cause of readmissions. Monitoring this ensures interventions like counseling and follow-ups are effective\n",
    "- Formula: $Medication Adherence Rate = \\frac{Number of patients adhering to medications}{Total number of patients} × 100$\n",
    "- Target: An increase in adherence correlates with better outcomes and fewer readmissions.\n",
    "\n",
    "**KPI 3: Follow-Up Appointment Compliance**\n",
    "- Definition: Percentage of discharged patients attending follow-up appointments within the recommended time frame.\n",
    "- Why Important: Follow-up visits can identify issues early and prevent readmissions.\n",
    "- Formula: $Compliance Rate= \\frac{Number of scheduled follow-ups}{Number of attended follow-ups} × 100$\n",
    "- Target: High compliance indicates improved patient engagement.\n",
    "\n",
    "**KPI 4: Average Length of Stay (LOS)**\n",
    "- Definition: Average number of days patients spend in the hospital.\n",
    "- Why Important: Shorter stays can indicate efficiency but might increase readmissions if patients are discharged prematurely.\n",
    "- Formula: $LOS= \\frac{Number of discharges}{Total inpatient days}$\n",
    "​- Target: Maintain an optimal LOS that balances cost and readmission prevention.\n",
    "\n",
    "**KPI 5: Percentage of High-Risk Patients Identified**\n",
    "- Definition: Proportion of discharged patients flagged as high-risk for readmission and targeted for interventions.\n",
    "- Why Important: Monitoring ensures that predictive models and risk stratification tools are working effectively.\n",
    "- Formula:$High-Risk Patients Identified = \\frac{Total number of discharged patients}{Number of flagged high-risk patients} × 100$\n",
    "- Target: Increase the identification rate while reducing actual readmissions.\n",
    "\n",
    "##### Presenting KPIs to Stakeholders\n",
    "\n",
    "**Visual Presentation**\n",
    "\n",
    "Use dashboards and visualizations:\n",
    "- Bar charts to compare readmission rates before and after interventions.\n",
    "- Line graphs showing trends over time for medication adherence and follow-up compliance.\n",
    "- Heatmaps for condition-specific readmission trends.\n",
    "\n",
    "Narrative\n",
    "- Highlight success: \"We reduced the 30-day readmission rate from 18% to 12%, saving $500,000 annually.\"\n",
    "- Focus on actionable insights: \"Medication adherence programs have been effective, with a 15% increase in adherence leading to a 5% drop in readmissions.\"\n",
    "\n",
    "Recommendations\n",
    "- Continue monitoring these KPIs for sustained improvements.\n",
    "- Scale successful interventions to other patient groups or hospitals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f51a4",
   "metadata": {},
   "source": [
    "## 2. Multiple Linear Regression:\n",
    "\n",
    "simple linear regression equation is as follows:\n",
    "\n",
    "$$Y = \\beta_{0} + \\beta_{1}X_1$$\n",
    "\n",
    "where:\n",
    "- $\\beta_{0}$ is the intercept, interpreted as the value of $Y$ when $X_1 = 0$;\n",
    "- $\\beta_{1}$ is the coefficient, interpreted as the effect on $Y$ for a one unit increase in $X_1$; and\n",
    "- $X_1$ is the single predictor variable.\n",
    "\n",
    "Extending that idea to multiple linear regression is as simple as adding an $X_{j}$ and corresponding $\\beta_{j}$ for each of the $p$ predictor variables, where $j$ is an element of the set $[1,p]$.\n",
    "   \n",
    "Hence in multiple linear regression, our regression equation becomes:   \n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $Y$ is the reponse variable which depends on the $p$ predictor variables;\n",
    "- $\\beta_0$ is the intercept, interpreted as the value of $Y$ when _all_ predictor variables are equal to zero;\n",
    "- $\\beta_j$ is the average effect on $Y$ of a one unit increase in $X_j$, assuming all other predictors are held fixed.\n",
    "\n",
    "Multiple linear regression is a technique to understand the relationship between a single dependent variable and multiple independent variables.\n",
    "\n",
    "$$ 𝑦=𝛽_{0}+𝛽_{1}𝑥_{1}+…+𝛽_{𝑛}𝑥_{𝑛}+ 𝜖 $$\n",
    "\n",
    "What it means:\n",
    "- It is used when two or more independent variables influence the dependant variable. \n",
    "\n",
    "- A linear equation defines the relationship, with the \n",
    "    - coefficients of the independent variables \n",
    "    \n",
    "- representing the effect of each variable on the dependant variable.\n",
    "\n",
    "# Assumptions of Multiple Linear Regression\n",
    "\n",
    "Regression is a parametric approach, which means that it makes assumptions about the data\n",
    "\n",
    "For successful regression analysis, it’s essential to validate the following assumptions.\n",
    "\n",
    "- Overfitting: When more and more variables are added to a model, the model may become far too complex and usually ends up memorizing all the data points in the training set\n",
    "    - This phenomenon is known as the overfitting of a model. \n",
    "    - This usually leads to high training accuracy and very low test accuracy.\n",
    "- Understanding of linearity and multicollinearity (predictors).\n",
    "    - It is the phenomenon where a model with several independent variables, may have some variables interrelated.\n",
    "- Understanding of independence, homoscedasticity, and normality (residuals).\n",
    "- Feature Selection: With more variables present, selecting the optimal set of predictors from the pool of given features (many of which might be redundant) becomes an important task for building a relevant and better model.\n",
    "\n",
    "We'll be moving through the following sections in order to achieve our objectives:\n",
    "\n",
    "- Investigating our predictor variables:\n",
    "    - Checking for linearity;\n",
    "    - Checking for multicollinearity;\n",
    "- Fitting a model with `statsmodels.OLS`;\n",
    "- Evaluating our fitted model:\n",
    "    - Checking for independence;\n",
    "    - Checking for homoscedasticity;\n",
    "    - Checking for normaility;\n",
    "    - Checking for outliers.\n",
    "\n",
    "### Checking for Linearity\n",
    "\n",
    "Linearity is a key assumption in multilinear regression. It states that the relationship between each predictor and the response variable should be linear. When this assumption is violated, the model's predictions may be biased or less effective.\n",
    "\n",
    "The first thing we need to check is the mathematical relationship between each predictor variable and the response variable. == linearity. \n",
    "- A linear relationship means that a change in the response *Y* due to a one-unit change in the predictor $X_j$ is constant, regardless of the value of $X_j$.\n",
    "\n",
    "If we fit a regression model to a dataset that is non-linear, \n",
    "- it will fail to adequately capture the relationship in the data - resulting in a mathematically inappropriate model. \n",
    "\n",
    "### Detecting Non-Linearity\n",
    "\n",
    "To check for linearity, \n",
    "- we can produce scatter plots of each individual predictor against the response variable. \n",
    "- The intuition here is that we are looking for obvious linear relationships.\n",
    "\n",
    "**Result**\n",
    "\n",
    "- State what appears of the variables that have an approximately linear relationship.\n",
    "- State that exhibits no linearity with resonse variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,5, figsize=(14,6),)\n",
    "fig.subplots_adjust(hspace = 0.5, wspace=.2)\n",
    "axs = axs.ravel()\n",
    "\n",
    "for index, column in enumerate(df.columns):\n",
    "    axs[index-1].set_title(\"{} vs. mpg\".format(column),fontsize=16)\n",
    "    axs[index-1].scatter(x=df[column],y=df['mpg'],color='blue',edgecolor='k')\n",
    "    \n",
    "fig.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17497ab2",
   "metadata": {},
   "source": [
    "Step 1: Diagnosing Non-Linearity\n",
    "\n",
    "Visual Inspection\n",
    "- Use scatter plots to visualize the relationship between predictors and the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20442d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plots for each predictor vs. response\n",
    "for predictor in [\"SquareFootage\", \"Bedrooms\", \"DistanceFromCityCenter\"]:\n",
    "    sns.scatterplot(x=df[predictor], y=df[\"HousePrice\"])\n",
    "    plt.title(f\"{predictor} vs. HousePrice\")\n",
    "    plt.xlabel(predictor)\n",
    "    plt.ylabel(\"HousePrice\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot to visualize relationships\n",
    "sns.pairplot(df, x_vars=[\"SquareFootage\", \"Bedrooms\", \"DistanceFromCityCenter\"], y_vars=\"HousePrice\", kind=\"reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bb6195",
   "metadata": {},
   "source": [
    "Residual Plots\n",
    "- Residual plots help check for linearity by plotting residuals against predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85a50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted values and residuals\n",
    "predicted = model.predict(X)\n",
    "residuals = Y - predicted\n",
    "\n",
    "# Residual plot\n",
    "plt.scatter(predicted, residuals)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb68e22",
   "metadata": {},
   "source": [
    "If you see a pattern (e.g., curves or increasing spread), it indicates non-linearity.\n",
    "\n",
    "If the relationship between variables is non-linear Applying Log Transformation:\n",
    "\n",
    "Step 2: Transforming Predictors or the Response Variable\n",
    "\n",
    "When to Transform\n",
    "- Predictors: Transform when individual predictors have non-linear relationships with the response.\n",
    "- Response Variable: Transform when the response itself shows a skewed distribution or non-linear relationship with predictors.\n",
    "\n",
    "|Transformation |Formula|Use Case|\n",
    "|---------------|-------|--------|\n",
    "|Log            |$log(x)$|Skewed data, multiplicative relationships, exponential growth.\n",
    "|Square Root\t|$\\sqrt{x}$ |Reduces spread while preserving the order of values.|\n",
    "|Polynomial\t    |$𝑥^2, x^3,...$|For non-linear relationships that resemble curves.|\n",
    "|Reciprocal\t\t|$\\frac{1}{x}$|When values decrease rapidly as the predictor increases.|\n",
    "|Box-Cox\t\t|$y^{𝜆}$|Optimal transformation for normalizing data or reducing variance.|\n",
    "\n",
    "Step 3: Applying Transformations in Python\n",
    "\n",
    "Example 1: Log Transformation\n",
    "- Suppose SquareFootage has a non-linear relationship with HousePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc9623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformation of SquareFootage\n",
    "df[\"Log_SquareFootage\"] = np.log(df[\"SquareFootage\"])\n",
    "\n",
    "# Fit the model again\n",
    "X_trans = df[[\"Log_SquareFootage\", \"Bedrooms\", \"DistanceFromCityCenter\"]]\n",
    "X_trans = sm.add_constant(X_trans)\n",
    "model_trans = sm.OLS(Y, X_trans).fit()\n",
    "\n",
    "print(model_trans.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36275b0",
   "metadata": {},
   "source": [
    "Example 2: Polynomial Transformation\n",
    "- Suppose DistanceFromCityCenter has a curved relationship with HousePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2da387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polynomial terms\n",
    "df[\"Distance_Squared\"] = df[\"DistanceFromCityCenter\"] ** 2\n",
    "\n",
    "# Fit model with polynomial term\n",
    "X_poly = df[[\"SquareFootage\", \"Bedrooms\", \"DistanceFromCityCenter\", \"Distance_Squared\"]]\n",
    "X_poly = sm.add_constant(X_poly)\n",
    "model_poly = sm.OLS(Y, X_poly).fit()\n",
    "\n",
    "print(model_poly.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930bf477",
   "metadata": {},
   "source": [
    "Example 3: Box-Cox Transformation for Response Variable\n",
    "- Normalize HousePrice if it's highly skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12520314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "# Box-Cox transformation\n",
    "Y_boxcox, lambda_boxcox = boxcox(Y)\n",
    "print(f\"Optimal lambda for Box-Cox: {lambda_boxcox}\")\n",
    "\n",
    "# Fit model with transformed response\n",
    "model_boxcox = sm.OLS(Y_boxcox, X).fit()\n",
    "print(model_boxcox.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6fdfd7",
   "metadata": {},
   "source": [
    "Step 4: Comparing Models\n",
    "Use metrics like Adjusted $𝑅^2$ , AIC, and BIC to compare the effectiveness of models before and after transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a398e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "print(\"Original Model AIC:\", model.aic)\n",
    "print(\"Log-Transformed Model AIC:\", model_log.aic)\n",
    "print(\"Polynomial Model AIC:\", model_poly.aic)\n",
    "print(\"Box-Cox Model AIC:\", model_boxcox.aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148429d4",
   "metadata": {},
   "source": [
    "Step 5: Visualizing and Validating Improvements\n",
    "- Visualizing Residuals After Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9597d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plot after transformation\n",
    "predicted_trans = model_log.predict(X_trans)\n",
    "residuals_trans = Y - predicted_trans\n",
    "\n",
    "plt.scatter(predicted_trans, residuals_trans)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.title(\"Residual Plot After Transformation\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32374997",
   "metadata": {},
   "source": [
    "Checking $𝑅^2$ and Adjusted $𝑅^2$ Compare values before and after applying transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0a58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original Model R^2: {model.rsquared}\")\n",
    "print(f\"Transformed Model R^2: {model_log.rsquared}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d75827",
   "metadata": {},
   "source": [
    "### Checking for Multicollinearity\n",
    "\n",
    "Multicollinearity occurs when predictors in a regression model are highly correlated. This can inflate standard errors, making it difficult to assess the individual impact of predictors on the response variable.\n",
    "- As multicollinearity makes it difficult to find out which variable is contributing towards the prediction of the response variable, it leads one to conclude incorrectly, the effects of a variable on the target variable.\n",
    "- Properly detect and deal with the multicollinearity present in the model, as random removal of any of these correlated variables from the model causes the coefficient values to swing wildly and even change signs.\n",
    "\n",
    "Multicollinearity refers to the presence of strong correlation among two or more of the predictor variables in the dataset. The presence of any correlation among predictors is detrimental to model quality for two reasons:\n",
    "\n",
    "- It tends to increase the standard error;\n",
    "\n",
    "- It becomes difficult to estimate the effect of any one predictor variable on the response variable.\n",
    "\n",
    "We will check for multicollinearity by generating \n",
    "- pairwise scatter plots among predictors\n",
    "- a correlation heatmap.\n",
    "\n",
    "Multicollinearity can be detected using the following methods.\n",
    "\n",
    "- Pairwise Correlations: Checking the pairwise correlations between different pairs of independent variables can throw useful insights into detecting multicollinearity.\n",
    "    - Pairwise correlations may not always be useful as it is possible that just one variable might not be able to completely explain some other variable but some of the variables combined could be ready to do this.  Thus, to check these sorts of relations between variables, one can use VIF:\n",
    "- Variance Inflation Factor (VIF): VIF explains the relationship of one independent variable with all the other independent variables. \n",
    "    - VIF is given by,\n",
    "\n",
    "$ VIF = \\frac{1}{1 - R^2}$\n",
    "\n",
    "where \n",
    "- $i$ refers to the $ith$ variable which is being represented as a linear combination of the rest of the independent variables.\n",
    "\n",
    "Heuristics\n",
    "- if VIF > 10 then the value is high and it should be dropped.\n",
    "- if the VIF=5 then it may be valid but should be inspected first.\n",
    "- if VIF < 5, then it is considered a good VIF value.\n",
    "\n",
    "**Step 1: Detecting Multicollinearity**\n",
    "\n",
    "(a) Pairwise scatter plots\n",
    "\n",
    "As can be inferred by the name, a pairwise scatter plot simply produces a visual $n \\times n$ matrix, where $n$ is the total number of variables compared, in which each cell represents the relationship between two variables. The diagonal cells of this visual represent the comparison of a variable with itself, and as such are substituted by a representation of the distribution of values taken by the visual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c028bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the number of visuals created, this codeblock takes about one minute to run.\n",
    "from seaborn import pairplot\n",
    "g = pairplot(df1.drop('mpg', axis='columns'))\n",
    "g.fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b9774",
   "metadata": {},
   "source": [
    "(b) Correlation Matrix\n",
    "- Use a correlation matrix to identify highly correlated predictors.\n",
    "\n",
    "Correlation heatmap\n",
    "\n",
    "Another way we can visually discover linearity between two or more variables within our dataset is through the use of a correlation heatmap. Similar to the pairwise scatter plot we produced above, this visual presents a matrix in which each row represents a distinct variable, with each colum representing the correlation between this variable and another one within the dataset.\n",
    "\n",
    "Result Interpretation\n",
    "- Look for correlations > 0.8 or < -0.8, which may indicate multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only compare the predictor variables, and thus drop the target `mpg` column.\n",
    "corr = df1.drop('mpg', axis='columns').corr()\n",
    "\n",
    "from statsmodels.graphics.correlation import plot_corr\n",
    "\n",
    "fig=plot_corr(corr,xnames=corr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = df[[\"SquareFootage\", \"Bedrooms\", \"DistanceFromCityCenter\"]].corr()\n",
    "\n",
    "# Display the heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166af992",
   "metadata": {},
   "source": [
    "(c) Variance Inflation Factor (VIF)\n",
    "- VIF quantifies how much the variance of a regression coefficient is inflated due to multicollinearity.\n",
    "\n",
    "Result Interpretation\n",
    "\n",
    "Rule of Thumb:\n",
    "- $VIF=1$: No multicollinearity.\n",
    "- $1<VIF<5$: Low multicollinearity.\n",
    "- $VIF>5$: High multicollinearity.\n",
    "- $VIF>10$: Severe multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac61546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Calculate VIF for each predictor\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7466f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Prepare data for VIF calculation\n",
    "X_vif = df[[\"SquareFootage\", \"Bedrooms\", \"DistanceFromCityCenter\"]]\n",
    "X_vif = sm.add_constant(X_vif)\n",
    "\n",
    "# Calculate VIF for each predictor\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed731934",
   "metadata": {},
   "source": [
    "**Step 2: Mitigating Multicollinearity**\n",
    "\n",
    "1. Drop Highly Correlated Predictors:\n",
    "- If two predictors are highly correlated, remove one to reduce redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876fcbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = X.drop(columns=[\"Bedrooms\"])  # Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad41427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Bedrooms' if it has high multicollinearity\n",
    "X_reduced = df[[\"SquareFootage\", \"DistanceFromCityCenter\"]]\n",
    "X_reduced = sm.add_constant(X_reduced)\n",
    "\n",
    "# Fit the model with reduced predictors\n",
    "model_reduced = sm.OLS(Y, X_reduced).fit()\n",
    "print(model_reduced.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819bb5c",
   "metadata": {},
   "source": [
    "2. Apply Ridge or Lasso Regression:\n",
    "\n",
    "- Ridge regression penalizes large coefficients to handle multicollinearity.\n",
    "- Lasso regression performs feature selection by shrinking some coefficients to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be237178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "ridge = Ridge(alpha=1.0)  # Regularization strength\n",
    "ridge.fit(X, Y)\n",
    "print(\"Ridge coefficients:\", ridge.coef_)\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, Y)\n",
    "print(\"Lasso coefficients:\", lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9fb022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ridge regression\n",
    "ridge = Ridge(alpha=1.0)  # Adjust alpha (regularization strength) as needed\n",
    "ridge.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate Ridge model\n",
    "ridge_predictions = ridge.predict(X_test)\n",
    "ridge_mse = mean_squared_error(Y_test, ridge_predictions)\n",
    "print(\"Ridge Regression MSE:\", ridge_mse)\n",
    "print(\"Ridge Coefficients:\", ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea2eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Lasso regression\n",
    "lasso = Lasso(alpha=0.1)  # Adjust alpha as needed\n",
    "lasso.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate Lasso model\n",
    "lasso_predictions = lasso.predict(X_test)\n",
    "lasso_mse = mean_squared_error(Y_test, lasso_predictions)\n",
    "print(\"Lasso Regression MSE:\", lasso_mse)\n",
    "print(\"Lasso Coefficients:\", lasso.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d20559",
   "metadata": {},
   "source": [
    "3. Principal Component Analysis (PCA):\n",
    "- PCA reduces dimensions by transforming correlated predictors into uncorrelated components.\n",
    "\n",
    "Interpreting PCA:\n",
    "- Principal components represent uncorrelated combinations of the original predictors.\n",
    "- The explained variance ratio tells you how much variance is captured by each componen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dd8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)  # Reduce dimensions\n",
    "X_pca = pca.fit_transform(X.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c0f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Scale predictors for PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X.iloc[:, 1:])  # Exclude constant term\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Choose number of components\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Fit model with PCA components\n",
    "model_pca = sm.OLS(Y, sm.add_constant(X_pca)).fit()\n",
    "print(model_pca.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a55d84",
   "metadata": {},
   "source": [
    "**Step 3: Comparing Models**\n",
    "\n",
    "Evaluate model performance before and after applying mitigation techniques using metrics such as:\n",
    "\n",
    "- Mean Squared Error (MSE)\n",
    "- Adjusted $𝑅^2$ \n",
    "- Akaike Information Criterion (AIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18bdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model AIC\n",
    "print(\"Original Model AIC:\", model.aic)\n",
    "print(\"Reduced Model AIC:\", model_reduced.aic)\n",
    "print(\"Ridge Model MSE:\", ridge_mse)\n",
    "print(\"Lasso Model MSE:\", lasso_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba46285",
   "metadata": {},
   "source": [
    "# Overfitting and Underfitting in Linear Regression\n",
    "\n",
    "When model performs well on training data but not on the test data.\n",
    "\n",
    "**Bias**\n",
    "\n",
    "Bias is a measure to determine how accurate a model’s predictions are likely to be on future unseen data.\n",
    "- Bias is errors made by training data.\n",
    "    - Complex models, assuming there is enough training data available, can make accurate model predictions. \n",
    "    - Models that are too naive, are very likely to perform badly concerning model predictions.\n",
    "- Linear algorithms have a high bias which makes them fast to learn and easier to understand but in general, are less flexible. \n",
    "    - Implying lower predictive performance on complex problems that fail to meet the expected outcomes.\n",
    "\n",
    "**Variance**\n",
    "\n",
    "Variance is the sensitivity of the model towards training data\n",
    "- it quantifies how much the model will react when input data is changed.\n",
    "    - model shouldn’t change too much from one training dataset to the next training data \n",
    "        - Whcih means that the algorithm is good at picking out the hidden underlying patterns between the inputs and the output variables.\n",
    "    - model should have lower variance which means that the model doesn’t change drastically after changing the training data(it is generalizable). \n",
    "        - Having higher variance will make a model change drastically even on a small change in the training dataset.\n",
    "\n",
    "**Bias Variance Tradeoff**\n",
    "\n",
    "A supervised machine learning algorithm seeks to strike a balance between low bias and low variance for increased robustness.\n",
    "\n",
    "The relationship between bias and variance is characterized by an inverse correlation.\n",
    "- Increased bias leads to reduced variance.\n",
    "- Conversely, heightened variance results in diminished bias.\n",
    "Finding an equilibrium between bias and variance is crucial, and algorithms must navigate this trade-off for optimal outcomes.\n",
    "\n",
    "**Overfitting**\n",
    "\n",
    "When a model learns every pattern and noise in the data to such an extent that it affects the performance of the model on the unseen future dataset.\n",
    "- model fits the data so well that it interprets noise as patterns in the data.\n",
    "\n",
    "Caused when a model has low bias and higher variance it ends up memorizing the data.\n",
    "\n",
    "Overfitting causes the model to become specific rather than generic. This usually leads to \n",
    "- high training accuracy and \n",
    "- very low test accuracy.\n",
    "\n",
    "There are several ways to prevent overfitting:\n",
    "- Cross-validation\n",
    "- If the training data is too small to train add more relevant and clean data.\n",
    "- If the training data is too large, do some feature selection and remove unnecessary features.\n",
    "- Regularization\n",
    "\n",
    "**Underfitting**\n",
    "\n",
    "When the model fails to learn from the training dataset and is also not able to generalize the test dataset.\n",
    "\n",
    "Detected by the performance metrics.\n",
    "\n",
    "When a model has high bias and low variance it ends up not generalizing the data and causing underfitting. \n",
    "- It is unable to find the hidden underlying patterns in the data. \n",
    "- This usually leads to low training accuracy and very low test accuracy.\n",
    "\n",
    "Ways to prevent underfitting:\n",
    "- Increase the model complexity\n",
    "- Increase the number of features in the training data\n",
    "- Remove noise from the data.\n",
    "\n",
    "### Fitting the model using `statsmodels.OLS`\n",
    "\n",
    "`sklearn` is limited in terms of metrics and tools available to evaluate the appropriateness of the regression models we fit.\n",
    "-As a means to expland our analysis, we import the `statsmodels` library which has a rich set of statistical tools to help us. \n",
    "\n",
    "##### Generating the regression string\n",
    "\n",
    "Those of you familiar with the R language will know that fitting a machine learning model requires a sort of string of the form:\n",
    "\n",
    "`y ~ X`\n",
    "\n",
    "which is read as follows: \"Regress y on X\". The `statsmodels` library works in a similar way, so we need to generate an appropriate string to feed to the method when we wish to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de013b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48989e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress target variable on all of the predictors.\n",
    "formula_str = df.columns[0]+' ~ '+'+'.join(df.columns[1:]); formula_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing seaborn library for visualizations\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# To plot all the scatterplots in a single plot\n",
    "sns.pairplot(df, x_vars=[ 'TV', ' Newspaper','Radio' ], y_vars = 'Sales', size = 4, kind = 'scatter' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f890be",
   "metadata": {},
   "source": [
    "##### Plotting 3D plot for multiple Linear regression\n",
    "\n",
    "To get a better idea of what a multi-dimensional dataset looks like, we'll generate a 3D scatter plot showing the `mpg` on the _z_-axis (height), with two predictor variables, `cyl` and `disp` on the _x_- and _y_-axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and 3d axes\n",
    "fig = plt.figure(figsize=(8,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# set axis labels\n",
    "ax.set_zlabel('MPG')\n",
    "ax.set_xlabel('No. of Cylinders')\n",
    "ax.set_ylabel('Weight (1000 lbs)')\n",
    "\n",
    "# scatter plot with response variable and 2 predictors\n",
    "ax.scatter(df['cyl'], df['wt'], df['mpg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e9a14",
   "metadata": {},
   "source": [
    "We know that in simple linear regression (2D), any model that we fit to data manifests in the form of a straight line. Extending this idea to 3D, the line becomes a plane - a flat surface which is chosen to minimise the squared vertical distances between each observation (red dots), and the plane, as shown in the figure below from ISLR.\n",
    "\n",
    "<img src=\"https://github.com/Explore-AI/Public-Data/raw/master/3D%20regression%20ISLR.jpg\" alt=\"plane\" style=\"width: 450px\"/>\n",
    "\n",
    "The result of a multivariate linear regression in higher dimensionality is known as a _hyperplane_ - similar to the flat surface in the figure above, but in a _p_-dimensional space, where $p>3$. Unfortunately, humans lack the ability to visualise any number of dimensions greater than three - so we have to be content with the idea that a hyperplane in _p_-dimensional space is effectively like a flat surface in 3-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e91c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot heatmap to find out correlations\n",
    "sns.heamap(df.corr(), cmap = 'YlGnBl', annot = True )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b531842",
   "metadata": {},
   "source": [
    "### Fitting the Multivariate Regression Model\n",
    "\n",
    "In `sklearn`, fitting a multiple linear regression model is much the same as fitting a simple linear regression. This time, of course, our $X$ contains multiple columns, where it only contained one before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb634e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, train_size = 0.7, test_size = 0.3, random_state = 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4ebbe",
   "metadata": {},
   "source": [
    "### Construct and fit the model\n",
    "\n",
    "We now go ahead and fit our model.\n",
    "- use the `ols` or Ordinary Least Squares regression model from the `statsmodels` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923aa12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to get an intercept\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "# Fit the resgression line using 'OLS'\n",
    "lr = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# OR\n",
    "\n",
    "model=sm.ols(formula=formula_str, data=df1)\n",
    "fitted = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a4053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the parameters,i.e. intercept and slope of the regression line obtained\n",
    "lr.params\n",
    "\n",
    "# extract model intercept\n",
    "beta_0 = float(lr.intercept_)\n",
    "\n",
    "# extract model coeffs\n",
    "beta_js = pd.DataFrame(lr.coef_, X.columns, columns=['Coefficient'])\n",
    "beta_js"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb2ae1",
   "metadata": {},
   "source": [
    "### Interpreting Coefficients of Multilinear Regression\n",
    "\n",
    "In a multilinear regression model, the coefficients represent the relationship between \n",
    "- each predictor (independent variable) and \n",
    "- the response (dependent variable), \n",
    "while controlling for the effects of other predictors in the model.\n",
    "\n",
    "Intercept ($𝛽_0$ | `beta_0`):\n",
    "- This is the predicted value of the response variable when all predictors are set to zero.\n",
    "- It is meaningful only if all predictors can realistically take a value of zero.\n",
    "\n",
    "Slope Coefficients ($𝛽_𝑖$ | `beta_js`):\n",
    "- Each $𝛽_𝑖$ measures the change in the response variable for a one-unit increase in predictor $𝑋_𝑖$, assuming all other predictors remain constant.\n",
    "- A positive $𝛽_𝑖$: Indicates that an increase in $𝑋_𝑖$ is associated with an increase in the response.\n",
    "- A negative $𝛽_𝑖$: Indicates that an increase in $𝑋_𝑖$ is associated with a decrease in the response.\n",
    "\n",
    "P-Values:\n",
    "- A p-value tests the null hypothesis that the coefficient $𝛽_{1}$ is zero (no effect). \n",
    "    - If the p-value is small (typically <0.05), the predictor is considered statistically significant in explaining the response variable.\n",
    "\n",
    "Standardized Coefficients:\n",
    "- If predictors are measured in different units, their coefficients can't be directly compared. Standardized coefficients (beta weights) are used to determine the relative importance of predictors.\n",
    "\n",
    "##### Explaining Multilinear Regression equation\n",
    "\n",
    "$$ 𝑦=𝛽_{0}+𝛽_{1}𝑥_{1}+…+𝛽_{2}𝑥_{2}+ 𝜖 $$\n",
    "\n",
    "- $𝛽_{1}$: if $𝛽_{1}$ = 2, then a one-unit increase in $𝑥_{1}$ is associated with an average increase of 2 units in 𝑌, holding $𝑥_{2}$ constant.\n",
    "- $𝛽_{2}$: if $𝛽_{2}$ = -3, then a one-unit increase in $𝑥_{2}$ is associated with an average decrease of 3 units in 𝑌, holding $𝑥_{1}$ constant.\n",
    "\n",
    "### Testing Relationships Between Response and Predictors\n",
    "\n",
    "Multilinear regression tests the relationship between the response variable (𝑌) and the predictors ($𝑥_{1}$,$𝑥_{2}$,…,$𝑥_{p}$) by modeling 𝑌 as a linear combination of the predictors:\n",
    "\n",
    "$$ 𝑦=𝛽_{0}+𝛽_{1}𝑥_{1}+ +𝛽_{2}𝑥_{2}+…+𝛽_{p}𝑥_{p}+ 𝜖 $$\n",
    "\n",
    "1. Hypothesis Testing:\n",
    "- For each predictor $𝑥_{p}$, Null Hypothesis ($𝐻_0): 𝛽_p =0$ (the predictor has no effect on response(𝑦) varaible | no relationship between predictor(x) response(𝑦)).\n",
    "- Alternative Hypothesis $(𝐻_𝑎): 𝛽_𝑗 ≠ 0$ (the predictor has an effect/ there is a relationship).\n",
    "\n",
    "2. **t-statistic: test is performed for each coefficient**\n",
    "\n",
    "How to Calculate the t-statistic in Linear regression\n",
    "\n",
    "The t-statistic in linear regression measures how many standard errors the estimated coefficient is away from zero. \n",
    "- It is used for hypothesis testing to determine if a predictor variable is statistically significant.\n",
    "\n",
    "The formula to calculate the t-statistic for a coefficient\n",
    "\n",
    "$$t = \\frac{\\hat{𝛽_p}}{SE_{\\hat{𝛽_p}}}$$\n",
    "\n",
    "Where:\n",
    "$𝛽_p$: Estimated coefficient (e.g., slope or intercept).\n",
    "$SE_{\\hat{𝛽_p}}$: Standard error of the estimated coefficient $\\hat{𝛽_p}$.\n",
    "\n",
    "### t-statistic maybe a misleading variable importance indicator:\n",
    "\n",
    "In multiple linear regression, the t-statistic evaluates the significance of individual predictor variables by testing the null hypothesis that a predictor's coefficient is zero ($𝐻_0): 𝛽_p =0$.\n",
    "\n",
    "It can be misleading as an indicator of variable importance in multilinear regression for the following reasons:\n",
    "\n",
    "- Multicollinearity\n",
    "    - When predictor variables are highly correlated, the variance of the coefficient estimates increases.\n",
    "    - This can lead to inflated standard errors and reduced t-statistics, causing variables to appear insignificant even if they are important.\n",
    "        - Conversely, some variables might have significant t-statistics due to correlation with other predictors rather than their actual contribution to the response variable.\n",
    "\n",
    "- Dependency on Units of Measurement\n",
    "    - The t-statistic depends on the scale of the predictor variables. \n",
    "        - For example, variables with larger numerical ranges can dominate, making direct comparisons between t-statistics across variables inappropriate without standardization.\n",
    "\n",
    "- Context of the Model\n",
    "- The importance of a variable depends on the context of other predictors in the model. \n",
    "    - Adding or removing predictors can change the coefficients and t-statistics, leading to different conclusions about importance.\n",
    "\n",
    "- Does Not Reflect Contribution to $R^2$\n",
    "    - The t-statistic evaluates the statistical significance of a single variable, but it does not measure its contribution to the model's overall explanatory power ($R^2$).\n",
    "    - A variable may be statistically significant (high t-statistic) yet contribute little to the variance explained.\n",
    "\n",
    "- Focuses on Statistical Significance Over Practical Significance\n",
    "    - A high t-statistic indicates statistical significance but does not imply that the variable is practically meaningful or contributes substantially to predictions.\n",
    "\n",
    "Best Practices to Assess Variable Importance\n",
    "- Use metrics like standardized coefficients to account for differences in units.\n",
    "- Evaluate variable importance metrics, such as partial $R^2$ , Shapley values, or permutation importance, especially in models with multicollinearity.\n",
    "- Perform model comparison using adjusted $R^2$ or the Akaike Information Criterion (AIC) to assess the model’s explanatory power with and without specific variables.\n",
    "\n",
    "##### Implementation of best practices for assessing variable importance in multilinear regression:\n",
    "\n",
    "- Standardized Coefficients: Calculates coefficients on a standardized scale for comparison.\n",
    "- Partial $R^2$: Measures the contribution of each variable to the overall $R^2$.\n",
    "- Permutation Importance: Evaluates the change in model performance when a variable's values are randomly shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8717cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Example data\n",
    "np.random.seed(42)\n",
    "X = pd.DataFrame({\n",
    "    'Variable_A': np.random.rand(100) * 100,\n",
    "    'Variable_B': np.random.rand(100) * 50,\n",
    "    'Variable_C': np.random.rand(100) * 10\n",
    "})\n",
    "y = 2 * X['Variable_A'] + 0.5 * X['Variable_B'] + 0.1 * X['Variable_C'] + np.random.randn(100) * 5\n",
    "\n",
    "# Step 1: Fit a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Step 2: Assess importance using standardized coefficients\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "model_scaled = LinearRegression()\n",
    "model_scaled.fit(X_scaled, y)\n",
    "standardized_coefficients = model_scaled.coef_\n",
    "\n",
    "# Step 3: Compute partial R-squared for each variable\n",
    "def partial_r2(X, y, variable):\n",
    "    X_partial = X.drop(columns=[variable])\n",
    "    model_partial = LinearRegression().fit(X_partial, y)\n",
    "    residuals = y - model_partial.predict(X_partial)\n",
    "    total_rss = np.sum((y - y.mean()) ** 2)\n",
    "    partial_rss = np.sum(residuals ** 2)\n",
    "    return 1 - (partial_rss / total_rss)\n",
    "\n",
    "partial_r2_values = {var: partial_r2(X, y, var) for var in X.columns}\n",
    "\n",
    "# Step 4: Compute permutation importance\n",
    "perm_importance = permutation_importance(model, X, y, n_repeats=30, random_state=42)\n",
    "\n",
    "# Step 5: Display results\n",
    "print(\"Standardized Coefficients:\")\n",
    "for var, coef in zip(X.columns, standardized_coefficients):\n",
    "    print(f\"{var}: {coef:.4f}\")\n",
    "\n",
    "print(\"\\nPartial R-squared Values:\")\n",
    "for var, r2 in partial_r2_values.items():\n",
    "    print(f\"{var}: {r2:.4f}\")\n",
    "\n",
    "print(\"\\nPermutation Importance:\")\n",
    "for var, importance in zip(X.columns, perm_importance.importances_mean):\n",
    "    print(f\"{var}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f4ceb",
   "metadata": {},
   "source": [
    "2. **F-Test for Overall Model Significance**:\n",
    "\n",
    "The F-statistic is used in hypothesis testing to evaluate the overall significance of a multiple linear regression model. Specifically, it tests whether at least one of the predictor variables in the model significantly explains variation in the dependent variable.\n",
    "- Tests the null hypothesis that all coefficients are zero ($𝛽_{1} = 𝛽_{2} = ... = 𝛽_{p} = 0$).\n",
    "    - If the F-statistic is significant, at least one predictor has a relationship with 𝑌.\n",
    "\n",
    "formula for the F-statistic is:\n",
    "\n",
    "$$ F= \\frac{Explained Mean Square (MSR)}{Residual Mean Square (MSE)}$$\n",
    "$$ F= \\frac{\\frac{TSS−RSS}{p}}{\\frac{RSS}{n−p−1}}$$\n",
    "\n",
    "Where:\n",
    "- TSS: Total Sum of Squares\n",
    "- RSS: Residual Sum of Squares\n",
    "- n: Number of observations\n",
    "- p: Number of predictors (excluding the intercept)\n",
    "- Mean Square Regression (MSR): $\\frac{TSS−RSS}{p}$\n",
    "- Mean Square Error (MSE): $frac{RSS}{n−p−1}$\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Calculate the degrees of freedom:\n",
    "- For regression: 𝑝\n",
    "- For error: 𝑛−𝑝−1\n",
    "\n",
    "2. Compute the explained variance: TSS−RSS\n",
    "\n",
    "3. Calculate Mean Square Regression (MSR) and Mean Square Error (MSE)\n",
    "\n",
    "4. Compute the F-statistic:\n",
    "$$F = \\frac{Explained Mean Square (MSR)}{Residual Mean Square (MSE)}$$\n",
    "\n",
    "When to Perform the F-Test?\n",
    "- Perform the F-test whenever you have a regression model and want to evaluate its overall significance. \n",
    "- It is especially relevant in multiple linear regression with several predictors.\n",
    "\n",
    "Why Perform the F-Test?\n",
    "- To determine if the model as a whole is useful for predicting the dependent variable.\n",
    "- It helps decide whether further analysis (e.g., testing individual predictors or refining the model) is warranted.\n",
    "\n",
    "##### Practical Steps in Hypothesis Testing:\n",
    "\n",
    "i.  Formulate the Hypotheses\n",
    "- Null Hypothesis ($𝐻_0$): All regression coefficients (except the intercept) are equal to zero, i.e., the predictors do not explain the variability in the dependent variable.\n",
    "$$ 𝐻_0: 𝛽_{1} = 𝛽_{2} = ... = 𝛽_{p} = 0$$\n",
    "- Alternative Hypothesis($𝐻_a$): At least one of the regression coefficients is not zero, i.e., at least one predictor contributes to explaining the variability.\n",
    "$$ 𝐻_a: at least one𝛽_{j}\\neq 0, for j = 1,2,...,p$$\n",
    "\n",
    "ii. Calculate the F-Statistic\n",
    "\n",
    "$$ F= \\frac{Explained Mean Square (MSR)}{Residual Mean Square (MSE)}$$\n",
    "$$ F= \\frac{\\frac{TSS−RSS}{p}}{\\frac{RSS}{n−p−1}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$MSR = \\frac{Explained Variance}{Degrees of Freedom for Regression (df_{reg})}$$\n",
    "\n",
    "$$MSE = \\frac{Residual Sum of Squares (RSS)}{Degrees of Freedom for Error (df_{error})}$$\n",
    "\n",
    "iii. Determine the Degrees of Freedom\n",
    "\n",
    "- $df_{reg}$ =p: Number of predictors.\n",
    "- $df_{error}$ =n−p−1: Residual degrees of freedom, where 𝑛 is the number of observations.\n",
    "\n",
    "iv. Find the Critical Value\n",
    "\n",
    "- Use the F-distribution table or Python to find the critical value for the given 𝛼 (commonly 0.05), $df_{reg}$ and $df_{error}$\n",
    "\n",
    "v. Compare F-Statistic with the Critical Value\n",
    "- If $𝐹 > 𝐹_{𝑐𝑟𝑖𝑡𝑖𝑐𝑎𝑙}$, reject $𝐻_0$. \n",
    "    - This implies that at least one predictor is significant.\n",
    "- If $𝐹 ≤ 𝐹_{𝑐𝑟𝑖𝑡𝑖𝑐𝑎𝑙}$, fail to reject $𝐻_0$. \n",
    "    - This implies the predictors do not collectively explain the variability better than random chance.\n",
    "\n",
    "vi. Use the p-Value (Optional)\n",
    "- Instead of using a critical value, you can calculate the p-value associated with the F-statistic:\n",
    "    - If p-value < α, reject $𝐻_0$.\n",
    "    - If p-value ≥ α, fail to reject $𝐻_0$.\n",
    "\n",
    "Interpreting Results\n",
    "- Significant F-statistic: Indicates the model has predictive power and at least one predictor is meaningful.\n",
    "- Non-significant F-statistic: Suggests the model does not explain variability better than a simple mean-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8deec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example parameters\n",
    "TSS = 1200  # Total Sum of Squares\n",
    "RSS = 300   # Residual Sum of Squares\n",
    "n = 50      # Number of observations\n",
    "p = 3       # Number of predictors (excluding the intercept)\n",
    "\n",
    "# Step 1: Degrees of freedom\n",
    "df_regression = p               # Degrees of freedom for regression\n",
    "df_error = n - p - 1           # Degrees of freedom for error\n",
    "\n",
    "# Step 2: Explained variance\n",
    "explained_variance = TSS - RSS\n",
    "\n",
    "# Step 3: Calculate MSR and MSE\n",
    "MSR = explained_variance / df_regression  # Mean Square Regression\n",
    "MSE = RSS / df_error                     # Mean Square Error\n",
    "\n",
    "# Step 4: Calculate the F-statistic\n",
    "F_statistic = MSR / MSE\n",
    "\n",
    "# Step 5: Perform hypothesis testing\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Calculate the critical value for the F-distribution\n",
    "alpha = 0.05  # Significance level\n",
    "F_critical = stats.f.ppf(1 - alpha, df_regression, df_error)\n",
    "\n",
    "# Calculate the p-value for the F-statistic\n",
    "p_value = 1 - stats.f.cdf(F_statistic, df_regression, df_error)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Degrees of Freedom (Regression): {df_regression}\")\n",
    "print(f\"Degrees of Freedom (Error): {df_error}\")\n",
    "print(f\"Explained Variance: {explained_variance}\")\n",
    "print(f\"Mean Square Regression (MSR): {MSR}\")\n",
    "print(f\"Mean Square Error (MSE): {MSE}\")\n",
    "print(f\"F-Statistic: {F_statistic}\")\n",
    "print(f\"Critical F-Value: {F_critical}\")\n",
    "print(f\"P-Value: {p_value}\")\n",
    "\n",
    "# Decision based on F-statistic\n",
    "if F_statistic > F_critical:\n",
    "    print(\"Reject the null hypothesis: At least one predictor is significant.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: The model is not significant.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d7e8e",
   "metadata": {},
   "source": [
    "3. Assessing Fit:\n",
    "- Coefficient of Determination $R^2$ : \n",
    "    - Proportion of variance in 𝑌 explained by the predictors.\n",
    "    - Purpose: Measures the proportion of variance in the dependent variable explained by the independent variables.\n",
    "    - When to Use: Always, as a baseline measure of model fit.\n",
    "- Adjusted $R^2$: \n",
    "    - Adjusts $R^2$ for the number of predictors, penalizing the inclusion of irrelevant predictors.\n",
    "    - Key Consideration: Adjusted $R^2$ accounts for the number of predictors, providing a better measure for models with multiple variables.\n",
    "- Residual Analysis\n",
    "    - Purpose: Examines the residuals (differences between observed and predicted values) to check assumptions of the regression model.\n",
    "    - How to Use:\n",
    "        - Plot residuals vs. predicted values to check for patterns (should appear random).\n",
    "        - Use a histogram or Q-Q plot of residuals to check normality.\n",
    "        - Examine residuals vs. independent variables to check for independence.\n",
    "    - When to Use: Always, to validate assumptions like linearity, homoscedasticity, and normality.\n",
    "- Mean Squared Error (MSE)\n",
    "    - Purpose: Measures the average squared difference between observed and predicted values.\n",
    "    - When to Use: To quantify model error; lower MSE indicates better fit.\n",
    "- F-Statistic\n",
    "    - Purpose: Tests the overall significance of the model by comparing explained variance to unexplained variance.\n",
    "    - When to Use: To test whether at least one predictor is significant in explaining the variance of the dependent variable.\n",
    "- Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)\n",
    "    - Purpose: Compare models, penalizing for model complexity.\n",
    "    - When to Use: When comparing multiple models with different numbers of predictors or structures.\n",
    "- Cross-Validation\n",
    "    - Purpose: Evaluates the model’s performance on unseen data.\n",
    "    - How to Use:\n",
    "        - Use k-fold cross-validation to divide data into training and test sets.\n",
    "        - Calculate metrics (e.g., $R^2$ , MSE) on test sets.\n",
    "    - When to Use: To assess model generalizability.\n",
    "- Variance Inflation Factor (VIF)\n",
    "    - Purpose: Detects multicollinearity among predictors.\n",
    "    - How to Use: Compute VIF for each predictor; values > 10 indicate high multicollinearity.\n",
    "    - When to Use: To assess stability of coefficient estimates.\n",
    "- Cook’s Distance and Leverage\n",
    "    - Purpose: Identifies influential observations that disproportionately affect the regression results.\n",
    "    - How to Use:\n",
    "        - Cook’s Distance: Observations with values > 1 are considered influential.\n",
    "        - Leverage: High-leverage points have significant potential to influence the model.\n",
    "    - When to Use: To identify outliers and influential data points.\n",
    "- Normalized Residual Standard Error (NRSE)\n",
    "    - Purpose: Provides a standardized measure of error in the model.\n",
    "    - When to Use: To compare models with different dependent variable scales.\n",
    "- Predictive Metrics (e.g., RMSE, MAE)\n",
    "    - Purpose: Evaluate model accuracy in predicting outcomes.\n",
    "    - When to Use: For regression models focused on prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348fade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.api import OLS, add_constant\n",
    "\n",
    "# Example dataset\n",
    "np.random.seed(42)\n",
    "X = pd.DataFrame({\n",
    "    'Variable_A': np.random.rand(100) * 100,\n",
    "    'Variable_B': np.random.rand(100) * 50,\n",
    "    'Variable_C': np.random.rand(100) * 10\n",
    "})\n",
    "y = 2 * X['Variable_A'] + 0.5 * X['Variable_B'] + 0.1 * X['Variable_C'] + np.random.randn(100) * 5\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 1. Coefficient of Determination (R^2)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "adjusted_r2 = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
    "print(f\"R^2: {r2:.4f}, Adjusted R^2: {adjusted_r2:.4f}\")\n",
    "\n",
    "# 2. Residual Analysis\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x=y_pred, y=residuals)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Predicted')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.xlabel('Residuals')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.show()\n",
    "\n",
    "# 3. Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "# 4. F-Statistic (using statsmodels)\n",
    "X_train_const = add_constant(X_train)\n",
    "ols_model = OLS(y_train, X_train_const).fit()\n",
    "print(ols_model.summary())\n",
    "\n",
    "# 5. Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)\n",
    "print(f\"AIC: {ols_model.aic:.4f}, BIC: {ols_model.bic:.4f}\")\n",
    "\n",
    "# 6. Cross-Validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "print(f\"Cross-Validation R^2 Scores: {cv_scores}\")\n",
    "print(f\"Mean CV R^2: {np.mean(cv_scores):.4f}\")\n",
    "\n",
    "# 7. Variance Inflation Factor (VIF)\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Variable'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(\"Variance Inflation Factor (VIF):\")\n",
    "print(vif_data)\n",
    "\n",
    "# 8. Cook's Distance and Leverage\n",
    "influence = ols_model.get_influence()\n",
    "cooks_d = influence.cooks_distance[0]\n",
    "high_influence_points = np.where(cooks_d > 4 / len(X_train))[0]\n",
    "print(f\"High Influence Points (Cook's Distance > 4/n): {high_influence_points}\")\n",
    "\n",
    "# 9. Residual Standard Error (RSE)\n",
    "rse = np.sqrt(mse)\n",
    "print(f\"Residual Standard Error (RSE): {rse:.4f}\")\n",
    "\n",
    "# 10. Predictive Metrics (RMSE and MAE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}, Mean Absolute Error (MAE): {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8217773",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Step 1: Import Libraries and Load Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate example dataset\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    \"SquareFootage\": np.random.uniform(500, 4000, 100),\n",
    "    \"Bedrooms\": np.random.randint(1, 6, 100),\n",
    "    \"DistanceFromCityCenter\": np.random.uniform(1, 20, 100),\n",
    "    \"HousePrice\": np.random.uniform(50000, 500000, 100),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print sample data\n",
    "print(df.head())\n",
    "\n",
    "############# Step 2: Fit the Multilinear Regression Model\n",
    "\n",
    "# Define predictors (X) and response (Y)\n",
    "X = df[[\"SquareFootage\", \"Bedrooms\", \"DistanceFromCityCenter\"]]\n",
    "Y = df[\"HousePrice\"]\n",
    "\n",
    "# Add a constant for the intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the model\n",
    "model = sm.OLS(Y, X).fit()\n",
    "\n",
    "# Summary of the model\n",
    "print(model.summary())\n",
    "\n",
    "############## Step 3: Interpret the Output\n",
    "\n",
    "# Performing a summary operation lists out all different parameters of the regression line fitted\n",
    "print(lr.summary())\n",
    "\n",
    "# OR\n",
    "\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5de94d",
   "metadata": {},
   "source": [
    "##### Interpretation of `summary`\n",
    "\n",
    "The model.summary() provides:\n",
    "\n",
    "Coefficients:\n",
    "- $𝛽_{0}$: The intercept.\n",
    "- $𝛽_{1}$,$𝛽_{2}$,$𝛽_{3}$: Coefficients for predictors.\n",
    "\n",
    "P-values:\n",
    "- Assess the significance of each predictor.\n",
    "  - If 𝑝 < 0.05, the predictor significantly explains variations in the response variable.\n",
    "\n",
    "$R^{2}$ and Adjusted $R^{2}$ :\n",
    "- Measure how much variance in the response is explained by the predictors.\n",
    "\n",
    "**F-statistic**:\n",
    "- Tests the overall significance of the model.\n",
    "- It tests whether at least one predictor variable in the model has a non-zero coefficient, meaning it contributes significantly to explaining the variance in the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example parameters\n",
    "TSS = 1200  # Total Sum of Squares\n",
    "RSS = 300   # Residual Sum of Squares\n",
    "n = 50      # Number of observations\n",
    "p = 3       # Number of predictors (excluding the intercept)\n",
    "\n",
    "# Step 1: Degrees of freedom\n",
    "df_regression = p               # Degrees of freedom for regression\n",
    "df_error = n - p - 1           # Degrees of freedom for error\n",
    "\n",
    "# Step 2: Explained variance\n",
    "explained_variance = TSS - RSS\n",
    "\n",
    "# Step 3: Calculate MSR and MSE\n",
    "MSR = explained_variance / df_regression  # Mean Square Regression\n",
    "MSE = RSS / df_error                     # Mean Square Error\n",
    "\n",
    "# Step 4: Calculate the F-statistic\n",
    "F_statistic = MSR / MSE\n",
    "\n",
    "# Print the results\n",
    "print(f\"Degrees of Freedom (Regression): {df_regression}\")\n",
    "print(f\"Degrees of Freedom (Error): {df_error}\")\n",
    "print(f\"Explained Variance: {explained_variance}\")\n",
    "print(f\"Mean Square Regression (MSR): {MSR}\")\n",
    "print(f\"Mean Square Error (MSE): {MSE}\")\n",
    "print(f\"F-Statistic: {F_statistic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34baee6c",
   "metadata": {},
   "source": [
    "4. Assumptions:\n",
    "- Linearity: The relationship between predictors and response is linear.\n",
    "- Independence of Errors: Errors are independent of each other.\n",
    "- Homoscedasticity: Constant variance of errors.\n",
    "- Normality of Errors: Errors are normally distributed.\n",
    "\n",
    "##### Practical Steps:\n",
    "1. Plot residuals to check assumptions.\n",
    "2. Use statistical tests (e.g., Shapiro-Wilk for normality, Breusch-Pagan for homoscedasticity).\n",
    "3. Apply transformations or alternative models if assumptions are violated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb5ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Step 4: Visualize Residuals to Check Assumptions\n",
    "\n",
    "# Linearity and Homoscedasticity\n",
    "# Plot predicted vs actual values\n",
    "predicted = model.predict(X)\n",
    "residuals = Y - predicted\n",
    "\n",
    "plt.scatter(predicted, residuals)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Predicted Values\")\n",
    "plt.show()\n",
    "\n",
    "# Normality of Errors\n",
    "# Plot residual distribution\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title(\"Residual Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Perform Shapiro-Wilk test for normality\n",
    "from scipy.stats import shapiro\n",
    "shapiro_test = shapiro(residuals)\n",
    "print(f\"Shapiro-Wilk test p-value: {shapiro_test.pvalue}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad84692b",
   "metadata": {},
   "source": [
    "Step 5: Interpretation\n",
    "\n",
    "Coefficient Interpretation\n",
    "- if coefficient of , $𝛽_{1}$ = 50 it means that for every additional square foot, the house price increases by $50, assuming other predictors are held constant.\n",
    "\n",
    "Model Fit\n",
    "- if $R^{2}$ = 0.85, it means 85% of the variance in house prices is explained by the predictors.\n",
    "- Check adjusted $R^{2}$ to ensure added predictors improve the model meaningfully.\n",
    "\n",
    "Assumptions\n",
    "- A residual plot with no pattern confirms linearity.\n",
    "- Homoscedasticity: Residuals should have constant variance (scatter evenly around zero).\n",
    "- Normality: Residuals should approximately follow a normal distribution.\n",
    "___________\n",
    "\n",
    "few 2-dimensional plots; plotting `wt`, `disp`, `cyl`, and `hp` vs. `mpg`, respectively (top-left to bottom-right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c99e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(9,7))\n",
    "\n",
    "axs[0,0].scatter(df['wt'], df['mpg'])\n",
    "axs[0,0].plot(df['wt'], lm.intercept_ + lm.coef_[4]*df['wt'], color='red')\n",
    "axs[0,0].title.set_text('Weight (wt) vs. mpg')\n",
    "\n",
    "axs[0,1].scatter(df['disp'], df['mpg'])\n",
    "axs[0,1].plot(df['disp'], lm.intercept_ + lm.coef_[1]*df['disp'], color='red')\n",
    "axs[0,1].title.set_text('Engine displacement (disp) vs. mpg')\n",
    "\n",
    "axs[1,0].scatter(df['cyl'], df['mpg'])\n",
    "axs[1,0].plot(df['cyl'], lm.intercept_ + lm.coef_[0]*df['cyl'], color='red')\n",
    "axs[1,0].title.set_text('Number of cylinders (cyl) vs. mpg')\n",
    "\n",
    "axs[1,1].scatter(df['hp'], df['mpg'])\n",
    "axs[1,1].plot(df['hp'], lm.intercept_ + lm.coef_[2]*df['hp'], color='red')\n",
    "axs[1,1].title.set_text('Horsepower (hp) vs. mpg')\n",
    "\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af29b5fd",
   "metadata": {},
   "source": [
    "### Assessing Model Accuracy\n",
    "\n",
    "Let's assess the fit of our multivariate model. For the purpose of a rudimentary comparison, let's measure model accuracy aginst a simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18155fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to X_test\n",
    "X_test_sm = sm.add_constant(X_test)\n",
    "# Predict the y values corresponding to X_test_sm\n",
    "y_pred = lr.predict(X_test_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9e55c",
   "metadata": {},
   "source": [
    "We have included a column *Test RMSE*, which is simply the square root of the *Test MSE*.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "RMSE & = \\sqrt{MSE} \\\\\n",
    "     & = \\sqrt{\\frac{1}{N}\\sum^{N} (\\hat{y_i} - y_i)^{2}}\n",
    "\\end{align}\n",
    "\n",
    "Where $y_i$ are the actual target values for a dataset with $N$ datapoints, and $\\hat{y_i}$ represent our corresponding predictions. RMSE is a more intuitive metric to use than MSE because it is in the same units as the underlying variable being predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec0210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import math\n",
    "\n",
    "# Imporitng libraries\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# dictionary of results\n",
    "results_dict = {'Training MSE':\n",
    "                    {\n",
    "                        \"SLR\": metrics.mean_squared_error(y_train, slr.predict(X_train[['disp']])),\n",
    "                        \"MLR\": metrics.mean_squared_error(y_train, lm.predict(X_train))\n",
    "                    },\n",
    "                'Test MSE':\n",
    "                    {\n",
    "                        \"SLR\": metrics.mean_squared_error(y_test, slr.predict(X_test[['disp']])),\n",
    "                        \"MLR\": metrics.mean_squared_error(y_test, lm.predict(X_test))\n",
    "                    },\n",
    "                'Test RMSE':\n",
    "                    {\n",
    "                        \"SLR\": math.sqrt(metrics.mean_squared_error(y_test, slr.predict(X_test[['disp']]))),\n",
    "                        \"MLR\": math.sqrt(metrics.mean_squared_error(y_test, lm.predict(X_test)))\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d2ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE value\n",
    "print(\"RMSE: \",np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "#R-squared value\n",
    "print(\"R-squared: \",r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c3b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lm = X_train_lm.values.reshape(-1,1)\n",
    "X_test_lm = X_test_lm.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa82d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_lm.shape)\n",
    "print(X_train_lm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ffa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "#Representing LinearRegression as lr (creating LinearRegression object)\n",
    "lr = LinearRegression()\n",
    "#Fit the model using lr.fit()\n",
    "lr.fit(X_train_lm,y_train_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba41d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get intercept\n",
    "print(lr.intercept_)\n",
    "#get slope\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b73f2",
   "metadata": {},
   "source": [
    "# Addressing Assumptions in Multilinear Regression\n",
    "\n",
    "Initial Diagnostics:\n",
    "- Examine scatter plots and residual plots.\n",
    "- Test assumptions (e.g., Breusch-Pagan for heteroscedasticity, Shapiro-Wilk for normality).\n",
    "\n",
    "Transform Data if Necessary:\n",
    "- Use log, Box-Cox, or polynomial transformations to address issues like non-linearity and heteroscedasticity.\n",
    "\n",
    "Refit and Compare Models:\n",
    "- Use metrics like Adjusted $𝑅^2$ , Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC) to compare models.\n",
    "\n",
    "Document Interpretations:\n",
    "- Explain coefficients in the context of transformed variables.\n",
    "- Discuss any trade-offs made during model selection.\n",
    "\n",
    "# Checking for Independence\n",
    "\n",
    "Independence of Errors: Errors should be independent (important for time series or clustered data).\n",
    "\n",
    "We have done checks for linearity and multicollinearity, which both referred to the predictor variables. \n",
    "\n",
    "To checking some of the artefacts of the fitted model for three more statistical phenomena which further help us determine its quality.\n",
    "\n",
    "#### Residuals vs. Predictor Variables Plots \n",
    "\n",
    "The first check we do involves plotting the residuals (vertical distances between each data point and the regression hyperplane). \n",
    "- We are looking to confirm the independence assumption here, i.e.: the residuals should be independent. \n",
    "\n",
    "If they are we will see:\n",
    "- Residuals approximately uniformly randomly distributed about the zero x-axes;\n",
    "- Residuals not forming specific clusters.\n",
    "\n",
    "Observing the plots two things should be relatively clear:\n",
    "\n",
    "- Residuals are slightly to skewed to the positive or negative (reaching +5 but only about -3);\n",
    "\n",
    "- check for clustering, \n",
    "    - Check which may present a cluster on the value 6.\n",
    "\n",
    "Conclusion: is the residuals are largely independent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,5, figsize=(14,6),sharey=True)\n",
    "fig.subplots_adjust(hspace = 0.5, wspace=.2)\n",
    "fig.suptitle('Predictor variables vs. model residuals', fontsize=16)\n",
    "axs = axs.ravel()\n",
    "\n",
    "for index, column in enumerate(df.columns):\n",
    "    axs[index-1].set_title(\"{}\".format(column),fontsize=12)\n",
    "    axs[index-1].scatter(x=df[column],y=fitted.resid,color='blue',edgecolor='k')\n",
    "    axs[index-1].grid(True)\n",
    "    xmin = min(df[column])\n",
    "    xmax = max(df[column])\n",
    "    axs[index-1].hlines(y=0,xmin=xmin*0.9,xmax=xmax*1.1,color='red',linestyle='--',lw=3)\n",
    "    if index == 1 or index == 6:\n",
    "        axs[index-1].set_ylabel('Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86f5750",
   "metadata": {},
   "source": [
    "# Checking for Homoscedasticity\n",
    "\n",
    "Homoscedasticity is an important assumption in linear regression. It implies that the variance of the residuals (errors) is constant across all levels of the independent variables. When this assumption is violated (heteroscedasticity), the model's standard errors, and p-values can become unreliable, potentially leading to incorrect inferences.\n",
    "\n",
    "Homoscedasticity: Residuals should have constant variance.\n",
    "\n",
    "What needs to be done: Check whether the variance of the residuals (the error terms) is constant as the fitted values increase. \n",
    "\n",
    "#### Fitted vs. Residuals\n",
    "\n",
    "Determine this by plotting the magnitude of the fitted values (i.e.: `mpg`) against the residuals. \n",
    "- What we are looking for is the plotted points to approximately form a rectangle.\n",
    "- The magnitude of the residuals should not increase as the fitted values increase (if that is the case, the data will form the shape of a cone on its side).\n",
    "\n",
    "**Observation**\n",
    "- If the variance is constant, we have observed _homoscedasticity_. \n",
    "- If the variance is not constant, we have observed _heteroscedasticity_. \n",
    "\n",
    "Use the same plot to check for outliers: any plotted points that are visibly seperate from the random pattern of the rest of the residuals.\n",
    "\n",
    "**Observation**\n",
    "- Look at data point on particular side of the plot and observe the scatteredness/ density.\n",
    "    - Points towards the right-hand side of the plot tend to be scattered slightly less densely, indicating the presence of heteroscedasticity.\n",
    "    - This violates our assumption of homoscedasticity. \n",
    "- Look at the presesnce of outliers\n",
    "    - The presence of these outliers means that those values are weighted too heavily in the prediction process, disproportionately influencing the model's performance. \n",
    "    - This in turn can lead to the confidence interval for out of sample predictions (unseen data) being unrealistically wide or narrow.\n",
    "\n",
    "if Heteroscedasticity, \n",
    "- Solution: Use transformations (log, Box-Cox) or weighted least squares regression.\n",
    "\n",
    "**Step 1: Diagnosing Heteroscedasticity**\n",
    "\n",
    "(a) Residual Plot\n",
    "- Plot the residuals against the predicted values to check for patterns.\n",
    "\n",
    "Interpretation:\n",
    "- If the points are randomly scattered, homoscedasticity is likely satisfied.\n",
    "- A funnel-shaped or other pattern suggests heteroscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "p=plt.scatter(x=fitted.fittedvalues,y=fitted.resid,edgecolor='k')\n",
    "xmin = min(fitted.fittedvalues)\n",
    "xmax = max(fitted.fittedvalues)\n",
    "plt.hlines(y=0,xmin=xmin*0.9,xmax=xmax*1.1,color='red',linestyle='--',lw=3)\n",
    "plt.xlabel(\"Fitted values\",fontsize=15)\n",
    "plt.ylabel(\"Residuals\",fontsize=15)\n",
    "plt.title(\"Fitted vs. residuals plot\",fontsize=18)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e086dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predicted values and residuals\n",
    "predicted = model.predict(X)\n",
    "residuals = Y - predicted\n",
    "\n",
    "# Residual plot\n",
    "plt.scatter(predicted, residuals)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da4c7c2",
   "metadata": {},
   "source": [
    "(b) Breusch-Pagan Test\n",
    "- This statistical test explicitly checks for heteroscedasticity.\n",
    "\n",
    "Interpretation:\n",
    "- Null Hypothesis: Homoscedasticity is present.\n",
    "- If p-value < 0.05, reject the null hypothesis, indicating heteroscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a3270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "# Breusch-Pagan test\n",
    "bp_test = het_breuschpagan(residuals, X)\n",
    "print(\"Breusch-Pagan Test Results:\")\n",
    "print(f\"LM Statistic: {bp_test[0]}\")\n",
    "print(f\"p-value: {bp_test[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fb49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breusch-Pagan test\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "bp_test = het_breuschpagan(residuals, X)\n",
    "print(f\"Breusch-Pagan test p-value: {bp_test[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6046fa9",
   "metadata": {},
   "source": [
    "(c) White Test\n",
    "- Another test for heteroscedasticity, more flexible than Breusch-Pagan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3446d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_white\n",
    "\n",
    "# White test\n",
    "white_test = het_white(residuals, X)\n",
    "print(\"White Test Results:\")\n",
    "print(f\"LM Statistic: {white_test[0]}\")\n",
    "print(f\"p-value: {white_test[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3904a128",
   "metadata": {},
   "source": [
    "**Step 2: Addressing Heteroscedasticity/ Handling Homoscedasticity**\n",
    "\n",
    "1. Transforming the Response Variable\n",
    "- Apply transformations to stabilize variance.\n",
    "\n",
    "(a) Log Transformation: Use when variance increases with the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Log_HousePrice\"] = np.log(df[\"HousePrice\"])\n",
    "model_log = sm.OLS(df[\"Log_HousePrice\"], X).fit()\n",
    "print(model_log.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192027e1",
   "metadata": {},
   "source": [
    "(b) Box-Cox Transformation: Automatically finds the best transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ceb289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "Y_transformed, lambda_boxcox = boxcox(Y)\n",
    "print(f\"Optimal lambda for Box-Cox: {lambda_boxcox}\")\n",
    "\n",
    "model_boxcox = sm.OLS(Y_transformed, X).fit()\n",
    "print(model_boxcox.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80012913",
   "metadata": {},
   "source": [
    "2. Applying Weighted Least Squares (WLS)\n",
    "\n",
    "If heteroscedasticity is detected:\n",
    "- Use WLS to assign weights inversely proportional to the variance of residuals.\n",
    "\n",
    "When to use:\n",
    "- When residual patterns vary predictably with certain predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b252f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "# Calculate weights as inverse of squared residuals\n",
    "weights = 1 / (residuals**2)\n",
    "\n",
    "# Fit WLS model\n",
    "model_wls = sm.WLS(Y, X, weights=weights).fit()\n",
    "print(model_wls.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b08694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a weighted least squares model\n",
    "weights = 1 / (residuals**2)\n",
    "model_wls = sm.WLS(Y, X, weights=weights).fit()\n",
    "\n",
    "print(model_wls.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4323e179",
   "metadata": {},
   "source": [
    "3. Heteroscedasticity-Robust Standard Errors\n",
    "- Use robust standard errors to correct inference without changing the model structure.\n",
    "\n",
    "Types of Robust Covariance:\n",
    "- \"HC0\": Basic robust variance.\n",
    "- \"HC1\", \"HC2\", \"HC3\": Variants of robust variance, with \"HC3\" being stricter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47eb07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit OLS model with robust standard errors\n",
    "model_robust = sm.OLS(Y, X).fit(cov_type=\"HC3\")\n",
    "print(model_robust.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c152c3",
   "metadata": {},
   "source": [
    "Check Residual Plots After Mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986274d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals of WLS model\n",
    "predicted_wls = model_wls.predict(X)\n",
    "residuals_wls = Y - predicted_wls\n",
    "\n",
    "plt.scatter(predicted_wls, residuals_wls)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.title(\"Residual Plot After WLS\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7451a7",
   "metadata": {},
   "source": [
    "**Step 3: Comparing Models**\n",
    "\n",
    "Evaluate and Compare Performance\n",
    "- Residual plots before and after adjustments.\n",
    "- Metrics like $𝑅^2$, AIC, and BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare AIC and BIC\n",
    "print(f\"Original Model AIC: {model.aic}\")\n",
    "print(f\"Log-Transformed Model AIC: {model_log.aic}\")\n",
    "print(f\"Box-Cox Model AIC: {model_boxcox.aic}\")\n",
    "print(f\"WLS Model AIC: {model_wls.aic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a586ca1f",
   "metadata": {},
   "source": [
    "Best Practices and Considerations\n",
    "\n",
    "Diagnosis:\n",
    "- Always check residual plots and use tests like Breusch-Pagan or White.\n",
    "\n",
    "Correction:\n",
    "- Start with transformations if patterns suggest non-linearity or skewed responses.\n",
    "- Use WLS or robust standard errors for complex variance structures.\n",
    "\n",
    "Validation:\n",
    "- Ensure improvements in residual plots and metrics.\n",
    "- Balance interpretability and complexity when applying advanced techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a82de17",
   "metadata": {},
   "source": [
    "# Checking for Normality\n",
    "\n",
    "The normality of residuals is a key assumption in linear regression, especially for inference. It ensures that t-tests and F-tests for significance are valid. If residuals are not normally distributed, it can lead to unreliable p-values and confidence intervals.\n",
    "\n",
    "To confirm our assumption of normality amongst the residuals. \n",
    "- If the residuals are non-normally distributed, confidence intervals can become too wide or too narrow, \n",
    "    - which leads to difficulty in estimating coefficients based on the minimisation of ordinary least squares.\n",
    "\n",
    "Check for violation of the normality assumption in two different ways:\n",
    "1. Plotting a histogram of the normalised residuals;\n",
    "2. Generating a Q-Q plot of the residuals.\n",
    "\n",
    "**Step 1: Testing for Normality**\n",
    "\n",
    "(a) Visual Inspection: Histogram and Q-Q Plot\n",
    "\n",
    "1. Histogram: Examine the residuals' distribution.\n",
    "\n",
    "Plot a histogram of the residuals to take a look at their distribution. \n",
    "- It is fairly easy to pick up when a distribution looks similar to the classic _bell curve_ shape of the normal distribution.\n",
    "\n",
    "Interpretation:\n",
    "- Histogram: A bell-shaped curve suggests normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(fitted.resid_pearson,bins=8,edgecolor='k')\n",
    "plt.ylabel('Count',fontsize=15)\n",
    "plt.xlabel('Normalized residuals',fontsize=15)\n",
    "plt.title(\"Histogram of normalized residuals\",fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f9d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = Y - model.predict(X)\n",
    "\n",
    "# Histogram\n",
    "plt.hist(residuals, bins=20, edgecolor='k', alpha=0.7)\n",
    "plt.title(\"Residual Histogram\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9660ce8",
   "metadata": {},
   "source": [
    "2. Q-Q plot of the residuals\n",
    "\n",
    "Compare residuals to a normal distribution.\n",
    "- A Q-Q plot, A.K.A quantile-quantile plot, attempts to plot the theoretical quantiles of the standard normal distribution against the quantiles of the residuals. \n",
    "- The one-to-one line, indicated in red below, is the ideal line indicating normality. \n",
    "- The closer the plotted points are to the red line, the closer the residual distribution is to the standard normal distribution.\n",
    "\n",
    "Quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities or dividing the observations in a sample in the same way.\n",
    "- 2 quantile is known as the Median\n",
    "- 4 quantile is known as the Quartile\n",
    "- 10 quantile is known as the Decile\n",
    "- 100 quantile is known as the Percentile\n",
    "\n",
    "Interpretation:\n",
    "- Q-Q Plot: Points should lie close to the 45° line for normality.\n",
    "\n",
    "10 quantile will divide the Normal Distribution into 10 parts each having 10 % of the data points. The Q-Q plot or quantile-quantile plot is a scatter plot created by plotting two sets of quantiles against one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5653d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We once again use the statsmodel library to assist us in producing our qqplot visualisation. \n",
    "from statsmodels.graphics.gofplots import qqplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8361da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "fig=qqplot(fitted.resid_pearson,line='45',fit='True')\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.xlabel(\"Theoretical quantiles\",fontsize=15)\n",
    "plt.ylabel(\"Sample quantiles\",fontsize=15)\n",
    "plt.title(\"Q-Q plot of normalized residuals\",fontsize=18)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6053ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = Y - model.predict(X)\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q Plot of Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afa905b",
   "metadata": {},
   "source": [
    "(b) Shapiro-Wilk Test\n",
    "- Statistical test for normality.\n",
    "\n",
    "Interpretation\n",
    "\n",
    "Null Hypothesis: Residuals follow a normal distribution.\n",
    "- If p-value < 0.05, reject the null hypothesis, indicating non-normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "shapiro_test = shapiro(residuals)\n",
    "print(f\"Shapiro-Wilk Test Statistic: {shapiro_test.statistic}, p-value: {shapiro_test.pvalue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426fb5e1",
   "metadata": {},
   "source": [
    "(c) Kolmogorov-Smirnov Test\n",
    "- Another test for normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29455622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest\n",
    "\n",
    "ks_test = kstest(residuals, 'norm', args=(residuals.mean(), residuals.std()))\n",
    "print(f\"KS Test Statistic: {ks_test.statistic}, p-value: {ks_test.pvalue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0215a753",
   "metadata": {},
   "source": [
    "(d) Anderson-Darling Test\n",
    "- Tests for how well data fits a specific distribution.\n",
    "\n",
    "Compare the statistic to critical values. \n",
    "- If the statistic exceeds the critical value for a given significance level, residuals deviate from normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189c667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import anderson\n",
    "\n",
    "anderson_test = anderson(residuals, dist=\"norm\")\n",
    "print(\"Anderson-Darling Test Results:\")\n",
    "print(f\"Statistic: {anderson_test.statistic}\")\n",
    "print(\"Critical Values:\", anderson_test.critical_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b093a117",
   "metadata": {},
   "source": [
    "**Step 2: Addressing Non-Normal Residuals/ Handling Normality of Errors**\n",
    "\n",
    "(a) Transform the Response Variable\n",
    "\n",
    "1. Log Transformation: Use if residuals are right-skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7599aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df[\"Log_HousePrice\"] = np.log(df[\"HousePrice\"])\n",
    "model_log = sm.OLS(df[\"Log_HousePrice\"], X).fit()\n",
    "print(model_log.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4c3d4b",
   "metadata": {},
   "source": [
    "2. Applying Box-Cox Transformation\n",
    "- Finds the best transformation parameter (𝜆)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a0c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "# Apply Box-Cox to the response variable\n",
    "Y_boxcox, lambda_boxcox = boxcox(Y)\n",
    "print(f\"Optimal lambda for Box-Cox: {lambda_boxcox}\")\n",
    "\n",
    "# Fit the model again\n",
    "model_boxcox = sm.OLS(Y_boxcox, X).fit()\n",
    "print(model_boxcox.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "Y_transformed, lambda_boxcox = boxcox(Y)\n",
    "print(f\"Optimal lambda for Box-Cox: {lambda_boxcox}\")\n",
    "\n",
    "model_boxcox = sm.OLS(Y_transformed, X).fit()\n",
    "print(model_boxcox.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a92e7",
   "metadata": {},
   "source": [
    "3. Square Root Transformation: Helps stabilize variance and normalize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bec93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sqrt_HousePrice\"] = np.sqrt(df[\"HousePrice\"])\n",
    "model_sqrt = sm.OLS(df[\"Sqrt_HousePrice\"], X).fit()\n",
    "print(model_sqrt.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3871b7",
   "metadata": {},
   "source": [
    "(b) Using Robust Regression\n",
    "\n",
    "If normality cannot be achieved:\n",
    "- Robust regression minimizes the influence of outliers and non-normal errors.\n",
    "\n",
    "1. Huber Regression: Combines linear regression with robustness to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f243552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "huber = HuberRegressor()\n",
    "huber.fit(X, Y)\n",
    "print(\"Huber Coefficients:\", huber.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102a6cdb",
   "metadata": {},
   "source": [
    "2. Quantile Regression: Models conditional medians instead of means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "model_quantile = sm.QuantReg(Y, X).fit(q=0.5)  # Median regression\n",
    "print(model_quantile.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06009f97",
   "metadata": {},
   "source": [
    "3. Robust linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463892e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.robust.robust_linear_model import RLM\n",
    "\n",
    "# Fit a robust linear model\n",
    "model_robust = sm.RLM(Y, X).fit()\n",
    "print(model_robust.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5573d3",
   "metadata": {},
   "source": [
    "(c) Bootstrap for Non-Normal Residuals\n",
    "- Bootstrapping creates confidence intervals without assuming normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f813857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Bootstrap residuals\n",
    "bootstrap_samples = 1000\n",
    "boot_means = []\n",
    "\n",
    "for _ in range(bootstrap_samples):\n",
    "    Y_boot, X_boot = resample(Y, X)\n",
    "    model_boot = sm.OLS(Y_boot, X_boot).fit()\n",
    "    boot_means.append(model_boot.params)\n",
    "\n",
    "boot_means = np.array(boot_means)\n",
    "print(\"Bootstrap Confidence Intervals:\")\n",
    "print(np.percentile(boot_means, [2.5, 97.5], axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0500c7a6",
   "metadata": {},
   "source": [
    "**Step 3: Evaluating Adjustments**\n",
    "\n",
    "Evaluate and Compare Performance\n",
    "- Residual plots before and after adjustments.\n",
    "- Normality tests on new residuals.\n",
    "- Performance Metrics like $𝑅^2$, AIC, and BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3cc496",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original Model AIC: {model.aic}\")\n",
    "print(f\"Log-Transformed Model AIC: {model_log.aic}\")\n",
    "print(f\"Box-Cox Model AIC: {model_boxcox.aic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd82f9c3",
   "metadata": {},
   "source": [
    "Plot Residuals After Adjustments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc79817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals from Box-Cox model\n",
    "residuals_boxcox = Y_transformed - model_boxcox.predict(X)\n",
    "\n",
    "plt.hist(residuals_boxcox, bins=20, edgecolor='k', alpha=0.7)\n",
    "plt.title(\"Residual Histogram After Box-Cox\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7a294",
   "metadata": {},
   "source": [
    "Testing:\n",
    "- Use visual methods like histograms and Q-Q plots.\n",
    "- Statistical tests (Shapiro-Wilk, Anderson-Darling) confirm non-normality.\n",
    "\n",
    "Correction:\n",
    "- Start with transformations like log or Box-Cox.\n",
    "- Use robust regression if transformations fail or residuals deviate significantly.\n",
    "\n",
    "Validation:\n",
    "- Reassess residual plots and metrics post-adjustment.\n",
    "- Ensure the model aligns with assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ee434",
   "metadata": {},
   "source": [
    "# Checking for Outliers in Residuals\n",
    "\n",
    "Check for outliers amongst the residuals.\n",
    "\n",
    "#### Plotting Cook's Distance\n",
    "\n",
    "Cook's distance is a calculation which measures the effect of deleting an observation from the data. \n",
    "- Observations with large Cook's distances should be earmarked for closer examination in the analysis due to their disproportionate impact on the model.\n",
    "\n",
    "**Observation**\n",
    "\n",
    "Check values with much higher Cook's distances than the rest. \n",
    "- A rule of thumb for determining whether a Cook's distance is too large is whether it is greater than four times the mean Cook's distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import OLSInfluence as influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b57b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf=influence(fitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f81425",
   "metadata": {},
   "outputs": [],
   "source": [
    "(c, p) = inf.cooks_distance\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Cook's distance plot for the residuals\",fontsize=16)\n",
    "plt.stem(np.arange(len(c)), c, markerfmt=\",\", use_line_collection=True)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d04294",
   "metadata": {},
   "source": [
    "#### Calculate the mean Cooks Distance\n",
    "\n",
    "Check which observation are 4 X higher the the average\n",
    "\n",
    "Implications: Highly influential in this dataset\n",
    "- warrant closer examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Cook\\'s distance: ', c.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6228e",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression\n",
    "\n",
    "Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).\n",
    "\n",
    "It is used to describe data and to explain the relationship between one dependent binary variable and one or more \n",
    "- nominal, \n",
    "- ordinal, \n",
    "- interval or \n",
    "- ratio-level independent variables.\n",
    "\n",
    "Logistic regression is a statistical model used for binary classification tasks.\n",
    "- The outcome variable is categorical with two possible values (e.g., 1/0, Yes/No, Positive/Negative).\n",
    "- Used to predict the Probabilities for classification problems.\n",
    "\n",
    "It predicts the probability of an event occurring, transforming the linear combination of predictors through a logistic function (sigmoid function) to ensure the predicted probabilities lie between 0 and 1.\n",
    "\n",
    "Model Equation: \n",
    "$ 𝑃(𝑦=1)= \\frac{1}{1+𝑒^{−(𝛽_{0}+𝛽_{1}𝑥_{1}+…+𝛽_{𝑛}𝑥_{𝑛}})}$\n",
    "\n",
    "**What It Means:** \n",
    "- Logistic regression estimates the probability of a binary outcome (e.g., yes/no, success/failure) based on predictor variables. \n",
    "    - It uses a logistic function to map predictions to probabilities between 0 and 1.\n",
    "\n",
    "- It is a statistical technique for investigating the relationship between a binary dependent variable (outcome) and one or more independent variables (predictors). \n",
    "\n",
    "- The goal of logistic regression is to find the best-fitting model to describe the relationship between the dependent variable and the independent variables and then use that model to predict the outcome variable.\n",
    "\n",
    "**Lay Explanation:**\n",
    "- Logistic regression is like a yes-or-no decision helper. It estimates the chances of an event happening (e.g., a customer buying a product) based on known factors.\n",
    "- It tries to find the best-fitted curve for the data\n",
    "\n",
    "**Why use Logistic Regression rather than Linear Regression?**\n",
    "\n",
    "Outlier Influence:\n",
    "- best fit line in linear regression shifts to fit that point.\n",
    "\n",
    "Predicted outcome out of range:\n",
    "- In linear regression, the predicted values may be out of range.\n",
    "\n",
    "Response Variable:\n",
    "- Linear regression is used when dependent variable is continuous\n",
    "- Logistic Regression is used when our dependent variable is binary.\n",
    "\n",
    "Logistic regression is ideal for this problem because:\n",
    "- Binary Outcome: The target variable is binary: Readmitted (1) or Not Readmitted (0).\n",
    "- Interpretability: It provides coefficients (log odds) that indicate how changes in predictors affect the likelihood of the event (readmission).\n",
    "- Insights: It helps identify the significant factors influencing readmissions.\n",
    "\n",
    "### Outcome Interpretation: \n",
    "- The model outputs probabilities that can be converted to binary outcomes. \n",
    "- Coefficients show how each predictor variable influences the likelihood of the outcome.\n",
    "\n",
    "### Performance Measures:\n",
    "- Accuracy: Proportion of correct predictions.\n",
    "- AUC-ROC: Measures the model's ability to distinguish between classes; values closer to 1 indicate a better model.\n",
    "\n",
    "### Types of Logistic Regression\n",
    "\n",
    "#### Binary Logistic Regression\n",
    "Binary logistic regression is used to predict the probability of a binary outcome, such as \n",
    "- yes or no, \n",
    "- true or false, or \n",
    "- 0 or 1. \n",
    "\n",
    "For example, it could be used to:\n",
    "- predict whether a customer will churn or not, \n",
    "- predict whether a patient has a disease or not, or \n",
    "- predict whether a loan will be repaid or not.\n",
    "\n",
    "#### Multinomial Logistic Regression\n",
    "Multinomial logistic regression is used to predict the probability of one of three or more possible outcomes, such as \n",
    "- the type of product a customer will buy, \n",
    "- the rating a customer will give a product, or \n",
    "- the political party a person will vote for.\n",
    "\n",
    "#### Ordinal Logistic Regression\n",
    "Used to predict the probability of an outcome that falls into a predetermined order, such as \n",
    "- the level of customer satisfaction, \n",
    "- the severity of a disease, or \n",
    "- the stage of cancer.\n",
    "\n",
    "#### Least-Squares Regression \n",
    "Is a foundational method in statistics and data science for modeling relationships between variables, particularly for continuous dependent variables. \n",
    "- It does so by finding the line (or hyperplane in higher dimensions) that minimizes the sum of the squared differences (residuals) between the observed and predicted values of the dependent variable.\n",
    "\n",
    "Application:\n",
    "- Continuous Outcomes: Least-squares regression is most commonly used for problems where the dependent variable is continuous, such as \n",
    "    - predicting house prices, \n",
    "    - stock prices, or \n",
    "    - blood pressure.\n",
    "- Exploratory Analysis: Identifying potential relationships between variables.\n",
    "\n",
    "##### Drawback of least-squares regression\n",
    "When applied to classification tasks like logistic regression, is that it assumes linearity and can lead to problems when modeling binary or categorical outcomes.\n",
    "\n",
    "Key Issue:\n",
    "\n",
    "1. Inappropriate Predictions\n",
    "- Least-squares regression is designed for continuous outcomes and does not restrict predictions to the range [0, 1], which is required for probabilities in classification problems.\n",
    "- For binary classification, it can result in predictions outside the valid probability range, such as negative values or values greater than 1, which are meaningless.\n",
    "\n",
    "2. Violation of Assumptions\n",
    "- The error terms (residuals) in least-squares regression are assumed to be normally distributed and homoscedastic (constant variance). \n",
    "    - However, in classification problems, these assumptions are violated because:\n",
    "        - The dependent variable is not continuous but binary.\n",
    "        - The variance of the binary response variable is a function of the mean (heteroscedasticity), not constant.\n",
    "\n",
    "3. Inefficient Parameter Estimation (relationship between the predictors and the binary outcome)\n",
    "- Linear least squares does not model this relationship (non-linear relationship between predictors and the outcome) correctly .\n",
    "    - As a result, least squares is inefficient in estimating parameters and may lead to biased coefficients.\n",
    "- In classification tasks, the relationship between the predictors and the binary outcome is often non-linear (sigmoid-shaped in logistic regression). \n",
    "\n",
    "4. Poor Performance for Separation\n",
    "- Least-squares regression does not inherently maximize the separation between the two classes in binary classification problems. \n",
    "- Logistic regression, on the other hand, maximizes the likelihood of the observed data, providing a more suitable objective for classification tasks.\n",
    "\n",
    "5. Susceptibility to Outliers\n",
    "- Least-squares regression is sensitive to outliers, as it minimizes the squared residuals. \n",
    "- In a classification context, outliers in the feature space can have a disproportionately large influence on the model, leading to poor generalization.\n",
    "\n",
    "##### Why Logistic Regression Instead of Least Squares?\n",
    "Logistic regression overcomes these drawbacks by:\n",
    "- Modeling the probability of the binary outcome using the logit function (log-odds), ensuring probabilities stay within [0, 1].\n",
    "- Using maximum likelihood estimation (MLE) to fit the model, which aligns with the probabilistic nature of classification problems.\n",
    "- Making no assumptions about normally distributed errors, as it focuses on the Bernoulli distribution of binary outcomes.\n",
    "\n",
    "\n",
    "##### Differences Between Linear and Logistic Regression\n",
    "The core difference lies in their target predictions.\n",
    "- Linear regression excels at predicting continuous values along a spectrum. \n",
    "    - resulting output would be a specific amount, a continuous value on the amount scale.\n",
    "- Linear regression answers “how much” questions, providing a specific value on a continuous scale.\n",
    "\n",
    "- Logistic regression deals with categories. \n",
    "    - It doesn’t predict a specific value but rather the likelihood of something belonging to a particular class.\n",
    "    - output here would be a probability between 0 (not likely spam) and 1 (very likely spam). \n",
    "    - This probability is then used to assign an email to a definitive category (spam or not spam) based on a chosen threshold.\n",
    "- Logistic regression tackles “yes or no” scenarios, giving the probability of something belonging to a certain category.\n",
    "\n",
    "### Problem Statement\n",
    "Objective:\n",
    "- The medical institute, we want to identify the likelihood of patients being readmitted within 30 days of discharge based on patient \n",
    "    - demographics, \n",
    "    - medical history, \n",
    "    - length of stay (LOS), and \n",
    "    - clinical metrics such as blood pressure, \n",
    "    - blood glucose levels, and \n",
    "    - medication adherence.\n",
    "\n",
    "**Key Assumptions of Logistic Regression**\n",
    "\n",
    "Data Specific\n",
    "- Binary Outcome: The dependent variable is binary.\n",
    "    - Logistic regression is designed for binary dependent variables. \n",
    "    - If your outcome has more than two categories, you might need a multinomial logistic regression or other classification techniques.\n",
    "- Independence of Observations: Observations are independent of each other.\n",
    "    -  This means no repeated measurements or clustering within the data.\n",
    "\n",
    "Relationship Between Variables\n",
    "- Linearity of Log-Odds: There is a linear relationship between the log-odds of the outcome and the independent variables.\n",
    "    - Outcome itself has a relationship with log-odds.\n",
    "    - Outcome does not have linear relationship with the independent variables.\n",
    "- No Multicollinearity: Independent variables are not highly correlated.\n",
    "    - Multicollinearity can cause instability in the model and make it difficult to interpret the coefficients.\n",
    "\n",
    "Other\n",
    "- Large Sample Size: Logistic regression performs well with larger datasets.\n",
    "    - To ensure reliable parameter estimates.\n",
    "- Absence of Outliers: outliers can significantly influence the model. \n",
    "    - It’s important to check for and address any outliers that might distort the results.\n",
    "\n",
    "**Step 1: Define the Problem**\n",
    "- Target Variable: Readmission within 30 days (1 = Yes, 0 = No).\n",
    "- Predictors:\n",
    "    - Patient Demographics: Age, gender, insurance status.\n",
    "    - Clinical Metrics: Blood glucose levels, blood pressure, medication adherence.\n",
    "    - Hospital Metrics: Length of Stay (LOS), number of previous visits.\n",
    "\n",
    "**Step 2: Collect and Prepare Data**\n",
    "- Gather historical patient data and ensure it's clean and consistent.\n",
    "    - Check for Missing Data:\n",
    "    - Impute missing values for predictors like glucose levels using median or mean.\n",
    "    - Standardize Continuous Variables:\n",
    "    - Standardize LOS, glucose levels, and blood pressure for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed07b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    'age': [45, 60, 50, 40, 70],\n",
    "    'los': [3, 7, 4, 2, 10],\n",
    "    'glucose': [150, 200, 180, 140, 220],\n",
    "    'med_adherence': [0.8, 0.6, 0.75, 0.9, 0.5],\n",
    "    'readmitted': [1, 1, 0, 0, 1]\n",
    "})\n",
    "\n",
    "# Features and target\n",
    "X = data[['age', 'los', 'glucose', 'med_adherence']]\n",
    "y = data['readmitted']\n",
    "\n",
    "# Add constant for intercept\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8ef54",
   "metadata": {},
   "source": [
    "**Step 3: Exploratory Data Analysis**\n",
    "- Univariate Analysis: Examine distributions of continuous variables.\n",
    "- Bivariate Analysis: Analyze relationships between predictors and the target variable.\n",
    "- Correlation Matrix: Identify multicollinearity among predictors.\n",
    "\n",
    "**Step 4: Perform Logistic Regression**\n",
    "\n",
    "How logistic regression squeezes the output of linear regression between 0 and 1.\n",
    "\n",
    "Best Fit Equation in Linear Regression\n",
    "\n",
    "$ y = 𝛽_{0}+𝛽_{1}𝑥_{1}$\n",
    "\n",
    "Now we want to take probabilities (P) instead of y.\n",
    "\n",
    "**Issue**: \n",
    "the value of (P) will exceed 1 or go below 0 and we know that range of Probability is (0-1)\n",
    "\n",
    "Odds and log-odds are central to understanding the relationship between predictors and the probability of an event occurring.\n",
    "\n",
    "**Overcome issue of $0 < P < 1$**\n",
    "\n",
    "by taking “odds” of P:\n",
    "\n",
    "Odds: The odds represent the ratio of the probability of an event occurring (P) to the probability of it not occurring (1−P).\n",
    "\n",
    "$$ Odds =  \\frac{P}{1-P}$$\n",
    "\n",
    "Log-Odds (Logit): The natural logarithm of the odds.\n",
    "\n",
    "$$ Log-Odds =  \\log(\\frac{P}{1-P})$$\n",
    "\n",
    "In logistic regression, the log-odds are modeled as a linear function of the predictors:\n",
    "\n",
    "$$ P = 𝛽_{0}+𝛽_{1}𝑥_{1}$$\n",
    "$$ \\frac{P}{1-P} = 𝛽_{0}+𝛽_{1}𝑥_{1}$$\n",
    "\n",
    "Odds can always be positive which means the range will always be ($0,+∞ $).\n",
    "- Odds are the ratio of the probability of success and probability of failure.\n",
    "\n",
    "Why ‘odds’?\n",
    "- odds are probably the easiest way to do this.\n",
    "\n",
    "Problem: is that the range is restricted and we don’t want a restricted range because if we do so then our correlation will decrease.\n",
    "- By restricting the range we are actually decreasing the number of data points and if we decrease our data points, our correlation will decrease.\n",
    "- Making it difficult to model a variable that has a restricted range.\n",
    "\n",
    "Control:\n",
    "- Control this we take the log of odds which has a range from (-∞,+∞)\n",
    "\n",
    "$ \\log(\\frac{P}{1-P}) = 𝛽_{0}+𝛽_{1}𝑥_{1}$\n",
    "\n",
    "Now we just want a function of P because we want to predict probability not log of odds. To do so we will \n",
    "- multiply by exponent on both sides and then solve for P.\n",
    "\n",
    "$ \\exp[\\log(\\frac{P}{1-P})] = \\exp(𝛽_{0}+𝛽_{1}𝑥_{1})$\n",
    "\n",
    "$ \\exp^{\\ln[\\frac{P}{1-P})} = \\exp^{(𝛽_{0}+𝛽_{1}𝑥_{1})} $\n",
    "\n",
    "$ \\frac{P}{1-P} = \\exp^{(𝛽_{0}+𝛽_{1}𝑥_{1})} $\n",
    "\n",
    "$ p = \\exp^{(𝛽_{0}+𝛽_{1}𝑥_{1})}  - p\\exp^{(𝛽_{0}+𝛽_{1}𝑥_{1})}$\n",
    "\n",
    "Now we have sigmoid function.\n",
    "\n",
    "Model Equation: \n",
    "$ 𝑃(𝑦=1)= \\frac{1}{1+𝑒^{−(𝛽_{0}+𝛽_{1}𝑥_{1}+…+𝛽_{𝑛}𝑥_{𝑛}})}$\n",
    "\n",
    "It squeezes a straight line into an S-curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2893d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the sigmoid function\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    The sigmoid function maps log-odds to probabilities between 0 and 1.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Function to calculate odds and log-odds\n",
    "def logistic_regression_predict(X, coefficients):\n",
    "    \"\"\"\n",
    "    Predict probabilities, odds, and log-odds using logistic regression.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Feature matrix (numpy array of shape [n_samples, n_features])\n",
    "    - coefficients: Coefficients including intercept (numpy array of shape [n_features + 1])\n",
    "    \n",
    "    Returns:\n",
    "    - probabilities: Predicted probabilities (numpy array of shape [n_samples])\n",
    "    - odds: Odds of event occurring (numpy array of shape [n_samples])\n",
    "    - log_odds: Log-Odds (numpy array of shape [n_samples])\n",
    "    \"\"\"\n",
    "    # Add intercept to the feature matrix\n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add a column of ones for the intercept\n",
    "    \n",
    "    # Calculate log-odds (z = X * coefficients)\n",
    "    log_odds = np.dot(X, coefficients)\n",
    "    \n",
    "    # Calculate probabilities using the sigmoid function\n",
    "    probabilities = sigmoid(log_odds)\n",
    "    \n",
    "    # Calculate odds\n",
    "    #  Derived from probabilities using the formula\n",
    "    odds = probabilities / (1 - probabilities)\n",
    "    \n",
    "    return probabilities, odds, log_odds\n",
    "\n",
    "# Example usage\n",
    "# Example dataset: X contains two features, and coefficients include intercept and weights\n",
    "X = np.array([[2, 3], [1, 0], [4, 5]])  # Feature matrix\n",
    "coefficients = np.array([-3, 0.5, 1])  # Coefficients (intercept + weights for features)\n",
    "\n",
    "# Predict probabilities, odds, and log-odds\n",
    "probabilities, odds, log_odds = logistic_regression_predict(X, coefficients)\n",
    "\n",
    "# Predicted Probabilities: Likelihood of the event occurring.\n",
    "# Odds: Ratio of the probability of success to failure.\n",
    "# Log-Odds: Linear transformation of the predictors.\n",
    "\n",
    "# Print results\n",
    "print(\"Predicted Probabilities:\", probabilities)\n",
    "print(\"Odds:\", odds)\n",
    "print(\"Log-Odds:\", log_odds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79dd7e3",
   "metadata": {},
   "source": [
    "**log-odds linear function**\n",
    "\n",
    "The log-odds linear function is a core concept in logistic regression and represents the relationship between the independent variables (predictors) and the log-odds of the dependent variable (outcome).\n",
    "\n",
    "$$ \\log(\\frac{P}{1-P}) = 𝛽_{0}+𝛽_{1}𝑥_{1}+𝛽_{2}𝑥_{2}+...+ +𝛽_{p}𝑥_{p}$$\n",
    "\n",
    "Where:\n",
    "- $𝛽_{0}$: Intercept (bias term).\n",
    "- $𝛽_{1}, 𝛽_{2},..., 𝛽_{p}$: Coefficients of the predictors $x_{1}, x_{2},..., x_{p}$\n",
    "- $x_{1}, x_{2},..., x_{p}$: Values of the independent variables.\n",
    "= $P$: Predicted probability of the event occurring.\n",
    "\n",
    "Steps to Calculate Log-Odds\n",
    "1. Start with the linear combination: Compute a weighted sum of the predictors and the intercept:\n",
    "\n",
    "$$ z = 𝛽_{0}+𝛽_{1}𝑥_{1}+𝛽_{2}𝑥_{2}+...+ +𝛽_{p}𝑥_{p}$$\n",
    "\n",
    "2. Interpret z as the log-odds: The value z is the log-odds, which can be converted to:\n",
    "- Odds using: \n",
    "$$ odds = e^z $$\n",
    "- Probability using the sigmoid function:\n",
    "$$ P= \\frac{1}{1+e^z}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1384637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate log-odds\n",
    "def calculate_log_odds(intercept, coefficients, predictors):\n",
    "    \"\"\"\n",
    "    Calculate log-odds for logistic regression.\n",
    "\n",
    "    Parameters:\n",
    "    - intercept: Intercept term (beta_0)\n",
    "    - coefficients: Coefficients for the predictors (list or array)\n",
    "    - predictors: Values of the predictors (list or array)\n",
    "\n",
    "    Returns:\n",
    "    - log_odds: Computed log-odds\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays\n",
    "    coefficients = np.array(coefficients)\n",
    "    predictors = np.array(predictors)\n",
    "    \n",
    "    # Compute log-odds\n",
    "    log_odds = intercept + np.dot(coefficients, predictors)\n",
    "    return log_odds\n",
    "\n",
    "# Example inputs\n",
    "intercept = -2\n",
    "coefficients = [0.8, -1.2]  # Beta coefficients\n",
    "predictors = [3, 5]         # Predictor values (x_1, x_2)\n",
    "\n",
    "# Calculate log-odds\n",
    "log_odds = calculate_log_odds(intercept, coefficients, predictors)\n",
    "print(\"Log-Odds:\", log_odds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6cf053",
   "metadata": {},
   "source": [
    "**Calculate class probabilities in logistic regression**\n",
    "\n",
    "the logistic (sigmoid) function is used to transform the log-odds into probabilities. \n",
    "- The logistic function ensures the probabilities range between 0 and 1, making it suitable for classification problems\n",
    "\n",
    "Logistic Function for Probability\n",
    "\n",
    "$$ P= \\frac{1}{1+e^z}$$\n",
    "\n",
    "Where:\n",
    "- P: Probability of the positive class (class 1).\n",
    "- z: Log-odds, calculated as:\n",
    "\n",
    "$$ z = 𝛽_{0}+𝛽_{1}𝑥_{1}+𝛽_{2}𝑥_{2}+...+ +𝛽_{p}𝑥_{p}$$\n",
    "\n",
    "- z is the weighted sum of the predictors and the intercept.\n",
    "\n",
    "The logistic function outputs:\n",
    "- P: Probability of the positive class (class 1).\n",
    "- 1−P: Probability of the negative class (class 0).\n",
    "\n",
    "Steps to Calculate Class Probability\n",
    "1. Calculate Log-Odds (z): Compute the linear combination of the intercept ($𝛽_0$) and the predictor variables.\n",
    "2. Apply the Logistic Function: \n",
    "Use the formula: \n",
    "$$ P= \\frac{1}{1+e^z}$$\n",
    "\n",
    "3. Interpret the Result:\n",
    "- If P≥0.5, classify the observation as the positive class (class 1).\n",
    "- If P<0.5, classify the observation as the negative class (class 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid (logistic) function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Function to calculate probability\n",
    "def calculate_probability(intercept, coefficients, predictors):\n",
    "    \"\"\"\n",
    "    Calculate class probability using the logistic function.\n",
    "\n",
    "    Parameters:\n",
    "    - intercept: Intercept term (beta_0)\n",
    "    - coefficients: Coefficients for predictors (list or array)\n",
    "    - predictors: Values of predictors (list or array)\n",
    "\n",
    "    Returns:\n",
    "    - probability: Probability of the positive class (class 1)\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays\n",
    "    coefficients = np.array(coefficients)\n",
    "    predictors = np.array(predictors)\n",
    "    \n",
    "    # Calculate log-odds\n",
    "    log_odds = intercept + np.dot(coefficients, predictors)\n",
    "    \n",
    "    # Apply sigmoid function to get probability\n",
    "    probability = sigmoid(log_odds)\n",
    "    return probability\n",
    "\n",
    "# Example inputs\n",
    "intercept = -2\n",
    "coefficients = [0.8, -1.2]  # Beta coefficients\n",
    "predictors = [3, 5]         # Predictor values (x_1, x_2)\n",
    "\n",
    "# Calculate class probability\n",
    "probability = calculate_probability(intercept, coefficients, predictors)\n",
    "print(\"Class Probability (P for class 1):\", probability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176fc5fc",
   "metadata": {},
   "source": [
    "### Decision boundary in logistic regression\n",
    "\n",
    "The decision boundary in logistic regression is the threshold at which the model predicts one class over the other. It represents the dividing line (or surface in higher dimensions) between the predicted classes in the feature space.\n",
    "\n",
    "Key Points about Decision Boundary in Logistic Regression\n",
    "1. Sigmoid Function and Threshold:\n",
    "- Logistic regression uses the sigmoid function to output probabilities between 0 and 1.\n",
    "- A commonly used threshold is 0.5\n",
    "    - If P≥0.5, classify as class 1 (positive class).\n",
    "    - If P<0.5, classify as class 0 (negative class).\n",
    "\n",
    "2. Log-Odds and Decision Boundary:\n",
    "- The decision boundary corresponds to where the log-odds (z) equals zero.\n",
    "- At z=0:\n",
    "\n",
    "$$ P= \\frac{1}{1+e^z}$$\n",
    "$$ P= \\frac{1}{1+e^0}$$\n",
    "$$ P= \\frac{1}{2}$$\n",
    "$$ P= 0.5$$\n",
    "\n",
    "- Thus, the decision boundary is the set of points where z=0, or equivalently:\n",
    "\n",
    "$$ z = 𝛽_{0}+𝛽_{1}𝑥_{1}+𝛽_{2}𝑥_{2}+...+ +𝛽_{p}𝑥_{p} = 0$$\n",
    "\n",
    "3. Geometric Interpretation:\n",
    "- In 2D (one predictor): The decision boundary is a line.\n",
    "- In 3D (two predictors): The decision boundary is a plane.\n",
    "- In higher dimensions: The decision boundary is a hyperplane.\n",
    "\n",
    "4. Linear Nature of Decision Boundary:\n",
    "- Logistic regression assumes a linear relationship between the predictors and the log-odds.\n",
    "- The decision boundary is linear unless the model is extended with non-linear transformations of the predictors (e.g., polynomial features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3791eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Coefficients\n",
    "beta_0 = -2      # Intercept\n",
    "beta_1 = 0.8     # Coefficient for x1\n",
    "beta_2 = -1.2    # Coefficient for x2\n",
    "\n",
    "# Generate a range of x1 values\n",
    "x1 = np.linspace(-10, 10, 100)\n",
    "\n",
    "# Calculate x2 for the decision boundary\n",
    "x2 = (-beta_0 - beta_1 * x1) / beta_2\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x1, x2, label=\"Decision Boundary\", color=\"red\")\n",
    "\n",
    "# Add some random points for class 0 and class 1\n",
    "np.random.seed(42)\n",
    "class_0 = np.random.multivariate_normal([3, 3], [[2, 1], [1, 2]], size=50)\n",
    "class_1 = np.random.multivariate_normal([-3, -3], [[2, 1], [1, 2]], size=50)\n",
    "\n",
    "plt.scatter(class_0[:, 0], class_0[:, 1], label=\"Class 0\", color=\"blue\", alpha=0.7)\n",
    "plt.scatter(class_1[:, 0], class_1[:, 1], label=\"Class 1\", color=\"green\", alpha=0.7)\n",
    "\n",
    "# Formatting\n",
    "plt.axhline(0, color=\"black\", linewidth=0.5, linestyle=\"--\")\n",
    "plt.axvline(0, color=\"black\", linewidth=0.5, linestyle=\"--\")\n",
    "plt.title(\"Decision Boundary of Logistic Regression\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018e34e",
   "metadata": {},
   "source": [
    "### Key properties of the logistic regression equation\n",
    "\n",
    "Expalin the Logistic regression model\n",
    "\n",
    "Sigmoid Function:\n",
    "- uses a special “S” shaped curve to predict probabilities. It ensures that the predicted probabilities stay between 0 and 1.\n",
    "\n",
    "Straightforward Relationship:\n",
    "- relationship between our inputs and the outcome is like drwing a straight line but a curve is there instead.\n",
    "\n",
    "Coefficients / parameters:\n",
    "- numbers that tell us how much each input affects the outcome in the logistic regression model.\n",
    "- coefficient tells us how much the outcome changes for every one unit increase in predictor variable.\n",
    "\n",
    "Best Guess: \n",
    "- Figure out the best coefficients for the logistic regression model by looking at the data we have and tweaking them until our predictions match the real outcomes as closely as possible.\n",
    "\n",
    "Basic Assumptions:\n",
    "- We assume that our observations are independent, meaning one doesn’t affect the other. \n",
    "- We assume that there’s not too much overlap between our predictors (like age and height), \n",
    "- We assume the relationship between our predictors and the outcome is kind of like a straight line.\n",
    "\n",
    "Probabilities, Not Certainties:\n",
    "- Logistic regression gives us probabilities.\n",
    "- Then decide on a cutoff point to make our final decision.\n",
    "\n",
    "Checking Our Work:\n",
    "- We make sure our predictions are good, like \n",
    "    - accuracy, \n",
    "    - precision, \n",
    "    - recall,\n",
    "    - ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "data = pd.read_csv('data.csv') # read data from csv file\n",
    "X = data[['Independent_Var_1', 'Independent_Var_2', 'Independent_Var_3']] # select independent variables\n",
    "Y = data['Dependent_Var'] # select dependent variable\n",
    "\n",
    "# Add a constant to the independent variable set\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = sm.Logit(Y, X).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3daca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train = data[:800]\n",
    "test = data[800:]\n",
    "\n",
    "# Define the independent variables\n",
    "X_train = train[['age', 'gender', 'income']]\n",
    "X_test = test[['age', 'gender', 'income']]\n",
    "\n",
    "# Define the dependent variable\n",
    "y_train = train['buy_product']\n",
    "y_test = test['buy_product']\n",
    "\n",
    "# Fit the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the outcomes for the test data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa885e4",
   "metadata": {},
   "source": [
    "**Step 5: Interpret Coefficients and Evaluate the Model**\n",
    "\n",
    "- Log Odds: Each coefficient represents the change in log odds of readmission for a unit increase in the predictor.\n",
    "- Odds Ratios: Use np.exp(model.params) to convert coefficients to odds ratios.\n",
    "\n",
    "1. Accuracy\n",
    "2. Confusion Matrix\n",
    "3. ROC Curve and AUC\n",
    "\n",
    "**Step 6: Optimisation**\n",
    "\n",
    "### Cost Function in Logistic Regression\n",
    "\n",
    "Linear regression, uses the Mean squared error which was the difference between y_predicted and y_actual\n",
    "- this is derived from the **maximum likelihood estimator**.\n",
    "\n",
    "logistic regression $Yi$ is a non-linear function ($ Ŷ= \\frac{1}​{1+ e-z}$).\n",
    "- If we use this in the above MSE equation then it will give a non-convex graph with many local minima.\n",
    "\n",
    "Problem: cost function will give results with local minima\n",
    "- End up miss out on our global minima and our error will increase.\n",
    "\n",
    "Solution: derive a different cost function for logistic regression\n",
    "- **log loss** which is also derived from the **maximum likelihood estimation method**.\n",
    "\n",
    "$ Log Loss = \\frac{1}{N} \\sum^{N}_{i = 1} - ( y_i * \\log(Y_i) + (1 - y_i) * log (1 - Y_i))$\n",
    "\n",
    "#### Maximum likelihood estimator\n",
    "\n",
    "Primary Objctive:\n",
    "- is to identify parameter values that maximize the likelihood function.\n",
    "- it represents the joint probability density function (pdf) of our sample observations.\n",
    "- it involves multiplying the conditional probabilities for observing each example given the distribution parameters.\n",
    "- this process aims to discover parameter values such that, when plugged into the model for P(x), it produces a value close to one for individuals with a predicted outcome and close to zero for those with a predicted outcome.\n",
    "\n",
    "Start by defining our likelihood function. \n",
    "- We now know that the labels are binary\n",
    "- we have two outcomes success and failure. \n",
    "- This means we can interpret each label as Bernoulli random variable.\n",
    "\n",
    "**Random experiment** whose outcomes are of two types, success S and failure F, occurring with probabilities p and q respectively is called a Bernoulli trial. If for this experiment a random variable X is defined such that it takes value 1 when S occurs and 0 if F occurs, then X follows a Bernoulli Distribution.\n",
    "\n",
    "#### Math behind this log loss function\n",
    "\n",
    "$ Y ~ Ber(P)$\n",
    "\n",
    "Where P is our sigmoid function\n",
    "\n",
    "$ P[Y=y | X=x] = \\sigma ( \\theta^{T} x^i)^y (1 - \\sigma(\\theta^{T} x^i))^{1-y} $\n",
    "\n",
    "where σ(θ^T*x^i) is the sigmoid function. Now for n observations\n",
    "\n",
    "$ L(\\theta) = \\prod^{n}_{1} \\sigma ( \\theta^{T} x^i)^y (1 - \\sigma(\\theta^{T} x^i))^{1-y} $\n",
    "\n",
    "We need a value for theta which will maximize this likelihood function. \n",
    "\n",
    "To make our calculations easier\n",
    "- we multiply the log on both sides. \n",
    "\n",
    "The function we get is also called the \n",
    "- log-likelihood function or \n",
    "- sum of the log conditional probability\n",
    "\n",
    "$ \\log(L(\\theta)) = \\sum^{n}_{1} * \\log[\\sigma ( \\theta^{T} x^i)] + (1-y) * \\log(1 - \\sigma(\\theta^{T} x^i)] $\n",
    "\n",
    "In ML, it is conventional to minimize a loss(error) function via gradient descent, rather than maximize an objective function via gradient ascent. \n",
    "- If we maximize this above function then we’ll have to deal with gradient ascent to avoid this we take negative of this log so that we use gradient descent.\n",
    "\n",
    "$ max[log(x)] = min[-log(x)] $\n",
    "\n",
    "The negative of this function is our cost function and what do we want with our cost function? That it should have a minimum value. \n",
    "It is common practice to minimize a cost function for optimization problems; therefore, we can invert the function so that we minimize the negative log-likelihood (NLL).\n",
    "\n",
    "$ - \\log(L(\\theta)) =  -\\sum^{n}_{1} * \\log[\\sigma ( \\theta^{T} x^i)] + (1-y) * \\log(1 - \\sigma(\\theta^{T} x^i)] $\n",
    "\n",
    "where \n",
    "- y represents the actual class and \n",
    "    - p(y) is the probability of 1.\n",
    "- log(σ(θ^T*x^i) ) is the probability of that class.\n",
    "    - 1-p(y) is the probability of 0.\n",
    "\n",
    "Get graph of cost function when y=1 and y=0.\n",
    "- By getting a convex graph with only 1 local minimum and now it’ll be easy to use gradient descent.\n",
    "    - red line here represents the 1 class (y=1), the right term of cost function will vanish. Now if the predicted probability is close to 1 then our loss will be less and when probability approaches 0, our loss function reaches infinity.\n",
    "    - black line represents 0 class (y=0), the left term will vanish in our cost function and if the predicted probability is close to 0 then our loss function will be less but if our probability approaches 1 then our loss function reaches infinity.\n",
    "\n",
    "$ Cost(h_{\\Theta}(x),y) = \\left\\{ \\begin{array}{rcl} - \\log(h_{\\Theta}(x)) if y = 1\\\\ - \\log(1 - h_{\\Theta}(x)) if y = 0 \\end{array}\\right.$\n",
    "\n",
    "Cost function is also called **log loss**\n",
    "\n",
    "It also ensures that as the\n",
    "- probability of the correct answer is maximized, \n",
    "- probability of the incorrect answer is minimized. \n",
    "    - Lower the value of this cost function higher will be the accuracy.\n",
    "\n",
    "### Gradient Descent Optimization\n",
    "\n",
    "How to use Gradient Descent to compute the minimum cost.\n",
    "\n",
    "- Gradient descent changes the value of our weights in such a way that it always converges to minimum point\n",
    "    - it aims at finding the optimal weights which minimize the loss function of our model.\n",
    "Gradient descent is an iterative method that finds the minimum of a function by figuring out the slope at a random point and then moving in the opposite direction.\n",
    "\n",
    "At first \n",
    "- gradient descent takes a random value of our parameters from our function. \n",
    "- need an algorithm that will tell us whether at the next iteration we should move left or right to reach the minimum point.\n",
    "    - The gradient descent algorithm \n",
    "        - finds the slope of the loss function at that particular point and then \n",
    "In the next iteration, \n",
    "- it moves in the opposite direction to reach the minima.\n",
    "\n",
    "Since we have a convex graph now we don’t need to worry about local minima. \n",
    "    - A convex curve will always have only 1 minima.\n",
    "\n",
    "Gradient descent algorithm\n",
    "\n",
    "$ \\theta_{new} = \\theta_{old} - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j} $\n",
    "\n",
    "where alpha is known as the learning rate. \n",
    "- It determines the step size at each iteration while moving towards the minimum point. \n",
    "    - a lower value of “alpha” is preferred, because if the learning rate is a big number then we may miss the minimum point and keep on oscillating in the convex curve.\n",
    "\n",
    "#### Derivation of Cost Function\n",
    "Derive this cost function w.r.t our parameters.\n",
    "\n",
    "$\n",
    "\\frac{d\\sigma(x)}{dx} = \\frac{d}{dx} \\left( \\frac{1}{1+e^{-x}} \\right) = \\frac{d}{dx} \\left( 1 + e^{-x} \\right)^{-1} $\n",
    "\n",
    "$\\Rightarrow -\\left(1 + e^{-x}\\right)^{-2} \\times \\frac{d}{dx} \\left(1 + e^{-x}\\right)$\n",
    "\n",
    "$\\Rightarrow -\\left(1 + e^{-x}\\right)^{-2} \\times \\left[ 0 + \\frac{d}{dx} \\left(e^{-x}\\right) \\right]$\n",
    "\n",
    "$\\Rightarrow -\\left(1 + e^{-x}\\right)^{-2} \\times \\left[e^{-x} \\times \\frac{d}{dx}(-x) \\right]$\n",
    "\n",
    "$\\Rightarrow -\\left(1 + e^{-x}\\right)^{-2} \\times \\left[e^{-x} \\times (-1) \\right]$\n",
    "\n",
    "$\\Rightarrow e^{-x} \\left(1 + e^{-x}\\right)^{-2}$\n",
    "\n",
    "$\\Rightarrow \\frac{e^{-x}}{(1+e^{-x})^2} = \\frac{e^{-x} + 1 - 1}{(1+e^{-x})(1+e^{-x})}$\n",
    "\n",
    "$\\Rightarrow \\frac{(1+e^{-x}) - 1}{(1+e^{-x})(1+e^{-x})} = \\frac{1}{(1+e^{-x})} \\left[ \\frac{(1+e^{-x})}{(1+e^{-x})} - \\frac{1}{(1+e^{-x})} \\right]$\n",
    "\n",
    "$\\Rightarrow \\frac{1}{(1+e^{-x})} \\left[ 1 - \\frac{1}{(1+e^{-x})} \\right]$\n",
    "\n",
    "Derive the cost function with the help of the chain rule as it allows us to calculate complex partial derivatives by breaking them down.\n",
    "\n",
    "**Step-1: Use chain rule and break the partial derivative of log-likelihood**\n",
    "\n",
    "$-\\frac{\\partial LL(\\theta)}{\\partial \\theta_j} = -\\frac{\\partial LL(\\theta)}{\\partial p} \\cdot \\frac{\\partial p}{\\partial \\theta} \\quad$\n",
    "$\\text{where } p= \\sigma\\left[\\theta^\\top x\\right]$\n",
    "\n",
    "$= -\\frac{\\partial LL(\\theta)}{\\partial p} \\cdot \\frac{\\partial p}{\\partial z} \\cdot \\frac{\\partial z}{\\partial \\theta_j} \\quad $\n",
    "$\\text{where } z =\\theta^\\top x$\n",
    "\n",
    "**Step-2: Find derivative of log-likelihood w.r.t p**\n",
    "\n",
    "We know,\n",
    "\n",
    "$LL(\\theta) = y \\log(p) + (1-y)\\log(1-p) \\quad \\text{where } p = \\sigma\\left[\\theta^\\top x\\right]$\n",
    "\n",
    "$\\frac{\\partial LL(\\theta)}{\\partial p} = \\frac{y}{p} + \\frac{(1-y)}{(1-p)}$\n",
    "\n",
    "**Step-3: Find derivative of ‘p’ w.r.t ‘z’**\n",
    "\n",
    "$ p= \\sigma(z)$\n",
    "\n",
    "$\\frac{\\partial p}{\\partial z} = \\frac{\\partial[ \\sigma (z)]}{\\partial z}$\n",
    "\n",
    "We know the derivative of sigmoid function is $\\sigma[\\theta^\\top x][1 - \\sigma(\\theta^\\top x)]$\n",
    "\n",
    "$\\Rightarrow \\frac{\\partial p}{\\partial z} =  \\sigma [z][1 - \\sigma(z)]$\n",
    "\n",
    "**Step-4: Find derivate of z w.r.t θ**\n",
    "\n",
    "$ z=\\theta^\\top x$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial \\theta_j} = x_j$\n",
    "\n",
    "**Step-5: Put all the derivatives in equation 1**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X) > 0.5\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc749e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a4e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y, model.predict(X))\n",
    "auc = roc_auc_score(y, model.predict(X))\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda6f67",
   "metadata": {},
   "source": [
    "**Understanding Factors Significantly Influencing Readmission**\n",
    "\n",
    "1. Use p-values from the logistic regression summary:\n",
    "- Predictors with $𝑝< 0.05$ are statistically significant.\n",
    "2. Assess the odds ratios:\n",
    "- For example, if the odds ratio for LOS is 2.0, each additional day in the hospital doubles the odds of readmission.\n",
    "3. Visualize relationships:\n",
    "- Plot odds ratios for key predictors to present to stakeholders.\n",
    "\n",
    "**Statistical Hypothesis Testing**\n",
    "\n",
    "Example 1: Relationship Between LOS and Readmission\n",
    "- Hypotheses:\n",
    "    - $𝐻_0$: LOS has no effect on readmission.\n",
    "    - $𝐻_𝑎$: LOS has a significant effect on readmission.\n",
    "- Approach: Perform a logistic regression test and check the p-value for LOS.\n",
    "\n",
    "Example 2: Age Group vs. Readmission\n",
    "- Hypotheses:\n",
    "    - $𝐻_0$: Age group is independent of readmission.\n",
    "    - $𝐻_𝑎$: Age group and readmission are dependent.\n",
    "- Approach: Use a Chi-Square test of independence (see previous example).\n",
    "\n",
    "**Actionable Insights**\n",
    "- Highlight key factors significantly influencing readmission (e.g., LOS, medication adherence).\n",
    "- Use odds ratios to explain how much each factor increases or decreases the likelihood of readmission.\n",
    "- Present findings visually (e.g., bar charts for odds ratios, ROC curves for model performance).\n",
    "\n",
    "### Variables and Variable Selection\n",
    "\n",
    "Learn how to:\n",
    "\n",
    "- Differentiate between Variable Types and Dummy Variables;\n",
    "- Select features based on correlation;\n",
    "- Select features based on variance thresholds.\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "**Variables** are the basic building blocks of datasets. \n",
    "- The quality of the variables present within your dataset has a direct impact on the intuition and overall outcome of your machine learning model. \n",
    "\n",
    "**Variable selection** and an in-depth knowledge of the domain you're building your model in remains essential when developing a predictive model.\n",
    "\n",
    "The purpose of regression is essentially to build associations between multiple variables. \n",
    "- Variable selection involves the \n",
    "    - elimination of input variables which may in turn reduce the computational cost of modeling \n",
    "    - improve the performance of the model. \n",
    "\n",
    "The model is structured around the belief that one of the variables in our dataset is a dependent variable (DV), that is explained or predicted in some way by the other independent variables (IVs). In this sense we work with: \n",
    "\n",
    "**Input variables** - are referred to as the independent variables (IVs) and used to explain or predict the target variable\n",
    "\n",
    "**Target variable** - are referred to as the dependent variable (DV) and is the target variable you want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f6c343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns have white space that we want to replace with an underscore (to avoid using the column names as variable names later on)\n",
    "df.columns = [col.replace(\" \",\"_\") for col in df.columns] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5d5b3f",
   "metadata": {},
   "source": [
    "##### Perfom preliminary data preprocessing\n",
    "\n",
    "to build some relationship between variables that are likely to indicate the dependent variable outcome once someone has taken a positive outcome (taken a loan), we really only want to consider instances (customers) who actually are on the positive predictive outcome (took personal loan) to build this relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e9549",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Personal_Loan'] == 1]\n",
    "df = df.drop(['Personal_Loan'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd704a80",
   "metadata": {},
   "source": [
    "##### Varaible types\n",
    "\n",
    "`df.info()` specifically outputs the number of non-null entries in each column. \n",
    "- We can be certain that our data has missing values if columns have a varying number of non-null entries.\n",
    "\n",
    "`df.describe()` show the summary statistics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4723f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6935c709",
   "metadata": {},
   "source": [
    "#### Dummy Variable Encoding\n",
    "From summary statistics of our numerical categorical data ('Online', 'CD_Account', 'Securities_Account') , Little to No information gotten.\n",
    "\n",
    "NB, All input data for regression model building purposes needs to be numerical. \n",
    "\n",
    "Transform the text data (found within columns such as 'Education','Gender', and 'Area') into numbers before we can train our machine learning model.\n",
    "\n",
    "To facilitate this transformation from textual-categorical data to numerical equivalents, \n",
    "- use a pandas method called get_dummies. \n",
    "- The text data are categorical variables, and get_dummies will transform all the categorical text data into numbers by adding a column for each distinct category. \n",
    "    - The new column has a 1 for observations which were in this category, and a 0 for observations that were not.\n",
    "\n",
    "For example, the dataframe:\n",
    "\n",
    "| Dog Age | Breed      |\n",
    "|---------|------------|\n",
    "| 15      | \"Bulldog\"  |\n",
    "| 12      | \"Labrador\" |\n",
    "| 10      | \"Labrador\" |\n",
    "| 22      | \"Beagle\"   |\n",
    "| 9       | \"Labrador\" |\n",
    "\n",
    "\n",
    "After `pd.dummies` becomes:\n",
    "\n",
    "| Dog Age | Breed_Labrador | Breed_Bulldog | Breed_Beagle |\n",
    "|---------|----------------|---------------|--------------|\n",
    "| 15      | 1              | 0             | 0            |\n",
    "| 12      | 0              | 1             | 0            |\n",
    "| 10      | 1              | 0             | 0            |\n",
    "| 22      | 0              | 0             | 1            |\n",
    "| 9       | 1              | 0             | 0            |\n",
    "\n",
    "This is a process known as [Dummy Variable Encoding]\n",
    "- important step in preprocessing data for regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff403e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df)\n",
    "\n",
    "# Again we make sure that all the column names have underscores instead of whitespaces\n",
    "df_dummies.columns = [col.replace(\" \",\"_\") for col in df_dummies.columns] \n",
    "\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3294b442",
   "metadata": {},
   "source": [
    "#### Correlations and Model Structure\n",
    "Now, we can build a model that predicts `Loan_Size` (our dependent variable) as a function of 43 different independent variables (IVs)\n",
    "\n",
    "1. reorder columns so that our dependent variable is the last column of the dataframe. \n",
    "- making a heatmap visualisation representing a correlation matrix of our data easier to interpret.\n",
    "\n",
    "2. Run correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db898f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_titles = [col for col in df_dummies.columns if col!= 'Loan_Size'] + ['Loan_Size']\n",
    "df_dummies=df_dummies.reindex(columns=column_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f5a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run corr matrix\n",
    "df_dummies.corr()\n",
    "\n",
    "from statsmodels.graphics.correlation import plot_corr\n",
    "\n",
    "fig = plt.figure(figsize=(15,15));\n",
    "ax = fig.add_subplot(111);\n",
    "plot_corr(df_dummies.corr(), xnames = df_dummies.corr().columns, ax = ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fc998e",
   "metadata": {},
   "source": [
    "#### Rerun your Model: Fitting the model using `statsmodels.OLS`\n",
    "\n",
    "##### Generating the regression string\n",
    "\n",
    "Importing the statsmodels library which has a rich set of statistical tools to help us. \n",
    "\n",
    "Those of you familiar with the R language will know that fitting a machine learning model requires a sort of string of the form:\n",
    "\n",
    "`y ~ X`\n",
    "\n",
    "- which is read as follows: \"Regress y on X\". \n",
    "\n",
    "`statsmodels` works in a similar way, so we need to generate an appropriate string to feed to the method when we wish to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Model DataFrame with all of the columns:\n",
    "dfm = df_dummies.copy()\n",
    "\n",
    "# The dependent variable:\n",
    "y_name = 'Loan_Size'\n",
    "# The independent variable\n",
    "# (let's first try all of the columns in the model DataFrame)\n",
    "X_names = [col for col in dfm.columns if col != y_name]\n",
    "\n",
    "# Build the OLS formula string \" y ~ X \"\n",
    "formula_str = y_name+\" ~ \"+\" + \".join(X_names);\n",
    "print('Formula:\\n\\t {}'.format(formula_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b08a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using the model dataframe\n",
    "model=ols(formula=formula_str, data=dfm)\n",
    "fitted = model.fit()\n",
    "\n",
    "# Output the fitted summary\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the OLS Regression Summary\n",
    "\n",
    "**Model Performance**\n",
    "\n",
    "|Measure           |Value             |\n",
    "|------------------|------------------|\n",
    "| Dep. Variable:   |        Loan_Size | \n",
    "| Model:           |              OLS | \n",
    "| Method:          |    Least Squares | \n",
    "| Date:            | Sat, 02 May 2020 |\n",
    "| Time:            |         13:21:01 |\n",
    "| No. Observations:|              471 |\n",
    "| Df Residuals:    |              430 |\n",
    "| Df Model:        |               40 |\n",
    "| Covariance Type: |        nonrobust |\n",
    "| R-squared:       |             0.777|\n",
    "| Adj. R-squared:  |             0.757|\n",
    "| F-statistic:     |             37.56|\n",
    "| Prob (F-statistic): |      1.71e-115|\n",
    "| Log-Likelihood:  |           -1387.0|\n",
    "| AIC:             |             2856.|\n",
    "| BIC:             |             3026.|\n",
    "\n",
    "Dependent Variable: Loan_Size\n",
    "- The target variable being modeled, indicating the size of loans in this context.\n",
    "\n",
    "R-squared: 0.777\n",
    "- Meaning: 77.7% of the variation in Loan_Size is explained by the independent variables in the model.\n",
    "- Thresholds: Higher values (closer to 1) indicate better model fit. However, 77.7% is a strong fit for real-world data.\n",
    "- Stakeholder Message: The model is effective at explaining the variability in loan sizes based on the input variables.\n",
    "\n",
    "Adj. R-squared: 0.757\n",
    "- Meaning: 75.7% of the variation in Loan_Size is explained by the independent variables in the model. but adjusts for the number of predictors to avoid overfitting. \n",
    "- A slight drop from R-squared suggests that some variables may add limited value to the model.\n",
    "\n",
    "**Statistical Significance of the Model**\n",
    "\n",
    "F-statistic: 37.56\n",
    "- looks at Statistical Significance of the Model\n",
    "- Meaning: The F-test checks if at least one of the predictors is statistically significant.\n",
    "\n",
    "Prob (F-statistic): 1.71e-115 (extremely small, close to 0)\n",
    "- Stakeholder Message: The overall model is statistically significant, indicating that the predictors together effectively explain variations in loan size.\n",
    "\n",
    "__________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "**Coefficients and Their Interpretation**\n",
    "\n",
    "|                          |   coef   | std err    |      t     | P>t    | [0.025      0.975]|\n",
    "|--------------------------|----------|------------|------------|-----------|-------------------|\n",
    "|Intercept                 | 6.4496   |   2.696    |  2.392     | 0.017     |  1.150      11.749|\n",
    "|Age                       |-0.3140   |   0.194    | -1.620     | 0.106     | -0.695       0.067|\n",
    "|Experience                | 0.2226   |   0.195    |  1.142     | 0.254     | -0.160       0.605|\n",
    "|Income                    | 0.1777   |   0.008    | 23.319     | 0.000     |  0.163       0.193|\n",
    "|Family                    | 1.3289   |   0.219    |  6.060     | 0.000     |  0.898       1.760|\n",
    "|CCAvg                     | 1.4333   |   0.114    | 12.521     | 0.000     |  1.208       1.658|\n",
    "|Mortgage                  |-0.0370   |   0.001    |-24.962     | 0.000     | -0.040      -0.034|\n",
    "|Securities_Account        | 1.5816   |   0.798    |  1.982     | 0.048     |  0.013       3.150|\n",
    "|CD_Account                |-0.6828   |   0.634    | -1.078     | 0.282     | -1.928       0.563|\n",
    "|Online                    | 0.1235   |   0.513    |  0.241     | 0.810     | -0.886       1.133|\n",
    "|Education_Postgrad        | 2.3492   |   0.941    |  2.496     | 0.013     |  0.499       4.199|\n",
    "|Education_Professional    | 1.9695   |   0.968    |  2.034     | 0.043     |  0.066       3.873|\n",
    "|Education_Undergrad       | 2.1309   |   0.988    |  2.156     | 0.032     |  0.188       4.074|\n",
    "|Gender_Female             | 3.6759   |   1.383    |  2.658     | 0.008     |  0.958       6.394|\n",
    "|Gender_Male               | 2.7737   |   1.352    |  2.052     | 0.041     |  0.117       5.431|\n",
    "|Area_Alameda              |-0.0350   |   0.854    | -0.041     | 0.967     | -1.714       1.644|\n",
    "|Area_Butte                |-2.9267   |   3.371    | -0.868     | 0.386     | -9.553       3.700|\n",
    "|Area_Contra_Costa         |-0.1349   |   1.435    | -0.094     | 0.925     | -2.956       2.686|\n",
    "|Area_Fresno               | 2.0428   |   3.397    |  0.601     | 0.548     | -4.634       8.719|\n",
    "|Area_Humboldt             | 0.0294   |   3.371    |  0.009     | 0.993     | -6.596       6.655|\n",
    "|Area_Kern                 | 1.1313   |   1.830    |  0.618     | 0.537     | -2.465       4.727|\n",
    "|Area_Los_Angeles          |-0.2556   |   0.653    | -0.392     | 0.696     | -1.538       1.027|\n",
    "|Area_Marin                | 0.2734   |   1.969    |  0.139     | 0.890     | -3.596       4.143|\n",
    "|Area_Mendocino            | 4.0507   |   4.756    |  0.852     | 0.395     | -5.297      13.398|\n",
    "|Area_Monterey             |-2.3811   |   1.289    | -1.847     | 0.065     | -4.914       0.152|\n",
    "|Area_Orange               | 0.5804   |   1.005    |  0.578     | 0.564     | -1.395       2.556|\n",
    "|Area_Placer               |-0.1183   |   3.351    | -0.035     | 0.972     | -6.706       6.469|\n",
    "|Area_Riverside            |-0.4246   |   1.991    | -0.213     | 0.831     | -4.339       3.489|\n",
    "|Area_Sacramento           | 0.9005   |   1.310    |  0.687     | 0.492     | -1.675       3.476|\n",
    "|Area_San_Bernardino       | 2.3827   |   2.770    |  0.860     | 0.390     | -3.062       7.827|\n",
    "|Area_San_Diego            | 0.4737   |   0.767    |  0.618     | 0.537     | -1.034       1.981|\n",
    "|Area_San_Francisco        |-1.4785   |   1.173    | -1.260     | 0.208     | -3.785       0.828|\n",
    "|Area_San_Joaquin          | 1.1931   |   4.742    |  0.252     | 0.801     | -8.128      10.514|\n",
    "|Area_San_Luis_Obispo      |-0.2345   |   2.408    | -0.097     | 0.922     | -4.968       4.499|\n",
    "|Area_San_Mateo            | 0.6569   |   1.559    |  0.421     | 0.674     | -2.408       3.722|\n",
    "|Area_Santa_Barbara        |-0.0998   |   1.498    | -0.067     | 0.947     |  -3.044       2.845|\n",
    "|Area_Santa_Clara          | 0.1681   |   0.729    |  0.231     | 0.818     |  -1.265       1.601|\n",
    "|Area_Santa_Cruz           |-0.3529   |   1.827    | -0.193     | 0.847     |  -3.944       3.238|\n",
    "|Area_Shasta               |-0.6051   |   2.779    | -0.218     | 0.828     |  -6.068       4.858|\n",
    "|Area_Solano               |-2.0356   |   2.749    | -0.740     | 0.459     |  -7.440       3.368|\n",
    "|Area_Sonoma               | 0.4197   |   1.987    |  0.211     | 0.833     |  -3.485       4.325|\n",
    "|Area_Stanislaus           |-0.9779   |   4.726    | -0.207     | 0.836     | -10.268       8.312|\n",
    "|Area_Ventura              | 1.7134   |   1.487    |   1.152    |  0.250    |   -1.210       4.636|\n",
    "|Area_Yolo                 | 2.4941   |   1.719    |  1.451     | 0.148     |  -0.885       5.873|\n",
    "\n",
    "\n",
    "The coef values represent the average change in Loan_Size for a one-unit change in each predictor, holding other variables constant.\n",
    "\n",
    "Significant Predictors:\n",
    "\n",
    "Income (coef = 0.1777, p < 0.001):\n",
    "- A one-unit increase in income is associated with an increase of 0.1777 in loan size, on average.\n",
    "- Stakeholder Message: Higher income levels significantly increase loan size, suggesting income is a major determinant of loan allocation.\n",
    "\n",
    "Family (coef = 1.3289, p < 0.001):\n",
    "- Loan size increases by 1.33 units for each additional family member.\n",
    "- Stakeholder Message: Family size positively influences loan size, which may reflect financial responsibilities influencing loan demand.\n",
    "\n",
    "CCAvg (coef = 1.4333, p < 0.001):\n",
    "- Average monthly credit card spending significantly increases loan size.\n",
    "- Stakeholder Message: High credit card spending is a strong indicator of higher loan eligibility or need.\n",
    "\n",
    "Mortgage (coef = -0.0370, p < 0.001):\n",
    "- A negative coefficient implies that larger mortgages slightly reduce loan size.\n",
    "- Stakeholder Message: Customers with higher mortgage liabilities may receive lower loans, possibly reflecting risk concerns.\n",
    "\n",
    "Non-Significant Predictors:\n",
    "\n",
    "Age (p = 0.106), Experience (p = 0.254), Online (p = 0.810), many Area variables:\n",
    "- These variables do not have a statistically significant relationship with loan size as p > 0.05.\n",
    "- Stakeholder Message: These factors might be excluded in future models unless they align with business insights or strategies.\n",
    "\n",
    "Categorical Predictors:\n",
    "\n",
    "Education:\n",
    "- Postgraduates, professionals, and undergraduates receive larger loans compared to the reference category (likely \"No Education\").\n",
    "- Stakeholder Message: Educational qualifications influence loan size, aligning with the idea that higher education may imply better creditworthiness.\n",
    "\n",
    "Gender:\n",
    "- Women receive loans that are on average larger by 3.68 units compared to men.\n",
    "- Stakeholder Message: Gender differences in loan sizes could reflect underlying demographic or financial patterns.\n",
    "\n",
    "___________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "**Diagnostic Measures**\n",
    "| Measure                  |  Value      |\n",
    "|--------------------------|-------------|\n",
    "|Omnibus:                  |  17.650 |\n",
    "|Durbin-Watson:            |     2.004  |\n",
    "|Prob(Omnibus):            |    0.000   |\n",
    "|Jarque-Bera (JB):         |    19.137|\n",
    "|Skew:                     |   -0.431   |\n",
    "|Prob(JB):                 |  6.99e-05|\n",
    "|Kurtosis:                 |    3.482 |\n",
    "|Cond. No.                 |  7.37e+16|\n",
    "\n",
    "Omnibus and Jarque-Bera Tests (p < 0.001):\n",
    "- Indicate that residuals (errors) may not be perfectly normally distributed.\n",
    "- Stakeholder Message: While the model is strong, residual non-normality could be further investigated to refine the model.\n",
    "\n",
    "Durbin-Watson Statistic (2.004):\n",
    "- A value close to 2 suggests no significant autocorrelation in residuals, meaning errors are independent.\n",
    "- Stakeholder Message: The model meets the independence of errors assumption.\n",
    "\n",
    "Condition Number (7.37e+16):\n",
    "- High values suggest multicollinearity issues (predictors are highly correlated).\n",
    "- Stakeholder Message: Some predictors may overlap in their explanatory power. This could be addressed through techniques like variable selection or regularization (e.g., Ridge/Lasso regression).\n",
    "\n",
    "Warnings:\n",
    "\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "[2] The smallest eigenvalue is 4.16e-27. This might indicate that there are\n",
    "strong multicollinearity problems or that the design matrix is singular.\n",
    "- likely as a result of the incorrect filtering of one hot encoded dummy variables\n",
    "- to ensure that we don't assume an underlying relationship between the categories\n",
    "    - call `pd.get_dummies` with the argument `drop_first=True` so that we only create n-1 columns for each variable with n categories\n",
    "        - (i.e. one variable/column with five categories will be transformed into four columns of 0's and 1's)\n",
    "\n",
    "_______________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "**Actionable Insights for Stakeholders**\n",
    "- Focus efforts on variables like Income, CCAvg, and Family, which are key drivers of loan sizes.\n",
    "- Investigate non-significant variables for possible removal to simplify the model and enhance interpretability.\n",
    "- Address potential multicollinearity by refining the input variables.\n",
    "- Consider segmentation by Education and Gender to tailor loan products effectively.\n",
    "- Reassess area-specific variables since many are non-significant; geographical targeting may not substantially impact loan size decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a7c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Again make sure that all the column names have underscores instead of whitespaces\n",
    "df_dummies.columns = [col.replace(\" \", \"_\") for col in df_dummies.columns]\n",
    "\n",
    "# Reorder columns with the dependent variable (claim_amount) the last column\n",
    "column_titles = [col for col in df_dummies.columns if col !=\n",
    "                 'Loan_Size'] + ['Loan_Size']\n",
    "df_dummies = df_dummies.reindex(columns=column_titles)\n",
    "\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545d4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll keep the model DataFrame, but only specify the columns we want to fit this time\n",
    "X_names = [col for col in df_dummies.columns if col != y_name]\n",
    "\n",
    "# Build the OLS formula string \" y ~ X \"\n",
    "formula_str = y_name+' ~ '+'+'.join(X_names)\n",
    "\n",
    "# Fit the model using the model dataframe\n",
    "model = ols(formula=formula_str, data=dfm)\n",
    "fitted = model.fit()\n",
    "\n",
    "# Output the fitted summary\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3b0ae2",
   "metadata": {},
   "source": [
    "## Making further selections on the variables now using their significance.\n",
    "\n",
    "### Variable Selection by Correlation and Significance\n",
    "\n",
    "We need to choose the best ones to be our predictors. \n",
    "\n",
    "One way is to \n",
    "- look at the correlations between the `Loan Size` and each variables in our DataFrame\n",
    "    - and select those with the strongest correlations (both positive and negative).\n",
    "- consider how significant those features are. \n",
    "\n",
    "Create a new DataFrame and store the correlation coefficents and p-values in that DataFrame for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae123c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations between predictor variables and the response variable\n",
    "corrs = df_dummies.corr()['Loan_Size'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece4d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Build a dictionary of correlation coefficients and p-values\n",
    "dict_cp = {}\n",
    "\n",
    "column_titles = [col for col in corrs.index if col!= 'Loan_Size']\n",
    "for col in column_titles:\n",
    "    p_val = round(pearsonr(df_dummies[col], df_dummies['Loan_Size'])[1],6)\n",
    "    dict_cp[col] = {'Correlation_Coefficient':corrs[col],\n",
    "                    'P_Value':p_val}\n",
    "    \n",
    "df_cp = pd.DataFrame(dict_cp).T\n",
    "df_cp_sorted = df_cp.sort_values('P_Value')\n",
    "df_cp_sorted[df_cp_sorted['P_Value']<0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda6f97a",
   "metadata": {},
   "source": [
    "Get a sorted list of the p-values and correlation coefficients for each of the features, when considered on their own.  \n",
    "\n",
    "If we were to use a logic test with a significance value of 5% (p-value < 0.05), \n",
    "- we could infer that the following features are statistically significant:\n",
    "    - List features\n",
    "\n",
    "Keep only the variables that have a significant correlation with the dependent variable. \n",
    "- Put them into an independent variable DataFrame `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ead79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dependent variable remains the same:\n",
    "y_data = df_dummies[y_name]  # y_name = 'Loan_Size'\n",
    "\n",
    "# Model building - Independent Variable (IV) DataFrame\n",
    "X_names = list(df_cp[df_cp['P_Value'] < 0.05].index)\n",
    "X_data = df_dummies[X_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63eb840",
   "metadata": {},
   "source": [
    "Also, look for predictor variable pairs which have a high correlation with each other to avoid autocorrelation.\n",
    "\n",
    "Easier to isolate the sections of the correlation matrix to where the off-diagonal correlations are high:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the correlation matrix\n",
    "corr = X_data.corr()\n",
    "\n",
    "# Find rows and columnd where correlation coefficients > 0.9 or <-0.9\n",
    "corr[np.abs(corr) > 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9701436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, we create the correlation matrix\n",
    "# and find rows and columnd where correlation coefficients > 0.9 or <-0.9\n",
    "corr = X_data.corr()\n",
    "r, c = np.where(np.abs(corr) > 0.9)\n",
    "\n",
    "# We are only interested in the off diagonal entries:\n",
    "off_diagonal = np.where(r != c)\n",
    "\n",
    "# Show the correlation matrix rows and columns where we have highly correlated off diagonal entries:\n",
    "corr.iloc[r[off_diagonal], c[off_diagonal]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603091c5",
   "metadata": {},
   "source": [
    "##### Resulting OLS fit summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf352120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a new subset of our potential independent variables\n",
    "X_remove = ['Age']\n",
    "X_corr_names = [col for col in X_names if col not in X_remove]\n",
    "\n",
    "# Create our new OLS formula based-upon our smaller subset\n",
    "formula_str = y_name+' ~ '+' + '.join(X_corr_names);\n",
    "print('Formula:\\n\\t{}'.format(formula_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f838918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the OLS model using the model dataframe\n",
    "model=ols(formula=formula_str, data=dfm)\n",
    "fitted = model.fit()\n",
    "\n",
    "# Display the fitted summary\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99313db",
   "metadata": {},
   "source": [
    "### Variable Selection by Variance Thresholds\n",
    "\n",
    "Variance Thresholds remove features whose values don't change much from observation to observation. \n",
    "\n",
    "The objective here is to remove all features that have a variance lower than the selected threshold.\n",
    "- Suppose that in our loans dataset 97% of observations were for 40-year-old women, then the *Age* and *Gender* features can be removed without a great loss in information.\n",
    "\n",
    "It is important to note that variance is dependent on scale, so the features will have to be normalized before implementing variance thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into independent (X) and independent (y) variables\n",
    "X_names = list(df_dummies.columns)\n",
    "X_names.remove(y_name)\n",
    "X_data = df_dummies[X_names]\n",
    "y_data = df_dummies[y_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1816ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "X_normalize = pd.DataFrame(X_scaled, columns=X_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f558fa07",
   "metadata": {},
   "source": [
    "#### Variance Threshold in Scikit Learn\n",
    "\n",
    "To implement Variance Threshold in Scikit Learn we have to do the following:\n",
    "\n",
    "Import and create an instance of the VarianceThreshold class;\n",
    "- Use the .fit() method to select subset of features based on the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create VarianceThreshold object\n",
    "selector = VarianceThreshold(threshold=0.03)\n",
    "\n",
    "# Use the object to apply the threshold on data\n",
    "selector.fit(X_normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100777d",
   "metadata": {},
   "source": [
    "##### Calculated variance for each predictive variable.\n",
    "\n",
    "Show the variances of the individual columns before any threshold is applied. \n",
    "\n",
    "It allows us to revise our initial variance threshold if we feel that we might exclude important variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f71b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column variances\n",
    "column_variances = selector.variances_\n",
    "\n",
    "vars_dict = {}\n",
    "vars_dict = [{\"Variable_Name\": c_name, \"Variance\": c_var}\n",
    "             for c_name, c_var in zip(X_normalize.columns, column_variances)]\n",
    "df_vars = pd.DataFrame(vars_dict)\n",
    "df_vars.sort_values(by='Variance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1318dc81",
   "metadata": {},
   "source": [
    "#### Extract the results and use them to select our new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select new columns\n",
    "X_new = X_normalize[X_normalize.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "# Save variable names for later\n",
    "X_var_names = X_new.columns\n",
    "\n",
    "# View first few entries\n",
    "X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Variance Threshold objects\n",
    "selector_1 = VarianceThreshold(threshold=0.05)\n",
    "selector_2 = VarianceThreshold(threshold=0.1)\n",
    "selector_3 = VarianceThreshold(threshold=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ee00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_1.fit(X_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2fad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_2.fit(X_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae070ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_3.fit(X_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select subset of columns\n",
    "X_1 = X_normalize[X_normalize.columns[selector_1.get_support(indices=True)]]\n",
    "X_2 = X_normalize[X_normalize.columns[selector_2.get_support(indices=True)]]\n",
    "X_3 = X_normalize[X_normalize.columns[selector_3.get_support(indices=True)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49658433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axes\n",
    "f, ax = plt.subplots(figsize=(8, 3), nrows=1, ncols=1)\n",
    "\n",
    "# Create list of titles and predictions to use in for loop\n",
    "subset_preds = [X_1.shape[1], X_2.shape[1], X_3.shape[1]]\n",
    "thresholds = ['0.05', '0.1', '0.15']\n",
    "\n",
    "# Plot graph\n",
    "ax.set_title('# of Predictors vs Thresholds')\n",
    "ax.set_ylabel('# of Predictors')\n",
    "ax.set_xlabel('Threshold')\n",
    "sns.barplot(x=thresholds, y=subset_preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e9bf69",
   "metadata": {},
   "source": [
    "\n",
    "##### Extract the predictor names of the 3 different datasets above?\n",
    "\n",
    "Results OLS fit summary for a threshold of 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is our new OLS formula?\n",
    "formula_str = y_name+' ~ '+' + '.join(X_new.columns)\n",
    "print('Formula:\\n\\t{}'.format(formula_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d94f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using the model dataframe\n",
    "model = ols(formula=formula_str, data=df_dummies)\n",
    "fitted = model.fit()\n",
    "\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b8e7e5",
   "metadata": {},
   "source": [
    "#### Advantages & Disadvantages of Variance Thresholds\n",
    "\n",
    "Let's consider some trade-offs associated with using variance thresholds for variable selection: \n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* Applying variance thresholds is based on solid intuition: features that don't change much also don't add much information;\n",
    "* Easy and relatively safe way to reduce dimensionality (i.e. number of features) at the start of the modeling process.\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "* Not the ideal algorithm if dimensionality reduction is not really required;\n",
    "* The threshold must be manually tuned, which can be a fickle process requiring domain/problem expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45d0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68895683",
   "metadata": {},
   "source": [
    "Preprocess the data\n",
    "\n",
    "make sure that all models are trained and tested on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6796317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data,\n",
    "                                                    y_data,\n",
    "                                                    test_size=0.20,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9919e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and testing data for variance threshold model\n",
    "X_var_train = X_train[X_var_names]\n",
    "X_var_test = X_test[X_var_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a036d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and testing data for correlation threshold model\n",
    "X_corr_train = X_train[X_corr_names]\n",
    "X_corr_test = X_test[X_corr_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44630c3b",
   "metadata": {},
   "source": [
    "##### Fit models\n",
    "\n",
    "instantiate and fit our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a3eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm_corr = LinearRegression()\n",
    "lm_var = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff51e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(X_train, y_train);\n",
    "lm_corr.fit(X_corr_train,y_train);\n",
    "lm_var.fit(X_var_train,y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ea6ba",
   "metadata": {},
   "source": [
    "##### Assess model accuracy \n",
    "Let's see how our linear models performed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb5467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create figure and axes\n",
    "f, ax = plt.subplots(figsize=(15, 5), nrows=1, ncols=3, sharey=True)\n",
    "\n",
    "# Create list of titles and predictions to use in for loop\n",
    "train_pred = [lm.predict(X_train),\n",
    "              lm_corr.predict(X_corr_train),\n",
    "              lm_var.predict(X_var_train)]\n",
    "test_pred = [lm.predict(X_test),\n",
    "             lm_corr.predict(X_corr_test),\n",
    "             lm_var.predict(X_var_test)]\n",
    "title = ['No threshold', 'Corr threshold', 'Var threshold']\n",
    "\n",
    "# Key:\n",
    "# No threshold - linear regression with all predictive variables\n",
    "# Corr threshold - linear regression with correlation thresholded predictive variables\n",
    "# Var threshold - linear regression with variance thresholded predictive variables\n",
    "\n",
    "\n",
    "# Loop through all axes to plot each model's results\n",
    "for i in range(3):\n",
    "    test_mse = round(mean_squared_error(test_pred[i], y_test), 4)\n",
    "    test_r2 = round(r2_score(test_pred[i], y_test), 4)\n",
    "    train_mse = round(mean_squared_error(train_pred[i], y_train), 4)\n",
    "    train_r2 = round(r2_score(train_pred[i], y_train), 4)\n",
    "    title_str = f\"Linear Regression({title[i]}) \\n train MSE = {train_mse} \\n \" + \\\n",
    "                f\"test MSE = {test_mse} \\n training $R^{2}$ = {train_r2} \\n \" + \\\n",
    "                f\"test $R^{2}$ = {test_r2}\"\n",
    "    ax[i].set_title(title_str)\n",
    "    ax[i].set_xlabel('Actual')\n",
    "    ax[i].set_ylabel('Predicted')\n",
    "    ax[i].plot(y_test, y_test, 'r')\n",
    "    ax[i].scatter(y_test, test_pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d73324",
   "metadata": {},
   "source": [
    "### Regularisation Preprocessing: Scaling Data for Regularisation\n",
    "\n",
    "Scaling data is a critical to regularisation as the penalty on particular coefficients in regularisation techniques namely L1 and L2, depends largely on the scale associated with the variables. \n",
    "\n",
    "Regularisation puts constraints on the size of the coefficients related to each variable.\n",
    "- Rescaling is very important for methods with regularisation because the size of the variables affects how much regularisation will be applied to that specific variable. \n",
    "- To make it fair, we need to get all the features on the same scale. \n",
    "\n",
    "There are two common scaling techniques: \n",
    "\n",
    "#### Normalisation\n",
    "\n",
    "One way to do this is with $[0,1]$-normalisation: \n",
    "- Squeezing your data into the range $[0,1]$. \n",
    "\n",
    "Through normalisation, \n",
    "- the maximum value of a variable becomes one, \n",
    "- the minimum becomes zero, and \n",
    "- the values in-between become decimals between zero and one.\n",
    "\n",
    "We implement this transformation by applying the following operation to each of the values of a predictor variable:\n",
    "\n",
    "$$\\hat{x}_{ij} = \\frac{x_{ij}-min(x_j)}{max(x_j)-min(x_j)},$$\n",
    "\n",
    "where \n",
    "- $\\hat{x}_{ij}$ is the value after normalisation, \n",
    "- $x_{ij}$ is the $i^{th}$ item of $x_j$, \n",
    "- and $min()$, $max()$ return the smallest and largest values of variable $x_j$ respectively. \n",
    "\n",
    "Normalisation is useful because it ensures all variables share the same range: $[0,1]$. \n",
    "\n",
    "Problem with normalisation,\n",
    "- drawback: if there are outliers, the bulk of your data will all lie in a small range, so you would lose information.\n",
    "\n",
    "#### Standardisation\n",
    "\n",
    "Z-score standardisation, or simply standardisation,\n",
    "- does not suffer from this drawback as it handles outliers gracefully. \n",
    "\n",
    "We implement Z-score standardisation by applying the following operation to each of our variables: \n",
    "\n",
    "$$\\hat{x}_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}.$$\n",
    "\n",
    "where, \n",
    "- $\\mu_j$ represents the mean of variable $x_j$, \n",
    "- while $\\sigma_j$ is the variable's standard deviation. As can be \n",
    "- seen from the above formula, instead of dividing by the full range of our variable, we instead divide by a more distribution-aware measure in the standard deviation. \n",
    "- While this doesn't completely remove the effects of outliers, it does consider them in a more conservative manner. \n",
    "\n",
    "As a trade-off to using this transformation, our variable is no longer contained within the $[0,1]$ range as it was during normalisation\n",
    "- it can now take on a range which includes negative values\n",
    "- This means that all our variables won't be bound to the exact same range \n",
    "    - they can have slightly different influence levels on the learnt regression coefficients during regularisation\n",
    "    - but they are far closer to one another then they were before the use of standardisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9444b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d76bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/regression_sprint/regression_sprint_data_2.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c223876",
   "metadata": {},
   "source": [
    "Using monthly data for the Rand/Dollar exchange rate, as well as a few potential predictor variables. \n",
    "\n",
    "The goal is to try and model the exchange rate, using the other 19 variables.   \n",
    "\n",
    "The way we write this is as follows:   \n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$   \n",
    "\n",
    "- $Y$ is the reponse variable which depends on the _p_ predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into predictors and response\n",
    "X = df.drop('ZAR/USD', axis=1)\n",
    "y = df['ZAR/USD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4546458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scaler method from sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# create scaled version of the predictors (there is no need to scale the response)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# convert the scaled predictor values into a dataframe\n",
    "X_standardise = pd.DataFrame(X_scaled,columns=X.columns)\n",
    "X_standardise.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d74620",
   "metadata": {},
   "source": [
    "Taking a look at one of the variables as an example (Value of Exports (USD)), we can see that standarizing the data has caused it to be centered around zero.\n",
    "\n",
    "The variance within each variable in the data is now equal to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X_standardise['Value of Exports (USD)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_standardise.describe().loc['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ceee9",
   "metadata": {},
   "source": [
    "### 3. Regularisation Methods: Ridge Regression\n",
    "\n",
    "Understand what regularisation is and how to implement it using the ridge method\n",
    "\n",
    "Linear regression is a popular choice, but it often faces the challenge of overfitting, especially with a high number of parameters. \n",
    "\n",
    "This is where ridge and lasso regression comes in, offering practical solutions to enhance model accuracy and make informed decisions in data analysis. \n",
    "\n",
    "Regularization techniques are used to address overfitting and enhance model generalizability. \n",
    "- Ridge and lasso regression are effective methods in machine learning, that introduce penalties on the magnitude of regression coefficients. \n",
    "    - They work by penalizing the magnitude of coefficients of features and minimizing the error between predicted and actual observations. These are called ‘regularization’ techniques.\n",
    "\n",
    "Ridge and Lasso regression, are powerful techniques generally used for creating parsimonious models in the presence of a ‘large’ number of features. \n",
    "- ‘Large’ can typically mean either of two things:\n",
    "    - Large enough to enhance the tendency of a model to overfit (as low as 10 variables might cause overfitting)\n",
    "    - Large enough to cause computational challenges. With modern systems, this situation might arise in the case of millions or billions of features.\n",
    "\n",
    "#### Shrinkage Methods\n",
    "\n",
    "Ridge regression, aims to modify and potentially improve the test-set performance of a least squares regression model by reducing the magnitude of some subset of the coefficients $\\hat{\\beta}$.\n",
    "- The ridge regression process of reducing the magnitude of those coefficients is a type of _shrinkage_ method - we are attempting to shrink the values of those less important coefficients.\n",
    "- In ridge regression, it is possible to shrink a coefficient's value towards zero, but never reaching exactly zero.\n",
    "\n",
    "#### Usage of Ridge Regression:\n",
    "- When we have the independent variables which are having high collinearity between them, general linear or polynomial regression will fail\n",
    "    - Solve problems, Ridge regression can be used.\n",
    "- If we have more parameters than the samples,\n",
    "    - Ridge regression helps to solve the problems.\n",
    "\n",
    "#### Limitation of Ridge Regression:\n",
    "\n",
    "Does not helps in Feature Selection: \n",
    "- It decreases the complexity of a model but does not reduce the number of independent variables since it never leads to a coefficient being zero rather only minimizes it. \n",
    "    - This technique is not good for feature selection.\n",
    "\n",
    "Model Interpretability: \n",
    "- It shrinks the coefficients for least important predictors, very close to zero but it will never make them exactly zero. \n",
    "- The final model will include all the independent variables, also known as predictors.\n",
    "\n",
    "### Ridge Regression\n",
    "\n",
    "Description\n",
    "- Ridge regression, also known as Tikhonov regularization, \n",
    "- is a technique that introduces a penalty term to the linear regression model to shrink the coefficient values.\n",
    "\n",
    "Penalty Type\n",
    "- Ridge regression utilizes an L2 penalty, \n",
    "    - which adds the sum of the squared coefficient values multiplied by a tuning parameter (lambda).\n",
    "\n",
    "Coefficient Impact\n",
    "- The L2 penalty in ridge regression discourages large coefficient values, pushing them towards zero but never exactly reaching zero. This shrinks the less important features’ impact.\n",
    "\n",
    "Feature Selection\n",
    "- Ridge regression retains all features in the model, reducing the impact of less important features by shrinking their coefficients.\n",
    "\n",
    "Use Case\n",
    "- Ridge regression is useful when the goal is to minimize the impact of less important features while keeping all variables in the model.\n",
    "\n",
    "Model Complexity\n",
    "- Ridge regression tends to favor a model with a higher number of parameters, as it shrinks less important coefficients but keeps them in the model.\n",
    "\n",
    "Interpretability\n",
    "- The results of ridge regression may be less interpretable due to the inclusion of all features, each with a reduced but non-zero coefficient.\n",
    "\n",
    "Sparsity\n",
    "- Ridge regression does not yield sparse models since all coefficients remain non-zero.\n",
    "\n",
    "Sensitivity\n",
    "- More robust and less sensitive to outliers compared to lasso regression.\n",
    "\n",
    "#### Regularisation: The theory behind regularisation.\n",
    "\n",
    "When performing variable selection, \n",
    "- manual variable selection is often performed to improve the predictive accuracy of a model.\n",
    "\n",
    "The process of variable selection is discrete in that we either keep a variable, or we throw it away.   \n",
    "\n",
    "**Regularisation** offers an alternative method in which all predictor variables are included, but are subject to constraint. \n",
    "\n",
    "Recall that the least squares method seeks to minimise the sum of the squares of the residuals:\n",
    "\n",
    "$$RSS = \\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$   \n",
    "\n",
    "which can be written in terms of the predictor variable coefficients, [$b_1, b_2, b_p$], and slope, $a$:   \n",
    "\n",
    "$$RSS = \\sum_{i=1}^n(y_i-(a+\\sum_{j=1}^pb_jx_{ij}))^2$$\n",
    "\n",
    "where \n",
    "- _n_ is the number of observations, and \n",
    "- _p_ is the number of predictor variables. \n",
    "\n",
    "In the case of **ridge regression**, the regression coefficients are calculated as the values that minimise:\n",
    "\n",
    "$$\\sum_{i=1}^n(y_i-(a+\\sum_{j=1}^pb_jx_{ij}))^2 + \\alpha\\sum_{j=1}^pb_j^2$$\n",
    "\n",
    "which is rewritten simply as:\n",
    "\n",
    "$$\\min_{\\beta} (RSS + \\alpha\\sum_{j=1}^pb_j^2)$$\n",
    "\n",
    "Objective = RSS + α * (sum of the square of coefficients)\n",
    "\n",
    "In minimising _RSS_ , we improve the overall fit of the model. \n",
    "\n",
    "Ridge regression performs ‘L2 regularization‘, i.e., it adds a factor of the sum of squares of coefficients in the optimization objective.\n",
    "\n",
    "In the newly introduced term, $\\alpha\\sum_{j=1}^pb_j^2$, \n",
    "- the intention is to penalise those individual coefficients that get too large (those that contribute the most to reducing the fit).\n",
    "- $\\alpha$ is a tuning parameter (which we calculate later on), which controls the degree to which the regression coefficients are penalised. \n",
    "    - The effect of this penalty parameter is to create a tradeoff between how much a coefficient contributes to minimising RSS and the size of the coefficient. \n",
    "    - In other words: _training fit_ vs. _size of coefficients_. \n",
    "- $\\alpha$, we can see that the penalty parameter is applied to the sum of the squares of the coefficients. \n",
    "- This means that as we increase the size of the coefficients, the penalty will increase too. \n",
    "- This has the effect of _shrinking_ the coefficients towards zero.\n",
    "\n",
    "$\\alpha$(alpha) is the parameter that balances the amount of emphasis given to minimizing RSS vs minimizing the sum of squares of coefficients. α can take various values:\n",
    "\n",
    "$\\alpha$ = 0:\n",
    "- The objective becomes the same as simple linear regression.\n",
    "    - We’ll get the same coefficients as simple linear regression.\n",
    "\n",
    "$\\alpha$ = ∞:\n",
    "- The coefficients will be zero. Why? \n",
    "    - Because of infinite weightage on the square of coefficients, anything less than zero will make the objective infinite.\n",
    "\n",
    "0 < $\\alpha$ < ∞:\n",
    "- The magnitude of $\\alpha$ will decide the weightage given to different parts of the objective.\n",
    "- The coefficients will be somewhere between 0 and ones for simple linear regression.\n",
    "\n",
    "non-zero value would give values less than that of simple linear regression.\n",
    "\n",
    "##### Example\n",
    "\n",
    "Dataset which contains monthly data for the Rand/Dollar exchange rate, as well as a few potential predictor variables.\n",
    "- the goal is to try and model the exchange rate, using the other 19 variables.\n",
    "\n",
    "The way we write this is as follows:   \n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$   \n",
    "\n",
    "where\n",
    "- $Y$ is the reponse variable which depends on the _p_ predictor variables.\n",
    "\n",
    "##### Review of Data Scaling\n",
    "\n",
    "Data scaling is essential in regularisation as regularising penalizes a model for large coefficients. \n",
    "\n",
    "The magnitude of coefficients is dependent on the following:\n",
    "\n",
    "* The strength of the relationship between the predictor variables (`x`) and the output variable (`y`)\n",
    "* The units of measurement of x(eg. distance measured in millimetres or metres).\n",
    "\n",
    "For example, if x is measured in metres, and its coefficient is 5; if it is expressed in kilometres, its coefficient will be 5*10³.\n",
    "\n",
    "We want regularisation to be impacted by the strength of the relationship that exists between `x` and `y` variables and not the magnitude of the coefficients.\n",
    "- Thus, to eliminate the impact of the units of measurement of the variables on the coefficients, \n",
    "- Performed data scaling to ensure variables are fairly scaled. \n",
    "\n",
    "**Z-score standardisation** is a great way to scale variables such that they have similar (though not identical) ranges, in a way that is fairly robust to outlier values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced8c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into predictors and response\n",
    "X = df.drop('ZAR/USD', axis=1)\n",
    "y = df['ZAR/USD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scaler method from sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create scaled version of the predictors (there is no need to scale the response)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert the scaled predictor values into a dataframe\n",
    "X_standardise = pd.DataFrame(X_scaled,columns=X.columns)\n",
    "X_standardise.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da65ea",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "Split our data into a training and a testing set.\n",
    "- Use the first eight years of data as our training set and \n",
    "- test the model on the final two years. \n",
    "\n",
    "Note that with time-series data it isn't appropriate to sample rows randomly for the training and testing sets because **chronological order** remains important.\n",
    "\n",
    "Fit and test our model.\n",
    "- Create a `Ridge()` object without modifying any of the parameters. \n",
    "    - This means that we will use the default value of $\\alpha=1$. \n",
    "    \n",
    "We'll learn about choosing a better value for this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952cc64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train/test splitting function from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test, being sure to use the standardised predictors\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standardise, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b4fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ridge regression module from sklearn\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad28d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ridge model\n",
    "ridge = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b91239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed0de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the model intercept value\n",
    "b0 = float(ridge.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b899da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the model coefficient value\n",
    "coeff = pd.DataFrame(ridge.coef_, X.columns, columns=['Coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae779ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intercept:\", float(b0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d06d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the coefficients\n",
    "coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c27c4",
   "metadata": {},
   "source": [
    "##### Interpretation of the intercept and coefficients\n",
    "\n",
    "Sincee standardised the features,\n",
    "- compare coefficients to each other,\n",
    "- respective variables are all on the same scale.\n",
    "- interpret the intercepts as the expected exchange rate when all the features are equal to their respective means and the coefficients are interpreted as the expected change in exchange rate given an increase of 1 in the **scaled feature value**. \n",
    "\n",
    "We can intepret variables with smaller coefficients as less important as they have suffered more in the shrinkage tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89613be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a basic linear model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create model object\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Train model\n",
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a52c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metrics module\n",
    "from sklearn import metrics\n",
    "\n",
    "# Check training accuracy\n",
    "train_lm = lm.predict(X_train)\n",
    "train_ridge = ridge.predict(X_train)\n",
    "\n",
    "print('Training MSE')\n",
    "print('Linear:', metrics.mean_squared_error(y_train, train_lm))\n",
    "print('Ridge :', metrics.mean_squared_error(y_train, train_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lm = lm.predict(X_test)\n",
    "test_ridge = ridge.predict(X_test)\n",
    "\n",
    "print('Testing MSE')\n",
    "print('Linear:', metrics.mean_squared_error(y_test, test_lm))\n",
    "print('Ridge :', metrics.mean_squared_error(y_test, test_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b56f10b",
   "metadata": {},
   "source": [
    "Ridge regression achieves a much lower score on the testing set at the expense of a slightly higher score on the training set.\n",
    " \n",
    "The increase in training MSE is not anything to be worried about since we want to avoid overfitting on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76429ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to plot the train and test response variables as a continuous line\n",
    "train_plot = y_train.append(pd.Series(y_test[0], index=['2016M01']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be369e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(y)), ridge.predict(X_standardise), label='Predicted')\n",
    "plt.plot(np.arange(len(train_plot)), train_plot, label='Training')\n",
    "plt.plot(np.arange(len(y_test))+len(y_train), y_test, label='Testing')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc1bf09",
   "metadata": {},
   "source": [
    "#### Ridge regression in Sine / polynomial problem as below under GLMs\n",
    "\n",
    "Function for Ridge Regression\n",
    "\n",
    "It takes ‘alpha’ as a parameter on initialization.\n",
    "\n",
    "Remember that normalizing the inputs generally benefits every type of regression and should apply to ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734497e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def ridge_regression(data, predictors, alpha, models_to_plot={}):\n",
    "    #Fit the model\n",
    "    ridgereg = Ridge(alpha=alpha,normalize=True)\n",
    "    ridgereg.fit(data[predictors],data['y'])\n",
    "    y_pred = ridgereg.predict(data[predictors])\n",
    "    \n",
    "    #Check if a plot is to be made for the entered alpha\n",
    "    if alpha in models_to_plot:\n",
    "        plt.subplot(models_to_plot[alpha])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['x'],y_pred)\n",
    "        plt.plot(data['x'],data['y'],'.')\n",
    "        plt.title('Plot for alpha: %.3g'%alpha)\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    ret.extend([ridgereg.intercept_])\n",
    "    ret.extend(ridgereg.coef_)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14328f30",
   "metadata": {},
   "source": [
    "Analyze the result of Ridge regression for 10 different values of α ranging from 1e-15 to 20. \n",
    "\n",
    "These values have been chosen so that we can easily analyze the trend with changes in values of $\\alpha$.\n",
    "\n",
    "These 10 models will contain all the 15 variables, and only the value of alpha would differ. \n",
    "- This differs from the simple linear regression case, where each model had a subset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize predictors to be set of 15 powers of x\n",
    "predictors=['x']\n",
    "predictors.extend(['x_%d'%i for i in range(2,16)])\n",
    "\n",
    "#Set the different values of alpha to be tested\n",
    "alpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n",
    "\n",
    "#Initialize the dataframe for storing coefficients.\n",
    "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
    "ind = ['alpha_%.2g'%alpha_ridge[i] for i in range(0,10)]\n",
    "coef_matrix_ridge = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "models_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}\n",
    "for i in range(10):\n",
    "    coef_matrix_ridge.iloc[i,] = ridge_regression(data, predictors, alpha_ridge[i], models_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1387018e",
   "metadata": {},
   "source": [
    "Observation: \n",
    "- As the value of alpha increases, the model complexity reduces. \n",
    "    - Though higher values of alpha reduce overfitting, significantly high values can cause underfitting as well (e.g., alpha = 5). \n",
    "        - Thus alpha should be chosen wisely. \n",
    "- A widely accepted technique is **cross-validation**, i.e., the value of alpha is iterated over a range of values, and the one giving a higher cross-validation score is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d67f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the display format to be scientific for ease of analysis\n",
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_matrix_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f771d",
   "metadata": {},
   "source": [
    "inferences:\n",
    "\n",
    "- The RSS increases with an increase in alpha.\n",
    "- An alpha value as small as 1e-15 gives us a significant reduction in the magnitude of coefficients. \n",
    "    - How? Compare the coefficients in the first row of this table to the last row of the simple linear regression table.\n",
    "- High alpha values can lead to significant underfitting. Note the rapid increase in RSS for values of alpha greater than 1\n",
    "    - Though the coefficients are really small, they are NOT zero.\n",
    "\n",
    "Reconfirm the same by determining the number of zeros in each row of the coefficients data set:\n",
    "\n",
    "This should confirm that all 15 coefficients are greater than zero in magnitude (can be +ve or -ve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_matrix_ridge.apply(lambda x: sum(x.values==0),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94991f9f",
   "metadata": {},
   "source": [
    "### 3.1. Regularisation Methods: LASSO Regression\n",
    "\n",
    "- Understand the difference between L1 and L2 regularisation\n",
    "- Understand the concept of sparsity.\n",
    "\n",
    "#### Shrinkage Methods\n",
    "\n",
    "In ridge regression, we learned that it is possible to modify and potentially improve the test-set performance of a least squares regression model by reducing the magnitude of some subset of the coefficients $\\hat{\\beta}$.\n",
    "- The ridge regression process of reducing the magnitude of those coefficients is a type of _shrinkage_ method - we are attempting to shrink the values of those less important coefficients.\n",
    "- In ridge regression, it is possible to shrink a coefficient's value towards zero, but never reaching exactly zero.\n",
    "\n",
    "#### Sparsity\n",
    "\n",
    "L1 penalty has the eﬀect of forcing some of the coeﬃcient estimates to be exactly equal to zero which means there is a complete removal of some of the features for model evaluation when the tuning parameter λ is suﬃciently large.\n",
    "- Therefore, the lasso method also performs Feature selection and is said to yield sparse models.\n",
    "\n",
    "#### Limitation of Lasso Regression:\n",
    "\n",
    "Problem - types of Dataset: \n",
    "- If the number of predictors is greater than the number of data points, \n",
    "    - Lasso will pick at most n predictors as non-zero, even if all predictors are relevant.\n",
    "\n",
    "Multicollinearity Problem: \n",
    "- If there are two or more highly collinear variables then LASSO regression selects one of them randomly which is not good for the interpretation of our model.\n",
    "\n",
    "### LASSO Regression\n",
    "\n",
    "Description\n",
    "- Lasso regression, or Least Absolute Shrinkage and Selection Operator, \n",
    "- is a regularization method that also includes a penalty term but can set some coefficients exactly to zero, effectively selecting relevant features.\n",
    "\n",
    "Penalty Type\n",
    "- Lasso regression employs an L1 penalty, \n",
    "    - which sums the absolute values of the coefficients multiplied by lambda.\n",
    "\n",
    "Coefficient Impact\n",
    "- The L1 penalty in lasso regression can drive some coefficients to exactly zero when the lambda value is large enough, performing feature selection and resulting in a sparse model.\n",
    "\n",
    "Feature Selection\n",
    "- Lasso regression can set some coefficients to zero, effectively selecting the most relevant features and improving model interpretability.\n",
    "\n",
    "Use Case\n",
    "- Lasso regression is preferred when the goal is feature selection, resulting in a simpler and more interpretable model with fewer variables.\n",
    "\n",
    "Model Complexity\n",
    "- Lasso regression can lead to a less complex model by setting some coefficients to zero, reducing the number of effective parameters.\n",
    "\n",
    "Interpretability\n",
    "- Lasso regression can improve interpretability by selecting only the most relevant features, making the model’s predictions more explainable.\n",
    "\n",
    "Sparsity\n",
    "- Lasso regression can produce sparse models by setting some coefficients to exactly zero.\n",
    "\n",
    "Sensitivity\n",
    "- More sensitive to outliers due to the absolute value in the penalty term.\n",
    "\n",
    "### L1 (LASSO) vs. L2(Ridge) Regularization Techniques\n",
    "\n",
    "The key difference is in how they assign penalties to the coefficients:\n",
    "\n",
    "Ridge Regression:\n",
    "- Performs L2 regularization, i.e., adds penalty equivalent to the square of the magnitude of coefficients\n",
    "    - Minimization objective = LS Obj + α * (sum of square of coefficients)\n",
    "\n",
    "Lasso Regression:\n",
    "- Performs L1 regularization, i.e., adds penalty equivalent to the absolute value of the magnitude of coefficients\n",
    "    - Minimization objective = LS Obj + α * (sum of the absolute value of coefficients)\n",
    "\n",
    "LS Obj refers to the ‘least squares objective,’ i.e., the linear regression objective without regularization.\n",
    "\n",
    "#### Key Differences between Ridge and Lasso Regression\n",
    "- Ridge regression helps us to reduce only the overfitting in the model while keeping all the features present in the model.\n",
    "    - It reduces the complexity of the model by shrinking the coefficients whereas Lasso regression helps in reducing the problem of overfitting in the model as well as automatic feature selection.\n",
    "- Lasso Regression tends to make coefficients to absolute zero whereas Ridge regression never sets the value of coefficient to absolute zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990f3cf9",
   "metadata": {},
   "source": [
    "Recall the optimisation expression for ridge regression:\n",
    "\n",
    "$$\\min_{\\beta} (RSS + \\alpha\\sum_{j=1}^pb_j^2)$$\n",
    "\n",
    "where we attempt to minimise the RSS and some penalty term. This can be rewritten:\n",
    "\n",
    "$$\\min_{\\beta} (RSS + \\alpha(L2\\_norm))$$\n",
    "\n",
    "where $L2\\_norm$ is the *sum of the squares of the coefficients*.\n",
    "\n",
    "In LASSO regularisation, \n",
    "- we replace the $L2\\_norm$ with what is known as the $L1\\_norm$: the *sum of the _absolute_ values of the coefficients*.\n",
    "\n",
    "This is a relatively recent adaptation of ridge regression which is capable of shrinking predictors to exactly zero - effectively removing them from the model entirely and creating what we call a sparse model (one which uses some subset of all of the available predictors).\n",
    "\n",
    "LASSO achieves both shrinkage and subset selection.\n",
    "\n",
    "A LASSO model is fit under the constraint of minimizing the following equation:\n",
    "\n",
    "$$\\sum_{i=1}^n(y_i-(a+\\sum_{j=1}^pb_jx_{ij}))^2 + \\alpha\\sum_{j=1}^p|b_j|$$\n",
    "\n",
    "which can be rewritten as follows:\n",
    "\n",
    "$$\\min_{\\beta} (RSS + \\alpha\\sum_{j=1}^p|b_j|)$$\n",
    "\n",
    "or,\n",
    "\n",
    "$$\\min_{\\beta} (RSS + \\alpha(L1\\_norm))$$\n",
    "\n",
    "Lasso regression performs L1 regularization, i.e., it adds a factor of the sum of the absolute value of coefficients in the optimization objective.\n",
    "\n",
    "Objective = RSS + $\\alpha$ * (sum of the absolute value of coefficients)\n",
    "\n",
    "$\\alpha$ (alpha) works similar to that of the ridge and provides a trade-off between balancing RSS and the magnitude of coefficients. \n",
    "\n",
    "Like that of the ridge, $\\alpha$ can take various values.\n",
    "\n",
    "- $\\alpha$ = 0: Same coefficients as simple linear regression\n",
    "- $\\alpha$ = ∞: All coefficients zero (same logic as before)\n",
    "- 0 < $\\alpha$ < ∞: coefficients between 0 and that of simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea71ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features from the response\n",
    "X = df.drop('ZAR/USD', axis=1)\n",
    "y = df['ZAR/USD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff9fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scaling module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create standardization object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Save standardized features into new variable\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f232b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train/test split module\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, \n",
    "                                                    y, \n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=1,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e0252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LASSO module\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Create LASSO model object, setting alpha to 0.01\n",
    "lasso = Lasso(alpha=0.01)\n",
    "\n",
    "# Train the LASSO model\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Extract intercept from model\n",
    "intercept = float(lasso.intercept_)\n",
    "\n",
    "# Extract coefficient from model\n",
    "coeff = pd.DataFrame(lasso.coef_, X.columns, columns=['Coefficient'])\n",
    "\n",
    "# Extract intercept\n",
    "print(\"Intercept:\", float(intercept))\n",
    "\n",
    "coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bed0d7",
   "metadata": {},
   "source": [
    "##### Interpretation of the intercept and coefficients\n",
    "\n",
    "We interpret the values of the intercept and coefficients the same way as before:\n",
    "\n",
    " - The intercept can be interpreted as the **expected exchange rate when all the features are equal to their means**.\n",
    " - Each coefficient is interpreted as the expected change in the response variable given an increase of 1 in the **scaled feature value**.\n",
    " \n",
    "See from the list of coefficients above that some of the coefficients have indeed been shrunk to exactly zero.\n",
    "\n",
    "##### Assessment of predictive accuracy\n",
    "fit the following models as well, in order to compare the LASSO results thoroughly:\n",
    "\n",
    "- A least squares model using all available predictors;\n",
    "- A least squares model using the predictors with non-zero coefficients from LASSO;\n",
    "- A ridge regression model using all available predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a basic linear model\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "X_subset = df.drop(['ZAR/USD',\n",
    "                   'Total Reserves excl Gold (USD)',\n",
    "                   'IMF Reserve Position (USD)',\n",
    "                   'Claims on Non-residents (USD)',\n",
    "                   'Central Bank Policy Rate',\n",
    "                   'Treasury Bill Rate',\n",
    "                   'Savings Rate',\n",
    "                   'Deposit Rate',\n",
    "                   'Lending Rate',\n",
    "                   'Government Bonds'], axis=1)\n",
    "\n",
    "X_subset_scaled = scaler.fit_transform(X_subset)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_subset, \n",
    "                                                        y, \n",
    "                                                        test_size=0.20, \n",
    "                                                        random_state=1,\n",
    "                                                        shuffle=False)\n",
    "\n",
    "# Least squares using non-zero variables from LASSO\n",
    "lm_subset = LinearRegression()\n",
    "\n",
    "# Least squares using all predictors\n",
    "lm_all = LinearRegression()\n",
    "\n",
    "# Ridge using all predictors\n",
    "ridge = Ridge()\n",
    "\n",
    "lm_subset.fit(X_train2, y_train2)\n",
    "lm_all.fit(X_train, y_train)\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bccf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Make training set predictions for each model\n",
    "train_lm_subset = lm_subset.predict(X_train2)\n",
    "train_lm_all = lm_all.predict(X_train)\n",
    "train_ridge = ridge.predict(X_train)\n",
    "train_lasso = lasso.predict(X_train)\n",
    "\n",
    "# Make test set predictions for each model\n",
    "test_lm_subset = lm_subset.predict(X_test2)\n",
    "test_lm_all = lm_all.predict(X_test)\n",
    "test_ridge = ridge.predict(X_test)\n",
    "test_lasso = lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f35d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of results\n",
    "results_dict = {'Training MSE':\n",
    "                    {\n",
    "                        \"Least Squares, Subset\": metrics.mean_squared_error(y_train2, train_lm_subset),\n",
    "                        \"Least Squares, All\": metrics.mean_squared_error(y_train, train_lm_all),\n",
    "                        \"Ridge\": metrics.mean_squared_error(y_train, train_ridge),\n",
    "                        \"LASSO\": metrics.mean_squared_error(y_train, train_lasso)\n",
    "                    },\n",
    "                    'Test MSE':\n",
    "                    {\n",
    "                        \"Least Squares, Subset\": metrics.mean_squared_error(y_test2, test_lm_subset),\n",
    "                        \"Least Squares, All\": metrics.mean_squared_error(y_test, test_lm_all),\n",
    "                        \"Ridge\": metrics.mean_squared_error(y_test, test_ridge),\n",
    "                        \"LASSO\": metrics.mean_squared_error(y_test, test_lasso)\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from dictionary\n",
    "results_df = pd.DataFrame(data=results_dict)\n",
    "\n",
    "# View the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f17d2",
   "metadata": {},
   "source": [
    "##### Result interpretation\n",
    "\n",
    "LASSO was able to perform subset selection, while also performing shrinkage. \n",
    "- The result is a more generalised model with greater predictive capacity. \n",
    "\n",
    "The least squares model which we trained on the same subset of variables that LASSO retained as non-zero scored a higher MSE on the test set, \n",
    "- indicating that the shrinkage that LASSO applied to those remaining variables was effective.\n",
    "\n",
    "LASSO achieved the best MSE on the test set, followed by ridge regression.\n",
    "\n",
    "##### Plot our results to end off.\n",
    "plot the the test set versus the three primary methods explored here:\n",
    "\n",
    "- Least squares using all predictors;\n",
    "- Ridge using all predictors;\n",
    "- LASSO using all predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c540e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### we want to plot the train and test response variables as a continuous line\n",
    "train_plot = y_train.append(pd.Series(y_test[0], index=['2016M01']))\n",
    "\n",
    "plt.plot(np.arange(96,120), lasso.predict(X_test), label='LASSO')\n",
    "plt.plot(np.arange(96,120), ridge.predict(X_test), label='Ridge')\n",
    "plt.plot(np.arange(96,120), lm_all.predict(X_test), label='Least Squares')\n",
    "plt.plot(np.arange(96,120), y_test, label='Testing')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ebe8a",
   "metadata": {},
   "source": [
    "#### Losso regression in Sine / polynomial problem as below under GLMs\n",
    "\n",
    "LASSO stands for Least Absolute Shrinkage and Selection Operator.\n",
    "\n",
    "2 keywords here – \n",
    "- absolute and\n",
    "- selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dd4f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "def lasso_regression(data, predictors, alpha, models_to_plot={}):\n",
    "    #Fit the model\n",
    "    lassoreg = Lasso(alpha=alpha,normalize=True, max_iter=1e5)\n",
    "    lassoreg.fit(data[predictors],data['y'])\n",
    "    y_pred = lassoreg.predict(data[predictors])\n",
    "    \n",
    "    #Check if a plot is to be made for the entered alpha\n",
    "    if alpha in models_to_plot:\n",
    "        plt.subplot(models_to_plot[alpha])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['x'],y_pred)\n",
    "        plt.plot(data['x'],data['y'],'.')\n",
    "        plt.title('Plot for alpha: %.3g'%alpha)\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    ret.extend([lassoreg.intercept_])\n",
    "    ret.extend(lassoreg.coef_)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6776e3b6",
   "metadata": {},
   "source": [
    "Additional parameters defined in the Lasso function – \n",
    "- max_iter.\n",
    "    - This is the maximum number of iterations for which we want the model to run if it doesn’t converge before. \n",
    "    - This exists for Ridge as well, but setting this to a higher than default value was required in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize predictors to all 15 powers of x\n",
    "predictors=['x']\n",
    "predictors.extend(['x_%d'%i for i in range(2,16)])\n",
    "\n",
    "#Define the alpha values to test\n",
    "alpha_lasso = [1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 1, 5, 10]\n",
    "\n",
    "#Initialize the dataframe to store coefficients\n",
    "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
    "ind = ['alpha_%.2g'%alpha_lasso[i] for i in range(0,10)]\n",
    "coef_matrix_lasso = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "#Define the models to plot\n",
    "models_to_plot = {1e-10:231, 1e-5:232,1e-4:233, 1e-3:234, 1e-2:235, 1:236}\n",
    "\n",
    "#Iterate over the 10 alpha values:\n",
    "for i in range(10):\n",
    "    coef_matrix_lasso.iloc[i,] = lasso_regression(data, predictors, alpha_lasso[i], models_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1eaa9",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "Model complexity decreases with an increase in the values of alpha. But notice the straight line at alpha=1.\n",
    "\n",
    "Expected inference: \n",
    "- higher RSS for higher alphas\n",
    "- For the same values of alpha, the coefficients of lasso regression are much smaller than that of ridge regression (compare row 1 of the 2 tables).\n",
    "- For the same alpha, lasso has higher RSS (poorer fit) as compared to ridge regression.\n",
    "- Many of the coefficients are zero, even for very small values of alpha.\n",
    "\n",
    "Check the number of coefficients that are zero in each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_matrix_lasso.apply(lambda x: sum(x.values==0),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09446f8",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- small value of alpha, a significant number of coefficients are zero. \n",
    "- This also explains the horizontal line fit for alpha=1 in the lasso plots; it’s just a baseline model! \n",
    "This phenomenon, where most coefficients become zero, is called **sparsity**. \n",
    "- Although lasso performs feature selection, we achieve this level of sparsity only in special cases\n",
    "\n",
    "#### Mathematics behind why coefficients are zero in the case of lasso but not ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LINEAR, RIDGE AND LASSO REGRESSION\n",
    "'''\n",
    "# importing requuired libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "# read test and train file\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "print('\\n\\n---------DATA---------------\\n\\n')\n",
    "print(train.head())\n",
    "\n",
    "#splitting into training and test\n",
    "## try building model with the different features and compare the result.\n",
    "X = train.loc[:,['Outlet_Establishment_Year','Item_MRP']]\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(X,train.Item_Outlet_Sales,random_state=5)\n",
    "\n",
    "print('--------Trainig Linear Regression Model---------------')\n",
    "lreg = LinearRegression()\n",
    "#training the model\n",
    "lreg.fit(x_train,y_train)\n",
    "\n",
    "#predicting on cv\n",
    "pred = lreg.predict(x_cv)\n",
    "\n",
    "#calculating mse\n",
    "mse = np.mean((pred - y_cv)**2)\n",
    "print('\\nMean Sqaured Error = ',mse )\n",
    "\n",
    "#Let us take a look at the coefficients of this linear regression model.\n",
    "# calculating coefficients\n",
    "coeff = DataFrame(x_train.columns)\n",
    "\n",
    "coeff['Coefficient Estimate'] = Series(lreg.coef_)\n",
    "\n",
    "print(coeff)\n",
    "\n",
    "print('\\n\\nModel performance on Test data = ')\n",
    "print(lreg.score(x_cv,y_cv))\n",
    "\n",
    "print('\\n\\n---------Training Ridge Regression Model----------------')\n",
    "\n",
    "ridge = Ridge()\n",
    "ridge.fit(x_train,y_train)\n",
    "pred1 = ridge.predict(x_cv)\n",
    "mse_1 = np.mean((pred1-y_cv)**2)\n",
    "\n",
    "print('\\n\\nMean Squared Error = ',mse_1)\n",
    "\n",
    "# calculating coefficients\n",
    "coeff = DataFrame(x_train.columns)\n",
    "coeff['Coefficient Estimate'] = Series(ridge.coef_)\n",
    "print(coeff)\n",
    "\n",
    "print('\\n\\nModel performance on Test data = ')\n",
    "print(ridge.score(x_cv,y_cv))\n",
    "\n",
    "\n",
    "print('\\n\\n---------Training Lasso Regression Model----------------')\n",
    "\n",
    "lasso = Lasso()\n",
    "lasso.fit(x_train,y_train)\n",
    "pred2 = lasso.predict(x_cv)\n",
    "mse_2 = np.mean((pred2-y_cv)**2)\n",
    "\n",
    "print('\\n\\nMean Squared Error = ',mse_2)\n",
    "\n",
    "# calculating coefficients\n",
    "coeff = DataFrame(x_train.columns)\n",
    "coeff['Coefficient Estimate'] = Series(lasso.coef_)\n",
    "print(coeff)\n",
    "\n",
    "print('\\n\\nModel performance on Test data = ')\n",
    "print(lasso.score(x_cv,y_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b8436",
   "metadata": {},
   "source": [
    "### 4. Generalized Linear Models (GLMs)\n",
    "What It Means: \n",
    "- GLMs extend linear regression by allowing different types of data distributions\n",
    "    - Poisson for count data. \n",
    "- It models the mean of the outcome variable based on a link function.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- The coefficients explain how each predictor affects the mean outcome, given the distribution.\n",
    "\n",
    "Performance Measures:\n",
    "- Deviance: Measures how well the model fits compared to a perfect model; lower values are better.\n",
    "\n",
    "Lay Explanation: \n",
    "- GLMs are like flexible versions of linear regression that can handle different data types (like counts or binary data), giving predictions that respect the data’s nature.\n",
    "\n",
    "Use Case: \n",
    "- Extends linear regression for non-normal distributions (e.g., Poisson regression for count data).\n",
    "\n",
    "Model Types: \n",
    "- Poisson regression, \n",
    "- Binomial regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48888f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "poisson_model = sm.GLM(y_train, X_train, family=sm.families.Poisson()).fit()\n",
    "predictions = poisson_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1559de0b",
   "metadata": {},
   "source": [
    "#### To understand why penalizing the magnitude of coefficients should work in the first place.\n",
    "\n",
    "To understand the impact of model complexity on the magnitude of coefficients, simulated a sine curve (between 60° and 300°) and added some random noise.\n",
    "\n",
    "Resembles a sine curve but not exactly because of the noise.\n",
    "\n",
    "Estimate the sine function using polynomial regression with powers of x from 1 to 15. Let’s add a column for each power upto 15 in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries. The same will be used throughout the article.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 10\n",
    "\n",
    "#Define input array with angles from 60deg to 300deg converted to radians\n",
    "x = np.array([i*np.pi/180 for i in range(60,300,4)])\n",
    "np.random.seed(10)  #Setting seed for reproducibility\n",
    "y = np.sin(x) + np.random.normal(0,0.15,len(x))\n",
    "data = pd.DataFrame(np.column_stack([x,y]),columns=['x','y'])\n",
    "plt.plot(data['x'],data['y'],'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38689bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,16):  #power of 1 is already there\n",
    "    colname = 'x_%d'%i      #new var will be x_power\n",
    "    data[colname] = data['x']**i\n",
    "print(data.head()) # add a column for each power upto 15 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01f2a8",
   "metadata": {},
   "source": [
    "#### Making 15 Different Linear Regression Models\n",
    "\n",
    "we have all the 15 powers, let’s make 15 different linear regression models, with each model containing variables with powers of x from 1 to the particular model number.\n",
    "\n",
    "Define a generic function that takes in the required maximum power of x as an input and returns a list containing \n",
    "- model RSS, \n",
    "- intercept, \n",
    "- coef_x, \n",
    "- coef_x2, … upto entered power \n",
    "\n",
    "Here RSS refers to the ‘Residual Sum of Squares,’ which is nothing but the sum of squares of errors between the predicted and actual values in the training data set and is known as the cost function or the loss function.\n",
    "\n",
    "The function will not plot the model fit for all the powers but will return the RSS and coefficient values for all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd6d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Linear Regression model from scikit-learn.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def linear_regression(data, power, models_to_plot):\n",
    "    #initialize predictors:\n",
    "    predictors=['x']\n",
    "    if power>=2:\n",
    "        predictors.extend(['x_%d'%i for i in range(2,power+1)])\n",
    "    \n",
    "    #Fit the model\n",
    "    linreg = LinearRegression(normalize=True)\n",
    "    linreg.fit(data[predictors],data['y'])\n",
    "    y_pred = linreg.predict(data[predictors])\n",
    "    \n",
    "    #Check if a plot is to be made for the entered power\n",
    "    if power in models_to_plot:\n",
    "        plt.subplot(models_to_plot[power])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['x'],y_pred)\n",
    "        plt.plot(data['x'],data['y'],'.')\n",
    "        plt.title('Plot for power: %d'%power)\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    ret.extend([linreg.intercept_])\n",
    "    ret.extend(linreg.coef_)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7824ea",
   "metadata": {},
   "source": [
    "##### Store all the Results in Pandas Dataframe\n",
    "\n",
    "Store all the results in a Pandas dataframe and plot 6 models to get an idea of the trend.\n",
    "\n",
    "Expection: the models with increasing complexity to better fit the data and result in lower RSS values.\n",
    "- As the model complexity increases, the models tend to fit even smaller deviations in the training data set. \n",
    "- Though this leads to overfitting, let’s keep this issue aside for some time and come to our main objective, i.e., the impact on the magnitude of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dataframe to store the results:\n",
    "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
    "ind = ['model_pow_%d'%i for i in range(1,16)]\n",
    "coef_matrix_simple = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "# Define the powers for which a plot is required:\n",
    "models_to_plot = {1:231,3:232,6:233,9:234,12:235,15:236}\n",
    "\n",
    "# Iterate through all powers and assimilate results\n",
    "for i in range(1,16):\n",
    "    coef_matrix_simple.iloc[i-1,0:i+2] = linear_regression(data, power=i, models_to_plot=models_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1567427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the display format to be scientific for ease of analysis\n",
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_matrix_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cc6727",
   "metadata": {},
   "source": [
    "Its evident that the size of coefficients increases exponentially with an increase in model complexity.\n",
    "- Intuition: into why putting a constraint on the magnitude of coefficients can be a good idea to reduce model complexity.\n",
    "\n",
    "##### Large Coefficents Significance\n",
    "\n",
    "It means that we’re putting a lot of emphasis on that feature, i.e., the particular feature is a good predictor for the outcome. \n",
    "- When it becomes too large, the algorithm starts modeling intricate relations to estimate the output and ends up overfitting the particular training data.\n",
    "\n",
    "Solution\n",
    "- ridge and lasso regression in detail \n",
    "- see how well they work for the same problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3ef82",
   "metadata": {},
   "source": [
    "##### 5. Decision Trees and Random Forests\n",
    "What It Means: \n",
    "- Decision trees split data based on conditions, creating branches that lead to a prediction. \n",
    "- Random forests use multiple trees to improve accuracy and reduce overfitting.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each \"branch\" shows how different conditions affect the outcome, \n",
    "- and random forests average the results of many trees for robust predictions.\n",
    "\n",
    "Performance Measures:\n",
    "- Accuracy: Proportion of correctly classified samples.\n",
    "- Gini Index / Entropy: Used to measure the purity of the splits; lower values are better.\n",
    "\n",
    "Lay Explanation: \n",
    "- Decision trees are like flowcharts that guide predictions based on conditions. \n",
    "- Random forests combine many trees to make stronger, more reliable decisions.\n",
    "\n",
    "Use Case: \n",
    "- For classification or regression problems with non-linear relationships and high dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "tree_model = DecisionTreeClassifier()\n",
    "tree_model.fit(X_train, y_train)\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "predictions_tree = tree_model.predict(X_test)\n",
    "predictions_rf = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f382cd",
   "metadata": {},
   "source": [
    "# Classification via Mathematics Functions\n",
    "\n",
    "Classification Using the Equation of a Straight Line\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Begin with the Equation of a Line: The general equation of a straight line in a 2D plane is:\n",
    "\n",
    "$$𝑦 = 𝑚 \\times 𝑥 + 𝑐 $$\n",
    "\n",
    "- m: Slope of the line (how steep it is)\n",
    "- c: Intercept (where the line crosses the y-axis)\n",
    "\n",
    "2. Connect it to Classification:\n",
    "- In binary classification, the goal is to separate two classes (e.g., Class 0 and Class 1).\n",
    "- The equation of a line can act as a decision boundary: \n",
    "    - points on one side of the line belong to Class 0, while \n",
    "    - points on the other side belong to Class 1.\n",
    "\n",
    "3. Interactive Example: \n",
    "- Imagine a dataset with two features, $𝑥_1$ and $𝑥_2$\n",
    "\n",
    "For simplicity:\n",
    "- $𝑥_1$: Horizontal axis\n",
    "- $𝑥_2$: Vertical axis\n",
    "\n",
    "A simple decision boundary can be represented as:\n",
    "$$ 𝑥_2 = 𝑚 \\times 𝑥_1 + 𝑐 $$\n",
    "\n",
    "**How the Slope (m) and Intercept (c) Influence the Boundary**\n",
    "\n",
    "The slope and intercept determine the orientation and position of the decision boundary in the feature space.\n",
    "\n",
    "- Slope (m):\n",
    "    - Controls the steepness or angle of the line.\n",
    "    - A larger absolute value of m means the line is steeper; a smaller absolute value means it is flatter.\n",
    "    - Example: In $ 𝑥_2 = 𝑚 \\times 𝑥_1 + 𝑐 $\n",
    "        - If m > 0, the line slopes upward.\n",
    "        - If m < 0, the line slopes downward.\n",
    "        - If m = 0, the line is horizontal.\n",
    "- Intercept (c):\n",
    "    - Determines where the line crosses the $𝑥_2$ (vertical) axis.\n",
    "    - Changing c shifts the line up or down without changing its slope.\n",
    "    - Example: If c=1, the line crosses the x_2 axis at 1.\n",
    "\n",
    "Together, m and c define how the decision boundary separates the feature space. Adjusting these values can change which points fall into Class 0 or Class 1.\n",
    "\n",
    "4. Decision Boundary in Classification: Modify the equation to reflect classification logic:\n",
    "$$ 𝑥_2 - 𝑚 \\times 𝑥_1 - 𝑐 = 0 $$\n",
    "\n",
    "- Points where this equation equals 0 lie exactly on the line.\n",
    "- Points where $ 𝑥_2 - 𝑚 \\times 𝑥_1 - 𝑐 > 0 $ belong to Class 1.\n",
    "- Points where $ 𝑥_2 - 𝑚 \\times 𝑥_1 - 𝑐 < 0 $ belong to Class 0.\n",
    "\n",
    "5. Visualization: Plot this line on a 2D plane with some example data points:\n",
    "- Red points for Class 0\n",
    "- Blue points for Class 1\n",
    "- The line $𝑦 = 𝑚 \\times 𝑥 + 𝑐 $ separates the two clesses\n",
    "\n",
    "6. Extend to Higher Dimensions: In higher dimensions, the decision boundary becomes a hyperplane:\n",
    "\n",
    "$$ w_1𝑥_1 + w_2𝑥_2 + ... + w_n𝑥_n + b = 0 $$\n",
    "\n",
    "- Where: \n",
    "    - $w_1, w_2, ..., w_n$ are weights (equivalent to slopes) and\n",
    "    - $𝑏$ is the intercept.\n",
    "\n",
    "**What Happens When the Data Points Overlap Significantly?**\n",
    "\n",
    "When data points from different classes overlap, the decision boundary may not cleanly separate the two classes, leading to misclassification. Here’s what happens:\n",
    "\n",
    "Misclassification:\n",
    "- Points from one class appear on the \"wrong\" side of the decision boundary.\n",
    "- This results in a classification error (false positives or false negatives).\n",
    "\n",
    "Impact on Model:\n",
    "- A linear decision boundary (a straight line) may not be flexible enough to separate overlapping or complex distributions.\n",
    "- Performance metrics like accuracy, precision, and recall can degrade.\n",
    "\n",
    "Example Scenario:\n",
    "- Consider a dataset where the two classes form concentric circles. A straight-line boundary cannot separate the classes, leading to significant misclassification.\n",
    "\n",
    "**Transition from Linear to Non-Linear Decision Boundaries**\n",
    "\n",
    "Linear decision boundaries work well when data is linearly separable. However, real-world data is often complex, requiring non-linear boundaries. Here’s how we transition:\n",
    "\n",
    "Extend the Feature Space:\n",
    "- Use techniques like polynomial features to introduce non-linear relationships.\n",
    "    - where $𝑥_1$ and $𝑥_2$ can be transformed to:\n",
    "        - $𝑥^2_1$ and $𝑥^2_2$\n",
    "        - $𝑥_1 \\times 𝑥_2$\n",
    "- The linear classifier now operates in this transformed space, creating a non-linear boundary in the original feature space.\n",
    "\n",
    "1. Kernel Methods (e.g., in SVMs):\n",
    "- Apply kernel functions like RBF (Radial Basis Function) to map data into a higher-dimensional space where it is linearly separable.\n",
    "- The decision boundary in the original space appears non-linear.\n",
    "\n",
    "2. Neural Networks:\n",
    "- Multi-layer perceptrons (MLPs) can learn complex, non-linear decision boundaries by stacking layers of non-linear activation functions.\n",
    "- Neural networks are particularly powerful for high-dimensional and unstructured data.\n",
    "\n",
    "3. Ensemble Models:\n",
    "- Techniques like random forests or gradient boosting combine multiple weak learners to create flexible decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "np.random.seed(0)\n",
    "x1_class0 = np.random.rand(50)\n",
    "x2_class0 = 2 * x1_class0 + 0.5 + np.random.normal(0, 0.1, 50)\n",
    "x1_class1 = np.random.rand(50)\n",
    "x2_class1 = 2 * x1_class1 - 0.5 + np.random.normal(0, 0.1, 50)\n",
    "\n",
    "# Equation of line: x2 = m*x1 + c\n",
    "m = 2  # slope\n",
    "c = 0  # intercept\n",
    "x_line = np.linspace(0, 1, 100)\n",
    "y_line = m * x_line + c\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x1_class0, x2_class0, color='red', label='Class 0')\n",
    "plt.scatter(x1_class1, x2_class1, color='blue', label='Class 1')\n",
    "plt.plot(x_line, y_line, color='black', label='Decision Boundary')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend()\n",
    "plt.title('Linear Decision Boundary for Classification')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b6d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Generate non-linear dataset\n",
    "X, y = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=0)\n",
    "\n",
    "# Plot raw data\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Class 0')\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Class 1')\n",
    "plt.title('Non-linear Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Linear decision boundary (fails for non-linear data)\n",
    "linear_svm = SVC(kernel='linear', C=1)\n",
    "linear_svm.fit(X, y)\n",
    "\n",
    "# Non-linear decision boundary using kernel trick\n",
    "nonlinear_svm = SVC(kernel='rbf', C=1, gamma=2)\n",
    "nonlinear_svm.fit(X, y)\n",
    "\n",
    "# Visualize decision boundaries\n",
    "def plot_decision_boundary(clf, X, y, title):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', edgecolor='k', label='Class 0')\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', edgecolor='k', label='Class 1')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Linear decision boundary\n",
    "plot_decision_boundary(linear_svm, X, y, title='Linear Decision Boundary (Fails)')\n",
    "\n",
    "# Non-linear decision boundary\n",
    "plot_decision_boundary(nonlinear_svm, X, y, title='Non-linear Decision Boundary (Succeeds)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667bf3f6",
   "metadata": {},
   "source": [
    "### **Linear Discriminant Analysis (LDA)**\n",
    "Linear Discriminant Analysis (LDA) is a classification technique that uses a linear combination of features to separate classes. \n",
    "\n",
    "It assumes:\n",
    "- The data within each class is normally distributed.\n",
    "- The covariance of each class is identical (homoscedasticity).\n",
    "\n",
    "LDA works by finding a linear decision boundary that maximizes the separation between classes.\n",
    "\n",
    "Goal:\n",
    "- Project data onto a lower-dimensional space (usually 1D for binary classification).\n",
    "- Maximize the distance between class means while minimizing the variance within each class.\n",
    "\n",
    "**Calculating the best values for the parameters of a linear discriminant**\n",
    "- Estimate the coefficients that define the linear decision boundary based on your dataset.\n",
    "- In Linear Discriminant Analysis (LDA), these coefficients are derived by maximizing the separation between the means of the classes while minimizing the variance within each class.\n",
    "\n",
    "Steps:\n",
    "1. Define the Linear Discriminant Function\n",
    "\n",
    "The linear discriminant function for binary classification can be written as:\n",
    "\n",
    "$$ y = w_0 + w_1𝑥_1 + w_2𝑥_2 + ... + w_d𝑥_d $$\n",
    "\n",
    "- where:\n",
    "    - $w_0$: Intercept (bias term).\n",
    "    - $w_1, w_2, ..., w_d$: Coefficients for each feature $x_1, x_2, ..., x_d$\n",
    "    - y: The decision score. A threshold is applied to classify points.\n",
    "\n",
    "2. Estimate Class Statistics\n",
    "\n",
    "To compute the parameters, you first need the following statistics from the data:\n",
    "\n",
    "- Compute Class Means ($\\mu_0 and \\mu_1): Calculate the mean vector for each class.\n",
    "    - for each class $C_0 and C_1$\n",
    "$$ \\mu_k = \\frac{1}{N_k} \\sum_{x \\in C_i} x $$\n",
    "- where \n",
    "    - $N_k$ is the number of instances in class k.\n",
    "\n",
    "- Compute Pooled Covariance Matrix ($𝑆_𝑤$):\n",
    "    - Within-Class Scatter Matrix ($𝑆_𝑤$): Measures the spread of points within each class.\n",
    "$$ S_w = \\sum^{[c]}_{i = 1} \\sum_{x \\in C_i} (x - \\mu_i)(x - \\mu_i)^T $$\n",
    "    - Divide by the total number of samples (N) to get the pooled covariance matrix.\n",
    "- Compute Between-Class Scatter Matrix ($𝑆_b$): Measures the separation between class means.\n",
    "$$ S_b = \\sum^{[c]}_{i = 1} N_i (\\mu_i - \\mu)(\\mu_i - \\mu)^T $$\n",
    "- where \n",
    "    - $N_i$ is the number of samples in class i\n",
    "    - $\\mu$ is the overall mean.\n",
    "\n",
    "- Prior Probabilities (P($C_0$) and P($C_1$)):\n",
    "    - These are the proportions of each class in the dataset:\n",
    "$$ P(C_k) = \\frac{N_k}{N} $$\n",
    "\n",
    "3. Compute the Parameters\n",
    "\n",
    "- Find Optimal Projection: Solve the eigenvalue problem for $S^{-1}_w S_b$, and select the eigenvector with the largest eigenvalue.\n",
    "    - The parameters of the discriminant function are calculated as follows:\n",
    "        - Linear Coefficients ($w$):\n",
    "$$ w = S^{-1}_w (\\mu_1 - \\mu_0)$$\n",
    "- where:\n",
    "    - $S^{-1}_w$ is the inverse of the pooled covariance matrix.\n",
    "\n",
    "        - Intercept($w_0$):\n",
    "$$ w_0 = -\\frac{1}{2} (\\mu^T_1 S^{-1}_w \\mu_1 - \\mu^T_0 S^{-1}_w \\mu_0 ) + ln \\frac{P(C_1)}{P(C_0)}$$\n",
    "\n",
    "4. Predict Class Labels (Decision Rule:)\n",
    "\n",
    "- Project data points onto the linear discriminant.\n",
    "    - For a new instance 𝑥, compute the linear discriminant score:\n",
    "$$ y = w_0 + w^T x$$\n",
    "\n",
    "- Use a threshold (e.g., midpoint between means) to classify points.\n",
    "- Classify based on the threshold (usually $y> 0 \\Rightarrow C_1 , y \\leq 0 \\Rightarrow C_0$ )\n",
    "\n",
    "**Interpretation of Parameters**\n",
    "\n",
    "Linear Coefficients (𝑤):\n",
    "- Feature Weight Represent?\n",
    "    - A feature weight (coefficient) indicates the change in the predicted outcome associated with a unit change in the feature, keeping all other features constant.\n",
    "    - These determine how much each feature contributes to the decision boundary.\n",
    "\n",
    "- A larger magnitude of $𝑤_𝑖$ means the corresponding feature $𝑥_𝑖$ has more influence.\n",
    "    -  Interpreting the Magnitude (Absolute Magnitude):\n",
    "        - Larger Magnitudes: Indicate that a feature has a stronger effect on the outcome.\n",
    "        - Smaller Magnitudes: Suggest that the feature has less influence on the outcome.\n",
    "    - Interpreting the Magnitude (Positive or Negative Sign)\n",
    "        - Positive Weight: Indicates a positive relationship between the feature and the outcome.\n",
    "        - Negative Weight: Indicates a negative relationship between the feature and the outcome.\n",
    "\n",
    "- Impact of Scaling on Magnitudes\n",
    "    - Feature magnitudes are meaningful only if the features are on the same scale. If features differ in scale:\n",
    "        - Larger scales will lead to larger coefficients, even if the feature has less relative importance.\n",
    "        - Standardizing or normalizing the features (e.g., using z-scores or min-max scaling) ensures that coefficient magnitudes are comparable.\n",
    "\n",
    "Linear Regression Example\n",
    "- Model: Predict house price (y) using square footage ($𝑥_1$) and number of bedrooms ($𝑥_2$):\n",
    "\n",
    "$$ y = w_0 + w_1𝑥_1 + w_2𝑥_2 $$\n",
    "$$ y = 50 + 300𝑥_1 + 10000𝑥_2 $$\n",
    "\n",
    "- Interpretation:\n",
    "    - $w_1$ = 300: Increasing square footage by 1 unit increases the house price by R300.\n",
    "    - $w_2$ = 10,000: Adding one bedroom increases the house price by R10,000.\n",
    "\n",
    "1. Interpretation of Weight Magnitude in Logistic Regression\n",
    "- In logistic regression, weights do not directly represent the change in the outcome but the log-odds of the outcome.\n",
    "\n",
    "$$ log(\\frac{P(y = 1)}{P(y = 0)}) = w_0 + w_1𝑥_1 + w_2𝑥_2 + ... + w_d𝑥_d $$\n",
    "\n",
    "- Exponentiated coefficients ($e^{w_i}$) indicate the multiplicative effect on the odds for a unit change in $x_i$\n",
    "\n",
    "Logitic Regression Example\n",
    "- Model: Predict customer churn (y) based on monthly charges ($𝑥_1$) and contract length ($𝑥_2$):\n",
    "\n",
    "$$ log(\\frac{P(y = 1)}{P(y = 0)}) = w_0 + w_1𝑥_1 + w_2𝑥_2 $$\n",
    "$$ log(\\frac{P(y = 1)}{P(y = 0)}) = -3 + 0.05𝑥_1 + 2𝑥_2 $$\n",
    "\n",
    "- Interpretation:\n",
    "    - $w_1$ = 0.05: for every R1 increase in monthly charges, the log-odds of churn increase by 0.05.\n",
    "    - $w_2$ = 10,000: For each additional month of contract length, the log-odds of churn increase by 2.\n",
    "\n",
    "2. Interpretation of Weight Magnitude in Regularized Models (Lasso and Ridge)\n",
    "- Coefficients may be shrunk or set to zero based on regularization strength, which impacts their magnitude.\n",
    "- Regularization ensures that larger weights correspond to truly important features.\n",
    "\n",
    "Intercept ($𝑤_0$):\n",
    "- Adjusts the position of the decision boundary.\n",
    "\n",
    "Decision Rule:\n",
    "- $y>0$: Class 1.\n",
    "- $y≤0$: Class 0.\n",
    "\n",
    "Considerations for interpretations\n",
    "\n",
    "Multicollinearity:\n",
    "- If features are highly correlated, the magnitude of weights can become unstable and misleading.\n",
    "    - Techniques like Variance Inflation Factor (VIF) or regularization can mitigate this.\n",
    "\n",
    "Standardization:\n",
    "- Always standardize features to ensure meaningful comparisons between coefficients.\n",
    "\n",
    "Model-Specific Meaning:\n",
    "- Interpretations vary slightly across linear regression, logistic regression, and other models.\n",
    "    - In logistic regression, remember that coefficients affect the log-odds, not the raw probabilities.\n",
    "\n",
    "**Interpretation of Results**\n",
    "\n",
    "Confusion Matrix and Classification Report:\n",
    "- The confusion matrix indicates true positives, true negatives, false positives, and false negatives.\n",
    "- The classification report shows metrics like precision, recall, F1-score, and accuracy.\n",
    "\n",
    "Decision Boundary:\n",
    "- The plot shows the LDA decision boundary, which is linear. \n",
    "    - It separates the two classes by maximizing the ratio of between-class variance to within-class variance.\n",
    "- Data points on either side of the boundary are classified into their respective classes.\n",
    "\n",
    "**Assumptions and Limitations:**\n",
    "\n",
    "Assumptions:\n",
    "- Classes have a normal distribution.\n",
    "- Classes share the same covariance matrix.\n",
    "\n",
    "Limitations:\n",
    "- LDA struggles with non-linear boundaries or when the assumptions of normality and homoscedasticity are violated.\n",
    "\n",
    "**When to Use LDA**\n",
    "\n",
    "Advantages:\n",
    "- Works well when the data satisfies its assumptions.\n",
    "- Provides interpretable results with clear decision boundaries.\n",
    "\n",
    "Use Cases:\n",
    "- Medical diagnosis (e.g., distinguishing between disease states).\n",
    "- Marketing (e.g., classifying customer preferences).\n",
    "- Text classification (when transformed into vector space).\n",
    "\n",
    "Not Suitable:\n",
    "- When classes are non-linearly separable (use non-linear methods like quadratic discriminant analysis or kernel methods in such cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746e08a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_informative=2, n_redundant=0,\n",
    "                           n_clusters_per_class=1, class_sep=2, random_state=42)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Class 0')\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Class 1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Synthetic Dataset')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Apply LDA\n",
    "lda = LDA()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lda.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Visualize decision boundary\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\n",
    "plt.scatter(X_test[y_test == 0][:, 0], X_test[y_test == 0][:, 1], color='red', edgecolor='k', label='Class 0')\n",
    "plt.scatter(X_test[y_test == 1][:, 0], X_test[y_test == 1][:, 1], color='blue', edgecolor='k', label='Class 1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('LDA Decision Boundary')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2971349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data: two classes, each with 2 features\n",
    "class_0 = np.array([[2, 3], [3, 4], [4, 5]])\n",
    "class_1 = np.array([[6, 8], [7, 9], [8, 10]])\n",
    "\n",
    "# Combine data and calculate class statistics\n",
    "X = np.vstack([class_0, class_1])\n",
    "y = np.array([0] * len(class_0) + [1] * len(class_1))\n",
    "\n",
    "# Calculate class means\n",
    "mu_0 = np.mean(class_0, axis=0)\n",
    "mu_1 = np.mean(class_1, axis=0)\n",
    "\n",
    "# Calculate within-class scatter matrix\n",
    "S_w = np.zeros((X.shape[1], X.shape[1]))\n",
    "for xi in class_0:\n",
    "    S_w += np.outer(xi - mu_0, xi - mu_0)\n",
    "for xi in class_1:\n",
    "    S_w += np.outer(xi - mu_1, xi - mu_1)\n",
    "\n",
    "# Calculate linear coefficients\n",
    "w = np.linalg.inv(S_w).dot(mu_1 - mu_0)\n",
    "\n",
    "# Calculate intercept\n",
    "prior_0 = len(class_0) / len(X)\n",
    "prior_1 = len(class_1) / len(X)\n",
    "intercept = -0.5 * (mu_1.T @ np.linalg.inv(S_w) @ mu_1 - mu_0.T @ np.linalg.inv(S_w) @ mu_0) + np.log(prior_1 / prior_0)\n",
    "\n",
    "# Display results\n",
    "print(\"Linear Coefficients (w):\", w)\n",
    "print(\"Intercept (w0):\", intercept)\n",
    "\n",
    "# Predict for a new sample\n",
    "sample = np.array([5, 6])\n",
    "decision_score = intercept + w.T.dot(sample)\n",
    "prediction = 1 if decision_score > 0 else 0\n",
    "print(\"Prediction for sample {}: Class {}\".format(sample, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838246d",
   "metadata": {},
   "source": [
    "### Optimizing the Objective Function in a Linear Discriminant Model\n",
    "\n",
    "Objective of a Linear Discriminant Analysis (LDA) model is to:\n",
    "- Find a linear combination of features that best separates two or more classes. This is achieved by \n",
    "    - optimizing an objective function that \n",
    "        - maximizes the separation between classes while \n",
    "        - minimizing the spread (variance) within each class.\n",
    "\n",
    "##### **Objective Function of LDA**\n",
    "The objective function in LDA is based on two key matrices:\n",
    "1. Between-Class Variance ($S_B$):\n",
    "    - Measures the separation between the class means.\n",
    "    - Defined as:\n",
    "$$ S_B = \\sum^k_{i= 1} n_i (\\mu - \\mu)(\\mu_i - \\mu)^T$$\n",
    "\n",
    "- where:\n",
    "    - k:Number of classes.\n",
    "    - $n_i$: Number of instances in class i\n",
    "    - $\\mu_i$: Mean vector of class i\n",
    "    - $\\mu$: Overall mean vector.\n",
    "\n",
    "2. Within-Class Variance ($S_W$): \n",
    "- Measures the spread of data points within each class.\n",
    "Defined as:\n",
    "$$ S_B = \\sum^k_{i= 1} \\sum^k_{x \\in C_1} (\\mu - \\mu)(\\mu_i - \\mu)^T$$\n",
    "\n",
    "- where:\n",
    "    - $𝐶_𝑖$ represents all instances belonging to class i.\n",
    "\n",
    "The objective function to optimize in LDA is:\n",
    "\n",
    "$$ J(w) = \\frac{w^T S_B w}{w^T S_W w}$$\n",
    "\n",
    "- Where:\n",
    "    - w : is the weight vector that defines the linear discriminant\n",
    "\n",
    "##### **Optimizing the Objective Function**\n",
    "To maximize J(w):\n",
    "1. Solve the generalized eigenvalue problem:\n",
    "$$ S^-1_W S_Bw = \\lambda w$$\n",
    "\n",
    "- Where:\n",
    "    - $\\lambda$ is the eigenvalue and \n",
    "    - w is the eigenvector.\n",
    "\n",
    "2. Select the eigenvector corresponding to the largest eigenvalue $\\lambda_1$,  as it maximizes the class separation.\n",
    "\n",
    "3. For multiclass problems, select the top k-1 eigenvectors (for k classes) to project data into a lower-dimensional space with maximum discrimination. \n",
    "\n",
    "### Scoring and Ranking Instances\n",
    "Once the linear discriminant function is computed, it can be used to score and rank instances as follows:\n",
    "\n",
    "Scoring:\n",
    "- The discriminant score for an instance x is calculated as:\n",
    "\n",
    "$$ y = w^T x + b$$\n",
    "\n",
    "- Where\n",
    "    - w is the optimized weight vector.\n",
    "    - b is the intercept (bias term).\n",
    "    - y is the scalar discriminant score.\n",
    "\n",
    "- The score indicates how far x lies from the decision boundary:\n",
    "    - Positive scores suggest the instance is likely to belong to one class.\n",
    "    - Negative scores suggest the instance is likely to belong to the other class.\n",
    "\n",
    "Ranking:\n",
    "- Instances can be ranked based on their discriminant scores y.\n",
    "    - Larger absolute scores indicate greater confidence in classification.\n",
    "    - Instances closer to zero are near the decision boundary, indicating uncertainty.\n",
    "\n",
    "##### Practical Example\n",
    "\n",
    "Given Dataset\n",
    "Suppose you have two classes (Class A and Class B) and two features $x_1, x_2$\n",
    "\n",
    "Steps to Optimize and Use the Objective Function:\n",
    "1. Compute Class Means:\n",
    "- $\\mu_A and \\mu_B$ are the mean vectors for Class A and Class B.\n",
    "- $\\mu$ is the overall mean.\n",
    "\n",
    "2. Compute Variance Matrices:\n",
    "- Calculate $S_B, S_W$\n",
    "\n",
    "3. Solve for w:\n",
    "- Find the eigenvector corresponding to the largest eigenvalue of $S^-1_W S_b$\n",
    "\n",
    "4. Calculate Scores:\n",
    "- For each instance $x_i$, compute the discriminant score:\n",
    "$$ y_i = w^T x_i + b$$\n",
    "\n",
    "5. Rank Instances:\n",
    "- Sort instances by their discriminant scores to rank them by their likelihood of belonging to a specific class\n",
    "\n",
    "__________________\n",
    "\n",
    "Disclaimer: `decision_function()` method comes from specific machine learning models in libraries like scikit-learn, and it is used to compute the distance of a sample to the decision boundary in classification tasks. \n",
    "- This function is particularly useful in models that rely on decision boundaries, such as \n",
    "    - Linear Discriminant Analysis (LDA), \n",
    "    - Support Vector Machines (SVM),\n",
    "    - Logistic Regression.\n",
    "\n",
    "What Does `decision_function()` Return?\n",
    "\n",
    "Binary Classification (2 Classes):\n",
    "- Returns a 1D array of scores where each score indicates the distance of the instance from the decision boundary.\n",
    "    - Positive scores suggest one class (e.g., Class 1), and negative scores suggest the other class (e.g., Class 0).\n",
    "\n",
    "Multiclass Classification (More than 2 Classes):\n",
    "- Returns a 2D array of scores (one score per class for each instance).\n",
    "    - The classifier assigns a class label based on the highest score.\n",
    "\n",
    "Why Use decision_function()?\n",
    "- To understand the confidence of predictions.\n",
    "- To enable custom ranking or thresholding based on discriminant scores.\n",
    "- To analyze how far instances are from the boundary, providing insight into borderline cases.\n",
    "\n",
    "Use `decision_function()` in Ranking and Thresholding\n",
    "\n",
    "- Ranking: Instances can be ranked by their scores. \n",
    "    - Higher absolute values indicate greater confidence in classification.\n",
    "- Thresholding: The decision scores can be used to apply custom thresholds to refine classification decisions.\n",
    "\n",
    "Decision Boundary\n",
    "\n",
    "In the case of Linear Discriminant Analysis:\n",
    "- The decision boundary corresponds to where the decision_function() outputs 0.\n",
    "- This boundary is a hyperplane that separates the feature space into regions corresponding to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Example data\n",
    "X = np.array([[2, 3], [3, 5], [5, 7], [6, 8], [8, 10], [9, 12]])  # Features\n",
    "y = np.array([0, 0, 0, 1, 1, 1])  # Labels (0: Class A, 1: Class B)\n",
    "\n",
    "# Fit LDA model\n",
    "lda = LDA()\n",
    "lda.fit(X, y)\n",
    "\n",
    "# Compute discriminant scores\n",
    "scores = lda.decision_function(X)\n",
    "\n",
    "# Print scores and rankings\n",
    "print(\"Discriminant Scores:\", scores)\n",
    "print(\"Ranking of Instances:\", np.argsort(-scores))  # Descending order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb45c1",
   "metadata": {},
   "source": [
    "Interpretation of Scores and Rankings\n",
    "\n",
    "Discriminant Scores:\n",
    "- Positive scores suggest membership in Class 1.\n",
    "- Negative scores suggest membership in Class 0.\n",
    "\n",
    "Rankings:\n",
    "- Instances with higher absolute scores are ranked higher, as the classifier is more confident in their classification.\n",
    "\n",
    "Decision Boundary:\n",
    "- The boundary is where the discriminant score y = 0\n",
    "\n",
    "### Analyzing the relationship between the distance from the decision boundary of a linear discriminant and the likelihood of response\n",
    "Helps us understanding how confident the model is in its predictions.\n",
    "\n",
    "- The distance from the decision boundary (discriminant score) relates directly to the confidence in classification.\n",
    "- Scores are transformed into posterior probabilities using logistic (binary) or softmax (multiclass) functions.\n",
    "- These probabilities are interpretable as the likelihood of response and can be used for scoring, ranking, and applying thresholds for decision-making.\n",
    "\n",
    "##### Theoretical Relationship : Distance from the decision boundary\n",
    "\n",
    "The decision boundary in a Linear Discriminant Analysis (LDA) separates classes by \n",
    "- maximizing the distance between class means while \n",
    "- minimizing variance within each class. \n",
    "\n",
    "The discriminant score $y = w^T x + b$ represents the signed distance of an instance x from the decision boundary:\n",
    "- Positive scores indicate the instance is classified into one class (e.g., Class 1).\n",
    "- Negative scores indicate the instance is classified into the other class (e.g., Class 0).\n",
    "\n",
    "The magnitude of the score reflects the confidence in classification:\n",
    "- Larger absolute values imply that the instance is far from the decision boundary and thus more confidently classified.\n",
    "- Smaller absolute values (near zero) indicate that the instance is close to the boundary, suggesting uncertainty.\n",
    "\n",
    "##### Likelihood of Response\n",
    "In LDA, we can link the discriminant score to the posterior probability of a class, which represents the likelihood of the instance belonging to that class:\n",
    "\n",
    "$$ P(C_k | x) = \\frac{exp(y_k)}{\\sum^K_{j = 1} exp(y_i)} $$\n",
    "\n",
    "- Where: \n",
    "    - $P(C_k | x)$: is the posterior probability for class k.\n",
    "    - $ y_k = w^T x_i + b_k$: is the discriminant score for class k.\n",
    "    - The denominator is the normalization factor across all classes K.\n",
    "\n",
    "The posterior probability serves as a soft classification metric:\n",
    "- Probabilities closer to 1 indicate high confidence.\n",
    "- Probabilities closer to 0.5 (in a binary classification) indicate uncertainty.\n",
    "\n",
    "#####  Practical Example\n",
    "Let’s calculate the relationship between discriminant scores and posterior probabilities for a **Binary classification**.\n",
    "\n",
    "Example Dataset\n",
    "\n",
    "Suppose we have a binary classification problem with discriminant scores:\n",
    "$$y=[2.0,0.5,0.0,−0.5,−2.0]$$\n",
    "\n",
    "We can compute the posterior probabilities using the logistic function:\n",
    "$$ P(C_1 | x) = \\frac{1}{1 + exp(-y)} $$\n",
    "\n",
    "_______________\n",
    "\n",
    "Generalization to **Multiclass Classification**\n",
    "- In multiclass problems, the discriminant scores $𝑦_𝑘$ are normalized using the softmax function to compute posterior probabilities for each class:\n",
    "\n",
    "$$ P(C_k | x) = \\frac{exp(y_k)}{\\sum^K_{j = 1} exp(y_i)} $$\n",
    "\n",
    "- And the class with the highest posterior probability is the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37545e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Discriminant scores\n",
    "scores = np.array([2.0, 0.5, 0.0, -0.5, -2.0])\n",
    "\n",
    "# Compute posterior probabilities using the logistic function\n",
    "posterior_probabilities = 1 / (1 + np.exp(-scores))\n",
    "\n",
    "# Print results\n",
    "print(\"Scores:\", scores)\n",
    "print(\"Posterior Probabilities:\", posterior_probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88fcac1",
   "metadata": {},
   "source": [
    "Interpretation of Results\n",
    "\n",
    "1. Scores Far from Zero:\n",
    "- y=2.0: High confidence in Class 1 ($P(C_1∣x)=0.88$).\n",
    "- y=−2.0: High confidence in Class 0 ($P(C_0∣x)=0.88$).\n",
    "\n",
    "2. Scores Near Zero:\n",
    "- y=0.0: The posterior probability is 0.5, indicating complete uncertainty.\n",
    "\n",
    "3. Intermediate Scores:\n",
    "- y=0.5: Moderately confident in Class 1 ($P(C_1∣x)=0.62$).\n",
    "- y=−0.5: Moderately confident in Class 0 ($P(C_0∣x)=0.62$).\n",
    "\n",
    "Insights\n",
    "\n",
    "Distance and Likelihood:\n",
    "- Instances farther from the boundary (large ∣y∣) have posterior probabilities close to 0 or 1, indicating higher confidence in classification.\n",
    "- Instances near the boundary (y≈0) have probabilities close to 0.5, indicating uncertainty.\n",
    "\n",
    "Scoring and Ranking:\n",
    "- By sorting instances based on posterior probabilities, you can rank them in terms of likelihood of response (e.g., likelihood of belonging to Class 1).\n",
    "\n",
    "Visualization\n",
    "To better understand the relationship, plot the discriminant score against the posterior probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd92fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot scores vs posterior probabilities\n",
    "plt.plot(scores, posterior_probabilities, marker='o')\n",
    "plt.axvline(0, color='gray', linestyle='--', label='Decision Boundary')\n",
    "plt.title('Discriminant Score vs Posterior Probability')\n",
    "plt.xlabel('Discriminant Score (y)')\n",
    "plt.ylabel('Posterior Probability')\n",
    "plt.legend(['Scores', 'Decision Boundary'])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23090e15",
   "metadata": {},
   "source": [
    "### Understanding Decision Boundaries in Depth\n",
    "\n",
    "Decision boundaries are surfaces (lines, planes, or hypersurfaces) that separate data points into different classes in a feature space.\n",
    "- These boundaries are derived based on the decision rules of a classifier, and they indicate the regions where the classifier predicts different outcomes.\n",
    "\n",
    "##### Decision Boundaries in 2D\n",
    "In 2D space, the decision boundary is a \n",
    "- line (for linear classifiers) or a \n",
    "- curve (for non-linear classifiers).\n",
    "\n",
    "1. Linear Decision Boundaries\n",
    "- For a binary classification problem, a linear decision boundary is represented as:\n",
    "\n",
    "$$ w_0 + w_1𝑥_1 + w_2𝑥_2 = 0 $$\n",
    "\n",
    "- where:\n",
    "    - $ w_1, w_2$ are the coefficients.\n",
    "    - $ 𝑥_1 , 𝑥_2$ are the features.\n",
    "    - $ w_0 $  is the intercept.\n",
    "\n",
    "Separating red and blue points in a 2D space, the decision boundary is a straight line. Points on one side belong to one class, while points on the other side belong to the other class.\n",
    "\n",
    "2. Non-Linear Decision Boundaries\n",
    "- For complex data distributions, non-linear classifiers create curved decision boundaries. These are:\n",
    "    - SVM with kernel trick or \n",
    "    - neural networks \n",
    "- Example: A circular boundary might separate inner and outer regions in a concentric circle dataset.\n",
    "\n",
    "Visualization\n",
    "- The boundary is typically visualized by plotting the equation in 2D space and showing the classification regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Generate 2D data\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_informative=2, n_redundant=0, class_sep=1.5, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Create grid for decision boundary\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "# Plot data and decision boundary\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap='coolwarm')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('2D Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1d68a",
   "metadata": {},
   "source": [
    "##### Decision Boundaries in 3D\n",
    "In 3D space, the decision boundary becomes a \n",
    "- plane.\n",
    "\n",
    "1. Linear Decision Boundaries\n",
    "- Represented as:\n",
    "\n",
    "$$ w_0 + w_1𝑥_1 + w_2𝑥_2 + w_3𝑥_3= 0 $$\n",
    "\n",
    "- where:\n",
    "    - $ w_1, w_2,  w_3 $ are the coefficients.\n",
    "    - $ 𝑥_1 , 𝑥_2, 𝑥_3$ are the features.\n",
    "    - $ w_0 $  is the intercept.\n",
    "\n",
    "For the features, the plane separates the feature space into two regions for classification.\n",
    "\n",
    "2. Non-Linear Decision Boundaries\n",
    "\n",
    "- Non-linear models define curved surfaces in 3D space\n",
    "    - spheres, - parabolas.\n",
    "- Example: In 3D, the boundary might look like a bowl separating one region (inside the bowl) from another (outside the bowl).\n",
    "\n",
    "Visualization\n",
    "- Visualizing a plane or curved surface in 3D is possible with tools like Matplotlib's 3D plotting. It shows how the boundary divides the space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01975d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generate 3D data\n",
    "X = np.random.rand(200, 3)\n",
    "y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Linear decision boundary\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X, y)\n",
    "\n",
    "# Create grid for decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(0, 1, 50), np.linspace(0, 1, 50))\n",
    "zz = (-model.intercept_[0] - model.coef_[0][0] * xx - model.coef_[0][1] * yy) / model.coef_[0][2]\n",
    "\n",
    "# Plot data and decision boundary\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='coolwarm', edgecolor='k')\n",
    "ax.plot_surface(xx, yy, zz, alpha=0.5, color='gray')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_zlabel('Feature 3')\n",
    "ax.set_title('3D Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079f3b5f",
   "metadata": {},
   "source": [
    "##### Decision Boundaries in Higher Dimensions\n",
    "In higher-dimensional spaces, the decision boundary becomes a \n",
    "- hyperplane \n",
    "- more complex hypersurface.\n",
    "\n",
    "1. Linear Decision Boundaries\n",
    "- For a d-dimensional feature space, the equation is:\n",
    "$$ w_0 + w_1𝑥_1 + w_2𝑥_2 + w_3𝑥_3 + ... + w_d𝑥_d = 0 $$\n",
    "- This hyperplane divides the d-dimensional space into regions for classification.\n",
    "\n",
    "In a 4D feature space $𝑥_1, 𝑥_2, 𝑥_3, 𝑥_4$, the decision boundary is a 3D hyperplane.\n",
    "\n",
    "2. Non-Linear Decision Boundaries\n",
    "- Non-linear models use transformations (e.g., polynomial, kernel tricks) to create non-linear hypersurfaces.\n",
    "- These hypersurfaces can separate data points that are non-linearly separable in their original feature space.\n",
    "\n",
    "Visualization\n",
    "- Direct visualization becomes challenging beyond 3 dimensions. \n",
    "- However, techniques like dimensionality reduction (PCA, t-SNE, UMAP) can project high-dimensional data and decision boundaries into 2D or 3D for interpretation.\n",
    "\n",
    "**Impact of Dimension on Decision Boundaries**\n",
    "\n",
    "Curse of Dimensionality:\n",
    "- As dimensions increase, data points become sparse, making classification harder.\n",
    "- Models like LDA or logistic regression may underperform without feature selection.\n",
    "\n",
    "Model Complexity:\n",
    "- Non-linear decision boundaries require more complex models (e.g., SVM with RBF kernels, neural networks).\n",
    "- Overfitting is a significant risk in high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156965d",
   "metadata": {},
   "source": [
    "### **6. Support Vector Machines (SVM)**\n",
    "What It Means: \n",
    "- SVMs classify data by finding the best “boundary” (hyperplane) that separates classes with the widest possible margin.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Data points on either side of the boundary belong to different classes, with \"support vectors\" helping to define the boundary.\n",
    "\n",
    "Performance Measures:\n",
    "- Accuracy: Proportion of correct classifications.\n",
    "- Precision and Recall: Used when classes are imbalanced; precision is the correctness of positive predictions, and recall measures coverage.\n",
    "\n",
    "Lay Explanation: \n",
    "- SVMs are like drawing a line to separate different groups, ensuring the groups are as distinct as possible with the help of a few key points.\n",
    "\n",
    "Use Case: \n",
    "- Used for classification and regression in high-dimensional spaces, often for non-linearly separable data.\n",
    "\n",
    "### **Support Vector Machines (SVM): Key Idea**\n",
    "\n",
    "The idea behind SVM is to find the optimal hyperplane that best separates data points of different classes in the feature space.\n",
    "- This basic idea of the SVM is to separate points using a $(p - 1)$ dimensional hyperplane. \n",
    "\n",
    "What does it mean to separate points? \n",
    "- This means that the SVM will construct a decision boundary such that points on the left are assigned a label of $A$ and points on the right are assigned a label of $B$.  \n",
    "- When finding this separating hyperplane we wish to maximise the distance of the nearest points to the hyperplane. \n",
    "    - The technical term for this is **maximum separating hyperplane**.\n",
    "- The data points which dictate where the separating hyperplane goes are called **support vectors**.\n",
    "\n",
    "How It works in laymans terms:\n",
    "\n",
    "Pretend that you want to classify data points into group $A$ or group $B$. An SVM will plot your labelled training data as points in space and will:\n",
    "- look for the widest, clearest gap between points belonging to group A and points belonging to group B. \n",
    "- It will then use this newly identified dividing line (known as a hyperplane) and the margin around it to classify new observations. \n",
    "- An unseen data point will be classified into group A or B depending on which side of the margin it is closest to. \n",
    "\n",
    "##### Important Concepts in SVM\n",
    "1. Hyperplane:\n",
    "- A decision boundary that separates classes in the feature space.\n",
    "    - In 2D, it’s a line; \n",
    "        -  when your data only has 2 features. You only need a simple one-dimensional decision boundary (which is basically a line) to classify the data.\n",
    "        - line only has one dimension\n",
    "    - In 3D, it’s a plane; \n",
    "    - In higher dimensions, it’s a hyperplane.\n",
    "        - more features get added the line needs to take on more dimensions,\n",
    "        - 4 or more dimensions\n",
    "        - In SVM, the hyperplane will always have one less dimension ($-1$) than the number of input features ($p$), or a total of $(p-1)$ dimensions.\n",
    "\n",
    "2. Margin:\n",
    "- The distance between the hyperplane and the closest data points (called support vectors) of either class.\n",
    "    - SVM maximizes this margin to create the most robust separation.\n",
    "\n",
    "3. Support Vectors:\n",
    "- The data points closest to the hyperplane, which influence its position and orientation.\n",
    "4. Optimal Hyperplane:\n",
    "- The hyperplane that maximizes the margin while correctly classifying the training data (or minimizing misclassifications).\n",
    "\n",
    "Support Vector Machines in a nutshell:\n",
    "- Like logistic regression, SVMs fit a linear decision boundary. \n",
    "- Unlike logistic regression, SVMs do this in a non-proabilistic way and are able to fit to non-linear data using an algorithm known as the [kernel trick](https://en.wikipedia.org/wiki/Kernel_method).\n",
    "\n",
    "SVMs can be used for both classification and regression. In `sklearn`, these are called:\n",
    "- `SVC` (Support Vector Classifier)\n",
    "- `SVR` (Support Vector Regression) \n",
    "\n",
    "SVC can also refer to Support Vector **Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f9f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738d5af7",
   "metadata": {},
   "source": [
    "##### Generate synthetic data\n",
    "Generate a random dataset to experiment with.\n",
    "- by taking a multi-dimensional **standard normal distribution** and defining classes separated by nested concentric multi-dimensional spheres such that roughly equal numbers of\n",
    "samples are in each class (quantiles of the $\\chi^2$ distribution).\n",
    "    - generated a donut shaped dataset, where \n",
    "        - the samples belonging to one class are generally located in the centre\n",
    "        - the samples belonging to the other class are generally located in the outer ring.\n",
    "\n",
    "\n",
    "##### **Reasons for Normalizing Data in SVMs**\n",
    "- because of how the SVM algorithm calculates margins and distances between data points.\n",
    "\n",
    "SVM is Sensitive to Feature Scales\n",
    "- SVM relies on calculating distances (e.g., Euclidean distance) between data points to determine margins and support vectors. \n",
    "    - If one feature has a much larger range than others, it will dominate the distance calculation, leading to biased results.\n",
    "- Example: In a dataset with two features—age (ranging from 0 to 100) and income (ranging from 0 to 100,000)—income will heavily influence the decision boundary, even if age is equally or more important.\n",
    "\n",
    "Ensures Proper Margins\n",
    "- The SVM objective is to find the hyperplane that maximizes the margin between classes. \n",
    "    - Without normalization, the margin calculation may become skewed, resulting in suboptimal or incorrect decision boundaries.\n",
    "- Example: If one feature has a larger scale, the margin might stretch disproportionately along that dimension, ignoring other features.\n",
    "\n",
    "Improves Kernel Performance\n",
    "- SVMs often use kernels (e.g., RBF, polynomial) to project data into higher dimensions. \n",
    "    - Kernels are sensitive to the relative scaling of features. Normalization ensures that all features contribute equally to the projection.\n",
    "- Example: An RBF kernel requires well-scaled data to compute meaningful similarity measures between points. Poorly scaled data may lead to ineffective kernel computations.\n",
    "\n",
    "Reduces Convergence Time\n",
    "- SVM optimization involves iterative calculations that are influenced by feature scaling. \n",
    "    - Normalized data leads to faster and more stable convergence of the optimization algorithm.\n",
    "- Example: When features are on drastically different scales, the optimization problem may take longer to converge or fail to converge entirely.\n",
    "\n",
    "Handles Non-linear Decision Boundaries Better\n",
    "- Why? For non-linear kernels (like RBF), the distance between points in feature space influences the shape of the decision boundary. \n",
    "    - Normalization ensures these distances are meaningful, leading to smoother and more accurate decision boundaries.\n",
    "\n",
    "##### **Consequences of Not Normalizing**\n",
    "- Poor Decision Boundaries: The SVM may create biased or incorrect hyperplanes, reducing model performance.\n",
    "- Misclassification: The model may misclassify data, especially when features with large ranges dominate.\n",
    "- Kernel Inefficiency: Kernels may fail to project the data effectively, leading to poor separation of classes.\n",
    "- Increased Training Time: Optimization takes longer, impacting the efficiency of training.\n",
    "\n",
    "##### **How to Normalize Data for SVMs**\n",
    "\n",
    "1. **Standardization**: Subtract the mean and divide by the standard deviation for each feature\n",
    "$$ z = (\\frac{x - \\mu}{\\sigma})$$\n",
    "- This scales features to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "2. **Min-Max Scaling**: Rescale each feature to a fixed range, typically [0, 1] \n",
    "$$ z = (\\frac{x - min(x)}{max(x)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f9f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "# Set the feature dimensionality\n",
    "p = 2\n",
    "\n",
    "# Construct the dataset\n",
    "X, y = make_gaussian_quantiles(cov=3.,\n",
    "                                 n_samples=1000, n_features=p,\n",
    "                                 n_classes=2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca6aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training and testing data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f6727",
   "metadata": {},
   "source": [
    "##### Fit a SVM classifier with a linear decision boundary\n",
    "We are going to fit an SVC model with a `linear kernel`. This means that we are telling the SVC to fit the data using a linear decision boundary. Let's also take a look at the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a42c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "print(\"The accuracy score of the SVC is:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b8a25",
   "metadata": {},
   "source": [
    "##### Plot the decision boundary for the SVC\n",
    "When accuracy score doesn't seem very good. To help us understand what's going on use: Visualisation.\n",
    "\n",
    "The SVC calculates and implements a $p-1$ dimensional decision boundary (hyperplane) over the input features.\n",
    "- Since we are only looking at 2 features (our synthetic dataset only has two features, or $p=2$), our hyperplane will only have 1 dimension ($p-1$)\n",
    "    - look like a single line.\n",
    "- if your model has more than 2 features, you can plot the hyperplane for any 2 features you choose.\n",
    "\n",
    "##### **Calculating the Dimensions of a separating Hyperplane**\n",
    "The dimensions of a separating hyperplane depend on the number of features (or predictors) in the dataset.\n",
    "\n",
    "Definition of a Hyperplane\n",
    "\n",
    "A hyperplane in n-dimensional space is defined as:\n",
    "$$ 𝑤 \\times 𝑥 + 𝑏 = 0$$\n",
    "\n",
    "where:\n",
    "- $w = [w_1, w_2, ..., w_n]$: Weight vector normal to the hyperplane.\n",
    "- $x = [x_1, x_2, ..., x_n]$: Feature vector of an instance.\n",
    "- $b$: Bias term (offset from the origin).\n",
    "\n",
    "The hyperplane separates data into two classes:\n",
    "- $ 𝑤 \\times 𝑥 + 𝑏 > 0$: Class 1\n",
    "- $ 𝑤 \\times 𝑥 + 𝑏 < 0$: Class 2\n",
    "\n",
    "Dimensions of a Hyperplane\n",
    "\n",
    "The dimensionality of the hyperplane is determined by the number of features in the dataset:\n",
    "- If the dataset has n features, the hyperplane is an (n−1)-dimensional subspace.\n",
    "\n",
    "Examples:\n",
    "- 2 Features (2D): The hyperplane is a 1D line.\n",
    "- 3 Features (3D): The hyperplane is a 2D plane.\n",
    "- 4 Features (4D): The hyperplane is a 3D subspace (hard to visualize, but mathematically valid).\n",
    "\n",
    "Intuition Behind Dimensions\n",
    "\n",
    "- The hyperplane must divide the feature space into two regions corresponding to different classes.\n",
    "- Higher dimensions mean more complex hyperplanes, allowing SVM to handle more intricate patterns.\n",
    "- Kernels: When data is mapped to a higher-dimensional feature space using kernels (e.g., RBF), the hyperplane exists in the higher-dimensional space, though its exact dimensions depend on the kernel's transformation\n",
    "\n",
    "When Are the Dimensions Relevant?\n",
    "\n",
    "- At Training Time: The dimensions of the hyperplane are implicitly calculated when the SVM solves the optimization problem to find 𝑤 and b.\n",
    "    - The optimization ensures the hyperplane maximizes the margin between support vectors of the two classes.\n",
    "- During Prediction: The dimensionality of the hyperplane affects how data points are classified. The model computes:\n",
    "    - Decision Function: $𝑤 \\times 𝑥 + 𝑏$\n",
    "        - The sign of this value determines the predicted class.\n",
    "\n",
    "##### **Calculation of the Dimensions**\n",
    "- The dimensions are calculated implicitly when the SVM solves its optimization problem to find 𝑤 and b.\n",
    "- The dimensionality of the hyperplane is directly tied to the feature space in which the data resides.\n",
    "\n",
    "Steps:\n",
    "1. Input Data Dimension: Count the number of features n in your dataset.\n",
    "- Example: If your dataset has features $x = [x_1, x_2, x_3]$ , it’s a 3-dimensional space.\n",
    "\n",
    "2. Hyperplane Dimension: The hyperplane will have (n−1) dimensions.\n",
    "- For the 3-feature example, the hyperplane is a 2D plane.\n",
    "\n",
    "In this case, donut-shaped data is not `linearly separable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ec1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 # Feature 1\n",
    "j = 1 # Feature 2\n",
    "\n",
    "svc.fit(X[:, [i, j]], y)\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    " \n",
    "x_min, x_max = X[:, i].min(), X[:, i].max()\n",
    "y_min, y_max = X[:, j].min(), X[:, j].max()\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 1000), np.linspace(y_min, y_max, 1000))\n",
    "\n",
    "y_hat = svc.predict(np.concatenate((xx.reshape(-1,1), yy.reshape(-1,1)), axis=1))\n",
    "y_hat = y_hat.reshape(xx.shape)\n",
    "\n",
    "ax1.pcolormesh(xx, yy, y_hat, cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.scatter(X[:, i], X[:, j], c=y, edgecolors='k', cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.set_xlim(xx.min(), xx.max())\n",
    "ax1.set_ylim(yy.min(), yy.max())\n",
    "ax1.set_xticks(())\n",
    "ax1.set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c0d22",
   "metadata": {},
   "source": [
    "Solution: Use of SVM's [kernel trick](https://en.wikipedia.org/wiki/Kernel_method) to use a **non-linear** decision boundary instead.\n",
    "\n",
    "\n",
    "\n",
    "##### Fit a SVC classifier with a non-linear decision boundary\n",
    "\n",
    "Use the rbf kernel (Radial_basis_function_kernel), which allows the SVC to fit a non-linear decision boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64fed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf')\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "print(\"The accuracy score of the SVC is:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0875d9",
   "metadata": {},
   "source": [
    "##### Plot the decision boundary for the SVC using the non-linear rbf kernel\n",
    "\n",
    "Plot the 1 dimensional decision boundary between the 2 features present in our synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 # Feature 1\n",
    "j = 1 # Feature 2\n",
    "\n",
    "svc.fit(X[:, [i, j]], y)\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    " \n",
    "x_min, x_max = X[:, i].min(), X[:, i].max()\n",
    "y_min, y_max = X[:, j].min(), X[:, j].max()\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 1000), np.linspace(y_min, y_max, 1000))\n",
    "\n",
    "y_hat = svc.predict(np.concatenate((xx.reshape(-1,1), yy.reshape(-1,1)), axis=1))\n",
    "y_hat = y_hat.reshape(xx.shape)\n",
    "\n",
    "ax1.pcolormesh(xx, yy, y_hat, cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.scatter(X[:, i], X[:, j], c=y, edgecolors='k', cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.set_xlim(xx.min(), xx.max())\n",
    "ax1.set_ylim(yy.min(), yy.max())\n",
    "ax1.set_xticks(())\n",
    "ax1.set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9625da",
   "metadata": {},
   "source": [
    "### **Objective Function of SVM**\n",
    "The objective of SVM is twofold:\n",
    "- Maximize the margin (maximize separation between classes).\n",
    "- Minimize classification errors for non-linearly separable data (using a hinge-loss function).\n",
    "\n",
    "It is designed to maximize the margin between the two classes while minimizing classification errors. \n",
    "- For m features, the objective function considers the weight vector $w \\in R^m$, which defines the orientation of the separating hyperplane in the feature space.\n",
    "\n",
    "#####  Primal Form of the Objective Function: Mathematical Formulation\n",
    "Given:\n",
    "- A dataset with n training samples $(𝑥_𝑖, 𝑦_𝑖)$ where \n",
    "    - $𝑥_𝑖 \\in 𝑅^𝑑$ are feature vectors\n",
    "    - $y_𝑖 \\in {−1,1}$ are class labels.\n",
    "- A weight vector 𝑤 and bias b defining the hyperplane.\n",
    "\n",
    "The decision boundary is represented by:\n",
    "$$ f(x) = w^T x + b $$\n",
    "\n",
    "The **SVM primal objective function** is:\n",
    "\n",
    "$$ Minimize_{w,b,\\xi}: \\frac{1}{2} ||w||^2 + C \\sum^n_{i=1} Hinge Loss$$\n",
    "\n",
    "- Where:\n",
    "    - $\\frac{1}{2} ||w||^2$: Ensures the margin is maximized, promoting simplicity.\n",
    "        - Encourages a large margin between the two classes (simpler decision boundary).\n",
    "        - It minimizes the norm of the weight vector w, maximizing the margin between the classes.\n",
    "    - $C$: Regularization parameter that controls the trade-off between margin maximization and classification errors.\n",
    "        - The choice of C determines where this balance lies.\n",
    "    - $\\sum^n_{i=1} \\xi_i$: A penalty for all margin violations. \n",
    "        - A higher sum of $\\xi_i$ implies more violations.\n",
    "    - $\\xi$: Classification errors / Slack variables that represent misclassification or margin violations.\n",
    "        - measure the extent of misclassification or margin violation for each data point.\n",
    "        - It represents the extent to which the i-th data point violates the margin (misclassification or lying within the margin).\n",
    "\n",
    "\n",
    "The constraints for correctly classified data points are / subject to:\n",
    "\n",
    "$$ y_i  \\cdot (w^T x_i + b) \\geq 1  - \\xi_i , \\xi_i \\geq 0 \\forall i $$\n",
    "\n",
    "##### Hinge-Loss Function\n",
    "\n",
    "The hinge-loss function is used to penalize misclassifications and points close to the margin. It is defined as:\n",
    "\n",
    "$$ Hinge Loss: L(y,f(x)) = max(0,1 - y  \\cdot f(x)) $$\n",
    "\n",
    "- If $y  \\cdot f(x) \\geq 1$, the loss is 0 (correctly classified and beyond the margin).\n",
    "- If $y  \\cdot f(x) < 1$, the loss increases linearly as the point moves closer to or across the margin.\n",
    "\n",
    "he **SVM primal objective function** is now:\n",
    "\n",
    "$$ Minimize_{w,b}: \\frac{1}{2} ||w||^2 + C \\sum^n_{i=1} max(0,1 - y_i  \\cdot (w^T x_i + b))$$\n",
    "\n",
    "### **Understanding the Objective Function**\n",
    "\n",
    "1. Margin Maximization ($\\frac{1}{2} ||w||^2$): \n",
    "- The first term ensures that the hyperplane has the largest margin by minimizing the norm of the weight vector ($||w||$).\n",
    "    - A smaller ||w|| corresponds to a larger margin.\n",
    "\n",
    "2. Hinge Loss ($C \\sum^n_{i=1} max(0,1 - y  \\cdot f(x))$)\n",
    "- The second term $\\sum^n_{i=1} \\xi_i$ penalizes points that are misclassified or fall within the margin.\n",
    "- $max(0,1 - y_i  \\cdot (w^T x_i + b)$ penalizes points that are either misclassified or lie within the margin.\n",
    "- The parameter C>0 is a `regularization parameter` that controls the trade-off between maximizing the margin and minimizing classification errors.\n",
    "    - How C Works:\n",
    "        - Large C: Strongly penalizes misclassifications, leading to a tighter fit to the training data.\n",
    "        - Small C: Allows for more margin violations, leading to a simpler, more generalizable model.\n",
    "\n",
    "Regularization parameter trade-off:\n",
    "- Regularization adjusts the balance between two objectives:\n",
    "    - Maximizing the margin:\n",
    "        - Maximizing the margin: Keeping $||w||^2$ small promotes a large margin and simpler models.\n",
    "        - Minimizing misclassification error: Penalizing $\\sum^n_{i=1} \\xi_i$ ensures the model correctly classifies most training instances.\n",
    "\n",
    "\n",
    "##### Interpretation of the Objective Function\n",
    "The function combines two objectives:\n",
    "1. Maximizing the margin: Achieved by minimizing $\\frac{1}{2} ||w||^2$ resulting in a decision boundary that is as far as possible from the nearest data points (support vectors).\n",
    "    - A larger margin improves the model's generalization ability (i.e., it performs better on unseen data).\n",
    "2. Minimizing classification errors: Achieved by penalizing the slack variables $\\xi_i$ via the term $C \\sum^n_{i=1} \\xi_i$, which accounts for points within or outside the margin.\n",
    "    - Points misclassified or within the margin are penalized, encouraging the model to position the hyperplane optimally.\n",
    "\n",
    "#### How SVM Uses Hinge Loss\n",
    "\n",
    "##### 2.1 How violations and misclassification are measured in soft margin classification.\n",
    "Soft Margin Classification:\n",
    "- The soft margin SVM introduces flexibility by allowing violations of the margin through $\\xi_i$, making it suitable for non-linearly separable and noisy datasets.\n",
    "- For linearly inseparable data, SVM introduces slack variables ($\\xi_i$) to allow some points to violate the margin constraints.\n",
    "- The hinge loss incorporates these violations, enabling SVM to work with noisy or overlapping data.\n",
    "\n",
    "In soft margin classification, violations and misclassification are measured using slack variables ($\\xi_i$), which represent the extent to which a data point deviates from the ideal separation defined by the decision boundary and margin.\n",
    "\n",
    "The Role of the Slack Variables ($\\xi_i$)\n",
    "- Slack variables are introduced in the soft margin SVM to allow for some data points to:\n",
    "    - Lie inside the margin (violations).\n",
    "    - Be misclassified (on the wrong side of the decision boundary).\n",
    "- Each data point i has an associated slack variable ($\\xi_i \\geq 0$), which quantifies its violation of the margin constraints.\n",
    "\n",
    "Decision Boundary and Constraints\n",
    "- The decision boundary in soft margin classification is defined by:\n",
    "$$ y_i (w \\cdot x_i + b) \\geq 1  - \\xi_i , \\xi_i \\geq 0 $$\n",
    "\n",
    "- When $ y_i (w \\cdot x_i + b) \\geq 1$:\n",
    "    - The data point is correctly classified and outside the margin. No violation occurs, so $\\xi_i = 0$.\n",
    "- When $ 0 < y_i (w  \\cdot x_i + b) < 1$: \n",
    "    - The data point is correctly classified but lies inside the margin. The margin is violated, and $\\xi_i > 0$.\n",
    "- When $ y_i (w \\cdot x_i + b) < 0$:\n",
    "    - The data point is misclassified and on the wrong side of the decision boundary. This is a severe violation, with a larger $\\xi_i$.\n",
    "\n",
    "Measuring Margin Violations\n",
    "- The slack variable $\\xi_i$ measures the distance a point falls short of the margin boundary. Specifically:\n",
    "    - $\\xi_i = 0$, the point lies on or outside the correct margin.\n",
    "    - $0 < \\xi_i \\leq 1$, the point is inside the margin but correctly classified.\n",
    "    - $\\xi_i > 1$, the point is misclassified.\n",
    "\n",
    "- Total Margin Violation\n",
    "    - The total violation across all data points is:\n",
    "$$ \\sum^N_{i=1} \\xi_i$$\n",
    "\n",
    "Misclassification\n",
    "- Misclassification occurs when a data point lies on the wrong side of the decision boundary:\n",
    "$$y_i (w \\cdot x_i + b)$$\n",
    "\n",
    "- For misclassified points, $\\xi_i >1$\n",
    "    - The slack variable $\\xi_i - 1$ represents the extent of misclassification.\n",
    "\n",
    "- Misclassification Count\n",
    "    - The number of misclassified points can be roughly estimated as:\n",
    "        - number of misclassifications $\\approx \\sum^N_{i=1} 1 (\\xi_i >1)$ where:\n",
    "            - $1(\\cdot)$ is an indicator function that equals 1 if the condition is true, and 0 otherwise.\n",
    "\n",
    "- Example: assume we have the following\n",
    "    - $y_i = +1$: Positive class.\n",
    "        - The margin for $y_i = +1$ is defined as $(w \\cdot x_i + b) \\geq +1$\n",
    "\n",
    "- Possible Cases:\n",
    "    - Correct Classification Outside the Margin: $y_i(w \\cdot x_i + b) \\geq +1)$\n",
    "        - No violation $\\xi_i = 0$\n",
    "    - Correct Classification Inside the Margin  $0 < y_i(w \\cdot x_i + b) < 1)$\n",
    "        - Margin violation occurs $\\xi_i > 0$\n",
    "    - Misclassified Point  $ y_i(w \\cdot x_i + b) < 0)$\n",
    "        - Severe violation $\\xi_i > 1$\n",
    "\n",
    "##### 2.2 Reasons for using `Regularization` in SVM\n",
    "Regularization in Support Vector Machines (SVMs) is crucial to ensure that the model generalizes well to unseen data. \n",
    "- Regularization introduces a penalty for overly complex models, preventing overfitting.\n",
    "- Regularization in SVM controls the trade-off between:\n",
    "    - Maximizing the margin: Ensuring the decision boundary is as far as possible from the nearest data points.\n",
    "    - Minimizing misclassification errors: Allowing some points to fall inside the margin or on the wrong side of the decision boundary for better generalization.\n",
    "\n",
    "Control Overfitting\n",
    "- Reason: SVM aims to maximize the margin between classes while minimizing misclassification errors. Without regularization, the model might try to perfectly classify the training data, resulting in overfitting.\n",
    "- Solution: Regularization balances the trade-off between achieving a larger margin (simpler model) and minimizing classification errors.\n",
    "    - A larger regularization parameter (C): penalizes misclassifications more heavily, potentially leading to overfitting.\n",
    "    - A smaller regularization parameter (C):  favors a larger margin and allows for more misclassifications, promoting generalization.\n",
    "\n",
    "Handle Noisy Data\n",
    "- Reason: Real-world datasets often contain noise, outliers, or mislabeled data points. Without regularization, SVM may overemphasize these noisy points, leading to a distorted decision boundary.\n",
    "- Solution: Regularization reduces the influence of such noisy points by allowing some tolerance for misclassification, leading to a more robust model.\n",
    "\n",
    "Promote Simpler Decision Boundaries\n",
    "- Reason: Complex decision boundaries can lead to poor generalization on new data.\n",
    "- Solution: Regularization encourages the SVM to find a simpler decision boundary by controlling the weight vector (w) through a regularization term in the objective function.\n",
    "\n",
    "Avoid Curse of Dimensionality\n",
    "- Reason: In high-dimensional spaces, the risk of overfitting increases because the model has more capacity to fit the training data perfectly.\n",
    "- Solution: Regularization reduces the model's flexibility, preventing overfitting in high-dimensional feature spaces.\n",
    "\n",
    "Improve Generalization Performance\n",
    "- Reason: A model that fits the training data too closely may fail to generalize to unseen data.\n",
    "- Solution: Regularization ensures that the SVM focuses on the most informative patterns in the data, improving performance on test data.\n",
    "\n",
    "Kernel Methods and Regularization\n",
    "- Reason: When using kernel functions (e.g., RBF, polynomial), the feature space is transformed into a higher dimension, increasing the model's capacity to overfit.\n",
    "- Solution: Regularization mitigates overfitting by constraining the optimization process, ensuring the model finds a balance between complexity and accuracy.\n",
    "\n",
    "##### **Type of regularization used in Soft Margin Classification for Support Vector Machines (SVMs)** \n",
    "Type used is L2 regularization.\n",
    "\n",
    "L2 Regularization in the Objective Function: \n",
    "$$Minimize_{w,b,\\xi}: \\frac{1}{2} ||w||^2 + C \\sum^n_{i=1} \\xi_i$$\n",
    "\n",
    "- Where:\n",
    "    - $\\frac{1}{2} ||w||^2$: represents L2 regularization, as it minimizes the squared Euclidean norm of the weight vector w. \n",
    "        - It helps in maximizing the margin by penalizing larger weight values, which results in a smoother and more generalizable decision boundary.\n",
    "    - $C \\sum^n_{i=1} \\xi_i$: his term penalizes margin violations (misclassification or points lying within the margin). \n",
    "        - The parameter C determines the penalty strength.\n",
    "\n",
    "Why L2 Regularization?\n",
    "- L2 regularization is chosen because:\n",
    "    - It encourages smaller weight magnitudes ($w_i^2), which leads to a more stable model less sensitive to noise in the data.\n",
    "    - It avoids overfitting by penalizing complex decision boundaries.\n",
    "    - The quadratic term $∥w∥^2$ ensures that the solution is smooth and generalizes well to unseen data.\n",
    "\n",
    "Mathematical Interpretation\n",
    "- The L2 regularization term $\\frac{1}{2} ||w||^2$ ensures that the weight vector w remains small, which effectively controls the model's complexity. \n",
    "    - Smaller weights correspond to a more stable and less overfitted model.\n",
    "\n",
    "Connection to Dual Formulation\n",
    "- In the dual formulation, the regularization parameter C indirectly limits the Lagrange multipliers $\\alpha_i$:\n",
    "\n",
    "$$ 0 \\leq \\alpha_i \\leq C $$\n",
    "\n",
    "This constraint ensures that the influence of each data point on the decision boundary is limited, balancing the trade-off between margin maximization and classification accuracy.\n",
    "\n",
    "##### 2.3 Hyperparameter C in soft margin classification\n",
    "The objective function for soft margin SVM is:\n",
    "\n",
    "$$Minimize_{w,b,\\xi}: \\frac{1}{2} ||w||^2 + C \\sum^n_{i=1} \\xi_i$$\n",
    "\n",
    "Hyperparameter C\n",
    "- Determines the trade-off between maximizing the margin (pathway width) and minimizing classification errors.\n",
    "- Determines how much weight is given to minimizing slack variables relative to maximizing the margin width.\n",
    "\n",
    "Trade-Off Parameter (C):\n",
    "- Governs the trade-off between the two components of the objective function:\n",
    "    - Large C: Focuses on minimizing misclassification, potentially at the cost of a smaller margin (risk of overfitting).\n",
    "        - Strongly penalizes misclassification.\n",
    "        - Results in a smaller margin as the model tries to classify every point correctly.\n",
    "        - May lead to overfitting, especially on noisy data.\n",
    "    - Small C: Focuses on maximizing the margin, tolerating some misclassifications (risk of underfitting).\n",
    "        - Allows more margin violations (misclassified points).\n",
    "        - Results in a larger margin and simpler decision boundary.\n",
    "        - Promotes better generalization, reducing the risk of overfitting.\n",
    "\n",
    "High C: Narrower Pathway Width\n",
    "- The penalty for margin violations ($C \\sum \\xi_i$) becomes significant.\n",
    "- The SVM prioritizes classifying training points correctly over maximizing the margin width.\n",
    "- The model becomes more sensitive to individual data points, which can lead to:\n",
    "    - A narrower pathway width (smaller margin).\n",
    "    - Overfitting, where the decision boundary conforms too closely to the training data.\n",
    "- Behavior:\n",
    "    - The margin shrinks to fit the data points tightly.\n",
    "    - Misclassified points are heavily penalized, so the model tries to minimize their number at the cost of a smaller margin.\n",
    "\n",
    "Low C: Wider Pathway Width\n",
    "- The penalty for margin violations becomes less significant.\n",
    "- The SVM focuses on maximizing the margin width, even if it means allowing some misclassified points.\n",
    "- The model becomes less sensitive to noise and outliers, leading to:\n",
    "    - A wider pathway width (larger margin).\n",
    "    - Better generalization to unseen data.\n",
    "- Behavior:\n",
    "    - The decision boundary prioritizes a larger margin over perfect classification.\n",
    "    - Misclassified points are tolerated, reducing the risk of overfitting.\n",
    "\n",
    "##### **Relationship Between C and Pathway Width**\n",
    "The pathway width (or margin width) is inversely related to C:\n",
    "- High C: Narrower pathway (small margin).\n",
    "- Low C: Wider pathway (large margin).\n",
    "\n",
    "This trade-off reflects the bias-variance trade-off:\n",
    "- High C: Low bias, high variance (more complex model).\n",
    "- Low C: High bias, low variance (simpler model).\n",
    "\n",
    "Practical Analysis of C and Pathway Width\n",
    "- To analyze the relationship between C and margin width:\n",
    "    - Train the SVM Model: Train models with different values of C.\n",
    "    - Visualize the Decision Boundary:\n",
    "        - Plot the decision boundary and margins for low, medium, and high C values.\n",
    "        - Observe how the margin width and boundary placement change.\n",
    "    - Evaluate Performance:\n",
    "        - On training data, high C often results in lower misclassification rates.\n",
    "        - On test data, low C often results in better generalization.\n",
    "\n",
    "Impact of C on Generalization\n",
    "- High C: The model prioritizes accuracy on the training data but risks overfitting due to a narrow margin.\n",
    "- Low C: The model sacrifices some accuracy on the training data but generalizes better due to a wider margin.\n",
    "\n",
    "##### **Dual Formulation of the Soft Margin Objective**\n",
    "Objective function for soft margin classification in Support Vector Machines (SVMs) allows for some misclassification or margin violations in the dataset. \n",
    "- This makes the model more robust to noisy and non-linearly separable data.\n",
    "\n",
    "The dual formulation is more computationally efficient for many datasets, especially when using kernels. It is expressed as:\n",
    "\n",
    "$$ max_{\\alpha} \\sum^{N}_{i = 1} \\alpha_i - \\frac{1}{2} \\sum^{N}_{i = 1} \\sum^{N}_{j = 1} \\alpha_i \\alpha_i y_i y_j K(x_i, x_j) $$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$ 0 \\leq \\alpha_i \\leq C, \\sum^{N}_{i = 1} \\alpha_i  y_i = 0 $$\n",
    "\n",
    "Where: \n",
    "- $\\alpha_i$: Lagrange multipliers.\n",
    "- $K(x_i, x_j)$: Kernel function, used for non-linear decision boundaries.\n",
    "- C: Controls the range of $\\alpha_i$, balancing margin width and classification error.\n",
    "\n",
    "##### **Components in Relation to m Features**\n",
    "The objective function in SVM for m-features balances the goals of maximizing the margin and minimizing misclassification through the weight vector w, bias b, and slack variables $\\xi_i$. Regularization, via C, plays a key role in ensuring that the model generalizes well to unseen data.\n",
    "\n",
    "- w: Weight vector of dimension m, one weight per feature, defines the hyperplane's orientation.\n",
    "- $𝑥_𝑖 \\in 𝑅^m$ Feature vectors in the m-dimensional space.\n",
    "- Kernal $K(x_i, x_j)$: Allows mapping of $𝑥_𝑖$ into a higher-dimensional feature space for non-linear separability, indirectly involving m.\n",
    "\n",
    "##### Intuition for m Features\n",
    "- The dimension m dictates the complexity of the weight vector w, which defines the separating hyperplane.\n",
    "- Larger m means a higher-dimensional feature space, potentially increasing the model's capacity but also the risk of overfitting.\n",
    "- Regularization (C) ensures that the optimization remains robust, even with a large number of features.\n",
    "\n",
    "### **Optimization**\n",
    "- To solve the SVM objective, quadratic programming methods or optimization algorithms (e.g., SMO—Sequential Minimal Optimization) are used. \n",
    "- For large datasets, kernels or approximate methods are often applied.\n",
    "\n",
    "##### **Tuning an SVM model**\n",
    "Use `sklearn`'s [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). \n",
    "- This procedure allows us to specify a set of possible parameters for a specific model.\n",
    "    - `GridSearchCV` will then go through those parameters and try every possible combination of them (kind of like it's working through a grid in a systematic way - that's where the name comes from). \n",
    "    - `GridSearchCV` will then return the combination of parameters that resulted in a model with the best score. \n",
    "    - `GridSearchCV` makes use of **cross validation**, helping to ensure the robustness of it's results.\n",
    "\n",
    "Grid search is a systematic method for hyperparameter optimization that evaluates a predefined set of hyperparameters for a machine learning model, such as an SVM.\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "C: Regularization parameter.\n",
    "- Higher C: Focuses on minimizing classification errors (lower margin, more overfitting).\n",
    "- Lower C: Allows more classification errors (larger margin, more underfitting).\n",
    "\n",
    "Kernel: Specifies the kernel function.\n",
    "- Linear: Best for linearly separable data.\n",
    "- Polynomial/RBF (Radial Basis Function): Handles nonlinear decision boundaries.\n",
    "\n",
    "Gamma: Used with RBF and polynomial kernels.\n",
    "- Controls the influence of a single training example.\n",
    "    - Lower values: More generalized decision boundaries.\n",
    "    - Higher values: Tighter fit around data points.\n",
    "\n",
    "Degree: Relevant for polynomial kernels, representing the polynomial degree.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Create a dictionary that contains the parameters you want to tune as `keys` and all the different options you want to test for those parameters as `values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel':('linear', 'rbf'), \n",
    "              'C':(0.25,1.0),\n",
    "              'gamma': (1,2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40da9c",
   "metadata": {},
   "source": [
    "2. Instantiate an SVC classifier and tell `GridSearchCV` to test it using the parameters we previously specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d3333",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "clf = GridSearchCV(svm, parameters)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b23a2b2",
   "metadata": {},
   "source": [
    "**Understanding the Output of Grid Search for SVM**\n",
    "\n",
    "1. Best Parameters (best_params_)\n",
    "\n",
    "This indicates the combination of hyperparameters that resulted in the best cross-validation score.\n",
    "\n",
    "    - {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
    "        - A regularization strength of C=10.\n",
    "        - A kernel function of Radial Basis Function (RBF).\n",
    "        - Gamma value of 0.1.\n",
    "\n",
    "Extract the Best Parameters:\n",
    "- Use grid_search.best_params_ to identify the best-performing combination.\n",
    "\n",
    "2. Best Score (best_score_)\n",
    "\n",
    "This is the highest cross-validation score achieved for the best parameter combination. It indicates how well the model generalized during validation.\n",
    "\n",
    "    - 0.93\n",
    "    - The best parameter combination resulted in 93% accuracy during cross-validation.\n",
    "\n",
    "Examine the Best Score:\n",
    "- Use grid_search.best_score_ to see the best validation accuracy achieved (e.g., 0.90).\n",
    "\n",
    "Make Predictions:\n",
    "- Use the grid_search.best_estimator_ to make predictions on new data.\n",
    "\n",
    "3. Complete Results (cv_results_)\n",
    "\n",
    "- A dictionary containing detailed results for all parameter combinations evaluated during grid search. Key fields:\n",
    "    - mean_test_score: Average cross-validation score for each parameter set.\n",
    "    - std_test_score: Standard deviation of scores across folds (indicates variability).\n",
    "    - params: Parameter combinations corresponding to the scores.\n",
    "\n",
    "            - {'mean_test_score': [0.91, 0.93, 0.89],\n",
    "            'std_test_score': [0.01, 0.02, 0.03],\n",
    "            'params': [{'C': 1, 'gamma': 0.1, 'kernel': 'rbf'},\n",
    "                        {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'},\n",
    "                        {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}]}\n",
    "\n",
    "            - The best score (0.93) corresponds to {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
    "            - The variability of scores (e.g., 0.02) reflects the model's consistency during cross-validation.\n",
    "\n",
    "4.  Best Estimator (best_estimator_)\n",
    "\n",
    "The trained SVM model with the best parameters. This can be used for predictions.\n",
    "\n",
    "    - SVC(C=10, gamma=0.1, kernel='rbf')\n",
    "\n",
    "##### How to Use the Grid Search Results\n",
    "- Best Parameters: Use `grid_search.best_params_` to train the final SVM model on the full training data for optimal performance.\n",
    "- Best Estimator: Use `grid_search.best_estimator_` directly for prediction.\n",
    "- Scoring and Ranking: The `mean_test_score in cv_results_` can be used to evaluate how different parameter combinations perform.\n",
    "- Variability: Use `std_test_score` to assess how consistent the model performance is across folds. Lower variability indicates a robust model.\n",
    "\n",
    "**GridSearch Output**\n",
    "\n",
    "    - GridSearchCV(cv=None, error_score=nan,\n",
    "                estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
    "                            class_weight=None, coef0=0.0,\n",
    "                            decision_function_shape='ovr', degree=3,\n",
    "                            gamma='scale', kernel='rbf', max_iter=-1,\n",
    "                            probability=False, random_state=None, shrinking=True,\n",
    "                            tol=0.001, verbose=False),\n",
    "                iid='deprecated', n_jobs=None,\n",
    "                param_grid={'C': (0.25, 1.0), 'gamma': (1, 2),\n",
    "                            'kernel': ('linear', 'rbf')},\n",
    "                pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
    "                scoring=None, verbose=0)\n",
    "\n",
    "1. cv=None:\n",
    "- By default, cv=None uses 5-fold cross-validation to evaluate the performance of each hyperparameter combination.\n",
    "\n",
    "2. error_score=nan:\n",
    "- Specifies what happens if a model fails during training. \n",
    "    - If set to nan, the model skips that combination and assigns a score of nan.\n",
    "\n",
    "3. estimator=SVC(...):\n",
    "- The base model being optimized, in this case, an SVM classifier (SVC).\n",
    "- The parameters within the SVC object (e.g., C=1.0, kernel='rbf') represent its default settings, which may be overridden by the grid search.\n",
    "\n",
    "4. param_grid={'C': (0.25, 1.0), 'gamma': (1, 2), 'kernel': ('linear', 'rbf')}:\n",
    "- The hyperparameter combinations being evaluated:\n",
    "    - C: Regularization parameter values [0.25,1.0].\n",
    "    - gamma: Kernel coefficient values [1,2].\n",
    "    - kernel: Kernel types [linear ,  rbf].\n",
    "- The grid search will test all possible combinations of these parameters (a total of 2×2×2=8 combinations).\n",
    "\n",
    "5. iid='deprecated':\n",
    "- Refers to the Independent Identically Distributed assumption, which has been deprecated in Scikit-learn 0.24 and later. \n",
    "    - It is safe to ignore this unless you’re using an older version of Scikit-learn.\n",
    "\n",
    "6. n_jobs=None:\n",
    "- Specifies the number of CPU cores to use for parallel computation. None means it will run in serial mode on a single core.\n",
    "\n",
    "7. pre_dispatch='2*n_jobs':\n",
    "- Controls the number of jobs that get dispatched during parallel computation. \n",
    "    - Since n_jobs=None, this has no effect.\n",
    "\n",
    "8. refit=True:\n",
    "- After finding the best hyperparameter combination, the grid search automatically refits the model on the entire training dataset using those parameters.\n",
    "\n",
    "9. return_train_score=False:\n",
    "- If True, the results would include training scores in addition to validation scores. \n",
    "- Here, it is False, so only validation scores are calculated.\n",
    "\n",
    "10. scoring=None:\n",
    "- Indicates that the default scoring metric for the estimator (e.g., accuracy for classification) is used.\n",
    "\n",
    "**This Configuration Means**\n",
    "\n",
    "The grid search is tuning an SVM classifier with:\n",
    "- Two values of C [0.25,1.0],\n",
    "- Two values of gamma [1,2], and\n",
    "- Two kernel types (linear and rbf).\n",
    "Each of these 2×2×2=8 combinations is evaluated using 5-fold cross-validation.\n",
    "\n",
    "The performance of each combination is assessed using the default scoring metric (accuracy for classification).\n",
    "\n",
    "The best-performing combination is automatically selected and refitted on the entire training dataset.\n",
    "\n",
    "### **Reasons for not using squared loss function in classification problems**\n",
    "\n",
    "1. It is sensitive to outliers.\n",
    "2. It does not align with the probabilistic interpretation of classification tasks.\n",
    "3. It fails to emphasize the separation of classes effectively.\n",
    "4. Alternatives like cross-entropy or hinge loss are better suited for optimizing classification models, focusing on class separation and meaningful probabilities.\n",
    "\n",
    "Non-robustness to Outliers\n",
    "- In Classification: Misclassified points, especially outliers, can disproportionately influence the decision boundary, leading to poor generalization.\n",
    "- Squared loss penalizes large errors quadratically, \n",
    "    - meaning that a few instances with large prediction errors can dominate the loss function.\n",
    "\n",
    "Misalignment with Classification Goals\n",
    "- Nature of Classification: Classification problems aim to predict discrete labels or probabilities for class membership, focusing on correctly separating classes.\n",
    "- Squared Loss Behavior: Squared loss minimizes the difference between predicted and true values. \n",
    "    - In classification, true labels are usually encoded as 0 or 1, and predictions outside [0,1] are meaningless probabilities. \n",
    "        - This can result in illogical outcomes for probabilities and suboptimal boundaries.\n",
    "\n",
    "Poor Handling of Probabilities\n",
    "- Probabilistic Interpretation: Classification models often interpret predictions as probabilities of class membership.\n",
    "- Squared Loss Issues: It does not naturally account for the probabilistic nature of classification. \n",
    "    - loss functions, like log-loss (cross-entropy), directly optimize for probability-based interpretations, ensuring that predictions align better with actual class probabilities.\n",
    "\n",
    " Inappropriate Gradients for Classification\n",
    "- Gradient Shape: Squared loss gradients are linear, meaning the gradient changes linearly with the error.\n",
    "- Impact: In classification problems, small classification errors might still produce significant gradients, leading to inefficient updates. \n",
    "    - loss functions like hinge loss or cross-entropy loss prioritize the misclassified or uncertain points more effectively, which aligns with the goal of improving class separation.\n",
    "\n",
    "Squared Loss Leads to Non-optimal Decision Boundaries\n",
    "- Decision Boundary Nature: In classification, the goal is to maximize the margin between classes or ensure good separation.\n",
    "- Squared Loss Focus: By trying to minimize the distance between predicted and true labels, squared loss tends to favor a compromise boundary, potentially leading to poorly separated classes, especially in non-linear classification problems.\n",
    "\n",
    "Better Alternatives Exist\n",
    "- Hinge Loss: Used in SVMs, it focuses on maximizing the margin and ensures only points near or across the boundary contribute to the loss.\n",
    "- Cross-Entropy Loss: Used in logistic regression and neural networks, it optimizes probabilities directly, aligning well with classification goals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a6bfa2",
   "metadata": {},
   "source": [
    "### Advantages of SVMs\n",
    "1. Effective in High-Dimensional Spaces\n",
    "-  SVMs perform well when the number of features is large relative to the number of observations.\n",
    "- Example: Applications in text classification or genomics, where the feature space is often very high-dimensional.\n",
    "\n",
    "2. Works Well with Clear Margins of Separation\n",
    "- SVM aims to find the optimal hyperplane that maximizes the margin between classes, which ensures robust classification when classes are well-separated.\n",
    "- Example: Binary classification tasks where the data is linearly separable.\n",
    "\n",
    "3. Kernel Trick for Non-linear Data\n",
    "- SVM uses the \"kernel trick\" to map non-linearly separable data into a higher-dimensional space where a linear separation is possible.\n",
    "- Example: Radial Basis Function (RBF) and polynomial kernels can handle complex decision boundaries.\n",
    "\n",
    "4. Regularization Through C Parameter\n",
    "- The regularization parameter C controls the trade-off between maximizing the margin and minimizing classification errors, making SVMs flexible to different types of data distributions.\n",
    "- Example: Adjusting C to avoid overfitting on small datasets or noisy data.\n",
    "\n",
    "5. Robust to Overfitting (with Proper Tuning)\n",
    "- By controlling the margin size and kernel functions, SVMs can generalize well, especially for small datasets.\n",
    "- Example: SVMs perform better than other models when data has limited examples but a high feature count.\n",
    "\n",
    "6. Effective for Outlier Detection\n",
    "- SVM variants, such as one-class SVM, are used to detect anomalies by learning the boundaries of the majority class.\n",
    "- Example: Fraud detection or network intrusion detection.\n",
    "\n",
    "### Disadvantages of SVMs\n",
    "1. High Computational Cost\n",
    "- SVM training involves solving a convex optimization problem, which can become computationally expensive for large datasets.\n",
    "- Example: For datasets with millions of samples, training can be significantly slower compared to models like logistic regression or decision trees.\n",
    "\n",
    "2. Sensitive to Choice of Kernel\n",
    "- The performance of SVM heavily depends on the choice of kernel function and its parameters (e.g., RBF kernel with parameters $𝛾$ and C).\n",
    "- Example: Incorrect kernel choice may lead to poor performance or overfitting.\n",
    "\n",
    "3. Inefficient for Large Datasets\n",
    "- The complexity of SVMs scales with the size of the dataset ($O(n^2$) to ($O(n^3$), making it less suitable for massive datasets.\n",
    "- Example: SVM may struggle with datasets containing millions of instances compared to neural networks or gradient-boosted trees.\n",
    "\n",
    "4. Difficulty Handling Noisy Data\n",
    "- SVMs try to maximize the margin and are sensitive to mislabeled data points, which can shift the decision boundary significantly.\n",
    "- Example: In datasets with a high degree of label noise, SVMs may underperform compared to models with robust loss functions.\n",
    "\n",
    "5. Lack of Probabilistic Output\n",
    "- SVMs do not naturally provide probabilities for predictions. While this can be approximated using Platt scaling or cross-validation, the results are not as interpretable as probabilistic models.\n",
    "- Example: Logistic regression offers direct probabilities, which are more useful in some applications, like medical diagnosis.\n",
    "\n",
    "6. Hyperparameter Tuning is Non-trivial\n",
    "- Choosing the right values for hyperparameters like C, $𝛾$, and the kernel function often requires extensive grid search or cross-validation.\n",
    "- Example: Poorly tuned parameters can lead to overfitting or underfitting, requiring careful experimentation.\n",
    "\n",
    "7. Not Easily Scalable for Multiclass Problems\n",
    "- SVMs are inherently binary classifiers. \n",
    "- For multiclass classification, strategies like \n",
    "    - one-vs-rest (OVR) or \n",
    "    - one-vs-one (OVO) must be used, adding complexity and computational cost.\n",
    "- Example: For 10 classes, OVO requires 10×(10−1)/2=45 classifiers to be trained.\n",
    "\n",
    "When to Use SVMs\n",
    "- Best Use Cases:\n",
    "    - High-dimensional datasets with clear margins of separation.\n",
    "    - Small-to-medium-sized datasets with complex decision boundaries.\n",
    "    - Applications where interpretability of the decision boundary is important (e.g., feature weights in a linear kernel).\n",
    "- Not Ideal For:\n",
    "    - Large datasets due to computational cost.\n",
    "    - Noisy datasets where robust models like random forests or neural networks might outperform.\n",
    "    - Problems requiring probabilistic outputs or interpretable probabilities.\n",
    "\n",
    "##### Five common use cases that require probabilistic outputs or interpretable probabilities that SVM poorly performs to.\n",
    "\n",
    "Key Challenges with SVMs in Probabilistic Scenarios\n",
    "- Calibration Issues: SVM probabilities (from methods like Platt Scaling) are often less reliable than probabilities from inherently probabilistic models.\n",
    "- Interpretability: Decision boundaries and margins are not intuitive for users who need to interpret confidence levels.\n",
    "- Actionable Insights: Many use cases (e.g., credit scoring, fraud detection) require actionable thresholds or prioritization, which hinge on well-calibrated probabilities.\n",
    "\n",
    "Medical Diagnosis\n",
    "- Why Probabilities are Needed: In medical applications, probabilistic outputs help determine the likelihood of a disease or condition, allowing practitioners to weigh risks and make informed decisions.\n",
    "    - Example: Predicting whether a patient has cancer with a 90% probability versus 55%.\n",
    "- Why SVM Fails: SVM outputs are distances from the decision boundary, which don’t naturally translate to probabilities. While calibration techniques like Platt Scaling can convert these into probabilities, they often yield less reliable and less interpretable probabilities than models like logistic regression.\n",
    "\n",
    "Fraud Detection\n",
    "- Why Probabilities are Needed: In fraud detection, probabilities allow for setting thresholds based on the acceptable level of risk. For instance, transactions with a probability of fraud >95% may trigger an immediate block, while transactions with 60%-80% may require manual review.\n",
    "    - Example: Flagging fraudulent transactions on an e-commerce platform.\n",
    "- Why SVM Fails: SVMs don’t inherently provide probabilities for these thresholds, making it difficult to prioritize actions based on the confidence level of predictions. This lack of interpretability can lead to either overreaction (blocking too many transactions) or underreaction.\n",
    "\n",
    "Customer Churn Prediction\n",
    "- Why Probabilities are Needed: Businesses use churn probability to allocate resources effectively, targeting high-probability churners with retention offers. Probabilities help prioritize interventions.\n",
    "    - Example: Predicting that a customer has a 70% chance of leaving allows the company to offer personalized discounts or incentives.\n",
    "- Why SVM Fails: SVM’s non-probabilistic nature makes it hard to prioritize customers effectively. In contrast, logistic regression or gradient boosting models provide reliable churn probabilities, directly guiding resource allocation.\n",
    "\n",
    "Marketing Campaign Effectiveness\n",
    "- Why Probabilities are Needed: Campaign optimization often relies on the likelihood of conversion or engagement. For example, targeting customers with an 80% chance of responding to an ad is more efficient than targeting those with only 20%.\n",
    "    - Example: Predicting the probability that a customer will click on an ad or make a purchase.\n",
    "- Why SVM Fails: SVM outputs distances, not probabilities, making it harder to assign confidence levels to predictions. This lack of probabilistic output complicates the ranking of prospects for targeted campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe25b8cd",
   "metadata": {},
   "source": [
    "# 7. Clustering Models (e.g., K-Means)\n",
    "What It Means: \n",
    "- Clustering groups similar data points together without predefined labels, often used for segmenting customers or finding patterns.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each cluster represents a natural grouping in the data, with data points in the same cluster sharing similar characteristics.\n",
    "\n",
    "Performance Measures:\n",
    "- Silhouette Score: Measures how well each point fits within its cluster; values closer to 1 indicate better-defined clusters.\n",
    "- Within-Cluster Sum of Squares (WCSS): Measures the compactness of clusters; lower values are better.\n",
    "\n",
    "Lay Explanation: \n",
    "- Clustering is like sorting items into bins based on similarity, helping us identify groups in our data.\n",
    "\n",
    "Use Case: \n",
    "- To group similar observations without predefined labels.\n",
    "\n",
    "Model Types: \n",
    "- K-Means, \n",
    "- Hierarchical Clustering, \n",
    "- DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3cfc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "clusters = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea1408",
   "metadata": {},
   "source": [
    "##### 8. Principal Component Analysis (PCA)\n",
    "What It Means: \n",
    "- PCA reduces the number of variables in the data by finding combinations of variables that capture the most information (variance).\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each \"principal component\" explains a percentage of the total variance, helping simplify the data without losing much information.\n",
    "\n",
    "Performance Measures:\n",
    "- Explained Variance Ratio: Shows how much information each principal component holds; higher is better.\n",
    "\n",
    "Lay Explanation: \n",
    "- PCA is like summarizing a book by keeping only the most important points, making data easier to work with without losing key insights.\n",
    "\n",
    "Use Case: \n",
    "- Dimensionality reduction while retaining the most critical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23bf1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6d396",
   "metadata": {},
   "source": [
    "##### 9. Bayesian Models\n",
    "What It Means: \n",
    "- Bayesian models incorporate prior knowledge or beliefs with the data to update the probability of outcomes as new evidence is available.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each output is a probability distribution reflecting both prior knowledge and the new data, offering a range of likely outcomes.\n",
    "\n",
    "Performance Measures:\n",
    "- Log-Likelihood: Measures how well the model explains the data; higher values indicate better fit.\n",
    "\n",
    "Lay Explanation: \n",
    "- Bayesian models are like revising a guess based on new evidence—updating beliefs as we get more information.\n",
    "\n",
    "Use Case: \n",
    "- To incorporate prior knowledge and quantify uncertainty.\n",
    "\n",
    "Model Types: \n",
    "- Bayesian Linear Regression, \n",
    "- Bayesian Networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6500107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "\n",
    "with pm.Model() as model:\n",
    "    alpha = pm.Normal('alpha', mu=0, sigma=1)\n",
    "    beta = pm.Normal('beta', mu=0, sigma=1, shape=len(X_train.columns))\n",
    "    epsilon = pm.HalfNormal('epsilon', sigma=1)\n",
    "    mu = alpha + pm.math.dot(X_train, beta)\n",
    "    y_pred = pm.Normal('y_pred', mu=mu, sigma=epsilon, observed=y_train)\n",
    "    trace = pm.sample(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac011c6c",
   "metadata": {},
   "source": [
    "##### 10. Survival Analysis (e.g., Cox Proportional Hazards)\n",
    "What It Means: \n",
    "- Survival analysis predicts the time until an event occurs, such as customer churn or equipment failure.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each output shows the likelihood of the event happening over time, considering various risk factors.\n",
    "\n",
    "Performance Measures:\n",
    "- Concordance Index (C-Index): Measures the model’s ability to correctly rank predictions; values closer to 1 indicate better performance.\n",
    "\n",
    "Lay Explanation: \n",
    "Survival analysis is like tracking how long something will last, based on factors that might speed it up or slow it down.\n",
    "\n",
    "Use Case: \n",
    "- For time-to-event data, such as time until a customer churns or equipment fails.\n",
    "\n",
    "Model Types: \n",
    "- Kaplan-Meier estimator, Cox Proportional Hazards Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995ebf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(data, 'time', event_col='event')\n",
    "cph.predict_survival_function(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b4e50",
   "metadata": {},
   "source": [
    "##### 4. Time Series Models (e.g., ARIMA)\n",
    "What It Means: \n",
    "- Time series models account for:\n",
    "    - trends, \n",
    "    - seasonality, and \n",
    "    - temporal dependencies in data collected over time, often used for forecasting future values.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each prediction is based on patterns in past data points, accounting for recent trends and cycles.\n",
    "\n",
    "Performance Measures:\n",
    "- Mean Absolute Percentage Error (MAPE): Shows the average prediction error in percentage terms.\n",
    "- Root Mean Squared Error (RMSE): Measures the prediction accuracy; lower values mean better predictions.\n",
    "\n",
    "Lay Explanation: \n",
    "- Time series models are like weather forecasts—they predict future values based on past patterns, like trends and cycles.\n",
    "\n",
    "Use Case: \n",
    "- Forecasting for data with a temporal component (e.g., sales data, stock prices).\n",
    "\n",
    "Model Types: \n",
    "- ARIMA, \n",
    "- SARIMA, \n",
    "- Exponential Smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e901a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "model = ARIMA(time_series_data, order=(1,1,1))\n",
    "model_fit = model.fit()\n",
    "predictions = model_fit.forecast(steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc25f7",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf07849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to compute True Positives, True Negatives, False Positives and False Negatives\n",
    "\n",
    "def true_positive(y_true, y_pred):\n",
    "    tp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 1:\n",
    "            tp += 1\n",
    "    return tp\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    tn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 0:\n",
    "            tn += 1        \n",
    "    return tn\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    fp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 1:\n",
    "            fp += 1       \n",
    "    return fp\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    fn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 0:\n",
    "            fn += 1        \n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea735dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "TP = np.diag(cnf_matrix)\n",
    "TN = cnf_matrix.sum() - (FP + FN + TP)FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy for each class\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf443ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation for table metrics:\n",
    "import sklearn.metrics\n",
    "import mathdef matrix_metrix(real_values,pred_values,beta):\n",
    "CM = confusion_matrix(real_values,pred_values)\n",
    "TN = CM[0][0]\n",
    "FN = CM[1][0] \n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "Population = TN+FN+TP+FP\n",
    "Prevalence = round( (TP+FP) / Population,2)\n",
    "Accuracy   = round( (TP+TN) / Population,4)\n",
    "Precision  = round( TP / (TP+FP),4 )\n",
    "NPV        = round( TN / (TN+FN),4 )\n",
    "FDR        = round( FP / (TP+FP),4 )\n",
    "FOR        = round( FN / (TN+FN),4 ) \n",
    "check_Pos  = Precision + FDR\n",
    "check_Neg  = NPV + FOR\n",
    "Recall     = round( TP / (TP+FN),4 )\n",
    "FPR        = round( FP / (TN+FP),4 )\n",
    "FNR        = round( FN / (TP+FN),4 )\n",
    "TNR        = round( TN / (TN+FP),4 ) \n",
    "check_Pos2 = Recall + FNR\n",
    "check_Neg2 = FPR + TNR\n",
    "LRPos      = round( Recall/FPR,4 ) \n",
    "LRNeg      = round( FNR / TNR ,4 )\n",
    "DOR        = round( LRPos/LRNeg)\n",
    "F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)\n",
    "FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "BM         = Recall+TNR-1\n",
    "MK         = Precision+NPV-1   \n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos','check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','F1','FBeta','MCC','BM','MK'],     \n",
    "                        'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,F1,FBeta,MCC,BM,MK]})   \n",
    "\n",
    "return (mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Implementation\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplotfpr, tpr, thresholds = roc_curve(real_values, prob_values)\n",
    "\n",
    "auc = roc_auc_score(real_values, prob_values)\n",
    "print('AUC: %.3f' % auc)pyplot.plot(fpr, tpr, linestyle='--', label='Roc curve')\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()pyplot.show()\n",
    "\n",
    "# Precision-recall implementation\n",
    "\n",
    "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(real_values,prob_values)pyplot.plot(recall, precision, linestyle='--', label='Precision versus Recall')\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "pyplot.legend()pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba51b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for get many metrics directly from sklearn\n",
    "\n",
    "def sk_metrix(real_values,pred_values,beta):\n",
    "Accuracy = round( sklearn.metrics.accuracy_score(real_values,pred_values) ,4)\n",
    "Precision= round( sklearn.metrics.precision_score(real_values,pred_values),4 )\n",
    "Recall   = round( sklearn.metrics.recall_score(real_values,pred_values),4 )   \n",
    "F1       = round ( sklearn.metrics.f1_score(real_values,pred_values),4)\n",
    "FBeta    = round ( sklearn.metrics.fbeta_score(real_values,pred_values,beta) ,4)\n",
    "MCC      = round ( sklearn.metrics.matthews_corrcoef(real_values,pred_values)  ,4)   \n",
    "Hamming  = round ( sklearn.metrics.hamming_loss(real_values,pred_values) ,4)   \n",
    "Jaccard  = round ( sklearn.metrics.jaccard_score(real_values,pred_values) ,4)   \n",
    "Prec_Avg = round ( sklearn.metrics.average_precision_score(real_values,pred_values) ,4)   \n",
    "Accu_Avg = round ( sklearn.metrics.balanced_accuracy_score(real_values,pred_values) ,4)   \n",
    "\n",
    "mat_met = pd.DataFrame({\n",
    "'Metric': ['Accuracy','Precision','Recall','F1','FBeta','MCC','Hamming','Jaccard','Precision_Avg','Accuracy_Avg'],\n",
    "'Value': [Accuracy,Precision,Recall,F1,FBeta,MCC,Hamming,Jaccard,Prec_Avg,Accu_Avg]})   \n",
    "\n",
    "return (mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics For Multi-class Classification\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate accuracy\n",
    "    -> param y_true: list of true values\n",
    "    -> param y_pred: list of predicted values\n",
    "    -> return: accuracy score\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "# Intitializing variable to store count of correctly predicted classes\n",
    "    correct_predictions = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == yp:\n",
    "            correct_predictions += 1\n",
    "    #returns accuracy\n",
    "    return correct_predictions / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eeb9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of macro-averaged precision\n",
    "\n",
    "def macro_precision(y_true, y_pred):\n",
    "\n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize precision to 0\n",
    "    precision = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(y_true.unique()):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false positive for current class\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute precision for current class\n",
    "        temp_precision = tp / (tp + fp + 1e-6)\n",
    "        # keep adding precision for all classes\n",
    "        precision += temp_precision\n",
    "        \n",
    "    # calculate and return average precision over all classes\n",
    "    precision /= num_classes\n",
    "    \n",
    "    return precision\n",
    "\n",
    "print(f\"Macro-averaged Precision score : {macro_precision(y_test, y_pred) }\")\n",
    "\n",
    "# implement marco-averaged precision using sklearn\n",
    "macro_averaged_precision = metrics.precision_score(y_test, y_pred, average = 'macro')\n",
    "print(f\"Macro-Averaged Precision score using sklearn library : {macro_averaged_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of micro-averaged precision\n",
    "\n",
    "def micro_precision(y_true, y_pred):\n",
    "\n",
    "\n",
    "    # find the number of classes \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in y_true.unique():\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate true positive for current class\n",
    "        # and update overall tp\n",
    "        tp += true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate false positive for current class\n",
    "        # and update overall tp\n",
    "        fp += false_positive(temp_true, temp_pred)\n",
    "        \n",
    "    # calculate and return overall precision\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision\n",
    "\n",
    "print(f\"Micro-averaged Precision score : {micro_precision(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "#  implement mirco-averaged precision using sklearn\n",
    "micro_averaged_precision = metrics.precision_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged Precision score using sklearn library : {micro_averaged_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ed0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of macro-averaged recall\n",
    "\n",
    "def macro_recall(y_true, y_pred):\n",
    "\n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize recall to 0\n",
    "    recall = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(y_true.unique()):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false negative for current class\n",
    "        fn = false_negative(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute recall for current class\n",
    "        temp_recall = tp / (tp + fn + 1e-6)\n",
    "        \n",
    "        # keep adding recall for all classes\n",
    "        recall += temp_recall\n",
    "        \n",
    "    # calculate and return average recall over all classes\n",
    "    recall /= num_classes\n",
    "    \n",
    "    return recall\n",
    "\n",
    "print(f\"Macro-averaged recall score : {macro_recall(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "# implement macro-averaged recall using sklearn\n",
    "\n",
    "macro_averaged_recall = metrics.recall_score(y_test, y_pred, average = 'macro')\n",
    "print(f\"Macro-averaged recall score using sklearn : {macro_averaged_recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd95acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of micro-averaged recall\n",
    "\n",
    "def micro_recall(y_true, y_pred):\n",
    "\n",
    "\n",
    "    # find the number of classes \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in y_true.unique():\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate true positive for current class\n",
    "        # and update overall tp\n",
    "        tp += true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate false negative for current class\n",
    "        # and update overall tp\n",
    "        fn += false_negative(temp_true, temp_pred)\n",
    "        \n",
    "    # calculate and return overall recall\n",
    "    recall = tp / (tp + fn)\n",
    "    return recall\n",
    "\n",
    "print(f\"Micro-averaged recall score : {micro_recall(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "#  implement micro-averaged recall using sklearn\n",
    "\n",
    "micro_averaged_recall = metrics.recall_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged recall score using sklearn library : {micro_averaged_recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d50779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of macro-averaged f1 score\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "\n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize f1 to 0\n",
    "    f1 = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(y_true.unique()):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false negative for current class\n",
    "        fn = false_negative(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false positive for current class\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute recall for current class\n",
    "        temp_recall = tp / (tp + fn + 1e-6)\n",
    "        \n",
    "        # compute precision for current class\n",
    "        temp_precision = tp / (tp + fp + 1e-6)\n",
    "        \n",
    "        \n",
    "        temp_f1 = 2 * temp_precision * temp_recall / (temp_precision + temp_recall + 1e-6)\n",
    "        \n",
    "        # keep adding f1 score for all classes\n",
    "        f1 += temp_f1\n",
    "        \n",
    "    # calculate and return average f1 score over all classes\n",
    "    f1 /= num_classes\n",
    "    \n",
    "    return f1\n",
    "\n",
    "\n",
    "print(f\"Macro-averaged f1 score : {macro_f1(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "# implement macro-averaged F1 score using sklearn\n",
    "\n",
    "macro_averaged_f1 = metrics.f1_score(y_test, y_pred, average = 'macro')\n",
    "print(f\"Macro-Averaged F1 score using sklearn library : {macro_averaged_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of micro-averaged fi score\n",
    "\n",
    "def micro_f1(y_true, y_pred):\n",
    "\n",
    "\n",
    "    #micro-averaged precision score\n",
    "    P = micro_precision(y_true, y_pred)\n",
    "\n",
    "    #micro-averaged recall score\n",
    "    R = micro_recall(y_true, y_pred)\n",
    "\n",
    "    #micro averaged f1 score\n",
    "    f1 = 2*P*R / (P + R)    \n",
    "\n",
    "    return f1\n",
    "\n",
    "print(f\"Micro-averaged recall score : {micro_f1(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "# implement micro-averaged F1 score using sklearn\n",
    "\n",
    "micro_averaged_f1 = metrics.f1_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged F1 score using sklearn library : {micro_averaged_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe51cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC AUCurve Computation\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def roc_auc_score_multiclass(actual_class, pred_class, average = \"macro\"):\n",
    "    \n",
    "    #creating a set of all the unique classes using the actual class list\n",
    "    unique_class = set(actual_class)\n",
    "    roc_auc_dict = {}\n",
    "    for per_class in unique_class:\n",
    "        \n",
    "        #creating a list of all the classes except the current class \n",
    "        other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "        #marking the current class as 1 and all other classes as 0\n",
    "        new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "        new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "\n",
    "        #using the sklearn metrics method to calculate the roc_auc_score\n",
    "        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n",
    "        roc_auc_dict[per_class] = roc_auc\n",
    "\n",
    "    return roc_auc_dict\n",
    "\n",
    "roc_auc_dict = roc_auc_score_multiclass(y_test, y_pred)\n",
    "roc_auc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC implementation: \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from itertools import cycle\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Load the iris data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target# Binarize the output\n",
    "y_bin = label_binarize(y, classes=[0, 1, 2])\n",
    "n_classes = y_bin.shape[1]# We split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size= 0.5, random_state=0)\n",
    "\n",
    "\n",
    "# We define the model as an SVC in OneVsRestClassifier setting.\n",
    "# this means that the model will be used for class 1 vs class 2, \n",
    "# class 2vs class 3 and class 1 vs class 3. \n",
    "# So, we have 3 cases at #the end and within each case, the bias will be varied in order to \n",
    "# Get the ROC curve of the given case - 3 ROC curves as output.\n",
    "\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=0))\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "# Plotting and estimation of FPR, TPR\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "colors = cycle(['blue', 'red', 'green'])\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=1.5, label='ROC curve of class {0} (area = {1:0.2f})' ''.format(i+1, roc_auc[i]))\n",
    "    plt.plot([0, 1], [0, 1], 'k-', lw=1.5)\n",
    "    plt.xlim([-0.05, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic for multi-class data')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69585b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
