{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb638b74",
   "metadata": {},
   "source": [
    "# Statisitcs and Research methods\n",
    "\n",
    "## Definition of Instances in Machine Learning\n",
    "An instance (also called an example, observation, or sample) refers to a single data point in a dataset. Each instance represents an individual record with multiple features (variables) that the model uses for learning and prediction.\n",
    "\n",
    "## Advantages of Replacing human decision-making with machines\n",
    "Consistency & Objectivity\n",
    "- Machines follow predefined rules and algorithms, reducing biases, emotions, or personal preferences that can affect human decisions.\n",
    "\n",
    "Efficiency & Speed\n",
    "- Automated decision-making is significantly faster than human processing, especially in large-scale data-driven applications like finance, healthcare, and logistics.\n",
    "\n",
    "Scalability\n",
    "- Machines can handle vast amounts of data and make thousands or millions of decisions simultaneously, which is impossible for humans.\n",
    "\n",
    "Reduction in Human Error \n",
    "- Machines don’t suffer from fatigue, stress, or cognitive overload, reducing the likelihood of mistakes in decision-making.\n",
    "\n",
    "Data-Driven Insights \n",
    "- Machine learning and AI can analyze complex patterns and relationships in data that humans might overlook, leading to more informed and evidence-based decisions.\n",
    "\n",
    "Cost Savings \n",
    "- Automating decisions can reduce labor costs and the need for human oversight in routine or repetitive tasks.\n",
    "\n",
    "24/7 Availability \n",
    "- Unlike humans, machines don’t need rest, enabling continuous operations in industries such as customer service, healthcare, and security.\n",
    "\n",
    "Fairness in Some Contexts \n",
    "- When properly designed, AI systems can eliminate certain human biases and discrimination in decision-making, such as in hiring or loan approvals.\n",
    "\n",
    "Handling Complexity \n",
    "- Machines can process multidimensional data and complex scenarios beyond human cognitive limits, leading to better optimization and strategic planning.\n",
    "\n",
    "Improved Safety \n",
    "- In high-risk industries (e.g., autonomous vehicles, industrial automation, medical diagnostics), machine-based decision-making can minimize accidents and improve safety outcomes.\n",
    "\n",
    "## Understanding Statistical Models vs. Machine Learning Models\n",
    "\n",
    "It's essential first to understand the distinctions between statistical models and machine learning models, as they serve different purposes, assumptions, and interpretative depth.\n",
    "\n",
    "- Statistical Models: \n",
    "    - These are rooted in traditional statistics and \n",
    "        - focus on relationships between variables through predefined equations. \n",
    "    - Statistical models aim to understand the underlying data-generating process, focusing on hypothesis testing and inference. \n",
    "    - These models often rely on strong assumptions like:\n",
    "        - linearity, \n",
    "        - normality, and \n",
    "        - homoscedasticity \n",
    "        - and are **interpretable**, making it easier to understand the impact of individual variables.\n",
    "\n",
    "- Machine Learning Models: \n",
    "    - These prioritize **predictive** power over interpretability. \n",
    "    - They are designed to automatically learn patterns and relationships within data, often with minimal assumptions. \n",
    "    - Machine learning models can handle complex and high-dimensional data but may lack transparency about how individual features affect the outcome, especially in “black box” models like neural networks or ensemble methods.\n",
    "\n",
    "| Aspect | Machine Learning   | Statistics            |\n",
    "|---------------|--------------------|----------------------------------|  \n",
    "|Goal\t|Prediction and pattern recognition, often focusing on optimizing performance on unseen data. |\tUnderstanding relationships between variables, inference, and hypothesis testing.|\n",
    "|Approach\t| Data-driven; models learn patterns from data without predefined assumptions. | Model-driven; relies on predefined mathematical models and assumptions about data. |\n",
    "|Focus\t|Generalization to new data (predictive power). |\tExplanation and inference (understanding relationships).|\n",
    "|Assumptions\t|Often makes fewer explicit assumptions about data distributions (e.g., neural networks, decision trees). |\tRelies on assumptions like normality, independence, and linearity (e.g., regression models).|\n",
    "|Data Needs\t|Often requires large datasets to perform well. |\tCan work well with smaller datasets when assumptions hold.|\n",
    "|Model Complexity |\tCan involve highly complex models (e.g., deep learning, random forests) with many parameters. |\tOften relies on simpler, interpretable models like linear regression or logistic regression..|\n",
    "|Interpretability\t|Many ML models (e.g., neural networks, ensemble models) are black boxes with limited interpretability. |\tStatistical models are generally interpretable, with well-defined coefficients and p-values.|\n",
    "|Use of Probability\t|Probability is often implicit (e.g., confidence scores in classification). |\tProbability theory is fundamental, with formal probability distributions and significance testing.|\n",
    "|Evaluation Metrics\t| \tFocuses on prediction accuracy (e.g., RMSE, F1-score, log-loss, AUC-ROC). |\tFocuses on statistical significance (e.g., p-values, confidence intervals, hypothesis tests).|\n",
    "|Handling of Bias & Variance\t| Uses techniques like cross-validation, regularization, and ensembling to balance bias-variance trade-off. |\tUses theoretical methods like maximum likelihood estimation and Bayesian inference.|\n",
    "|Common Applications| Image recognition, fraud detection, recommendation systems, NLP, autonomous systems. |\tMedical studies, economics, market research, A/B testing, survey analysis.|\n",
    "\n",
    "## Data Mining\n",
    "Data mining is the process of discovering patterns, correlations, trends, or useful information from large sets of data using statistical, machine learning, and computational techniques. It transforms raw data into meaningful insights for decision-making.\n",
    "\n",
    "###  Core Steps in Data Mining\n",
    "- Data Cleaning – Removing noise or irrelevant data.\n",
    "- Data Integration – Combining data from multiple sources.\n",
    "- Data Selection – Choosing relevant data for analysis.\n",
    "- Data Transformation – Converting data into appropriate formats.\n",
    "- Data Mining – Applying algorithms to extract patterns.\n",
    "- Pattern Evaluation – Identifying truly interesting patterns.\n",
    "- Knowledge Representation – Visualizing the mined data.\n",
    "\n",
    "Applications in Different Industries\n",
    "1. Retail & E-commerce\n",
    "    - Market Basket Analysis: Identifies products frequently bought together (e.g., Amazon's \"Customers who bought this also bought…\").\n",
    "    - Customer Segmentation: Target marketing strategies based on purchasing behavior.\n",
    "    - Inventory Management: Forecasting demand to optimize stock levels.\n",
    "\n",
    "2. Banking & Finance\n",
    "    - Fraud Detection: Uncover suspicious transactions using anomaly detection.\n",
    "    - Credit Scoring: Predicting creditworthiness of customers.\n",
    "    - Algorithmic Trading: Analyzing market patterns for automated trading.\n",
    "\n",
    "Finance\n",
    "- Fraud Detection\n",
    "    - How: Use classification algorithms (e.g., Decision Trees, Neural Networks) to detect unusual transaction patterns.\n",
    "    - Example: A credit card company flags a transaction in another country right after a local purchase, indicating possible fraud.\n",
    "- Credit Risk Assessment\n",
    "    - How: Apply logistic regression or machine learning models to historical customer data (income, debt, payment history).\n",
    "    - Example: A bank evaluates a loan applicant's likelihood of default based on mined patterns from past defaulters.\n",
    "\n",
    "Marketing\n",
    "- Market Basket Analysis\n",
    "    - How: Use association rule mining (e.g., Apriori algorithm) to find product combinations frequently purchased together.\n",
    "    - Example: A grocery store finds that people who buy diapers often buy baby wipes and adjusts shelf placement and promotions accordingly.\n",
    "- Campaign Response Modeling\n",
    "    - How: Use classification or uplift models to predict which customers are likely to respond to marketing campaigns.\n",
    "    - Example: An online retailer sends discount codes only to customers who are predicted to convert, improving ROI.\n",
    "\n",
    "Customer Relationship Management (CRM)\n",
    "- Customer Segmentation\n",
    "    - How: Use clustering techniques like K-means to group customers by behavior, preferences, or demographics.\n",
    "    - Example: A telecom company segments users into heavy data users vs. voice-call users to tailor service plans.\n",
    "- Churn Prediction\n",
    "    - How: Use predictive models to identify customers likely to leave based on usage patterns, complaints, or support interactions.\n",
    "    - Example: A streaming service targets users showing signs of disengagement with personalized recommendations to retain them.\n",
    "\n",
    "3. Healthcare\n",
    "    - Disease Prediction & Diagnosis: Identifying potential diseases based on symptoms and historical data.\n",
    "    - Patient Profiling: Personalizing treatment plans using patient history.\n",
    "    - Drug Discovery: Analyzing clinical trials and genomic data to discover new drugs.\n",
    "\n",
    "4. Telecommunications\n",
    "    - Churn Prediction: Identifying customers likely to leave and creating retention strategies.\n",
    "    - Network Optimization: Improving service quality by analyzing usage patterns.\n",
    "\n",
    "5. Manufacturing\n",
    "    - Predictive Maintenance: Preventing equipment failure by analyzing sensor data.\n",
    "    - Quality Control: Detecting anomalies in product quality during production.\n",
    "\n",
    "6. Education\n",
    "    - Student Performance Prediction: Identifying at-risk students for early intervention.\n",
    "    - Curriculum Personalization: Recommending personalized learning paths.\n",
    "\n",
    "7. Government & Public Sector\n",
    "    - Crime Analysis: Detecting crime hotspots and patterns.\n",
    "    - Tax Fraud Detection: Spotting inconsistencies in tax data.\n",
    "    - Policy Making: Analyzing census and survey data for informed decisions.\n",
    "\n",
    "## Data Science\n",
    "Data Science is a multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.\n",
    "\n",
    "It combines elements of:\n",
    "- Statistics & Mathematics – for analysis and inference\n",
    "- Computer Science – for data processing and algorithm development\n",
    "- Domain Knowledge – for interpreting results in context\n",
    "- Machine Learning & AI – for building predictive and intelligent systems\n",
    "- Data Engineering – for managing and transforming data at scale\n",
    "- Visualization – for communicating insights effectively\n",
    "\n",
    "🔄 Core Workflow of Data Science\n",
    "- Problem Definition – Understanding the business question.\n",
    "- Data Collection – Gathering data from various sources (databases, APIs, web scraping, sensors, etc.).\n",
    "- Data Cleaning & Preprocessing – Handling missing data, duplicates, and inconsistent formatting.\n",
    "- Exploratory Data Analysis (EDA) – Summarizing main characteristics using plots and statistics.\n",
    "- Feature Engineering – Creating or selecting relevant variables for modeling.\n",
    "- Model Building – Using statistical models or machine learning algorithms.\n",
    "- Model Evaluation – Checking performance using metrics like accuracy, RMSE, AUC, etc.\n",
    "- Deployment – Making models available in production (e.g., web apps, APIs).\n",
    "- Monitoring & Maintenance – Continuously tracking model performance.\n",
    "\n",
    "📌 Examples of Data Science in Action\n",
    "- Healthcare: Predicting disease outbreaks or diagnosing illnesses from medical images.\n",
    "- Finance: Credit scoring, fraud detection, algorithmic trading.\n",
    "- Retail: Personalized recommendations, inventory optimization.\n",
    "- Marketing: Customer segmentation, campaign effectiveness.\n",
    "- Public Health: Modeling the spread of diseases, evaluating the impact of health interventions.\n",
    "\n",
    "## Key Differences Between Data Science and Data Mining\n",
    "|Feature\t|Data Science\t|Data Mining|\n",
    "|-----------|---------------|-----------|\n",
    "|Definition\t|A broad interdisciplinary field focused on extracting insights and value from data using scientific methods.|\tA specific process of discovering patterns and relationships in large datasets.|\n",
    "|Scope\t|Broad: includes data collection, cleaning, modeling, interpretation, and communication.\t|Narrower: mainly focused on pattern discovery and knowledge extraction.|\n",
    "|Goal\t|To solve complex business problems and build predictive or decision-making systems.\t|To uncover hidden patterns and associations in data.|\n",
    "|Techniques Used|\tMachine learning, deep learning, statistical modeling, big data processing, AI.|\tClustering, classification, association rule mining, anomaly detection.|\n",
    "|Tools|\tPython, R, SQL, Hadoop, Spark, TensorFlow, Tableau.|\tWeka, RapidMiner, Orange, SQL, R, Python (with specific libraries).|\n",
    "|Output|\tModels, dashboards, predictive systems, business insights.|\tPatterns, rules, summaries of relationships in data.|\n",
    "|Data Types|\tStructured, semi-structured, unstructured (text, image, audio).\t|Mostly structured data.|\n",
    "|Domain Knowledge Importance|\tVery high – needed to guide the entire problem-solving process.|\tModerate – useful for interpreting discovered patterns.|\n",
    "|Interdisciplinary?|\tYes – blends statistics, computer science, business knowledge.|\tNot as interdisciplinary – rooted more in statistics and computer science.|\n",
    "\n",
    "🧠 Analogy\n",
    "- Data Science is like being a full chef – choosing the ingredients (data), preparing them, cooking the dish (model), and presenting it.\n",
    "- Data Mining is like discovering hidden recipes in a big cookbook – finding which ingredients often go together and what dishes they create.\n",
    "\n",
    "🧪 How They Work Together\n",
    "- Data Mining is a step within the Data Science workflow.\n",
    "- In Data Science, once data is prepared, data mining may be used to explore patterns before moving to more advanced modeling.\n",
    "\n",
    "Step-by-Step Breakdown\n",
    "\n",
    "✅ 1. Data Collection (Data Science)\n",
    "- Sources: Transaction logs, user profiles, device information, IP geolocation, past fraud history.\n",
    "- Tools: Python, SQL, Kafka, APIs\n",
    "- Purpose: Gather raw structured (transaction) and unstructured (browser behavior) data.\n",
    "\n",
    "🧼 2. Data Preprocessing (Data Science)\n",
    "- Cleaning missing values, formatting timestamps, normalizing amounts, flagging duplicate entries.\n",
    "- Encoding categorical data like merchant type or location.\n",
    "- Feature engineering: Time between transactions, transaction velocity, location changes.\n",
    "\n",
    "🔍 3. Data Mining (Pattern Discovery Phase)\n",
    "\n",
    "Here’s where Data Mining techniques are applied:\n",
    "\n",
    "a. Anomaly Detection\n",
    "- Identify outliers: unusually large transactions or transactions from unfamiliar locations.\n",
    "- Technique: Clustering (e.g., DBSCAN), distance-based anomaly detection.\n",
    "\n",
    "b. Association Rule Mining\n",
    "- Purpose: Find typical sequences of legitimate behavior.\n",
    "- Example: If a customer usually buys groceries and fuel, a sudden electronics purchase overseas could be suspicious.\n",
    "\n",
    "c. Frequent Pattern Mining\n",
    "- Find commonly occurring fraud patterns from past data.\n",
    "- Useful for building rules or heuristics.\n",
    "\n",
    "🤖 4. Predictive Modeling (Data Science)\n",
    "- Train a classification model (e.g., Logistic Regression, Random Forest, XGBoost, or Neural Networks) to predict:\n",
    "    - 1 for fraud\n",
    "    - 0 for legitimate\n",
    "- Input Features:\n",
    "    - Transaction amount, time of day, location, device ID, account age, historical fraud rate.\n",
    "- Output: Probability of fraud.\n",
    "\n",
    "📊 5. Model Evaluation & Deployment (Data Science)\n",
    "- Metrics: Precision, recall, F1-score, ROC-AUC (since fraud is rare and imbalanced).\n",
    "- Action: If the fraud score exceeds a threshold:\n",
    "- Flag for manual review\n",
    "- Automatically decline or challenge transaction (e.g., OTP)\n",
    "\n",
    "## Data-Driven Decision Making (DDDM) \n",
    "Refers to the process of making decisions based on data analysis and insights rather than intuition, guesswork, or personal experience alone.\n",
    "\n",
    "It involves collecting relevant data, analyzing it using statistical or machine learning tools, and using the results to guide strategic or operational choices. This approach helps organizations become more objective, consistent, and evidence-based in their actions.\n",
    "\n",
    "🔄 Key Elements of Data-Driven Decision Making\n",
    "- Collect: Gather high-quality, relevant data.\n",
    "- Analyze: Use analytics, statistical methods, or AI models to uncover patterns and trends.\n",
    "- Interpret: Understand what the data means in a business context.\n",
    "- Decide: Take action based on the evidence.\n",
    "- Measure: Monitor outcomes to refine future decisions.\n",
    "\n",
    "\n",
    "📌 Two Types of Decisions That Benefit from DDDM\n",
    "1. Strategic Decisions\n",
    "    - These are long-term, high-impact decisions that shape the direction of an organization.\n",
    "\n",
    "✅ Examples:\n",
    "- Market Expansion: Deciding which new region or country to enter based on customer demographics, purchasing behavior, and economic indicators.\n",
    "- Product Development: Launching a new feature or product based on customer feedback, usage patterns, and competitor analysis.\n",
    "- Resource Allocation: Investing in technology, R&D, or marketing channels based on ROI predictions and cost-benefit analysis.\n",
    "\n",
    "💡 Benefit of DDDM:\n",
    "- Reduces risk of costly missteps.\n",
    "- Aligns decision-making with real market needs and performance indicators.\n",
    "\n",
    "2. Operational Decisions\n",
    "    - These are day-to-day, tactical decisions that affect the efficiency and performance of processes.\n",
    "\n",
    "✅ Examples:\n",
    "- Customer Support: Routing customer service tickets based on issue type, past resolution time, or sentiment analysis.\n",
    "- Inventory Management: Reordering stock based on demand forecasts and sales trends.\n",
    "- Fraud Detection: Flagging suspicious transactions in real time using predictive analytics.\n",
    "\n",
    "💡 Benefit of DDDM:\n",
    "- Improves speed and accuracy of actions.\n",
    "- Enhances customer experience and operational efficiency.\n",
    "\n",
    "🎯 Summary:\n",
    "|Decision Type|\tDescription|\tExample|\tData-Driven Advantage|\n",
    "|----------------|------------|-----------|-----------------------|\n",
    "|Strategic|\tLong-term, directional|\tChoosing markets, product launches|\tInformed planning and reduced uncertainty|\n",
    "|Operational|\tDaily/tactical|\tInventory, customer support, fraud checks|\tEfficiency, automation, better performance|\n",
    "\n",
    "### Case Study: PayPal – Combating Fraud and Expanding Services with Data\n",
    "🏦 Company Overview\n",
    "- PayPal is a leading online payments platform serving millions of customers globally. With millions of transactions per day, real-time risk management and service personalization are essential to their success.\n",
    "\n",
    "🎯 1. Strategic Decision Example: Expanding into New Financial Services\n",
    "\n",
    "✅ The Challenge:\n",
    "- PayPal wanted to expand beyond peer-to-peer payments into offering credit, investment, and savings products. But which services should they prioritize?\n",
    "\n",
    "🔍 Data Used:\n",
    "- Transaction patterns of existing users\n",
    "- User demographics and income bands\n",
    "- Purchase categories (e.g., travel, subscriptions, bills)\n",
    "- Creditworthiness signals\n",
    "- Market trends in underserved regions\n",
    "\n",
    "📊 Data-Driven Decision:\n",
    "- Using customer segmentation and predictive analytics, PayPal identified:\n",
    "    - Users who regularly paid bills or sent remittances as potential candidates for micro-lending.\n",
    "    - Younger, digitally savvy users for investment products.\n",
    "    - Regions where competitors lacked strong financial infrastructure.\n",
    "\n",
    "💡 Outcome:\n",
    "- Rolled out PayPal Credit and PayPal Savings successfully.\n",
    "- Increased customer retention and cross-product adoption.\n",
    "- Strategic expansion aligned with actual customer behavior, not assumptions.\n",
    "\n",
    "⚙️ 2. Operational Decision Example: Real-Time Fraud Detection\n",
    "✅ The Challenge:\n",
    "- With thousands of global transactions per second, PayPal needed to detect and prevent fraud in real-time without blocking legitimate users.\n",
    "\n",
    "🔍 Data Used:\n",
    "- Device fingerprinting\n",
    "- IP address and geolocation\n",
    "- Time of transaction\n",
    "- Velocity of transactions\n",
    "- Historical fraud labels\n",
    "- Behavioral signals (mouse movement, app usage)\n",
    "\n",
    "🧠 Data-Driven Solution:\n",
    "- PayPal built a real-time fraud detection engine using:\n",
    "- Gradient Boosting Machines (GBMs) and Deep Learning models trained on historical fraud patterns.\n",
    "- Ensemble models combining rule-based alerts and machine learning scores.\n",
    "- Continuous feedback loops for model retraining.\n",
    "\n",
    "💡 Outcome:\n",
    "- Real-time detection of fraud with false positive rates under 1%.\n",
    "- Prevented millions in fraud losses annually.\n",
    "- Enabled smooth user experience by reducing false blocks.\n",
    "\n",
    "### Automated Decision System: Fraud Scoring in Insurance Claims\n",
    "🎯 Goal:\n",
    "To automatically identify and flag potentially fraudulent insurance claims in real-time or near real-time, minimizing losses and improving investigation efficiency.\n",
    "\n",
    "⚙️ How the System Works (Step-by-Step)\n",
    "1. 📥 Input Data Collection\n",
    "\n",
    "The system collects structured and unstructured data from:\n",
    "- Claim forms (amount, type of claim, time of filing)\n",
    "- Customer profile (age, past claims, location, occupation)\n",
    "- Policy details (coverage type, duration, premium)\n",
    "- External data sources (e.g., repair bills, police reports)\n",
    "- Historical fraud cases and red flags\n",
    "- Social network or geospatial data (in advanced setups)\n",
    "\n",
    "2. 🧠 Feature Engineering\n",
    "\n",
    "Key features are derived, including:\n",
    "- Time delay between incident and filing\n",
    "- Similarity to past fraudulent claims\n",
    "- Number of claims in short periods\n",
    "- Inconsistencies between claim details and supporting docs\n",
    "- Unusual claim amounts for the policy type\n",
    "\n",
    "3. 🤖 Machine Learning Model\n",
    "A fraud scoring model is trained using labeled historical data (fraudulent vs. genuine claims).\n",
    "\n",
    "Common algorithms used:\n",
    "- Logistic Regression – interpretable and fast\n",
    "- Random Forest / XGBoost – handles complex nonlinearities and interactions\n",
    "- Neural Networks – used when unstructured data (text, images) are involved\n",
    "- The model outputs a fraud probability score between 0 and 1.\n",
    "\n",
    "4. 📊 Decision Thresholds & Automation Rules\n",
    "\n",
    "Based on the fraud score, automated decision logic is applied:\n",
    "|Fraud Score|\tAction|\n",
    "|-----------|----------|\n",
    "|0.00 – 0.30|\tAuto-approve claim|\n",
    "|0.31 – 0.70|\tFlag for manual review|\n",
    "|0.71 – 1.00|\tAuto-reject or escalate to SIU 🔍|\n",
    "\n",
    "Rules may also include:\n",
    "- Blocking payment triggers\n",
    "- Sending alerts to investigators\n",
    "- Logging metadata for audits\n",
    "\n",
    "5. 🔁 Feedback Loop & Model Retraining\n",
    "- Investigators label outcomes (fraud/not fraud)\n",
    "- These labels are used to retrain the model periodically\n",
    "- System gets smarter over time (adaptive fraud detection)\n",
    "\n",
    "### Data Science for Decision-Makers – Cheat Sheet\n",
    "🧩 1. What Is Data Science?\n",
    "    - The art and science of turning data into decisions using statistics, machine learning, and domain expertise.\n",
    "\n",
    "🔑 2. Key Concepts (Simple Definitions)\n",
    "\n",
    "|Concept|\tWhat It Means|\tWhy It Matters for You|\n",
    "|---------|---------------|------------------------|\n",
    "|Data|\tFacts collected (e.g., sales, clicks, transactions)|\tFuel for all insights|\n",
    "|Descriptive Analytics|\tWhat happened? (e.g., \"Sales dropped 10% last week\")|\tFirst step in understanding trends|\n",
    "|Predictive Analytics|\tWhat will happen? (e.g., \"This customer may churn\")|\tPlan ahead; reduce risk|\n",
    "|Prescriptive Analytics|\tWhat should we do? (e.g., \"Offer a 10% discount now\")|\tTurn insight into action|\n",
    "|Machine Learning|\tAlgorithms that learn from data to make predictions|\tAutomates and improves decisions over time|\n",
    "|Model|\tA tool that finds patterns and makes predictions|\tThe engine behind data-driven decisions|\n",
    "|Feature|\tA variable or column (e.g., age, income, product views)|\tUsed to train models and find insights|\n",
    "|Bias|\tSkewed results due to bad data or wrong assumptions|\tCan lead to unfair or wrong outcomes|\n",
    "\n",
    "🧠 3. Must-Know Basics (for Interpreting Insights)\n",
    "\n",
    "|Principle|\tWhy It Matters for You|\n",
    "|---------|--------------------------|\n",
    "|Correlation ≠ Causation|\tJust because two things move together doesn’t mean one causes the other|\n",
    "|Sample Size Matters|\tSmall or skewed data can lead to misleading results|\n",
    "|Ask “Compared to What?”|\tAlways check baselines and control groups|\n",
    "|Data Can Be Dirty|\tMissing, inconsistent, or incorrect data affects results|\n",
    "|All Models Have Assumptions|\tNo model is perfect—always ask “what was assumed?”|\n",
    "\n",
    "🔍 4. Questions to Ask Your Data Team\n",
    "\n",
    "1. What business question are we answering?\n",
    "2. Where did the data come from?\n",
    "3. How was success measured?\n",
    "4. What assumptions does the model make?\n",
    "5. How confident are we in this prediction?\n",
    "\n",
    "\n",
    "When you're talking to a senior stakeholder like the CFO, your focus should be on understanding the business context, desired outcomes, and constraints.\n",
    "\n",
    "## What is Big Data?\n",
    "Big Data refers to extremely large and complex datasets that are too big or complex for traditional data processing software to handle efficiently.\n",
    "\n",
    "Big Data is commonly characterized by the 5 V’s:\n",
    "\n",
    "|V|\tMeaning|\tExample|\n",
    "|-----|-------|-------|\n",
    "|Volume|\tMassive amounts of data|\tBillions of credit card transactions|\n",
    "|Velocity|\tData generated in real-time or near-real-time|\tSocial media updates, sensor data|\n",
    "|Variety|\tDifferent data types (structured, semi-structured, unstructured)|\tImages, text, video, logs|\n",
    "|Veracity|\tUncertainty or quality of data\tInconsistent or noisy data from user inputs|\n",
    "|Value|\tTurning raw data into actionable insights|\tPredicting churn, detecting fraud|\n",
    "\n",
    "\n",
    "### Why Appropriate Data and Data Scientists Are Required to Extract Useful Knowledge\n",
    "\n",
    "1. 📊 Why Appropriate Data is Needed\n",
    "    - Having the right data is crucial because:\n",
    "        - Garbage in = garbage out: Poor quality, irrelevant, or biased data leads to incorrect conclusions.\n",
    "        - Context matters: You need domain-relevant features to solve specific problems (e.g., you can’t predict loan defaults without credit history).\n",
    "        - Completeness and Accuracy: Decisions rely on clean, complete, and well-structured data.\n",
    "    - Example: Predicting insurance fraud using only age and gender is likely to fail. You need detailed claim behavior, policy details, and historical fraud patterns.\n",
    "\n",
    "2. 🧠 Why Data Scientists Are Essential\n",
    "    - Data scientists bring a blend of:\n",
    "        - Statistics & Mathematics (to analyze trends, test hypotheses)\n",
    "        - Programming (to work with large-scale data using tools like Python, R, SQL)\n",
    "        - Machine Learning (to create predictive models)\n",
    "        - Domain Knowledge (to ask the right questions and interpret results)\n",
    "    - They ensure:\n",
    "        - The right questions are asked\n",
    "        - The right techniques are applied (e.g., regression vs. classification)\n",
    "        - The insights are valid, explainable, and actionable\n",
    "        - Without skilled data scientists, businesses might:\n",
    "        - Misinterpret correlations as causation\n",
    "        - Build biased or misleading models\n",
    "        - Miss key patterns hiding in the data\n",
    "\n",
    "### Why It's Necessary to Understand Data Science – Even If You Won’t Use It Directly\n",
    "Even if you're not building models yourself, understanding data science is critical in a modern, data-driven world, especially for decision-makers.\n",
    "\n",
    "1. 📣 Better Communication with Data Teams\n",
    "    - You can ask the right questions\n",
    "    - Understand reports, dashboards, and model limitations\n",
    "    - Collaborate more effectively with technical teams\n",
    "\n",
    "2. 🎯 Make Informed Decisions\n",
    "    - Know what the data is really saying\n",
    "    - Spot when results don’t make sense or when assumptions are flawed\n",
    "    - Ask for more analysis where needed (e.g., “Can we break this down by customer segments?”)\n",
    "\n",
    "3. 🛡️ Avoid Being Misled\n",
    "    - Understand basics like correlation ≠ causation, sampling bias, or p-values\n",
    "    - Protect yourself from overhyped or flawed analytics\n",
    "\n",
    "4. 💼 Gain a Competitive Edge\n",
    "    - Managers and leaders who understand data science are better at:\n",
    "        - Allocating resources\n",
    "        - Prioritizing projects\n",
    "        - Identifying growth opportunities\n",
    "\n",
    "## Four Fundamental Concepts of Data Science\n",
    "\n",
    "1. 📊 Data Wrangling (or Data Preparation)\n",
    "Definition: The process of cleaning, transforming, and organizing raw data into a usable format for analysis or modeling.\n",
    "\n",
    "Why it matters:\n",
    "- Real-world data is messy — it often has missing values, errors, duplicates, or inconsistencies. Effective data wrangling ensures that your dataset is accurate, complete, and structured properly, forming the foundation for trustworthy analysis.\n",
    "\n",
    "Key tasks:\n",
    "- Handling missing or inconsistent data\n",
    "- Merging or reshaping datasets\n",
    "- Feature engineering (creating meaningful variables)\n",
    "- Data type conversions\n",
    "\n",
    "2. 📈 Statistical Inference\n",
    "Definition: Drawing conclusions about a population based on a sample using statistical methods (e.g., confidence intervals, hypothesis testing).\n",
    "\n",
    "Why it matters:\n",
    "- It allows you to make data-driven decisions even when you can't measure everything — using probabilities to estimate relationships, detect patterns, and test assumptions.\n",
    "\n",
    "Key tools:\n",
    "- Hypothesis testing (t-tests, chi-square tests)\n",
    "- Regression analysis\n",
    "- Probability distributions\n",
    "- Confidence intervals and p-values\n",
    "\n",
    "3. 🤖 Machine Learning\n",
    "Definition: Algorithms that allow systems to learn patterns from data and make predictions or decisions without being explicitly programmed for every scenario.\n",
    "\n",
    "Why it matters:\n",
    "- It enables automation of decision-making, such as fraud detection, customer segmentation, churn prediction, or recommendation engines.\n",
    "\n",
    "Key categories:\n",
    "- Supervised learning: Prediction based on labeled data (e.g., classification, regression)\n",
    "- Unsupervised learning: Finding hidden patterns in unlabeled data (e.g., clustering)\n",
    "- Reinforcement learning: Learning from feedback (used in dynamic systems like trading bots)\n",
    "\n",
    "4. 📊 Data Visualization & Communication\n",
    "Definition: The graphical representation of data and insights using charts, graphs, dashboards, or storytelling techniques.\n",
    "\n",
    "Why it matters:\n",
    "- The value of analysis is only realized if stakeholders can understand and act on the insights. Visualization bridges the gap between technical analysis and business decision-making.\n",
    "\n",
    "Key techniques:\n",
    "- Exploratory data analysis (EDA)\n",
    "- Charts (bar, line, scatter, boxplots)\n",
    "- Dashboards (e.g., Tableau, Power BI)\n",
    "- Storytelling with data (context, clarity, visuals)\n",
    "\n",
    "## Selecting the right analytical approach and data mining algorithm \n",
    "Depends on the business problem, data characteristics, and desired outcome.\n",
    "\n",
    "#### Key Questions to Determine the Right Analysis Approach\n",
    "A. Understanding the Business Problem\n",
    "\n",
    "Step 1: What is the business objective?\n",
    "\n",
    "Ask:\n",
    "- Are we trying to describe, predict, explain, or optimize something?\n",
    "- Is the focus on understanding the past, making decisions now, or forecasting the future?\n",
    "\n",
    "**What is the business objective?**\n",
    "- Prediction? → Predictive Analytics\n",
    "- Understanding why something happened? → Diagnostic / Causal Analysis\n",
    "- Finding patterns in behavior? → Behavioral Segmentation / Clustering\n",
    "- Optimizing decisions? → Prescriptive Analytics\n",
    "- Forecasting future trends? → Time Series Decomposition / Forecasting\n",
    "\n",
    "**Do we need to compare groups or test a hypothesis?**\n",
    "- A/B Testing (if randomized controlled experiment)\n",
    "- Hypothesis Testing (if comparing means/proportions)\n",
    "- Inferential Statistics (if generalizing from a sample)\n",
    "\n",
    "**Are we trying to measure relationships?**\n",
    "- Correlation (if measuring association)\n",
    "- Causal Inference (if determining cause-effect, e.g., propensity score matching, DAGs, RCTs)\n",
    "\n",
    "**Is the data sequential/time-dependent?**\n",
    "- Time Series Analysis (ARIMA, Prophet, LSTM)\n",
    "- Survival Analysis (for churn/risk modeling)\n",
    "\n",
    "**Are we dealing with uncertainty or risk?**\n",
    "- Probabilistic Models (Bayesian Networks, Monte Carlo Simulation)\n",
    "- Probability Distributions (Poisson for counts, Normal for continuous)\n",
    "\n",
    "**Do we need to optimize marketing spend?**\n",
    "- Marketing Mix Modeling (MMM) (regression-based attribution)\n",
    "- Multi-Touch Attribution (MTA) (Shapley value, Markov chains)\n",
    "\n",
    "\n",
    "|Business Objective|\tAnalysis Type|\n",
    "|--------------------|----------------|\n",
    "|What happened?|\tDescriptive Analytics|\n",
    "|Why did it happen?|\tDiagnostic / Causal|\n",
    "|What is likely to happen next?|\tPredictive Analytics|\n",
    "|What should we do about it?|\tPrescriptive Analytics|\n",
    "\n",
    "\n",
    "🔍 Step 2: What is the nature of the data and outcome?\n",
    "\n",
    "Ask:\n",
    "- Do we have labeled data (target variable)?\n",
    "- Is the outcome categorical, continuous, binary, or unknown?\n",
    "- Are we measuring change over time?\n",
    "- Are we dealing with networks, groups, or events?\n",
    "\n",
    "This determines whether we use:\n",
    "- Statistical modeling (e.g., regression, hypothesis testing)\n",
    "- Machine learning (e.g., classification, clustering)\n",
    "- Time series analysis, or\n",
    "- Behavioral/structural approaches (e.g., graph theory, segmentation)\n",
    "\n",
    "### What Determines the Use of Statistical Modeling?\n",
    "Ask these core questions:\n",
    "\n",
    "✅ A. Do I need to understand relationships, not just predict outcomes?\n",
    "- Statistical modeling helps you interpret how and why variables relate (e.g., how age impacts claim risk).\n",
    "\n",
    "✅ B. Is interpretability more important than raw predictive power?\n",
    "- If stakeholders (regulators, doctors, underwriters) need transparent insights, go for statistical models.\n",
    "\n",
    "✅ C. Do I have a hypothesis or theory I want to test?\n",
    "- If you're trying to validate an assumption (e.g., \"Smoking increases risk of stroke\"), statistical inference is key.\n",
    "\n",
    "✅ D. Is my data sample small or medium-sized?\n",
    "- Statistical models are efficient with limited data and can quantify uncertainty through confidence intervals, p-values, etc.\n",
    "\n",
    "##### How to Break Down and Solve a Statistical Modeling Problem\n",
    "🧠 Step-by-Step Framework\n",
    "\n",
    "|Step|\tWhat to Do|\tExample (Loan Default)|\n",
    "|-----------|-----------------|--------------|\n",
    "|1. Define|\tWhat’s the question/hypothesis?|\tDoes higher DTI increase risk of default?|\n",
    "|2. Explore|\tUnderstand data structure, missing values|\tCheck distributions, missing income data|\n",
    "|3. Choose model|\tBased on outcome type & interpretability|\tLogistic Regression (binary outcome)|\n",
    "|4. Fit model|\tTrain on labeled data|\tUse statsmodels or sklearn|\n",
    "|5. Validate assumptions|\tLinearity, independence, etc.|\tCheck VIFs, ROC curve|\n",
    "|6. Interpret results|\tFocus on effect sizes, CIs, p-values|\tOdds of default ↑ by 1.5x with DTI > 40%|\n",
    "|7. Communicate|\tUse visuals + plain language|\t“Customers with low credit score & high DTI are 2x riskier”|\n",
    "\n",
    "Use statistical modeling when:\n",
    "- You want interpretable insights\n",
    "- You aim to test hypotheses\n",
    "- You want to explain relationships, not just predict\n",
    "- You need inference, not just accuracy\n",
    "\n",
    "##### 🏦 Financial Services\n",
    "\n",
    "Use Case: Predict likelihood of loan default based on income, credit score, and debt-to-income ratio.\n",
    "\n",
    "Model: Logistic Regression (binary outcome)\n",
    "\n",
    "Question: What factors significantly impact default risk?\n",
    "\n",
    "Solve:\n",
    "- Run logistic regression\n",
    "- Interpret coefficients (e.g., odds of default ↑ as DTI ↑)\n",
    "- Use model to flag risky borrowers\n",
    "\n",
    "Use Case: Test if a new digital onboarding process improves customer conversion.\n",
    "\n",
    "Model: Two-sample t-test or A/B testing\n",
    "\n",
    "Question: Is the difference in conversion rate statistically significant?\n",
    "\n",
    "Solve:\n",
    "- Randomly assign users to control and test groups\n",
    "- Run t-test\n",
    "- Check p-value to accept/reject the null\n",
    "\n",
    "##### 🏥 Health & Medical\n",
    "\n",
    "Use Case 1: Identify risk factors for readmission within 30 days.\n",
    "\n",
    "Model: Multiple Logistic Regression\n",
    "\n",
    "Question: Which comorbidities or treatment delays are associated with higher readmission?\n",
    "\n",
    "Solve:\n",
    "- Collect EHR data\n",
    "- Fit model with features like age, diagnosis, length of stay\n",
    "- Report odds ratios and confidence intervals\n",
    "\n",
    "Use Case 2: Estimate average recovery time for different surgical procedures.\n",
    "\n",
    "Model: ANOVA (Analysis of Variance)\n",
    "\n",
    "Question: Are there significant differences in recovery time across procedures?\n",
    "\n",
    "Solve:\n",
    "- Run one-way ANOVA\n",
    "- If significant, apply post-hoc tests (Tukey) to compare pairs\n",
    "\n",
    "##### 🛡 Insurance\n",
    "Use Case 1: Estimate claim severity based on policyholder and accident details.\n",
    "\n",
    "Model: Linear Regression\n",
    "\n",
    "Question: How does claim amount vary with age, location, vehicle type?\n",
    "\n",
    "Solve:\n",
    "- Fit linear regression\n",
    "- Check residuals, multicollinearity\n",
    "- Interpret impact of variables (e.g., luxury vehicles = higher payout)\n",
    "\n",
    "Use Case 2: Evaluate whether offering a telematics discount reduces claims.\n",
    "\n",
    "Model: Difference-in-Differences\n",
    "\n",
    "Question: Did claims drop more among those who opted into telematics vs. those who didn’t?\n",
    "\n",
    "Solve:\n",
    "- Use pre/post data with treatment/control groups\n",
    "- Fit DiD model\n",
    "- Assess interaction term\n",
    "\n",
    "\n",
    "🧠 Step 3: Decision Framework for Analytic Approach\n",
    "- Let’s map questions → analysis type → data mining algorithm:\n",
    "\n",
    "📊 **1. Descriptive Analytics**\n",
    "\n",
    "Goal: Summarize patterns or trends in historical data.\n",
    "\n",
    "Ask:\n",
    "- What are key metrics over time?\n",
    "- What are the distributions of variables?\n",
    "- What are the common customer segments?\n",
    "\n",
    "Approaches:\n",
    "- Summary statistics, cross-tabs\n",
    "- Behavioral segmentation\n",
    "- Time Series Decomposition\n",
    "\n",
    "Algorithms/Tools:\n",
    "- Profiling\n",
    "- Clustering (e.g., K-Means)\n",
    "- Data Reduction (PCA, t-SNE)\n",
    "\n",
    "🧪 **2. Inferential Statistics & Hypothesis Testing**\n",
    "\n",
    "Goal: Draw conclusions about a population from a sample.\n",
    "\n",
    "Ask:\n",
    "- Is the difference between groups statistically significant?\n",
    "- What’s the probability this result occurred by chance?\n",
    "\n",
    "Approaches:\n",
    "- Hypothesis Testing (t-test, ANOVA, chi-square)\n",
    "- Confidence Intervals\n",
    "- Probability distributions (Binomial, Normal, Poisson)\n",
    "- A/B Testing / Controlled Experiments\n",
    "\n",
    "Algorithms:\n",
    "- No ML; statistical techniques suffice\n",
    "- Difference-in-Differences (for observational causal studies)\n",
    "\n",
    "🧭 **3. Causal Analysis**\n",
    "\n",
    "Goal: Understand if X causes Y.\n",
    "\n",
    "Ask:\n",
    "- If we change X, will Y change?\n",
    "- Can we estimate the treatment effect?\n",
    "\n",
    "Approaches:\n",
    "- Randomized Controlled Trials (RCT)\n",
    "- Causal Inference (DoWhy, EconML)\n",
    "- Propensity Score Matching\n",
    "- Instrumental Variables\n",
    "\n",
    "Algorithms:\n",
    "- Causal Modeling (Graphical Models)\n",
    "- Link Prediction (in networked systems)\n",
    "\n",
    "📈 **4. Predictive Analytics**\n",
    "\n",
    "Goal: Predict future events based on historical data.\n",
    "\n",
    "Ask:\n",
    "- Can we forecast a value or classify an outcome?\n",
    "- Do we need to predict churn, fraud, claims, readmissions?\n",
    "\n",
    "Approaches:\n",
    "- Supervised Machine Learning\n",
    "- Time Series Forecasting (if time-related)\n",
    "- Behavioral or Risk Scoring\n",
    "\n",
    "Algorithms:\n",
    "- Classification: Logistic regression, Random Forest, XGBoost\n",
    "- Regression: Linear regression, Lasso, SVR\n",
    "- Similarity Matching: KNN, cosine similarity\n",
    "- Time Series: ARIMA, Prophet, LSTM (for deep learning)\n",
    "\n",
    "⏳ **5. Time Series Decomposition / Forecasting**\n",
    "\n",
    "Goal: Identify seasonality, trend, and irregular patterns.\n",
    "\n",
    "Ask:\n",
    "- How does this variable behave over time?\n",
    "- Are there predictable patterns?\n",
    "\n",
    "Approaches:\n",
    "- Time series decomposition (Trend/Seasonality/Noise)\n",
    "- Forecasting (univariate or multivariate)\n",
    "\n",
    "Algorithms:\n",
    "- ARIMA/SARIMA\n",
    "- Exponential Smoothing\n",
    "- Prophet\n",
    "- LSTM (deep learning)\n",
    "\n",
    "💰 **6. Marketing Effectiveness / MMM**\n",
    "\n",
    "Goal: Quantify how marketing inputs drive sales or ROI.\n",
    "\n",
    "Ask:\n",
    "- What’s the ROI of each channel?\n",
    "- What happens if I increase digital ad spend?\n",
    "\n",
    "Approaches:\n",
    "- Marketing Mix Modeling (MMM)\n",
    "- Attribution Modeling\n",
    "- Regression with lag effects and diminishing returns\n",
    "\n",
    "Algorithms:\n",
    "- Linear and non-linear regression\n",
    "- Bayesian MMM models\n",
    "- Regularized regression (Ridge, Lasso for variable selection)\n",
    "\n",
    "🧑‍🤝‍🧑 **7. Behavioral Segmentation**\n",
    "\n",
    "Goal: Group individuals/customers by shared behaviors.\n",
    "\n",
    "Ask:\n",
    "- Can we segment our population meaningfully?\n",
    "- Are there subgroups that behave differently?\n",
    "\n",
    "Approaches:\n",
    "- Clustering + Profiling\n",
    "- Latent Class Analysis\n",
    "- Customer Lifetime Value modeling\n",
    "\n",
    "Algorithms:\n",
    "- K-Means\n",
    "- DBSCAN\n",
    "- Gaussian Mixture Models (GMM)\n",
    "- Hierarchical Clustering\n",
    "\n",
    "🔗 **8. Link Prediction & Networks**\n",
    "\n",
    "Goal: Predict new connections/interactions in networks.\n",
    "\n",
    "Ask:\n",
    "- Who will transact with whom?\n",
    "- Can we detect fraud rings or referral networks?\n",
    "\n",
    "Approaches:\n",
    "- Graph embeddings\n",
    "- Social network analysis\n",
    "- Knowledge graphs\n",
    "\n",
    "Algorithms:\n",
    "- Node2Vec, DeepWalk\n",
    "- Common Neighbors, Jaccard Similarity\n",
    "- Graph Neural Networks\n",
    "\n",
    "🔄 Mapping to Data Mining Algorithms\n",
    "\n",
    "|Business Question Type|\tData Mining Task|\tExample Tools /Algorithms|\n",
    "|--------------------------|-------------------|----------------------------|\n",
    "|Is this A or B? (classification)|\tClassification|\tLogistic Regression, XGBoost|\n",
    "|How much/many? (numeric prediction)|\tRegression|\tLinear Regression, Lasso|\n",
    "|Who/what is similar to this?|\tSimilarity Matching|\tKNN, Cosine Similarity|\n",
    "|What groups exist in my data?|\tClustering|\tK-Means, DBSCAN|\n",
    "|What items occur together?|\tCo-occurrence Grouping|\tApriori, FP-Growth|\n",
    "|What does a typical X look like?|\tProfiling|\tSummary Stats, Decision Trees|\n",
    "|Will these entities connect?|\tLink Prediction|\tGraph Embeddings, Node2Vec|\n",
    "|Can I reduce noise or simplify features?|\tData Reduction|\tPCA, Autoencoders|\n",
    "|Does X cause Y?|\tCausal Modeling|\tDoWhy, EconML, PSM, DiD|\n",
    "\n",
    "## Knowing when and why to use each data mining algorithm\n",
    "- Definition and Use Cases\n",
    "- Business Questions That Lead You to It\n",
    "- How to Choose the Right Algorithm\n",
    "\n",
    "Key skill for a senior data scientist is not just knowing what each algorithm does, but asking the right questions to choose the right one. As a Data Science Lead, I often walk stakeholders through a simple process:\n",
    "\n",
    "\"What is the goal? Is it to **predict, explain, explore, or reduce**? What type of data and labels do we have?\"\n",
    "\n",
    "**1. Classification**\n",
    "\n",
    "🔹 What it is:\n",
    "- Supervised learning where the outcome is categorical (e.g., “Yes/No”, “High Risk/Low Risk”).\n",
    "\n",
    "💼 When to Use:\n",
    "- Fraud detection (fraudulent vs. non-fraudulent)\n",
    "- Disease diagnosis (cancer/no cancer)\n",
    "- Credit scoring (approve/decline loan)\n",
    "- Churn prediction (will customer leave or not)\n",
    "\n",
    "❓ Key Questions to Ask:\n",
    "- Do we have labeled data with known outcomes?\n",
    "- Is the outcome variable categorical?\n",
    "- Are we trying to assign new records to known groups?\n",
    "\n",
    "✅ Common Algorithms:\n",
    "- Logistic Regression\n",
    "- Decision Trees / Random Forest\n",
    "- Gradient Boosting (XGBoost)\n",
    "- Naive Bayes\n",
    "- Neural Networks (for complex cases)\n",
    "\n",
    "**2. Regression**\n",
    "\n",
    "🔹 What it is:\n",
    "- Supervised learning where the outcome is continuous (e.g., dollar value, time, rate).\n",
    "\n",
    "💼 When to Use:\n",
    "- Forecasting revenue, claims amount, or hospital stay length\n",
    "- Estimating patient recovery time\n",
    "- Predicting insurance premium costs\n",
    "\n",
    "❓ Key Questions to Ask:\n",
    "- Are we predicting a continuous number?\n",
    "- Do we want to quantify the influence of each feature?\n",
    "- Are we dealing with time trends?\n",
    "\n",
    "✅ Common Algorithms:\n",
    "- Linear Regression\n",
    "- Lasso / Ridge\n",
    "- Gradient Boosting Regression\n",
    "- Time Series Regression (ARIMA/SARIMA)\n",
    "\n",
    "**3. Clustering**\n",
    "\n",
    "🔹 What it is:\n",
    "Unsupervised learning for grouping similar items without predefined labels.\n",
    "\n",
    "💼 When to Use:\n",
    "- Customer segmentation\n",
    "- Patient stratification (e.g., grouping by health profiles)\n",
    "- Grouping similar insurance claims or policies\n",
    "\n",
    "❓ Key Questions to Ask:\n",
    "- Do we want to group or segment the data without labels?\n",
    "- Are we exploring patterns rather than predicting outcomes?\n",
    "- Do we want to understand common customer/patient types?\n",
    "\n",
    "✅ Common Algorithms:\n",
    "- K-Means\n",
    "- Hierarchical Clustering\n",
    "- DBSCAN\n",
    "- Gaussian Mixture Models (GMM)\n",
    "\n",
    "**4. Similarity Matching**\n",
    "\n",
    "🔹 What it is:\n",
    "Finding things similar to a given entity based on features.\n",
    "\n",
    "💼 When to Use:\n",
    "- Recommending similar products, policies, or treatments\n",
    "- Finding patient “twins” for treatment comparison\n",
    "- Matching insurance applicants to prior similar claims\n",
    "\n",
    "❓ Key Questions to Ask:\n",
    "- Do we want to recommend or match based on similarity?\n",
    "- Do we have a reference case or query item?\n",
    "- Are we comparing distances/similarities?\n",
    "\n",
    "✅ Common Techniques:\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Cosine Similarity\n",
    "- Euclidean/Manhattan Distance\n",
    "- Embedding-based similarity (in NLP or deep learning)\n",
    "\n",
    "**5. Co-Occurrence Grouping (Association Rule Mining)**\n",
    "\n",
    "🔹 What it is:\n",
    "Unsupervised learning that finds items that frequently occur together.\n",
    "\n",
    "💼 When to Use:\n",
    "- Market basket analysis\n",
    "- Diagnoses that tend to co-occur (comorbidity)\n",
    "- Cross-selling insurance products\n",
    "- Service bundling in healthcare or financial services\n",
    "\n",
    "❓ Key Questions to Ask:\n",
    "- Are we interested in items or events that occur together?\n",
    "- Are we analyzing transactional or event-based data?\n",
    "- Do we want to discover patterns or rules?\n",
    "\n",
    "✅ Common Algorithms:\n",
    "- Apriori Algorithm\n",
    "- FP-Growth\n",
    "- Eclat Algorithm\n",
    "\n",
    "**6. Profiling (Behavior Description)**\n",
    "\n",
    "🔹 What it is:\n",
    "Describing the typical behavior of a group or class.\n",
    "\n",
    "💼 When to Use:\n",
    "- Customer lifetime value profiling\n",
    "- Describing characteristics of high-risk patients\n",
    "- Profiling fraudulent claims\n",
    "\n",
    "❓ Key Questions to Ask:\n",
    "- Do we want to describe and understand the typical characteristics?\n",
    "- Are we analyzing groups rather than predicting?\n",
    "- Are we building a business persona or behavior archetype?\n",
    "\n",
    "✅ Common Techniques:\n",
    "\n",
    "Descriptive statistics\n",
    "- Clustering + Summary Statistics\n",
    "- Decision Trees (to explain differences between groups)\n",
    "\n",
    "**7. Link Prediction**\n",
    "\n",
    "🔹 What it is:\n",
    "Predicting whether a relationship will form between two entities in a network.\n",
    "\n",
    "💼 When to Use:\n",
    "- Social network formation\n",
    "- Identifying patient-doctor network changes\n",
    "- Fraud rings in insurance or financial networks\n",
    "- Recommending connections (doctors, specialists, business clients)\n",
    "\n",
    "❓ Key Questions to Ask:\n",
    "- Are we working with a network/graph of entities?\n",
    "- Are we predicting future connections or interactions?\n",
    "- Do we want to understand structural patterns?\n",
    "\n",
    "✅ Common Algorithms:\n",
    "- Graph Embeddings\n",
    "- Node2Vec / DeepWalk\n",
    "- Link-based classifiers (Common Neighbors, Jaccard, etc.)\n",
    "- Knowledge Graphs\n",
    "\n",
    "**8. Data Reduction**\n",
    "\n",
    "🔹 What it is:\n",
    "Reducing data size or dimensions while retaining important patterns.\n",
    "\n",
    "💼 When to Use:\n",
    "- Simplifying large datasets for visualization or modeling\n",
    "- Speeding up computation\n",
    "- Removing redundant features (e.g., in medical sensors, finance logs)\n",
    "\n",
    "❓ Key Questions to Ask:\n",
    "- Do we want to reduce complexity without losing insight?\n",
    "- Is our data high-dimensional?\n",
    "- Are we aiming to compress, filter, or summarize?\n",
    "\n",
    "✅ Common Techniques:\n",
    "- Principal Component Analysis (PCA)\n",
    "- Autoencoders (deep learning)\n",
    "- Feature selection / Variance Thresholding\n",
    "- t-SNE, UMAP (for visualization)\n",
    "\n",
    "**9. Causal Modeling**\n",
    "\n",
    "🔹 What it is:\n",
    "Uncovering causal relationships (not just correlations) between variables.\n",
    "\n",
    "💼 When to Use:\n",
    "- Determining treatment effects in healthcare (e.g., Drug A vs Drug B)\n",
    "- Impact of marketing on conversion (e.g., campaign A vs. B)\n",
    "- Policy effectiveness in financial or insurance interventions\n",
    "\n",
    "❓ Key Questions to Ask:\n",
    "- Do we want to understand the impact of one variable on another?\n",
    "- Is correlation not enough—do we need causal inference?\n",
    "- Is there an intervention or treatment whose effect needs quantifying?\n",
    "\n",
    "✅ Common Techniques:\n",
    "- Randomized Controlled Trials (RCT)\n",
    "- Difference-in-Differences (DiD)\n",
    "- Propensity Score Matching\n",
    "- Instrumental Variables\n",
    "- Causal Bayesian Networks\n",
    "- DoWhy / EconML (causal libraries in Python)\n",
    "\n",
    "Summary Table\n",
    "\n",
    "|Data Mining Task|\tSupervised?|\tOutput Type|\tBest For|\n",
    "|----------------|---------------|------------------|-----------------|\n",
    "|Classification|\t✅ Yes|\tCategorical|\tLabeling categories, decision-making|\n",
    "|Regression|\t✅ Yes|\tContinuous|\tPredicting quantities (costs, time, prices)|\n",
    "|Clustering|\t❌ No|\tGroup Labels|\tSegmenting customers, patients|\n",
    "|Similarity Matching|\t❌ No|\tRanking/Match|\tRecommendations, nearest neighbors|\n",
    "|Co-occurrence Grouping|\t❌ No|\tRules|\tBundling, cross-sell, comorbidity patterns|\n",
    "|Profiling|\t❌ No|\tDescriptive|\tGroup behavior analysis|\n",
    "|Link Prediction|\t✅/❌ Mix\tBinary/Score|\tPredicting new connections in networks|\n",
    "|Data Reduction|\t❌ No|\tCompressed Data\tPreprocessing, dimensionality reduction|\n",
    "|Causal Modeling|\t✅ Yes|\tEffect Estimate|\tUnderstanding \"what if\" and estimating causal relationships|\n",
    "\n",
    "### 📊 Data Mining vs. Use of Data Mining Results\n",
    "\n",
    "|Aspect | Data Mining | Use of Data Mining Results|\n",
    "|------------------------|--------------------------|-------------------------------|\n",
    "|Definition | The process of discovering patterns, relationships, and insights from large datasets using algorithms | The application of those discovered patterns to drive decisions, strategies, or actions|\n",
    "|Goal | To uncover hidden patterns, correlations, or trends within the data | To apply those findings to solve business problems or make data-informed decisions|\n",
    "|Focus | Exploration and model building | Operationalization and action|\n",
    "|Examples of Tasks | - Classification- Clustering- Regression- Association Rule Mining- Anomaly Detection | - Targeting high-value customers- Detecting fraud in real-time- Optimizing pricing|\n",
    "|Techniques Used | Machine learning algorithms, statistical models, and pattern recognition | Dashboards, business rules, decision support systems, campaign tools|\n",
    "|Who Performs It? | Data scientists, analysts, ML engineers | Business stakeholders, product teams, marketing, finance, and operations teams|\n",
    "|Timeframe | Often exploratory and iterative | Often ongoing, used for monitoring or acting on predictions|\n",
    "|Output | Models, patterns, clusters, predictive rules | Business insights, decisions, automation, customer actions|\n",
    "|Example (Finance) | Using clustering to segment loan applicants | Targeting low-risk segments with preferred interest rates|\n",
    "|Example (Healthcare) | Mining EHR data for disease progression patterns | Adjusting treatment plans or screening protocols based on insights|\n",
    "|Example (Insurance) | Using regression to predict claims costs | Adjusting premium pricing models based on predicted risk|\n",
    "\n",
    "Summary:\n",
    "- Data Mining is the \"discovery phase\"—you dig into the data to uncover what's going on.\n",
    "- Using Data Mining Results is the \"action phase\"—you take the insights and apply them to make decisions or build products.\n",
    "\n",
    "\n",
    "\n",
    "## Statistical and Machine Learning algorithms categorization \n",
    "These can be categorized based on different criteria such as \n",
    "- learning type, \n",
    "- assumptions about data, and \n",
    "- problem-solving approach.\n",
    "\n",
    "**Learning approach and methods**\n",
    "\n",
    "| Algorithm  | Parametric | Non-Parametric | Lazy learning | Eager learning | Probabilistic Models | Distance-Based Models | Rule-Based Models | Ensemble Methods | Linear Models | Non-Linear Models | Traditional Time-Series Models | Machine Learning for Time-Series | Feature Engineering Required | Automatic Feature Extraction |\n",
    "|-------|------|-------|-------|-------|------|-------|-------|-------|------|-------|-------|-------|------|-------|\n",
    "|Linear Regression | | | | | | | | | | | | | | |\n",
    "|Ridge Regression | | | | | | | | | | | | | | |\n",
    "|Lasso Regression | | | | | | | | | | | | | | |\n",
    "|Polynomial Regression | | | | | | | | | | | | | | |\n",
    "|Poisson Regression | | | | | | | | | | | | | | |\n",
    "|Gamma Regression | | | | | | | | | | | | | | |\n",
    "|Bayesian Linear Regression | | | | | | | | | | | | | | |\n",
    "|Support Vector Regression (SVR) | | | | | | | | | | | | | | |\n",
    "|Decision Tree Regression | | | | | | | | | | | | | | |\n",
    "|Random Forest Regression | | | | | | | | | | | | | | |\n",
    "|XGBoost Regression | | | | | | | | | | | | | | |\n",
    "|Neural Networks | | | | | | | | | | | | | | |\n",
    "|Logistic Regression | | | | | | | | | | | | | | |\n",
    "|Naïve Bayes | | | | | | | | | | | | | | |\n",
    "|k-Nearest Neighbors | | | | | | | | | | | | | | |\n",
    "|Decision Trees | | | | | | | | | | | | | | |\n",
    "|Random Forest | | | | | | | | | | | | | | |\n",
    "|Bagging (Bootstrap Aggregation) | | | | | | | | | | | | | | |\n",
    "|Boosting (AdaBoost, Gradient Boosting, XGBoost) | | | | | | | | | | | | | | |\n",
    "|Stacking | | | | | | | | | | | | | | |\n",
    "|Gradient Boosting Machines (GBM) | | | | | | | | | | | | | | |\n",
    "|XGBoost | | | | | | | | | | | | | | |\n",
    "|Support Vector Machines (SVM) | | | | | | | | | | | | | | |\n",
    "|Bayesian Networks | | | | | | | | | | | | | | |\n",
    "|Artificial Neural Networks (ANNs) | | | | | | | | | | | | | | |\n",
    "|Convolutional Neural Networks (CNNs) | | | | | | | | | | | | | | |\n",
    "|Recurrent Neural Networks (RNNs) | | | | | | | | | | | | | | |\n",
    "|Transformers | | | | | | | | | | | | | | |\n",
    "|k-Means Clustering | | | | | | | | | | | | | | |\n",
    "|Hierarchical Clustering | | | | | | | | | | | | | | |\n",
    "|DBSCAN | | | | | | | | | | | | | | |\n",
    "|Gaussian Mixture Models (GMM) | | | | | | | | | | | | | | |\n",
    "|Mean-Shift Clustering | | | | | | | | | | | | | | |\n",
    "|Self-Organizing Maps | | | | | | | | | | | | | | |\n",
    "|Principal Component Analysis (PCA) | | | | | | | | | | | | | | |\n",
    "|Independent Component Analysis (ICA) | | | | | | | | | | | | | | |\n",
    "|t-Distributed Stochastic Neighbor Embedding (t-SNE) | | | | | | | | | | | | | | |\n",
    "|Uniform Manifold Approximation and Projection (UMAP) | | | | | | | | | | | | | | |\n",
    "|Autoencoders | | | | | | | | | | | | | | |\n",
    "|Apriori Algorithm | | | | | | | | | | | | | | |\n",
    "|Eclat Algorithm | | | | | | | | | | | | | | |\n",
    "|FP-Growth Algorithm | | | | | | | | | | | | | | |\n",
    "|Self-training Models  | | | | | | | | | | | | | | |\n",
    "|Label Propagation | | | | | | | | | | | | | | |\n",
    "|Generative Adversarial Networks (GANs)  | | | | | | | | | | | | | | |\n",
    "|Q-Learning (Model-Free)  | | | | | | | | | | | | | | |\n",
    "|Deep Q-Networks (DQN)  | | | | | | | | | | | | | | |\n",
    "|Policy Gradient Methods (e.g., REINFORCE)  | | | | | | | | | | | | | | |\n",
    "|Actor-Critic Algorithms (e.g., A3C, PPO)  | | | | | | | | | | | | | | |\n",
    "|Monte Carlo Methods | | | | | | | | | | | | | | |\n",
    "|Temporal Difference (TD) Learning  | | | | | | | | | | | | | | |\n",
    "|Locally Weighted Regression | | | | | | | | | | | | | | |\n",
    "|Hidden Markov Models (HMM) | | | | | | | | | | | | | | |\n",
    "|ARIMA (AutoRegressive Integrated Moving Average) | | | | | | | | | | | | | | |\n",
    "|SARIMA (Seasonal ARIMA)  | | | | | | | | | | | | | | |\n",
    "|VAR (Vector AutoRegression)  | | | | | | | | | | | | | | |\n",
    "|Exponential Smoothing  | | | | | | | | | | | | | | |\n",
    "|STM (Long Short-Term Memory Networks)  | | | | | | | | | | | | | | |\n",
    "|GRU (Gated Recurrent Units)  | | | | | | | | | | | | | | |\n",
    "|Transformer Models for Time-Series (e.g., Time-Series Transformers)  | | | | | | | | | | | | | | |\n",
    "\n",
    "\n",
    "### Assumptions About Data: Parametric vs. Non-Parametric Methods\n",
    "##### **Parametric Methods**\n",
    "Definition: Parametric methods make strong assumptions about the underlying distribution of the data. They assume that the data follows a specific form (e.g., Gaussian distribution) and the model is defined by a fixed number of parameters.\n",
    "- Parametric models assume a fixed functional form (mathematical equation) that describes the relationship between input features and the output.\n",
    "\n",
    "🔹 Key Characteristics:\n",
    "- Assumes the data follows a specific distribution (e.g., normal distribution in linear regression).\n",
    "- The model is defined by a set of parameters (e.g., weights in linear regression).\n",
    "- Once trained, the number of parameters is fixed, regardless of additional data.\n",
    "- Computationally efficient since fewer parameters need to be estimated.\n",
    "\n",
    "🔹 Examples:\n",
    "- Linear Regression: Assumes a linear relationship between input features and output\n",
    "- Logistic Regression: Assumes a sigmoid function for binary classification.\n",
    "- Naïve Bayes: Assumes feature independence based on Bayes' Theorem.\n",
    "- Gaussian Mixture Models (GMM): Assumes data comes from a mixture of Gaussian distributions.\n",
    "- Linear Discriminant Analysis (LDA).\n",
    "\n",
    "Advantages:\n",
    "- Simpler and faster to train because they rely on a fixed number of parameters.\n",
    "- Require less data to train effectively.\n",
    "\n",
    "Disadvantages:\n",
    "- If the assumptions about the data distribution are incorrect, the model may perform poorly.\n",
    "- Less flexible in capturing complex patterns in the data.\n",
    "\n",
    "🔹 Limitations:\n",
    "- Misspecification risk: If the true relationship is non-linear, a parametric model might perform poorly.\n",
    "- Lack of flexibility: Assumptions may not hold for real-world data.\n",
    "\n",
    "##### **Non-Parametric Methods**\n",
    "Definition: Non-parametric methods do not make strong assumptions about the data distribution. They are more flexible and can adapt to the structure of the data.\n",
    "- Non-parametric models do not assume a fixed functional form. Instead, they infer structure from data.\n",
    "\n",
    "🔹 Key Characteristics:\n",
    "- More flexible because they do not assume a predefined equation.\n",
    "- The model complexity grows with data size (e.g., decision trees expand as data increases).\n",
    "- They can capture complex patterns that parametric models miss.\n",
    "\n",
    "🔹 Examples:\n",
    "- Decision Trees: Splits data into branches based on feature values.\n",
    "- k-Nearest Neighbors (k-NN): Memorizes all training data and makes predictions based on the closest examples.\n",
    "- Support Vector Machines (SVM, with non-linear kernels): Uses kernel functions to map data into a higher-dimensional space.\n",
    "- Random Forest: An ensemble of multiple decision trees.\n",
    "\n",
    "Advantages:\n",
    "- More flexible and can model complex relationships in the data.\n",
    "- No need to assume a specific form for the data distribution.\n",
    "\n",
    "Disadvantages:\n",
    "- Require more data to train effectively.\n",
    "- Computationally more expensive and slower to train.\n",
    "\n",
    "🔹 Limitations:\n",
    "- Higher computational cost due to storing training data.\n",
    "- May require more data for good performance compared to parametric models.\n",
    "\n",
    "Parametric vs Non-Parametric\n",
    "|Feature\t|Parametric\t|Non-Parametric |\n",
    "|-----------|-----------|---------------|\n",
    "|Assumptions\t|Strong assumptions about data distribution\t| Minimal assumptions |\n",
    "|Model Complexity\t|Fixed number of parameters\t|Flexible, grows with data\n",
    "|Example\t|Linear Regression\t|k-NN, Decision Trees |\n",
    "|Storage\t|No training data stored\t|May store training data|\n",
    "\n",
    "\n",
    "### Learning Approach: Lazy Learning vs. Eager Learning\n",
    "##### **Lazy Learning (Instance-Based Learning)**\n",
    "Definition: Lazy learning methods delay the processing of the training data until a prediction is required. The model simply stores the training data and uses it directly to make predictions.\n",
    "- Lazy learning algorithms do not learn a model during training. Instead, they store the training data and defer computation until a prediction is required.\n",
    "\n",
    "🔹 Key Characteristics:\n",
    "- No explicit model is built during training (training phase)\n",
    "- Predictions involve searching through stored data \n",
    "    - made by comparing new instances to stored training instances.\n",
    "- Works well for small datasets but is slow for large ones.\n",
    "    - Computationally expensive during prediction but fast during training.\n",
    "\n",
    "🔹 Examples:\n",
    "- k-Nearest Neighbors (k-NN): Stores training data and finds the closest k points at prediction time.\n",
    "- Case-Based Reasoning (CBR): Solves problems by referring to past cases.\n",
    "\n",
    "🔹 Advantages:\n",
    "- No need for assumptions about data distribution.\n",
    "- Adapts well to new data.\n",
    "- No loss of information during training.\n",
    "\n",
    "🔹 Disadvantages:\n",
    "- Computationally expensive at prediction time.\n",
    "- Memory-intensive as it must store all training data.\n",
    "    - Requires large memory to store the entire dataset.\n",
    "\n",
    "##### **Eager Learning (Model-Based Learning)**\n",
    "Definition: Eager learning methods build a generalized model during training, which is then used to make predictions on new data.\n",
    "- Eager learning algorithms build a model during training and use it for predictions.\n",
    "\n",
    "🔹 Key Characteristics:\n",
    "- Training phase involves learning parameters.\n",
    "    - A model is constructed during training, and predictions are made using this model.\n",
    "- Predictions are fast as the model is pre-built.\n",
    "    - Faster prediction times compared to lazy learning.\n",
    "\n",
    "🔹 Examples:\n",
    "- Linear Regression: Learns a fixed set of weights.\n",
    "- Decision Trees: Constructs a tree structure from training data.\n",
    "- Neural Networks: Learns representations via backpropagation.\n",
    "\n",
    "🔹 Advantages:\n",
    "- Fast inference (once trained).\n",
    "- Can generalize well if trained correctly.\n",
    "- Faster prediction times.\n",
    "- More efficient for large datasets.\n",
    "\n",
    "🔹 Disadvantages:\n",
    "- Training can be computationally expensive.\n",
    "- May require hyperparameter tuning.\n",
    "- Less adaptable to new data.\n",
    "- May lose information during model construction.\n",
    "\n",
    "Lazy Learning vs Eager Learning\n",
    "|Feature\t|Lazy Learning|\tEager Learning|\n",
    "|-----------|-------------|---------------|\n",
    "|Training Effort|\tMinimal|\tHeavy |\n",
    "|Prediction Time|\tExpensive|\tFast |\n",
    "|Example|\tk-NN|\tDecision Trees|\n",
    "\n",
    "### Problem-Solving Approach\n",
    "##### **Probabilistic Models**\n",
    "Definition: These models use probability theory to model uncertainty and make predictions. They often assume that the data is generated from a probabilistic process.\n",
    "- Use probability distributions to model relationships.\n",
    "\n",
    "Example: \n",
    "- Naïve Bayes, \n",
    "- Gaussian Mixture Models (GMMs).\n",
    "- Hidden Markov Models (HMM), \n",
    "- Bayesian Networks.\n",
    "\n",
    "Use Cases: \n",
    "- Spam detection, \n",
    "- speech recognition, \n",
    "- medical diagnosis.\n",
    "\n",
    "Bayesian vs Non-Bayesian\n",
    "|Feature\t|Bayesian\t|Non-Bayesian |\n",
    "|-----------|-----------|-------------|\n",
    "|Probability-Based\t|Uses probability distributions|\tDirectly optimizes functions|\n",
    "|Example\t|Naïve Bayes |\tDecision Trees|\n",
    "\n",
    "##### **Distance-Based Models**\n",
    "Definition: These models rely on measuring the similarity or distance between data points to make predictions.\n",
    "- Measure similarity or distance between data points.\n",
    "\n",
    "Example: \n",
    "- k-NN, \n",
    "- k-Means Clustering.\n",
    "\n",
    "Use Cases: \n",
    "- Recommendation systems, \n",
    "- clustering, \n",
    "- anomaly detection.\n",
    "\n",
    "##### **Rule-Based Models**\n",
    "Definition: These models use a set of rules (often in the form of if-then statements) to make decisions or predictions.\n",
    "- Use if-else conditions to classify data.\n",
    "\n",
    "Example: \n",
    "- Decision Trees, \n",
    "- Association Rule Mining.\n",
    "- Rule-Based Systems, \n",
    "- Fuzzy Logic.\n",
    "\n",
    "Use Cases: \n",
    "- Expert systems, \n",
    "- classification tasks.\n",
    "\n",
    "##### **Ensemble Methods**\n",
    "Definition: Ensemble methods combine multiple models to improve performance. They aim to reduce overfitting and increase generalization.\n",
    "- Combine multiple models to improve accuracy.\n",
    "\n",
    "Example: \n",
    "- Random Forest (Bagging), \n",
    "- XGBoost (Boosting).\n",
    "- Gradient Boosting Machines (GBM), \n",
    "- AdaBoost.\n",
    "\n",
    "Use Cases: \n",
    "- Predictive modeling, \n",
    "- classification, \n",
    "- regression.\n",
    "\n",
    "Ensemble vs Gradient Boosting\n",
    "|Feature\t|Ensemble (Bagging)\t| Gradient Boosting |\n",
    "|-----------|-------------------|------------------|\n",
    "|Core Idea\t|Multiple models trained independently\t|Models trained sequentially|\n",
    "|Example\t|Random Forest\t|XGBoost |\n",
    "|Error Handling\t|Reduces variance\t|Reduces bias |\n",
    "\n",
    "\n",
    "### Function Approximation\n",
    "##### **Linear Models**\n",
    "Definition: Linear models assume a linear relationship between the input features and the output.\n",
    "- Assumes a linear relationship.\n",
    "\n",
    "Example: \n",
    "- Linear Regression, \n",
    "- Logistic Regression.\n",
    "\n",
    "Advantages:\n",
    "- Simple and interpretable.\n",
    "- Computationally efficient.\n",
    "\n",
    "Disadvantages:\n",
    "- Limited ability to model complex relationships.\n",
    "\n",
    "##### **Non-Linear Models**\n",
    "Definition: Non-linear models can capture complex, non-linear relationships between input features and the output.\n",
    "- Can capture complex relationships.\n",
    "\n",
    "Example: \n",
    "- Decision Trees, \n",
    "- Neural Networks.\n",
    "- Support Vector Machines (SVM) with non-linear kernels.\n",
    "\n",
    "Advantages:\n",
    "- Can model complex patterns in the data.\n",
    "\n",
    "Disadvantages:\n",
    "- Computationally more expensive.\n",
    "- Harder to interpret.\n",
    "\n",
    "### Time-Series & Sequential Data\n",
    "##### **Traditional Time-Series Models**\n",
    "Definition: These models are specifically designed for time-series data, where the order of data points matters.\n",
    "- Assumes stationarity (constant statistical properties).\n",
    "\n",
    "Examples: \n",
    "- ARIMA  (AutoRegressive Integrated Moving Average), \n",
    "- SARIMA, \n",
    "- Exponential Smoothing.\n",
    "\n",
    "Use Cases: \n",
    "- Stock price prediction, \n",
    "- weather forecasting.\n",
    "\n",
    "##### **Machine Learning for Time-Series**\n",
    "Definition: Machine learning models can also be applied to time-series data, often by transforming the data into a format suitable for supervised learning.\n",
    "- Does not require stationarity.\n",
    "\n",
    "Examples: \n",
    "- LSTMs, \n",
    "- Transformers, \n",
    "- Gradient Boosting Trees\n",
    "- Gradient Boosting Machines (GBM)\n",
    "- Recurrent Neural Networks (RNN)\n",
    "\n",
    "Use Cases: \n",
    "- Anomaly detection, \n",
    "- sequence prediction.\n",
    "\n",
    "### Feature Learning\n",
    "##### **Feature Engineering Required**\n",
    "Definition: Feature engineering involves manually creating or selecting features from raw data to improve model performance.\n",
    "- Manual selection of features.\n",
    "\n",
    "Example: \n",
    "- Logistic Regression, \n",
    "- SVM.\n",
    "\n",
    "Used to:\n",
    "- Creating interaction terms, \n",
    "- scaling, \n",
    "- encoding categorical variables.\n",
    "\n",
    "Advantages:\n",
    "- Can significantly improve model performance.\n",
    "- Allows domain knowledge to be incorporated.\n",
    "\n",
    "Disadvantages:\n",
    "- Time-consuming and requires expertise.\n",
    "\n",
    "##### **Automatic Feature Extraction**\n",
    "Definition: Automatic feature extraction involves using algorithms to automatically discover useful features from raw data.\n",
    "- Uses algorithms to learn features automatically.\n",
    "\n",
    "Example: \n",
    "- Neural Networks, \n",
    "- Autoencoders.\n",
    "- Principal Component Analysis (PCA),\n",
    "- Convolutional Neural Networks (CNN)\n",
    "\n",
    "Advantages:\n",
    "- Reduces the need for manual feature engineering.\n",
    "- Can discover complex patterns in the data.\n",
    "\n",
    "Disadvantages:\n",
    "- May be less interpretable.\n",
    "\n",
    "\n",
    "### Learning Types\n",
    "##### **Supervised Learning**\n",
    "Definition: The model is trained on labeled data, where the input features and corresponding output labels are provided.\n",
    "- Learns from labeled data.\n",
    "\n",
    "Example:\n",
    "- Regression Algorithms \n",
    "    - Regression: Linear Regression (Parametric)\n",
    "        - Linear regression assumes a specific functional form\n",
    "        - It assumes linearity in relationships.\n",
    "        - It estimates a fixed number of parameters (coefficients β).\n",
    "        - Once trained, it does not store training data; predictions are based on learned parameters.\n",
    "    - Regression: Support Vector Regression (SVR) (Non-Parametric)\n",
    "        - SVR does not assume a fixed functional form.\n",
    "        - Uses a kernel trick to transform input features into a higher-dimensional space.\n",
    "        - The complexity of the model depends on the dataset, as it finds support vectors rather than learning fixed parameters.\n",
    "- Classification Algorithms\n",
    "    - Classification: Decision Trees  (Non-Parametric)\n",
    "        - It does not assume a mathematical relationship between inputs and outputs.\n",
    "        - It splits data based on information gain or Gini index, dynamically adjusting tree structure.\n",
    "    - Classification: Logistic Regression (Parametric)\n",
    "        - Uses a fixed logistic (sigmoid) function\n",
    "        - Assumes a fixed number of parameters.\n",
    "        - Does not store training data explicitly.\n",
    "    - Classification: k-Nearest Neighbors (k-NN) (Non-Parametric, Lazy Learning)\n",
    "        - Does not assume any functional form of the data.\n",
    "        - Instead of learning parameters, it memorizes training data.\n",
    "        - Why it is Lazy Learning: It only performs computations when making a prediction, not during training.\n",
    "    - Classification: Random Forest (Ensemble)\n",
    "        - It combines multiple Decision Trees.\n",
    "        - Uses Bootstrap Aggregation (Bagging) to improve performance.\n",
    "    - Classification: XGBoost (Gradient Boosting)\n",
    "        - Uses an iterative approach where each new tree corrects errors of previous trees.\n",
    "        - Each tree is weighted by its contribution.\n",
    "    - Classification: Naïve Bayes (Bayesian)\n",
    "        - Uses Bayes’ Theorem to compute class probabilities.\n",
    "        - Assumes feature independence\n",
    "    - Classification: Neural Networks (Deep Learning)\n",
    "        - Consists of multiple layers (input, hidden, output).\n",
    "        - Each layer applies transformations to data using activation functions.\n",
    "        - Learns from raw data features rather than requiring manual feature engineering.\n",
    "\n",
    "Supervised Learning Algorithms\n",
    "- Used when the target variable is known (e.g., predicting fraud, classifying diseases, forecasting churn)\n",
    "\n",
    "🔹 Classification (Categorical Outcome)\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- Random Forest\n",
    "- Gradient Boosting (XGBoost, LightGBM, CatBoost)\n",
    "- Support Vector Machines (SVM)\n",
    "- Naive Bayes\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Neural Networks / Deep Learning (for classification)\n",
    "\n",
    "🔹 Regression (Continuous Outcome)\n",
    "- Linear Regression\n",
    "- Ridge / Lasso / ElasticNet Regression\n",
    "- Random Forest Regressor\n",
    "- Gradient Boosting Regressor\n",
    "- Support Vector Regression (SVR)\n",
    "- Neural Networks / Deep Learning (for regression)\n",
    "\n",
    "Use Cases: \n",
    "- Image classification, \n",
    "- spam detection.\n",
    "\n",
    "Supervised Learning Subcategories\n",
    "- Regression (Predict Continuous Values)\n",
    "- Classification (Predict Discrete Labels)\n",
    "\n",
    "##### **Unsupervised Learning**\n",
    "Definition: The model is trained on unlabeled data, and the goal is to discover hidden patterns or structures.\n",
    "- Learns patterns in unlabeled data.\n",
    "\n",
    "Example: \n",
    "- Clustering Algorithms\n",
    "    - Clustering: k-Means (Partitioning)\n",
    "        - Divides data into k clusters based on distance metrics.\n",
    "        - Iteratively updates cluster centroids.\n",
    "    - Clustering: DBSCAN (Density-Based)\n",
    "        - Groups points based on high-density regions.\n",
    "        - Identifies noise and outliers.\n",
    "    - Dimensionality Reduction: PCA\n",
    "\n",
    "Use Cases: \n",
    "- Customer segmentation, \n",
    "- anomaly detection.\n",
    "\n",
    "Unsupervised Learning Algorithms\n",
    "- Used when the target variable is unknown (e.g., customer segmentation, anomaly detection, topic modeling)\n",
    "\n",
    "🔹 Clustering\n",
    "- K-Means\n",
    "- Hierarchical Clustering\n",
    "- DBSCAN\n",
    "- Gaussian Mixture Models (GMM)\n",
    "- Spectral Clustering\n",
    "\n",
    "🔹 Dimensionality Reduction / Data Compression\n",
    "- Principal Component Analysis (PCA)\n",
    "- t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
    "- UMAP (Uniform Manifold Approximation and Projection)\n",
    "- Autoencoders (unsupervised deep learning)\n",
    "\n",
    "🔹 Association / Co-occurrence Grouping\n",
    "- Apriori Algorithm\n",
    "- FP-Growth\n",
    "- Market Basket Analysis\n",
    "\n",
    "🔹 Anomaly Detection (Unsupervised)\n",
    "- Isolation Forest\n",
    "- One-Class SVM\n",
    "- Elliptic Envelope\n",
    "- Autoencoders (for anomaly scoring)\n",
    "\n",
    "##### Supervised Learning vs. Unsupervised Learning\n",
    "\n",
    "|Aspect | 🧠 Supervised Learning | 🧩 Unsupervised Learning|\n",
    "|-----------|-------------------------------|---------------------------|\n",
    "|Definition | Learns a mapping from input features to a known output | Learns patterns or structures from unlabeled data|\n",
    "|Goal | Predict or classify labeled outcomes | Discover hidden patterns, groupings, or structure|\n",
    "|Input Data | Features + known target (label) | Features only (no target/label)|\n",
    "|Output | Predicted label or value | Clusters, dimensions, or associations|\n",
    "|Examples of Problems | Fraud detection, disease classification, risk prediction | Customer segmentation, anomaly detection, topic modeling|\n",
    "|Key Algorithms | - Linear Regression- Logistic Regression- Decision Trees- Random Forest- XGBoost- SVM- Neural Networks | - K-Means- Hierarchica -Clustering- DBSCAN- PCA- t-SNE- Apriori- Autoencoders|\n",
    "|Evaluation Metrics | Accuracy, Precision, Recall, AUC, RMSE, MAE | Silhouette Score, Inertia, Cluster Purity, Explained Variance\n",
    "Requires Labeled Data? | ✅ Yes | ❌ No|\n",
    "Common Use Cases | - Predicting loan defaults- Diagnosing diseases- Forecasting stock prices | - Behavioral segmentation- Market basket analysis- Reducing dimensionality|\n",
    "|Interpretability | Often more interpretable depending on the model | Depends on method, often more exploratory|\n",
    "|Complexity | Can be high if overfitting/underfitting needs tuning | Often involves trial-and-error and visual validation|\n",
    "\n",
    "**How to Choose?**\n",
    "\n",
    "Ask:\n",
    "\n",
    "1. Do I know what I’m trying to predict?\n",
    "    - Yes → Supervised Learning\n",
    "    - No → Unsupervised Learning\n",
    "2. Do I have labeled outcome data?\n",
    "    - Yes → Supervised\n",
    "    - No → Unsupervised\n",
    "3. Am I trying to understand structure or reduce dimensionality?\n",
    "    - Yes → Unsupervised\n",
    "\n",
    "##### **Semi-Supervised Learning**\n",
    "Definition: Combines a small amount of labeled data with a large amount of unlabeled data to improve learning accuracy.\n",
    "- Uses both labeled and unlabeled data.\n",
    "- Used when only some data points have labels.\n",
    "\n",
    "Example: \n",
    "- Self-training (use confident predictions to label new data).\n",
    "- Label Propagation (propagate labels to similar data points).\n",
    "- Co-training.\n",
    "\n",
    "Use Cases: \n",
    "- Speech recognition, \n",
    "- web content classification.\n",
    "\n",
    "##### **Reinforcement Learning**\n",
    "Definition: The model learns by interacting with an environment and receiving feedback in the form of rewards or penalties.\n",
    "- Learns through trial and error.\n",
    "\n",
    "Example: \n",
    "- Q-Learning: Uses a Q-table to store action-reward values\n",
    "- Deep Q-Networks (DQN): Uses a neural network to approximate Q-values.\n",
    "\n",
    "Use Cases: \n",
    "- Game playing, \n",
    "- robotics.\n",
    "\n",
    "##### **Self-Supervised Learning**\n",
    "Definition: A form of unsupervised learning where the data provides its own supervision by generating labels from the input data.\n",
    "\n",
    "Examples: \n",
    "- Contrastive Learning, \n",
    "- Masked Language Models (e.g., BERT).\n",
    "\n",
    "Use Cases: \n",
    "- Natural language processing, \n",
    "- computer vision.\n",
    "\n",
    "##### **Transfer Learning**\n",
    "Definition: A model trained on one task is reused as the starting point for a model on a second task.\n",
    "\n",
    "Examples: \n",
    "- Fine-tuning pre-trained models (e.g., GPT, ResNet).\n",
    "\n",
    "Use Cases: \n",
    "- Image recognition, \n",
    "- NLP tasks.\n",
    "\n",
    "##### **Online Learning**\n",
    "Definition: The model is updated continuously as new data arrives, rather than being trained on a static dataset.\n",
    "\n",
    "Examples: \n",
    "- Stochastic Gradient Descent (SGD), \n",
    "- Online k-Means.\n",
    "\n",
    "Use Cases: \n",
    "- Real-time recommendation systems, \n",
    "- fraud detection.\n",
    "\n",
    "##### **Active Learning**\n",
    "Definition: The model actively queries the user or an oracle to label new data points that are most informative for improving the model.\n",
    "\n",
    "Examples: \n",
    "- Uncertainty Sampling, \n",
    "- Query-by-Committee.\n",
    "\n",
    "Use Cases: \n",
    "- Medical diagnosis, \n",
    "- rare event detection.\n",
    "\n",
    "Deep Learning vs Traditional Machine Learning\n",
    "|Feature\t|Deep Learning|\tTraditional ML|\n",
    "|-----------|-------------|---------------|\n",
    "Feature Engineering|\tAutomatic|\tManual |\n",
    "Model Complexity|\tHigh |\tLow  |\n",
    "Example|\tCNNs, RNNs|\tLogistic Regression, k-NN |\n",
    "\n",
    "## Choosing the Right Statistical Model\n",
    "\n",
    "The type of statistical model you use depends on your data and problem:\n",
    "\n",
    "- Linear Regression: For predicting a **continuous target variable** based on one or more predictors.\n",
    "- Logistic Regression: For predicting a **binary outcomes**, often used in classification problems.\n",
    "- ANOVA (Analysis of Variance): For comparing means across multiple groups.\n",
    "- Time Series Models: For data that’s ordered by time (e.g., ARIMA, SARIMA).\n",
    "- Survival Analysis: For time-to-event data, such as customer churn timing.\n",
    "- Multivariate Analysis: For understanding interactions across multiple variables (e.g., MANOVA, PCA).\n",
    "\n",
    "## Preprocessing the Data\n",
    "Prepare your data by cleaning and preprocessing it:\n",
    "\n",
    "- Missing Values: Decide whether to impute or drop missing values.\n",
    "- Outliers: Identify and consider handling outliers, especially in regression.\n",
    "- Data Transformation: Transform non-normal variables if required (e.g., using log transformations).\n",
    "- Feature Scaling: For some models, standardizing or normalizing data is essential.\n",
    "\n",
    "\n",
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "EDA is essential to understand: \n",
    "- patterns,\n",
    "    - visualizations\n",
    "- distributions,\n",
    "    - summary statistics\n",
    "- relationships\n",
    "    - correlation matrices\n",
    "    \n",
    "This is to identify relevant features and spot potential issues like multicollinearity.\n",
    "\n",
    "## Building the Statistical Model\n",
    "\n",
    "- **Statsmodels** provides \n",
    "    - coefficients, \n",
    "    - p-values, and \n",
    "    - confidence intervals for each variable, \n",
    "        - enabling hypothesis testing on whether each predictor significantly affects the outcome.\n",
    "\n",
    "## Evaluating Model Performance\n",
    "Regression Metrics: \n",
    "- Use R-squared, \n",
    "- Adjusted R-squared, \n",
    "- RMSE, and \n",
    "- MAE to evaluate regression models.\n",
    "\n",
    "Classification Metrics: \n",
    "- Use confusion matrix, \n",
    "- accuracy, \n",
    "- precision, \n",
    "- recall, and \n",
    "- AUC-ROC.\n",
    "\n",
    "Residual Analysis: \n",
    "- Residual plots help assess assumptions\n",
    "    - homoscedasticity, \n",
    "    - normality of residuals).\n",
    "\n",
    "## Model Interpretation\n",
    "Statistical models are highly interpretable. \n",
    "- In linear regression, each coefficient represents the expected change in the dependent variable for a one-unit change in the predictor, holding all else constant.\n",
    "\n",
    "Confidence Intervals: \n",
    "- Look at 95% CI for each coefficient; if it does not contain zero, it suggests the predictor has a statistically significant effect.\n",
    "\n",
    "P-Values: \n",
    "- A p-value below a threshold (usually 0.05) indicates that the predictor significantly affects the outcome.\n",
    "\n",
    "## Validating Assumptions\n",
    "- Linearity: Check scatter plots of residuals.\n",
    "- Normality of Residuals: Use a Q-Q plot to verify.\n",
    "- No Multicollinearity: Variance inflation factor (VIF) helps detect multicollinearity.\n",
    "- Homoscedasticity: Plot residuals vs. fitted values.\n",
    "\n",
    "## Engaging with Senior Stakeholders\n",
    "When you're talking to a senior stakeholder like the CFO, your focus should be on understanding the business context, desired outcomes, and constraints.\n",
    "\n",
    "🧠 1. Understanding the Business Problem\n",
    "\n",
    "- What is the core strategic objective you’re aiming to achieve?\n",
    "    - (e.g., “Reduce fraudulent claims,” “Optimize operating costs by 10%,” “Improve cash flow predictability”)\n",
    "- Why is this objective important now? What’s the urgency or driver?\n",
    "    - (Regulatory change? Competitive pressure? Rising losses?)\n",
    "- What does success look like in measurable terms?\n",
    "    - (E.g., “Cut fraud losses by $2M/year” or “Reduce manual processing time by 30%”)\n",
    "- Are there known bottlenecks, pain points, or suspicions we can explore with data?\n",
    "    - (e.g., “Most fraud seems to happen in claims above $5,000”)\n",
    "\n",
    "🔍 2. Scoping the Use Case\n",
    "\n",
    "- Which part of the process are we focusing on first?\n",
    "    - (Detection? Prevention? Recovery? A specific product or geography?)\n",
    "- Do we already have KPIs or benchmarks for this area?\n",
    "    - (That helps compare model performance later)\n",
    "- What constraints or risks should we be aware of?\n",
    "    - (Compliance, budget, timeline, staff resources, reputational concerns)\n",
    "\n",
    "📊 3. About Data & Resources\n",
    "\n",
    "- What internal systems or data sources can we tap into?\n",
    "    - (Claims system, finance ledger, transaction logs, case audit reports)\n",
    "- Who owns the data, and can we access it securely?\n",
    "- Is there a team or SME (subject matter expert) we can work with for context?\n",
    "\n",
    "💰 4. About Value & Prioritization\n",
    "\n",
    "- Where do you believe the biggest value or impact lies?\n",
    "    - (This guides prioritization—e.g., is it high-value fraud, or many small cases?)\n",
    "- If we find something actionable, are there teams ready to act on it?\n",
    "    - (E.g., adjust controls, launch an investigation, or redesign workflows)\n",
    "- How frequently do you want updates, and what format do you prefer for insights?\n",
    "    - (This sets stakeholder communication expectations: dashboard, report, meeting)\n",
    "\n",
    "5. 📣 Communication & Success Metrics\n",
    "\n",
    "- How would you like to receive updates?\n",
    "    - e.g., Executive summary, dashboard, workshop, email report\n",
    "- How often would you prefer updates or reviews?\n",
    "    - Weekly, biweekly, monthly?\n",
    "- What metrics or indicators matter most to you when tracking success?\n",
    "    - e.g., cost savings, detection rate, ROI, reduced processing time\n",
    "\n",
    "6. Final / Conclusion Questions\n",
    "- Who else should we speak with to understand the process or data better?\n",
    "- Are there past initiatives (successful or failed) that we should be aware of?\n",
    "- Are there strategic timelines or board deadlines we should align with?\n",
    "\n",
    "## Reporting and Communicating Results\n",
    "Present your findings by focusing on:\n",
    "\n",
    "- Key Coefficients: Explain which predictors significantly affect the outcome.\n",
    "- Model Fit: Interpret R-squared values (e.g., explaining how much variance in the target variable is explained).\n",
    "- Real-World Implications: Describe how insights from the model can impact business decisions.\n",
    "\n",
    "# Approach to statistical modeling\n",
    "\n",
    "Each model type has specific \n",
    "- applications, \n",
    "- strengths, and \n",
    "- limitations, \n",
    "\n",
    "Understand when and how to use them.\n",
    "\n",
    "### Step 1: Define Objectives and Hypotheses\n",
    "\n",
    "Identify the Problem and Objectives: \n",
    "- Clearly define the goal.\n",
    "    - Are you trying to predict, classify, find patterns, or estimate relationships? \n",
    "    - Setting objectives helps in choosing the right model.\n",
    "\n",
    "- Formulate Hypotheses: \n",
    "    - Based on the problem, develop hypotheses. \n",
    "        - For instance, in a sales prediction problem, you may hypothesize that `certain features like advertising spend, time of year, and economic indicators affect sales.`\n",
    "\n",
    "### Step 2: Data Collection and Preprocessing\n",
    "Data Collection: \n",
    "- Gather historical data related to the problem. \n",
    "\n",
    "Data Cleaning: \n",
    "- Handle missing values, remove duplicates, and ensure consistency.\n",
    "\n",
    "Feature Engineering: \n",
    "- Create new features if necessary. \n",
    "- This could involve \n",
    "    - transformations, \n",
    "    - encoding categorical variables, or \n",
    "    - creating interaction terms.\n",
    "\n",
    "full data cleaning pipeline in Python that automates common cleaning tasks using Pandas, NumPy, and Scikit-learn. This pipeline includes handling:\n",
    "- ✅ Missing values\n",
    "- ✅ Duplicates\n",
    "- ✅ Inconsistent data types\n",
    "- ✅ Categorical inconsistencies\n",
    "- ✅ Outliers\n",
    "- ✅ Imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86aa595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Automated Data Cleaning Pipeline\"\"\"\n",
    "    \n",
    "    # 1️⃣ Remove Duplicate Rows\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # 2️⃣ Handle Missing Values\n",
    "    num_cols = df.select_dtypes(include=np.number).columns\n",
    "    cat_cols = df.select_dtypes(include=\"object\").columns\n",
    "    \n",
    "    # Impute numerical missing values with median\n",
    "    num_imputer = SimpleImputer(strategy='median')\n",
    "    df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "\n",
    "    # Impute categorical missing values with mode\n",
    "    for col in cat_cols:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    \n",
    "    # 3️⃣ Standardize Categorical Values (Lowercase, Remove Spaces)\n",
    "    df[cat_cols] = df[cat_cols].apply(lambda x: x.str.lower().str.strip())\n",
    "\n",
    "    # 4️⃣ Convert Data Types (Ensure numerical columns are numeric)\n",
    "    for col in num_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # 5️⃣ Handle Outliers (Using IQR method)\n",
    "    for col in num_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])\n",
    "        df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])\n",
    "\n",
    "    # 6️⃣ Encode Categorical Variables\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example Usage:\n",
    "df = pd.DataFrame({\n",
    "    'Age': [25, np.nan, 30, 35, 200],  # Outlier in Age\n",
    "    'Salary': [50000, np.nan, 60000, 65000, 70000],\n",
    "    'City': ['New York', 'new york ', 'Los Angeles', np.nan, 'San Francisco'],\n",
    "    'Gender': ['M', 'F', 'm', np.nan, 'F']\n",
    "})\n",
    "\n",
    "cleaned_df = clean_data(df)\n",
    "print(cleaned_df)\n",
    "\n",
    "# Assume 'target' is the label column\n",
    "X = cleaned_df.drop(columns=['target'])\n",
    "y = cleaned_df['target']\n",
    "\n",
    "# Apply SMOTE for balancing\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Standardizing numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "# Splitting into Train/Test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77e8187",
   "metadata": {},
   "source": [
    "Advanced data cleaning pipeline that includes:\n",
    "- ✅ Handling missing values (numeric + categorical)\n",
    "- ✅ Removing duplicates\n",
    "- ✅ Fixing categorical inconsistencies\n",
    "- ✅ Handling outliers (IQR method)\n",
    "- ✅ Feature selection using correlation analysis\n",
    "- ✅ Handling imbalanced data (SMOTE)\n",
    "- ✅ Feature scaling (StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ca5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def clean_data(df, target_column=None):\n",
    "    \"\"\"Automated Data Cleaning Pipeline with Feature Selection and Imbalance Handling\"\"\"\n",
    "\n",
    "    # 1️⃣ Remove Duplicate Rows\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # 2️⃣ Identify Numerical and Categorical Columns\n",
    "    num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    cat_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "    \n",
    "    # 3️⃣ Handle Missing Values\n",
    "    num_imputer = SimpleImputer(strategy='median')\n",
    "    df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "\n",
    "    for col in cat_cols:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "    # 4️⃣ Standardize Categorical Values (Lowercase, Remove Spaces)\n",
    "    df[cat_cols] = df[cat_cols].apply(lambda x: x.str.lower().str.strip())\n",
    "\n",
    "    # 5️⃣ Convert Data Types (Ensure numerical columns are numeric)\n",
    "    for col in num_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # 6️⃣ Handle Outliers (Using IQR Method)\n",
    "    for col in num_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])\n",
    "        df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])\n",
    "\n",
    "    # 7️⃣ Encode Categorical Variables\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "\n",
    "    # 8️⃣ Feature Selection (Removing Low Variance Features)\n",
    "    selector = VarianceThreshold(threshold=0.01)\n",
    "    df = pd.DataFrame(selector.fit_transform(df), columns=df.columns[selector.get_support()])\n",
    "\n",
    "    # 9️⃣ Handle Imbalanced Data (SMOTE) if Target Column is Provided\n",
    "    if target_column and target_column in df.columns:\n",
    "        X = df.drop(columns=[target_column])\n",
    "        y = df[target_column]\n",
    "\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "        df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "        df[target_column] = y_resampled\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example Usage:\n",
    "df = pd.DataFrame({\n",
    "    'Age': [25, np.nan, 30, 35, 200],  # Outlier in Age\n",
    "    'Salary': [50000, np.nan, 60000, 65000, 70000],\n",
    "    'City': ['New York', 'new york ', 'Los Angeles', np.nan, 'San Francisco'],\n",
    "    'Gender': ['M', 'F', 'm', np.nan, 'F'],\n",
    "    'target': [0, 1, 0, 1, 0]  # Binary target variable for classification\n",
    "})\n",
    "\n",
    "cleaned_df = clean_data(df, target_column='target')\n",
    "print(cleaned_df)\n",
    "\n",
    "# Separate Features and Target\n",
    "X = cleaned_df.drop(columns=['target'])\n",
    "y = cleaned_df['target']\n",
    "\n",
    "# Scale Numerical Features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into Train/Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebac067",
   "metadata": {},
   "source": [
    "#### Data Splitting: \n",
    "- Split the data into training and testing sets. Typically, an 80-20 or 70-30 split is used.\n",
    "\n",
    "In machine learning, datasets are typically divided into three parts: training set, validation set, and test set. Each serves a specific purpose in building and evaluating models.\n",
    "\n",
    "**Training Set**\n",
    "\n",
    "🔹 Purpose: Used to train the machine learning model.\n",
    "\n",
    "🔹 What Happens Here?\n",
    "- The model learns patterns, relationships, and structures from the data.\n",
    "- It updates its parameters (e.g., weights in a neural network) to minimize error.\n",
    "    - 🔹 Size Considerations: Usually the largest portion of the dataset (e.g., 60-80%).\n",
    "\n",
    "**Validation Set**\n",
    "\n",
    "🔹 Purpose: Used to tune hyperparameters and prevent overfitting.\n",
    "\n",
    "🔹 What Happens Here?\n",
    "- Helps in model selection by comparing different algorithms or configurations.\n",
    "- Used for hyperparameter tuning (e.g., learning rate, number of layers in a neural network).\n",
    "- If the model performs well on the training set but poorly on the validation set, it might be overfitting.\n",
    "    - 🔹 Size Considerations: Typically 10-20% of the data.\n",
    "\n",
    "📌 Important Note: In some cases, cross-validation (e.g., k-fold cross-validation) is used instead of a fixed validation set.\n",
    "\n",
    "**Test Set**\n",
    "\n",
    "🔹 Purpose: Used to evaluate the final model’s performance on unseen data.\n",
    "\n",
    "🔹 What Happens Here?\n",
    "- It simulates real-world deployment by testing the model on completely new data.\n",
    "- The test set should not be used during training or tuning.\n",
    "    -🔹 Size Considerations: Usually 10-20% of the dataset.\n",
    "\n",
    "\n",
    "| Dataset| Used for  | Seen by Model During Training?    | Used for Model Selection?  |\n",
    "|---------------|--------|----------------------------------| -----------------------|  \n",
    "|Training Set| Learning patterns |\t✅ Yes |  ❌ No   |\n",
    "|Validation Set\t|Hyperparameter tuning, model selection | ✅ Yes (but not for learning)     |  ✅ Yes    |\n",
    "|Test Set | Final evaluation |\t\t❌ No     | ❌ No   | \n",
    "\n",
    "##### Key Concepts Related to Instances\n",
    "Labeled vs. Unlabeled Instances\n",
    "- Labeled Instance: Has both input features and the target variable (used in supervised learning).\n",
    "- Unlabeled Instance: Has only input features but no target variable (used in unsupervised learning).\n",
    "\n",
    "Training, Validation, and Test Instances\n",
    "- Training Instances: Used to train the model.\n",
    "- Validation Instances: Used to tune hyperparameters.\n",
    "- Test Instances: Used to evaluate final model performance.\n",
    "\n",
    "Instance vs. Feature\n",
    "- An instance is a full data point (row in a table).\n",
    "- A feature is an individual attribute describing an instance (column in a table).\n",
    "\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "### Why is EDA important?\n",
    "\n",
    "Exploratory Data Analysis (EDA) helps us to understand our data without making any assumptions. EDA is a vital component before we continue with the modelling phase as it provides context and guidance on the course of action to take when developing the appropriate model. It will also assist in interpreting the results correctly. Without doing EDA you will not understand your data fully.\n",
    "\n",
    "\n",
    "### The different types of EDA\n",
    "\n",
    "EDA are generally classified in two ways:\n",
    "\n",
    "    1) Non-graphical or Graphical\n",
    "    2) Univariate or Multivariate\n",
    "    \n",
    "<div align=\"left\" style=\"width: 600px; text-align: left;\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/f860f39251c523eda779dea0140316ccbefdd8e0/eda_map.jpg?raw=True\"\n",
    "     alt=\"EDA Diagram\"\n",
    "     style=\"padding-bottom=0.5em\"\n",
    "     width=600px/>\n",
    "</div>\n",
    "\n",
    "\n",
    "#### Non-graphical EDA\n",
    "Involves calculations of summary/descriptive statistics. \n",
    "\n",
    "#### Graphical EDA\n",
    "This type of analysis will contain data visualisations.\n",
    "\n",
    "#### Univariate Analysis \n",
    "This is performed on one variable at a time as the prefix 'uni' indicates. \n",
    "\n",
    "#### Multivariate Analysis \n",
    "This type of analysis explores the relationship between two or more variables. \n",
    "When only comparing two variables it is known as **bivariate analysis** as indicated by the prefix 'bi'.\n",
    "\n",
    "Read a more detailed explanation <a href=\"https://www.stat.cmu.edu/~hseltman/309/Book/chapter4.pdf\">here</a>.\n",
    "\n",
    "### 1. Basic Analysis\n",
    "\n",
    "For a practical example, we will be looking at the Medical Claims Data. Using these four commands, we will perform a basic analysis:\n",
    "\n",
    "    - df.head()\n",
    "    - df.shape\n",
    "    - df.info()\n",
    "        - feature (variable) is categorical the Dtype is object and if it is a numerical variable the Dtype is an int64 or float64. \n",
    "        - This command also shows us that out of the 1338 none of the features contain any null values.\n",
    "    - df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/regression_sprint/claims_data.csv')\n",
    "\n",
    "# Looking at the top five rows of our data\n",
    "df.head()\n",
    "\n",
    "# shape command shows us that we have x rows of data and y features.\n",
    "df.shape\n",
    "\n",
    "#  confirms our categorical and numerical features.\n",
    "df.info()\n",
    "\n",
    "# Null values for each feature can also be checked by using the following command\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83721513",
   "metadata": {},
   "source": [
    "# Population and Sample\n",
    "\n",
    "**Population**\n",
    "- Population is a collection of all data points of interest.\n",
    "    - eg: Total number of employees in the organization is known as population\n",
    "- **Parameter**\n",
    "    - numbers that are obtained when working with a population\n",
    "        - eg: total number of employees working in an organization and After completion of our survey, we arrive at number – 20000. \n",
    "    \n",
    "**Sample**\n",
    "- Sample is a subset of the population.\n",
    "    - eg: Total number of employees in a project is known as a sample.\n",
    "- **Statistic**\n",
    "    - numbers that are obtained when working with a sample\n",
    "        - eg: count the total number of employees working on a particular project. After completion of our survey, we arrive at number – 20.\n",
    "\n",
    "What to chose between Population and Sample?\n",
    "\n",
    "The real-life case scenarios, we always deal with sample data. \n",
    "- The reason behind this is that a sample is easy to collect and easier to compute than the population. \n",
    "- Based on the result that we obtained for a sample, we can then use predictive analytics to make predictions about the entire population.\n",
    "\n",
    "# The Measure of Central tendency\n",
    "\n",
    "The concept of central tendency is based on the below fact –\n",
    "- “Provided with a larger number of observations of similar type, most of the observations seems to cluster around central position when represented as a graph”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98438ff3",
   "metadata": {},
   "source": [
    "# Univariate Analysis: Non-Graphical\n",
    "\n",
    "The first univariate analysis will be non-graphical. This is where we will be looking at the **descriptive statistics** of each feature. \n",
    "\n",
    "## Continous/Numeric Feature\n",
    "\n",
    "##### **Descriptive Statistics**\n",
    "\n",
    "We can get the descriptive statistics of each **numerical feature** by using the following command:\n",
    "\n",
    "    - df.describe()\n",
    "\n",
    "This command will provide the \n",
    "- mean, \n",
    "    - also known as the arithmetic mean \n",
    "    - is the statistical average of all data points in question.\n",
    "- standard deviation and\n",
    "- The five number summary of each numerical feature.\n",
    "    - Minimum, \n",
    "    - Lower Quartile (Q1) = 25%,\n",
    "    - Median (Q2) = 50%, \n",
    "        - middlemost data point in the dataset when arranged in ascending or descending order.\n",
    "        - Higher resistance to outlier as compared to mean\n",
    "        - Median with even number of data points = average of the middle two numbers.\n",
    "        - Median with an odd number of data points = middlemost observation.\n",
    "    - Upper Quartile (Q3) = 75%, \n",
    "    - Maximum is also used for creating the box plot.\n",
    "        - exposes **Outlier**: is a data point that is significantly different from the rest of the data points in consideration.\n",
    "\n",
    "Individual statistical measures can also be calculated by using the following commands:\n",
    "\n",
    "    - df.count()\n",
    "    - df.mean()\n",
    "    - df.std()\n",
    "    - df.min()\n",
    "    - df.quantile([0.25, 0.5, 0.75], axis = 0)\n",
    "    - df.median()\n",
    "    - df.max()\n",
    "\n",
    "The three measures for central tendency are the:\n",
    "- mode\n",
    "    - Mode is basically the value that appears the most in the dataset. \n",
    "- mean and \n",
    "- median**. \n",
    "\n",
    "The command to determine the mode is:\n",
    "\n",
    "    - df.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed976189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "\n",
    "# statistics of a specific feature\n",
    "df.age.describe()\n",
    "df['age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f3953",
   "metadata": {},
   "source": [
    "##### **Dispersion of Data**\n",
    "\n",
    "Dispersion of data used to understands the distribution of data.\n",
    "- Helps to understand the variation of data and provides a piece of information about the distribution data.\n",
    "\n",
    "These include: \n",
    "- Range,\n",
    "     - measure by subtracting the lowest value from the massive Number. \n",
    "          - The wide range indicates high variability,\n",
    "          - The small range specifies low variability in the distribution.\n",
    "     - Range = Highest_value  – Lowest_value\n",
    "          - range can be influence by outliers\n",
    "- Interquartile Range (IQR),\n",
    "     - IQR is a range (the boundary between the first and second quartile) and Q3 (the boundary between the third and fourth quartile).\n",
    "     - IQR is preferred over a range as, like a range, IQR does not influence by outliers. \n",
    "     - IQR is used to measure variability by splitting a data set into four equal quartiles.\n",
    "          - IQR uses a box plot to find the outliers.\n",
    "               - Formula to find outliers: [Q1 – 1.5 * IQR, Q3 + 1.5 * IQR]\n",
    "- Variance, \n",
    "     - Variance measures how far each number in the dataset from the mean.\n",
    "\n",
    "Population variance\n",
    "$$\\sigma^2 = \\frac{\\sum (x_i - \\mu)^2}{n}$$\n",
    "sample variance\n",
    "$$ s^2 = \\frac{\\sum (x_i - \\bar{x})^2}{n - 1}$$  \n",
    "\n",
    "- Standard Deviation\n",
    "     - Standard deviation is a squared root of the variance to get original values. \n",
    "     - Low standard deviation indicates data points close to mean.\n",
    "         -  68 % of values lie within 1 standard deviation.\n",
    "         - 95 % of values lies within 2 standard deviation.\n",
    "         - 99.7 % of values lie within 3 standard deviation.\n",
    "\n",
    "Population std\n",
    "$$\\sigma = \\sqrt{\\frac{1}{N}\\sum (x_i - \\mu)^2}$$\n",
    "sample std\n",
    "$$ s = \\sqrt{\\frac{1}{n - 1}\\sum (x_i - \\bar{x})^2}$$\n",
    "\n",
    "##### Standard deviation and Mean Absolute deviation (Why SD is more reliable than MAD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00270bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d3f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8a37bde",
   "metadata": {},
   "source": [
    "# Univariate Analysis: Graphical\n",
    "\n",
    "Objective:\n",
    "- Trends and Patterns of data\n",
    "- Frequency\n",
    "- Distribution of the variables\n",
    "- Relationship that may exist between different variables\n",
    "\n",
    "You can look at the **distribution** of any numerical feature by using the following plots:\n",
    "- Scatter plot\n",
    "- histogram\n",
    "- density plot\n",
    "- box plot\n",
    "- violin plot\n",
    "    \n",
    "For a categorical feature we will use a:\n",
    "- bar plot\n",
    "\n",
    "## Continous/Numerical variable\n",
    "\n",
    "### Uni-variate summary plots :\n",
    "These plots give a more concise description of the location, dispersion, and distribution of a variable than an enumerative plot. \n",
    "- Summarizing every individual data value in a plot isn’t feasible, but it efficiently represents the entire dataset,\n",
    "\n",
    "#### Histogram and Density Plot\n",
    "\n",
    "For displaying a histogram and density plot we will be using the Matplotlib library and create a list of all numerical features to visualise these features at the same time.\n",
    "\n",
    " both the histogram and density plot display the same information. The density plot can be considered a smoothed version of the histogram and does not depend on the size of bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbdb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['age', 'bmi', 'steps', 'children', 'claim_amount'] # create a list of all numerical features\n",
    "df[features].hist(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[features].plot(kind='density', subplots=True, layout=(3, 2), sharex=False, figsize=(10, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bca4f0a",
   "metadata": {},
   "source": [
    "#### Box Plot and Violin Plot\n",
    "\n",
    "For the Box Plot and Violin Plot, we will use the seaborn library and only select one feature instead of all the numerical features. We can visualise all numerical features simultaneously, but as the range of values for each feature is different, it will not create a useful visualisation. Standardisation or normalisation can be applied to a feature to adjust the range, but we will not apply it in this notebook. Further reading on standardisation and normalisation can be done <a href=\"https://medium.com/@dataakkadian/standardization-vs-normalization-da7a3a308c64\">here</a>.\n",
    "\n",
    "The `bmi` feature will be used.\n",
    "\n",
    "Although both the box plot and violin plot display the distribution of the data, the boxplot provides certain statistics that are useful. \n",
    "\n",
    "The five vertical lines in the boxplot provide the information of the five number summary and the dots on the right hand side of the graph is a display of outliers. The violin plot focuses more on a smoothed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544a3fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='bmi', data=df)\n",
    "\n",
    "sns.set(rc={'figure.figsize':(9,9)})\n",
    "sns.boxplot(x = 'var', y = 'value', data = pd.melt(dfm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8885be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x='bmi', data=df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8647e096",
   "metadata": {},
   "source": [
    "### Univariate enumerative Plots\n",
    "\n",
    "#### Scatter plot\n",
    "\n",
    "Plots different observations/values of the same variable corresponding to the index/observation number.\n",
    "- plot the variable\n",
    "- against the corresponding observation number stored as the index of the data frame (df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36309ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.index, df['var1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6205931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x= df.index , y= ['var'], hue = df['variety'])\n",
    "\n",
    "# In seaborn, the ‘hue’ parameter, an interesting feature, determines which column in the data frame to use for color encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b30c5b",
   "metadata": {},
   "source": [
    "#### Line plot\n",
    "A line plot visualizes data by connecting the data points via line segments. \n",
    "- It resembles a scatter plot but differs by ordering the measurement points (usually by their x-axis value) and connecting them with straight line segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize': (7,7)})\n",
    "sns.set(font_scale= 1.5)\n",
    "\n",
    "fig = sns.lineplot(x = df.index, y= df['var2'], markevery = 1, marker = 'd', data = df, hue = df[variety])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5be95",
   "metadata": {},
   "source": [
    "#### Strip plot and Swarm Plot :\n",
    "- The strip plot is similar to a scatter plot.\n",
    "    - helps to plot the distribution of variables for each category as individual data points.\n",
    "- The swarm-plot, similar to a strip-plot, provides a visualization technique for univariate data to view the spread of values in a continuous variable.\n",
    "    - The only difference between the strip-plot and the swarm-plot is that the swarm-plot spreads out the data points of the variable automatically to avoid overlap and hence provides a better visual overview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0e25ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(y=df['var1'])\n",
    "sns.stripplot(x= df['variety',y=df['var1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a7ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = 'figure.figsize': (5,5))\n",
    "sns.swarmplot(x = df['var'])\n",
    "sns.swarmplot(x = df['variety'], y = df['var'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf932ae",
   "metadata": {},
   "source": [
    "### Catagorical Data\n",
    "\n",
    "#### Bar Plot\n",
    "\n",
    "For the categorical features, we can create a **bar plot** to display the frequency distribution. \n",
    "\n",
    "plot on a two-dimensional axis. \n",
    "- One axis is the category axis indicating the category, while the \n",
    "- second axis is the value axis that shows the numeric value of that category, indicated by the length of the bar.\n",
    "\n",
    "We'll generate a bar plot of the `children` feature, where each bar represents a unique number of children from the data, and the height represents how many times that number of children occurred. This can be done by using seaborn's `countplot`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bbe8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['var'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'children', data = df, palette=\"hls\")\n",
    "plt.title(\"Distribution of Children\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17982bc",
   "metadata": {},
   "source": [
    "##### Pie Chart:\n",
    "Shows the numerical proportion occupied by each category\n",
    "-  pass the array of values to the ‘labels’ parameter to add labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e58776",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df['var'].value_counts(), labels= ['cat1', 'cat2', 'cat3'], shadow= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6dd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df['var'].value_counts(), startangle= 90, autopct='%.3f', labels= ['cat1', 'cat2', 'cat3'], shadow= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db98a8b6",
   "metadata": {},
   "source": [
    "# Normal Distribution\n",
    "\n",
    "Examples like: Birth weight, the IQ Score, and stock price return often form a bell-shaped curve.\n",
    "\n",
    "Normal Distribution becomes essential for data scientists is the Central Limit Theorem\n",
    "- theorem explains the magic of mathematics and is the foundation for hypothesis testing techniques.\n",
    "\n",
    "### Properties of Normal Distribution\n",
    "- Bell-shaped curve\n",
    "    - curve is symmetric around the Mean\n",
    "    - Mean, Median, and Mode are all the same.\n",
    "- Normal Distribution is symmetric, which means its tails on one side are the mirror image of the other side\n",
    "- also call a Gaussian Distribution\n",
    "- simplify the Normal Distribution’s Probability Density by using only two parameters\n",
    "    - $\\mu$\n",
    "    - $\\sigma^2$\n",
    "- Normal distribution retains the normal shape throughout, unlike other probability distributions that change their properties after a transformation. \n",
    "\n",
    "For a Normal Distribution:\n",
    "- Product of two Normal Distribution results into a Normal Distribution\n",
    "- The Sum of two Normal Distributions is a Normal Distribution\n",
    "- Convolution of two Normal Distribution is also a Normal Distribution\n",
    "- Fourier Transformation of a Normal Distribution is also Normal\n",
    "\n",
    "Empirical Rule for Normal Distribution\n",
    "- According to the Empirical Rule for Normal Distribution:\n",
    "    - 68.27% of data lies within 1 standard deviation of the mean\n",
    "    - 95.45% of data lies within 2 standard deviations of the mean\n",
    "    - 99.73% of data lies within 3 standard deviations of the mean\n",
    "-  almost all the data lies within 3 standard deviations. \n",
    "\n",
    "This rule enables us to check for Outliers and is very helpful when determining the normality of any distribution.\n",
    "\n",
    "### Standard Normal Distribution\n",
    "Standard Normal Distribution is a special case of Normal Distribution when\n",
    "- $\\mu$ = 0\n",
    "- $\\sigma$ = 1\n",
    "\n",
    "Convert Normal Distribution into Standard Normal distribution with\n",
    "$$ Z = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "Example: Comparing Maths mark with History mark of 2 students\n",
    "- who ever get the higher z-score performed better.\n",
    "\n",
    "### Skewed Distribution\n",
    "\n",
    "When data points cluster on one side more than the other. These types of distributions are called Skewed Distributions.\n",
    "\n",
    "##### **kurtosis** and **skew**. \n",
    "\n",
    "Both kurtosis and skew are important statistical terms to be familiar with in data science. Kurtosis is the measure of outliers present in the data. **High kurtosis (>3)** indicates a large number of outliers and **low kurtosis (<3)** a lack of outliers.  Skew will indicate how symmetrical your data is. Below is a table that explains the range of values with regards to skew.\n",
    "\n",
    "Left skewed distribution\n",
    "- Mode > Median > Mean.\n",
    "\n",
    "Right Skewed Distribution\n",
    "- Mode < Median < Mean\n",
    "\n",
    "\n",
    "|   Skew Value (x)  |       Description of Data      |\n",
    "|:-------------------|:---------------:|\n",
    "| -0.5 < x < 0.5              |Fairly Symmetrical |\n",
    "| -1 < x < -0.5 | Moderate Negative Skew  | \n",
    "| 0.5 < x < 1             | Moderate Positive Skew  | \n",
    "|       x < -1     |High Negative Skew  | \n",
    "|       x > 1  |High Positve Skew | \n",
    "\n",
    "<div align=\"left\" style=\"width: 500px; font-size: 80%; text-align: left; margin: 0 auto\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/f3aeedd2c056ddd233301c7186063618c1041140/regression_analysis_notebook/skew.jpg?raw=True\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: left; padding-bottom=0.5em\"\n",
    "     width=500px/>\n",
    "     For a more detailed explanation on skew and kurtosis read <a href=\"https://codeburst.io/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa\">here</a>.\n",
    "</div>\n",
    "\n",
    "\n",
    "The commands used to determine the skewness of data are:\n",
    "\n",
    "    - df.skew()\n",
    "\n",
    "### Check the **Normality** of a Distribution\n",
    "- Histogram\n",
    "- KDE Plots\n",
    "- Q_Q Plots\n",
    "- Skewness\n",
    "- Kurtosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f58c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()\n",
    "\n",
    "# Closer to 0 implies fairly symmetrical.\n",
    "# Above 0.3 implies  moderately skewed in a positive direction.\n",
    "# Above 1 implies highly skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63643b2",
   "metadata": {},
   "source": [
    "### Kertosis\n",
    "\n",
    "Check for Normality is Kurtosis. \n",
    "\n",
    "Kurtosis gives the information regarding tailedness which basically indicates the data distribution along the tails.\n",
    "- For the symmetric type of distribution, the Kurtosis value will be close to Zero. We call such types of distributions as Mesokurtic distribution. \n",
    "    - Its tails are similar to Gaussian Distribution.\n",
    "\n",
    "- If there are extreme values present in the data, then it means that more data points will lie along with the tails. In such cases, the value of K will be greater than zero.\n",
    "    - Here, Tail will be fatter and will have longer distribution. We call such types of distributions as Leptokurtic Distribution.\n",
    "        - As we can clearly see here, the tails are fatter and denser as compared to Gaussian Distribution:\n",
    "\n",
    "- If there is a low presence of extreme values compared to Normal Distribution, then lesser data points will lie along the tail.\n",
    "    - The Kurtosis value will be less than zero. We call such types of distributions as Platykurtic Distribution. \n",
    "        - It will have a thinner tail and a shorter distribution in comparison to Normal distribution.\n",
    "\n",
    "The commands used to determine the kurtosis of data are:\n",
    "\n",
    "    - df.kurtosis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicates a lack of outliers for all features.\n",
    "df.kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb4e974",
   "metadata": {},
   "source": [
    "### Transform features into Normal/Gaussian Distribution\n",
    "- Models such as Linear Regression, Logistic Regression, Artificial Neural Networks assume that features are normally distributed\n",
    "- They perform much better if the features provided to them during modeling are normally distributed.\n",
    "\n",
    "**What do we do when data provided to us does not necessarily follow a normal distribution?**\n",
    "\n",
    "### Gaussian Distribution\n",
    "\n",
    "In probability theory, a normal (or Gaussian) distribution is a type of continuous probability distribution for a real-valued random variable.\n",
    "- general form of its probability density function is\n",
    "$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}$$\n",
    "\n",
    "Samples of the Gaussian Distribution follow a bell-shaped curve and lies around the mean. \n",
    "- The mean, median, and mode of Gaussian Distribution are the same.\n",
    "\n",
    "Steps:\n",
    "1. Check if a variable is following Normal Distribution (see above)\n",
    "- Checking the distribution of variables using a Q-Q plot\n",
    "    - Q-Q plot is a scatterplot created by plotting two sets of quantiles against one another. If both sets of quantiles came from the same distribution, we should see the points forming a roughly straight line.\n",
    "        -  if the data falls in a straight line then the variable follows normal distribution otherwise not.\n",
    "\n",
    "Example: if variable is highly positively skewed\n",
    "- plot the Q-Q plot for the variable and check.\n",
    "\n",
    "If data points of the feature are not falling on a straight line. This implies that it does not follow a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccbdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import scipy.stats as stats\n",
    "import pylab\n",
    "\n",
    "stats.probplot(cp.price,plot=pylab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82346226",
   "metadata": {},
   "source": [
    "##### Function in python which will take data and feature name as inputs and return the KDE plot and Q-Q plot of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf451ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return plots for the feature\n",
    "def normality(data,feature):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.kdeplot(data[feature])\n",
    "    plt.subplot(1,2,2)\n",
    "    stats.probplot(data[feature],plot=pylab)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1b178",
   "metadata": {},
   "source": [
    "### Performing the transformations\n",
    "\n",
    "##### **Logarithmic Transformation**\n",
    "Convert to its log value i.e log(Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd003720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing logarithmic transformation on the feature\n",
    "cp['price_log']=np.log(cp['price'])\n",
    "# plotting to check the transformation\n",
    "normality(cp,'price_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf561f",
   "metadata": {},
   "source": [
    "##### **Reciprocal Transformation**\n",
    "This will inverse values of Price i.e1/Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf75ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp['price_reciprocal']=1/cp.price\n",
    "normality(cp,'price_reciprocal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9f1ff9",
   "metadata": {},
   "source": [
    "##### **Square Root Transformation**\n",
    "\n",
    "This transformation will take the square root of the Price column i.e sqrt(Price)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f7d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp['price_sqroot']=np.sqrt(cp.price)\n",
    "normality(cp,'price_sqroot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf02f1",
   "metadata": {},
   "source": [
    "##### **Exponential Transformation**\n",
    "\n",
    "The exponential value of the Price variable will be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481728d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp['price_exponential']=cp.price**(1/1.2)\n",
    "normality(cp,'price_exponential')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac8769",
   "metadata": {},
   "source": [
    "##### **Box-Cox Transformation**\n",
    "\n",
    "$$ y_i^{(\\lambda)} = \\{ {\\frac{y_i^{(\\lambda)} - 1}{\\lambda} \\text{if} \\lambda \\neq 0, \\\\ \\ln(y_i) \\text{if} \\lambda = 0,}$$\n",
    "\n",
    "where:\n",
    "- y is the response variable and \n",
    "- λ is the transformation parameter. \n",
    "    - λ value varies from -5 to 5. \n",
    "\n",
    "During the transformation, all values of λ are considered and the optimal/best value for the variable is selected. \n",
    "- log(y) is only applied when λ=0.\n",
    "\n",
    "Box cox is more logic-based and involves the λ variable which is chosen as per the best skewness for the data so Box cox will be a better transformation to go with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660127d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp['price_Boxcox'],parameters=stats.boxcox(cp['price'])\n",
    "normality(cp,'price_Boxcox')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dcf821",
   "metadata": {},
   "source": [
    "# Types Of Probability Distribution Function in Univeriate Analysis\n",
    "\n",
    "Probability Distribution Function (PDF) is a mathematical way of showing how likely different outcomes are in a random event. \n",
    "- It gives probabilities to each possible result, and \n",
    "- Adding up all the probabilities, the total is always 1. \n",
    "The PDF helps us understand the chances of different outcomes in a random experiment.\n",
    "\n",
    "### Distribution Function\n",
    "- is a mathematical expression that describes the probability of different possible outcomes for an experiment.\n",
    "- denoted as Variable ~ Type (Characteristics)\n",
    "\n",
    "Data Types\n",
    "- We have Qualitative and Quantitative data. \n",
    "    - Quantitative data, we have \n",
    "        - Continuous data types/ random variables. \n",
    "            - Continuous data measures and can take any number of values within a given finite or infinite range.\n",
    "            - Continuous data represented in decimal format.\n",
    "            - Example: \n",
    "                - person’s height, \n",
    "                - Time, \n",
    "                - distance,\n",
    "        - Discrete data types.\n",
    "            - Discrete data is counted and can take only a limited number of values.\n",
    "            - Discrete data is represented as Whole number.\n",
    "            - Example:\n",
    "                - number of students in a class, \n",
    "                - number of workers in a company\n",
    "\n",
    "### Types of distribution functions\n",
    "\n",
    "|   Discrete distributions   |      Continuous distributions     |\n",
    "|:-------------------|:---------------:|\n",
    "|  Uniform distribution | Normal distribution |\n",
    "| Binomial distribution | Standard Normal distribution  | \n",
    "| Bernoulli distribution  | Student’s T distribution  | \n",
    "| Poisson distribution  | Chi-squared  distribution  |\n",
    "\n",
    "#### **Probability Density Function (PDF):**\n",
    "- Statistical term that describes the probability distribution of a **continuous** random variable.\n",
    "- Probability associate with a single value is always Zero.\n",
    "\n",
    "$$F(X) = P(a \\leq x \\leq b) = \\int^{b}_{a} f(x)dx \\geq 0$$\n",
    "\n",
    "#### **Probability Mass Function (PMF):**\n",
    "- Statistical term that describes the probability distribution of a **discrete** random variable.\n",
    "\n",
    "$$p(x) = P(X=x)$$\n",
    "\n",
    "Where:\n",
    "- probability of x = the probability X = one specific x\n",
    "\n",
    "#### **Cumulative Distribution Function (CDF):**\n",
    "- It is another method to describe the distribution of a random variable (either continuous or discrete).\n",
    "\n",
    "$$ F_X (x) = P(X \\leq x)$$\n",
    "\n",
    "Where:\n",
    "- F_X (x) = function of X\n",
    "- X = real value variable\n",
    "- P = probability that X will have a value less then or equal to x\n",
    "\n",
    "### Discrete Distribution\n",
    "\n",
    "##### **1. Discrete Uniform distribution**\n",
    "- Denoted as X ~ U (a, b)\n",
    "- where X is a discrete random variable that follows uniform distribution ranging from a to b.\n",
    "- Uniform distribution is when all the possible events are equally likely.\n",
    "- Example:\n",
    "    - Experiment of rolling a dice\n",
    "    - six possible events X = {1, 2, 3, 4, 5, 6} each having a probability of P(X) = 1/6.\n",
    "\n",
    "Formula for PMF, CDF of Uniform distribution function:\n",
    "\n",
    "|   Term   |     Fromula     |\n",
    "|:-------------------|:---------------:|\n",
    "|  Support | $K \\in {a, a + 1, ..., b-1, b}$ |\n",
    "| PMF | $\\frac{1}{n}$  | \n",
    "| CDF | $\\frac{[k] - a + 1}{n}$  |\n",
    "| Mean | $\\frac{(a + b)}{2}$  | \n",
    "| Variance | $\\frac{(n^2 - 1)}{12}$  |\n",
    "\n",
    "Case Study: Lottery Number Simulation\n",
    "\n",
    "A lottery system allows participants to pick a number between 1 and 6, inclusive, where each number has an equal chance of being selected. \n",
    "- This setup represents a discrete uniform distribution.\n",
    "\n",
    "PMF:\n",
    "- Since each outcome is equally likely, the probability for each number from 1 to 6 will be $\\frac{1}{6} ≈ 0.1667$.\n",
    "\n",
    "CDF:\n",
    "- The cumulative probabilities for the outcomes [1, 2, 3, 4, 5, 6] will increase incrementally as: [0.1667, 0.3334, 0.5001, 0.6668, 0.8335, 1.0]\n",
    "\n",
    "Mean:\n",
    "- For a discrete uniform distribution:\n",
    "\n",
    "$$ Mean = \\frac{Low + High}{2}$$\n",
    "$$ = \\frac{1 + 6}{2}$$\n",
    "$$ = 3.5 $$\n",
    "\n",
    "Variance:\n",
    "- For a discrete uniform distribution:\n",
    "\n",
    "$$ Variance = \\frac{(High - Low + 1)^2 - 1}{12}$$\n",
    "$$ = \\frac{(6 -1 + 1)^2}{12}$$\n",
    "$$ = \\frac{35}{12} $$\n",
    "$$ ≈ 2.92 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc94acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint\n",
    "\n",
    "# 1. Define the parameters of the discrete uniform distribution\n",
    "low, high = 1, 6  # Numbers range from 1 to 6, inclusive\n",
    "\n",
    "# 2. Simulate the discrete uniform distribution\n",
    "n_samples = 10000\n",
    "samples = np.random.randint(low, high + 1, size=n_samples)\n",
    "\n",
    "# 3. Calculate the PMF\n",
    "pmf = [1 / (high - low + 1)] * (high - low + 1)  # Since it's uniform, all probabilities are equal\n",
    "outcomes = np.arange(low, high + 1)\n",
    "\n",
    "# 4. Calculate the CDF\n",
    "cdf = np.cumsum(pmf)\n",
    "\n",
    "# 5. Mean and Variance\n",
    "mean = np.mean(samples)\n",
    "variance = np.var(samples)\n",
    "\n",
    "# 6. Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# PMF Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(outcomes, pmf, color='skyblue', alpha=0.7)\n",
    "plt.title(\"PMF of Discrete Uniform Distribution\")\n",
    "plt.xlabel(\"Outcomes\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xticks(outcomes)\n",
    "\n",
    "# CDF Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.step(outcomes, cdf, where='post', color='orange', label=\"CDF\")\n",
    "plt.title(\"CDF of Discrete Uniform Distribution\")\n",
    "plt.xlabel(\"Outcomes\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.xticks(outcomes)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Print results\n",
    "print(\"PMF:\", pmf)\n",
    "print(\"CDF:\", cdf)\n",
    "print(f\"Simulated Mean: {mean:.2f}\")\n",
    "print(f\"Simulated Variance: {variance:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea177ec7",
   "metadata": {},
   "source": [
    "##### **2. Binomial distribution**\n",
    "- Denoted as X ~ B(n, p).\n",
    "- where X is a discrete random variable that follows Binomial distribution with parameters n, p.\n",
    "    - n is the no. of trials,\n",
    "    - p is the success probability for each trial.\n",
    "- Probability distribution of the number of successes in ‘n’ independent experiments sequence.\n",
    "    - Binomial event suggests the no. of times a specific outcome can be expected.\n",
    "- The two outcomes of a Binomial trial could be \n",
    "    - Success is denoted as 1, and the probability associated with it is p.\n",
    "    - Failure is denoted as 0, and the probability associate with it is q = 1-p.\n",
    "- Example: \n",
    "    - Success/Failure, \n",
    "    - Pass/Fail/, \n",
    "    - Win/Lose,\n",
    "\n",
    "\n",
    "|   Term   |     Fromula     |\n",
    "|:-------------------|:---------------:|\n",
    "| PMF | $ \\left(^{n}_{k}\\right) p^k q^{n - k}$  | \n",
    "| CDF | $I_q ( n - k, 1 + k)$  |\n",
    "| Mean | $n \\times p$  | \n",
    "| Variance | $ n \\times p \\times q$  |\n",
    "\n",
    "Case Study: Quality Control in Manufacturing\n",
    "- A manufacturing plant produces light bulbs. We inspect a batch of 10 bulbs. Each light bulb has a: \n",
    "    - 90% probability of passing quality control (success) and a \n",
    "    - 10% probability of failing (failure). \n",
    "\n",
    "PMF: \n",
    "- The PMF provides the probability of having exactly 𝑘 successes in n trials:\n",
    "- For example, P(X=9) represents the probability that 9 out of 10 light bulbs pass quality control.\n",
    "\n",
    "$$ \\left(^{n}_{k}\\right) p^k q^{n - k}$$\n",
    "\n",
    "CDF: \n",
    "- The CDF provides the cumulative probability of having up to k successes:\n",
    "$$ P(X \\leq k) = \\sum P(X = i) $$\n",
    "\n",
    "\n",
    "Mean: For a Binomial distribution:\n",
    "$$ Mean = n \\times p$$\n",
    "$$ = 10 \\times 0.9 $$\n",
    "$$ =0 $$\n",
    "\n",
    "Variance: For a Binomial distribution:\n",
    "\n",
    "$$ Variance= n \\times p \\times (1−p) $$\n",
    "$$ =10⋅0.9⋅0.1 $$\n",
    "$$ =0.9 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "\n",
    "# 1. Define the parameters of the Binomial distribution\n",
    "n = 10  # Number of trials (light bulbs in the batch)\n",
    "p = 0.9  # Probability of success (passing quality control)\n",
    "\n",
    "# 2. Simulate the Binomial distribution\n",
    "n_samples = 10000\n",
    "samples = np.random.binomial(n, p, size=n_samples)\n",
    "\n",
    "# 3. Calculate the PMF\n",
    "x = np.arange(0, n + 1)  # Possible outcomes: 0 to n successes\n",
    "pmf = binom.pmf(x, n, p)\n",
    "\n",
    "# 4. Calculate the CDF\n",
    "cdf = binom.cdf(x, n, p)\n",
    "\n",
    "# 5. Mean and Variance\n",
    "mean = n * p  # Mean of a Binomial distribution\n",
    "variance = n * p * (1 - p)  # Variance of a Binomial distribution\n",
    "\n",
    "# 6. Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# PMF Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(x, pmf, color='skyblue', alpha=0.7, label='PMF')\n",
    "plt.title(\"PMF of Binomial Distribution\")\n",
    "plt.xlabel(\"Number of Successes\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend()\n",
    "\n",
    "# CDF Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.step(x, cdf, where='post', color='orange', label='CDF')\n",
    "plt.title(\"CDF of Binomial Distribution\")\n",
    "plt.xlabel(\"Number of Successes\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Print results\n",
    "print(\"PMF:\", pmf)\n",
    "print(\"CDF:\", cdf)\n",
    "print(f\"Theoretical Mean: {mean:.2f}\")\n",
    "print(f\"Theoretical Variance: {variance:.2f}\")\n",
    "print(f\"Simulated Mean: {np.mean(samples):.2f}\")\n",
    "print(f\"Simulated Variance: {np.var(samples):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4ee37e",
   "metadata": {},
   "source": [
    "##### **3. Bernoulli distribution**\n",
    "- denoted as X ~ Bern(p).\n",
    "- Where X is a discrete random variable that follows Bernoulli distribution with parameter p.\n",
    "    - Where p is the probability of the success.\n",
    "- Bernoulli is a Binomial experiment with a single trial.\n",
    "    - Bernoulli’s event suggests which outcome can be expected for a single trial.\n",
    "- Example: tossing a fair. The two possible outcomes are \n",
    "    - Heads, Tails. \n",
    "    - The probability (p) associated with each of them is 1/2.\n",
    "- Example: In an unfair coin\n",
    "    - Heads can have a probability of p = 0.8, then the probability of tail q = 1-p = 1-0.8 = 0.2\n",
    "\n",
    "|   Term   |     Fromula     |\n",
    "|:-------------------|:---------------:|\n",
    "| PMF | $ \\{ q = 1- p \\text{  if  } k = 0 \\\\ \\{ p \\text{  if  } k = 1 \\\\ p^k (1 - p)^{1 - k}$  | \n",
    "| CDF | $\\{ 0 = 1- p \\text{  if  } k < 0 \\\\ \\{ 1 - p \\text{  if  } 0 \\leq k < 1 \\\\ \\{ 0 = 1- p \\text{  if  } k \\geq  1$  |\n",
    "| Mean | $ p$  | \n",
    "| Variance | $ p( 1 - p) = p \\times q$  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d4abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# 1. Define the parameters of the Bernoulli distribution\n",
    "p = 0.5  # Probability of success (Heads)\n",
    "\n",
    "# 2. Simulate the Bernoulli distribution\n",
    "n_samples = 10000\n",
    "samples = np.random.binomial(1, p, size=n_samples)  # Equivalent to Bernoulli\n",
    "\n",
    "# 3. Calculate the PMF\n",
    "x = [0, 1]  # Possible outcomes: 0 (Tails), 1 (Heads)\n",
    "pmf = bernoulli.pmf(x, p)\n",
    "\n",
    "# 4. Calculate the CDF\n",
    "cdf = bernoulli.cdf(x, p)\n",
    "\n",
    "# 5. Mean and Variance\n",
    "mean = p  # Mean of a Bernoulli distribution\n",
    "variance = p * (1 - p)  # Variance of a Bernoulli distribution\n",
    "\n",
    "# 6. Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# PMF Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(x, pmf, color='skyblue', alpha=0.7, label='PMF')\n",
    "plt.title(\"PMF of Bernoulli Distribution\")\n",
    "plt.xlabel(\"Outcomes (0: Tails, 1: Heads)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xticks(x)\n",
    "plt.legend()\n",
    "\n",
    "# CDF Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.step(x, cdf, where='post', color='orange', label='CDF')\n",
    "plt.title(\"CDF of Bernoulli Distribution\")\n",
    "plt.xlabel(\"Outcomes (0: Tails, 1: Heads)\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.xticks(x)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Print results\n",
    "print(\"PMF:\", pmf)\n",
    "print(\"CDF:\", cdf)\n",
    "print(f\"Theoretical Mean: {mean:.2f}\")\n",
    "print(f\"Theoretical Variance: {variance:.2f}\")\n",
    "print(f\"Simulated Mean: {np.mean(samples):.2f}\")\n",
    "print(f\"Simulated Variance: {np.var(samples):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3717be0d",
   "metadata": {},
   "source": [
    "##### **4. Poisson Distribution**\n",
    "- Denoted as X ~ Po(λ). \n",
    "- Where X is a discrete random variable that follows Poisson Distribution with parameter λ.\n",
    "    - Where λ is the expected rate of occurrences.\n",
    "- It expresses the probability of a given number of events occurring in a fixed time interval.\n",
    "- Examples: \n",
    "    - The number of diners at a restaurant on a given day.\n",
    "    - Calls per hour at a call centre.\n",
    "\n",
    "|   Term   |     Fromula     |\n",
    "|:-------------------|:---------------:|\n",
    "| PMF | $ \\frac{\\lambda^k e^{-\\lambda}}{k!} $ | \n",
    "| CDF | $ e^{-\\lambda} \\sum^{[k]}_{i = 0} \\frac{\\lambda^i}{i!}$  |\n",
    "| Mean | $ \\lambda $  | \n",
    "| Variance | $ \\lambda $  |\n",
    "\n",
    "Case Study: Website Traffic\n",
    "- A website receives an average of λ=3 inquiries per minute. \n",
    "- The number of inquiries in any given minute can be modeled using a Poisson distribution.\n",
    "\n",
    "PMF:\n",
    "-  for λ=3 and k=2:\n",
    "\n",
    "$$ P(X = k ) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$$\n",
    "$$ P(X = 2 ) = \\frac{3^2 e^{-3}}{2!}$$\n",
    "$$ 0.224$$\n",
    "\n",
    "CDF: \n",
    "\n",
    "Mean:\n",
    "- Mean=λ=3\n",
    "\n",
    "Variance:\n",
    "- Variance=λ=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1738b0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# 1. Define the parameter of the Poisson distribution\n",
    "lam = 3  # Average rate (inquiries per minute)\n",
    "\n",
    "# 2. Simulate the Poisson distribution\n",
    "n_samples = 10000\n",
    "samples = np.random.poisson(lam, size=n_samples)\n",
    "\n",
    "# 3. Calculate the PMF\n",
    "x = np.arange(0, 15)  # Possible outcomes (0 to 14 inquiries)\n",
    "pmf = poisson.pmf(x, lam)\n",
    "\n",
    "# 4. Calculate the CDF\n",
    "cdf = poisson.cdf(x, lam)\n",
    "\n",
    "# 5. Mean and Variance\n",
    "mean = lam  # Mean of a Poisson distribution\n",
    "variance = lam  # Variance of a Poisson distribution\n",
    "\n",
    "# 6. Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# PMF Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(x, pmf, color='skyblue', alpha=0.7, label='PMF')\n",
    "plt.title(\"PMF of Poisson Distribution\")\n",
    "plt.xlabel(\"Number of Inquiries\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend()\n",
    "\n",
    "# CDF Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.step(x, cdf, where='post', color='orange', label='CDF')\n",
    "plt.title(\"CDF of Poisson Distribution\")\n",
    "plt.xlabel(\"Number of Inquiries\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Print results\n",
    "print(\"PMF:\", pmf)\n",
    "print(\"CDF:\", cdf)\n",
    "print(f\"Theoretical Mean: {mean:.2f}\")\n",
    "print(f\"Theoretical Variance: {variance:.2f}\")\n",
    "print(f\"Simulated Mean: {np.mean(samples):.2f}\")\n",
    "print(f\"Simulated Variance: {np.var(samples):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b304420",
   "metadata": {},
   "source": [
    "### Continuous Distributions\n",
    "\n",
    "##### **1. Normal or Gaussian Distribution**\n",
    "- denoted as $X ~ N (μ, σ^2)$. \n",
    "- Where  X is a continuous random variable that follows a Normal distribution with parameters μ, σ2.\n",
    "    - μ is the mean. \n",
    "    - $σ^2$ is the variance.\n",
    "- describes the probability of a continuous random variable that takes real values.\n",
    "- Examples:\n",
    "    - Heights of people, \n",
    "    - exam scores of students, \n",
    "    - IQ Scores,\n",
    "- Normal distribution follows the 68-95-99.7 rule (empirical rule). \n",
    "    - 68% of data lies in the first standard deviation range, \n",
    "    - 95% of data lies in the second standard deviation range, and \n",
    "    - 99.7% of data lies in the third standard deviation range.\n",
    "\n",
    "Properties of Normal distribution:\n",
    "- The random variable takes values from -∞ to +∞\n",
    "- The probability associate with any single value is Zero.\n",
    "- looks like a bell curve and is symmetric about x=μ. \n",
    "    - 50% of data lies on the left-hand side and \n",
    "    - 50% of the data lies on the right-hand side.\n",
    "- The area under the curve (AUC) = 1\n",
    "- All the measures of central tendency coincide i.e., mean = median = mode\n",
    "\n",
    "|   Term   |     Fromula     |\n",
    "|:-------------------|:---------------:|\n",
    "| PDF | $ \\frac{1}{\\sigma \\sqrt{2\\pi}} e{-\\frac{1}{2}(\\frac{x = \\mu}{\\sigma})^2} $ | \n",
    "| CDF | $ \\frac{1}{2} [ 1 = erf(\\frac{x = \\mu}{\\sigma\\sqrt{2}})$  |\n",
    "| Mean | $ \\mu $  | \n",
    "| Variance | $ \\sigma^2 $  |\n",
    "\n",
    "Case Study: Human Heights\n",
    "- Assume the heights of adults in a population are normally distributed with:\n",
    "    - μ=170 cm (average height).\n",
    "    - σ=10 cm (standard deviation).\n",
    "\n",
    "PDF:\n",
    "- μ=170, \n",
    "- σ=10, and\n",
    "- x=180:\n",
    "\n",
    "$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e{-\\frac{1}{2}(\\frac{x = \\mu}{\\sigma})^2}$$ \n",
    "\n",
    "CDF:\n",
    "- For x = 180, P(X≤180)≈0.841.\n",
    "\n",
    "Mean:\n",
    "- Mean=μ=170,\n",
    "\n",
    "Variance: \n",
    "- Variance= $σ^2$=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ffc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# 1. Define the parameters of the Normal distribution\n",
    "mu = 170  # Mean (average height in cm)\n",
    "sigma = 10  # Standard deviation (spread of height in cm)\n",
    "\n",
    "# 2. Simulate the Normal distribution\n",
    "n_samples = 10000\n",
    "samples = np.random.normal(mu, sigma, size=n_samples)\n",
    "\n",
    "# 3. Calculate the PDF\n",
    "x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 1000)  # Range of values (±4σ)\n",
    "pdf = norm.pdf(x, mu, sigma)\n",
    "\n",
    "# 4. Calculate the CDF\n",
    "cdf = norm.cdf(x, mu, sigma)\n",
    "\n",
    "# 5. Mean and Variance\n",
    "mean = mu  # Mean of a Normal distribution\n",
    "variance = sigma ** 2  # Variance of a Normal distribution\n",
    "\n",
    "# 6. Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# PDF Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, pdf, color='skyblue', label='PDF')\n",
    "plt.title(\"PDF of Normal Distribution\")\n",
    "plt.xlabel(\"Height (cm)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "\n",
    "# CDF Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, cdf, color='orange', label='CDF')\n",
    "plt.title(\"CDF of Normal Distribution\")\n",
    "plt.xlabel(\"Height (cm)\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Print results\n",
    "print(f\"Theoretical Mean: {mean:.2f}\")\n",
    "print(f\"Theoretical Variance: {variance:.2f}\")\n",
    "print(f\"Simulated Mean: {np.mean(samples):.2f}\")\n",
    "print(f\"Simulated Variance: {np.var(samples):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c575b0",
   "metadata": {},
   "source": [
    "# Multivariate Analysis: Non-Graphical \n",
    "\n",
    "### Continuous - Continuous\n",
    "\n",
    "##### **Covariance**\n",
    "\n",
    "Statistical tool that helps to quantify the total variance of random variables from their expected value(Mean).\n",
    "- it is a measure of the linear relationship between two random variables. \n",
    "- It can take any positive and negative values.\n",
    "    - Positive Covariance: \n",
    "        - It indicates that two variables tend to move in the same direction, which means that if we increase the value of one variable other variable value will also increase.\n",
    "    - Zero Covariance: \n",
    "        - It indicates that there is no linear relationship between them.\n",
    "    - Negative Covariance: \n",
    "        - It indicates that two variables tend to move in the opposite direction, which means that if we increase the value of one variable other variable value will decrease and vice versa.\n",
    "\n",
    "Formula:\n",
    "\n",
    "Limitations of Covariance\n",
    " -Covariance magnitude does not signify the strength of their relationship, so what only matters is the sign, whether it is positive or negative which tells the relationship.\n",
    "- If we convert or scale the measurements of the variable X and Y, then Cov(X’, Y’) ≠ Cov(X, Y) should not happen.\n",
    "- Covariance does not capture the non-linear relationship between two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d4dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau, pointbiserialr\n",
    "\n",
    "# Step 1: Simulate Two Random Variables\n",
    "np.random.seed(42)  # For reproducibility\n",
    "n = 100  # Number of samples\n",
    "\n",
    "# Variable X: Continuous Random Variable\n",
    "X = np.random.normal(loc=50, scale=10, size=n)  # Mean=50, Std Dev=10\n",
    "\n",
    "# Variable Y: Continuous Random Variable\n",
    "Y = 0.5 * X + np.random.normal(loc=0, scale=5, size=n)  # Linear relationship with noise\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "data = pd.DataFrame({'X': X, 'Y': Y})\n",
    "\n",
    "# Step 2: Covariance Calculation\n",
    "# Covariance measures how two variables vary together.\n",
    "cov_matrix = np.cov(X, Y)\n",
    "covariance = cov_matrix[0, 1]\n",
    "\n",
    "print(f\"Covariance between X and Y: {covariance:.4f}\")\n",
    "\n",
    "# Step 3: Pearson Correlation Coefficient\n",
    "# Measures linear correlation between X and Y.\n",
    "pearson_corr, pearson_p_value = pearsonr(X, Y)\n",
    "print(f\"Pearson Correlation Coefficient: {pearson_corr:.4f}, p-value: {pearson_p_value:.4f}\")\n",
    "\n",
    "# Step 4: Spearman's Rank Correlation\n",
    "# Measures monotonic relationship between variables.\n",
    "spearman_corr, spearman_p_value = spearmanr(X, Y)\n",
    "print(f\"Spearman's Rank Correlation: {spearman_corr:.4f}, p-value: {spearman_p_value:.4f}\")\n",
    "\n",
    "# Step 5: Kendall's Tau Rank Correlation\n",
    "# Measures ordinal association between variables.\n",
    "kendall_corr, kendall_p_value = kendalltau(X, Y)\n",
    "print(f\"Kendall's Tau Correlation: {kendall_corr:.4f}, p-value: {kendall_p_value:.4f}\")\n",
    "\n",
    "# Step 6: Point Biserial Correlation\n",
    "# Requires one continuous variable and one binary variable.\n",
    "# Simulate a binary variable from X.\n",
    "Z = (X > np.median(X)).astype(int)  # Binary variable based on X's median\n",
    "point_biserial_corr, point_biserial_p_value = pointbiserialr(Z, Y)\n",
    "print(f\"Point Biserial Correlation: {point_biserial_corr:.4f}, p-value: {point_biserial_p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b003ca2",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "For this analysis, we can **determine the relationship between any two numerical features** by calculating the **correlation coefficient**. \n",
    "- Correlation is a measure of the degree to which two variables change together, if at all. \n",
    "    - If two features have a strong positive correlation, it means that if the value of one feature increases, the value of the other feature also increases. \n",
    "    - There are three different correlation measures:\n",
    "        - Pearson correlation \n",
    "        - Spearman rank correlation\n",
    "        - Kendall correlation\n",
    "\n",
    "For this lesson, we will focus on the **Pearson correlation**. The Pearson correlation measures the linear relationship between features and assumes that the features are normally distributed. Below is a table that explains how to interpret the Pearson correlation measure:\n",
    "\n",
    "|   Pearson Correlation Coefficient (r)  |       Description of Relationship     |\n",
    "|:-------------------|:---------------:|\n",
    "|  r = -1              |Perfect Negative Correlation |\n",
    "| -1 < r < -0.8 | Strong Negative Correlation  | \n",
    "| - 0.8 < r < -0.5             | Moderate Negative Correlation  | \n",
    "|       - 0.5 < r < 0     |Weak Negative Correlation  | \n",
    "|       r = 0  |No Linear Correlation | \n",
    "| 0 < r < 0.5 | Weak Positive Correlation  | \n",
    "| 0.5 < r < 0.8             | Moderate Positive Correlation  | \n",
    "|       0.8 < r < 1     |Strong Positive Correlation  | \n",
    "|       r = 1  |Perfect Positive Correlation | \n",
    "\n",
    "\n",
    "<div align=\"left\" style=\"width: 800px; text-align: left;\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/f3aeedd2c056ddd233301c7186063618c1041140/regression_analysis_notebook/pearson_corr.jpg?raw=True\"\n",
    "     alt=\"Pearson Correlation\"\n",
    "     style=\"padding-bottom=0.5em\"\n",
    "     width=800px/>\n",
    "</div>\n",
    "\n",
    "For a more detailed explanation of correlations, read <a href=\"https://medium.com/fintechexplained/did-you-know-the-importance-of-finding-correlations-in-data-science-1fa3943debc2#:~:text=Correlation%20is%20a%20statistical%20measure,to%20forecast%20our%20target%20variable.&text=It%20means%20that%20when%20the,variable(s)%20also%20increases.\">here</a>.\n",
    "\n",
    "The command we will use to determine the correlation between features is:\n",
    "\n",
    "    - df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a15749",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1483a",
   "metadata": {},
   "source": [
    "# Multivariate Analysis: Graphical\n",
    "\n",
    "For the multivariate graphical analysis the following visualisations will be considered:\n",
    "\n",
    "    - Heatmap\n",
    "    - Scatter Plot\n",
    "    - Pair Plot\n",
    "    - Joint Plot\n",
    "    - Bubble Plot\n",
    "    \n",
    "#### Heatmap\n",
    "\n",
    "The relationship between features can also be displayed graphically using a **heatmap**. The Seaborn library will be used for this basic heatmap visualisation. \n",
    "\n",
    "To see how different heatmap variations can be created, read <a href=\"https://medium.com/@szabo.bibor/how-to-create-a-seaborn-correlation-heatmap-in-python-834c0686b88e\">here</a>.\n",
    "\n",
    "The correlation coefficient value will be displayed on the heatmap using the `vmin` and `vmax` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bcf0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6d815",
   "metadata": {},
   "source": [
    "#### Scatter Plot\n",
    "\n",
    "A Scatter plot is used to visualise the relationship between two different features and is most likely the primary multivariate graphical method. For this exercise, we will create a scatter plot to determine if there is a relationship between `bmi` and `age`. The parameter `hue` is set to the feature `insurance_claim`, colouring the points according to whether or not a claim was submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d5bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='age',y='bmi',hue='insurance_claim', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c43d2",
   "metadata": {},
   "source": [
    "#### Pair Plot\n",
    "\n",
    "A pair plot can be used to visualise the relationships between all the numerical features at the same time. \n",
    "\n",
    "The `hue` is once again set to the feature `insurance_claim` to indicate which data points submitted an insurance claim and which didn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afc8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.pairplot(df, hue=\"insurance_claim\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd213a",
   "metadata": {},
   "source": [
    "#### Joint Plot\n",
    "\n",
    "The joint plot can be used to provide univariate and multivariate analyses at the same time. The central part of the plot will be a scatter plot comparing two different features. The top and right visualisations will display the distribution of each feature as a histogram. \n",
    "\n",
    "For this joint plot, we will once again compare `age` and `bmi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5370b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x = 'age', y = 'bmi', data = df)\n",
    "\n",
    "# including the hue as insurance_claim\n",
    "sns.jointplot(x = 'age', y = 'bmi', data = df, hue='insurance_claim')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd4b7a1",
   "metadata": {},
   "source": [
    "#### Bubble Plots\n",
    "\n",
    "A bubble plot is a variation of a scatter plot. Bubbles vary in size, dependent on another feature in the data. The same applies to the colour of the bubbles; which can be set to vary with the values of another feature. This way, we can visualise up to four dimensions/features at the same time.\n",
    "\n",
    "For this bubble plot, `bmi` and `claim_amount` will be plotted on the x-axis and y-axis, respectively. The colours of the bubbles will vary based on whether the observation is a `smoker` or not, and lastly, the size of the bubbles will vary based on the number of `children` the observation has. We will create this bubble plot by using `seaborn`’s scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(x=\"bmi\", \n",
    "                y=\"claim_amount\",\n",
    "                size=\"children\",\n",
    "                sizes=(20,100),\n",
    "                alpha=0.8,\n",
    "                hue=\"smoker\",\n",
    "                data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bcc8a9",
   "metadata": {},
   "source": [
    "## Splitting the Data\n",
    "### Two-Way Split\n",
    "\n",
    "When fitting a machine learning model to some data, we ultimately intend to use that model to make predictions/forecasts on real-world data. \n",
    "- Real-world data is unseen - it doesn't exist in the dataset we have at our disposal - so in order to validate our model (check how well it performs), we need to test it on unseen data too.\n",
    "- Gathering unseen data is not as simple as collecting it from outside the window and exposing it to the model: any new data would need to be \n",
    "    - cleaned, \n",
    "    - wrangled and \n",
    "    - annotated just like the data in our dataset.\n",
    "- The next best thing, then, is to simulate some unseen data, which we can do using the existing dataset by splitting it into two sets:\n",
    "    - One for training the model; and\n",
    "    - A second for testing it.\n",
    "   \n",
    "We fit a model using the training data, and then assess its accuracy using the test set.\n",
    "- use 80% of the data for training and \n",
    "    - the training set will contain 80% of the rows, or data points,\n",
    "- keep 20% aside for testing. \n",
    "    - and the remaining 20% of rows will be in the test set.\n",
    "These rows are selected at random, to ensure that the mix of data in the train set is as close as possible to the mix in the test set.\n",
    "\n",
    "### Three-Way Split\n",
    "\n",
    "Many academic works on machine learning talk about splitting the dataset into three distinct parts: \n",
    "- `train`, \n",
    "    - training set is used to fit the model to the observations.\n",
    "- `validation,` and\n",
    "    -  during the model tuning process where hyperparameters are tweaked and decisions on the dataset is made, the validation set is used to test the performance of the model.\n",
    "- `test` sets. \n",
    "    - Once the model designer is satisfied with the performance of the model on the validation set, the previously unseen test set is brought out and used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "#### Caveats for using a validation set\n",
    "\n",
    "On small datasets, it may not be feasible to include a validation set for the following reasons, both of which should be intuitive:\n",
    "\n",
    "- The model may need every possible data point to adequately determine model values;\n",
    "- For small enough test sets, the uncertainty of the test set can be considerably large to the point where different test sets may produce very different results.\n",
    "\n",
    "Clearly, further splitting the training data into training and validation sets would remove precious observations for the training process.\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "In the case that the designer does not desire to use a validation set, or there is simply not enough data, \n",
    "- a technique known as cross validation may be used. \n",
    "A common version of cross validation is known as K-fold cross validation: \n",
    "- during the training process, some proportion of the training data, say 10%, is held back, and effectively used as a validation set while the model parameters are calcuated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import seaborn as sns\n",
    "\n",
    "# Import the split function from sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into the response, y, and features, X\n",
    "y = df['ZAR/USD']\n",
    "X = df.drop('ZAR/USD', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26658ff9",
   "metadata": {},
   "source": [
    "Understand the four parameters to hand to the splitting function.\n",
    "\n",
    "- `X` contains the features on which we will be training the model. In this case: just `exports`;\n",
    "- `y` is the response variable, that which we are trying to predict. In this case: `exchange rate`;\n",
    "- `test_size` is a value between 0 and 1: the proportion of our dataset that we want to be used as test data. Typically 0.2 (20%);\n",
    "- `random_state` is an arbitrary value which, when set, ensures that the _random_ nature in which rows are picked to be in the test set is the same each time the split is carried out. In other words, the rows are picked at random, but we can ensure these random picks are repeatable by using the same value here. This makes it easier to assess model performance across iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02311b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Call the train_test_split function:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd317a",
   "metadata": {},
   "source": [
    "Plotting the data points in each of the training and testing sets in different colours, we should be able to see that we have a similar spread of data in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1299f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.scatter(X_train, y_train, color='green', label='Training')  # plot the training data in green\n",
    "plt.scatter(X_test, y_test, color='darkblue', label='Testing')  # plot the testing data in blue\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d204818a",
   "metadata": {},
   "source": [
    "## Advanced plotting\n",
    "Let's try and create something a little more visually appealing than the two plots above.\n",
    "​\n",
    "- We'll plot both dependent data series on the same graph;\n",
    "- We'll assign two separate y-axes: one for each series;\n",
    "- We'll display a legend near the top of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc('mathtext', default='regular')\n",
    "# Create blank figure\n",
    "fig = plt.figure()\n",
    "\n",
    "# Split figure to allow two sets of y axes\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot the first line on its axis\n",
    "ax.plot(np.arange(len(df.Y)), df.Y, '-', label = 'ZAR/USD', color='orange')\n",
    "\n",
    "# Create second y axis and plot second line\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(np.arange(len(df.X)), df.X, '-', label = 'Exports (ZAR)')\n",
    "\n",
    "# Add legends for each axis\n",
    "ax.legend(loc=2)\n",
    "ax2.legend(loc=9)\n",
    "\n",
    "ax.grid()\n",
    "\n",
    "# Set labels of axes\n",
    "ax.set_xlabel(\"Months\")\n",
    "ax.set_ylabel(\"ZAR/USD\")\n",
    "ax2.set_ylabel(\"Exports (ZAR, millions)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda9b41d",
   "metadata": {},
   "source": [
    "### Step 3: Select the Type of Statistical Model\n",
    "Statistical models can be broadly categorized as:\n",
    "\n",
    "- **Descriptive Models**: Summarize data patterns.\n",
    "- **Inferential Models**: Help make inferences about the population.\n",
    "- **Predictive Models**: Used to predict future outcomes based on historical data.\n",
    "- **Prescriptive Models**: Suggest actions based on predictions.\n",
    "\n",
    "Let's go through common types of statistical models and their applications.\n",
    "\n",
    "# Regression Analysis\n",
    " \n",
    "Regression Analysis is a statistical method to analyze the relationship between a dependent variable and one or more independent variables.\n",
    "\n",
    "Use regression analysis for one of two purposes: \n",
    "- predict the value of the dependent variable when you know the independent variables or \n",
    "- predict the effect of an independent variable on the dependent variable.\n",
    "\n",
    "### Types of regression analysis\n",
    "\n",
    "- **Simple linear regression**\n",
    "    - Assumes a linear connection between a dependent variable (Y) and an independent variable (X).\n",
    "    - linear regression model can be simple \n",
    "        - with only one dependent and one independent variable.\n",
    "    - A real estate agent wants to determine the relationship between the size of a house (in square feet) and its selling price. They can use simple linear regression to predict the selling price of a house based on its size.\n",
    "    \n",
    "-  **Multiple Linear Regression / Multivariate Linear Regression**\n",
    "    - Assumes a linear connection between a dependent variable (Y) and an independent variable (X).\n",
    "    - linear regression model can be complex \n",
    "        - with numerous dependent and independent variables\n",
    "        - with one dependent variable and more than one independent variable.\n",
    "    - A car manufacturer wants to predict the fuel efficiency of their vehicles based on various independent variables such as engine size, horsepower, and weight.\n",
    "    \n",
    "- **Logistic regression**\n",
    "    - Used When the dependent variable is discrete.\n",
    "        - the target variable can take on only one of two values, \n",
    "    - The sigmoid curve represents its connection to the independent variable, and probability has a value between 0 and 1.\n",
    "    - A bank wants to predict whether a customer will default on their loan based on their credit score, income, and other factors. By using logistic regression, the bank can estimate the probability of default and take appropriate measures to minimize their risk.\n",
    "\n",
    "- **Polynomial Regression**\n",
    "    - Represents a non-linear relationship between dependent and independent variables. \n",
    "    - This technique is a variant of the multiple linear regression model, but the best fit line is curved rather than straight.\n",
    "\n",
    "- **Ridge Regression**\n",
    "    - Applied when the independent variables are highly correlated.\n",
    "        - When data exhibits multicollinearity\n",
    "    - While least squares estimates are unbiased in multicollinearity, their variances are significant enough to cause the observed value to diverge from the actual value. \n",
    "    - Ridge regression reduces standard errors by biassing the regression estimates.\n",
    "    - The lambda (λ) variable in the ridge regression equation resolves the multicollinearity problem.\n",
    "\n",
    "- **Lasso Regression**\n",
    "    - Lasso regression (Least Absolute Shrinkage and Selection Operator) technique penalizes the absolute magnitude of the regression coefficient. \n",
    "    - The lasso regression technique employs variable selection, which leads to the shrinkage of coefficient values to absolute zero.\n",
    "\n",
    "- **Quantile Regression**\n",
    "    - The quantile regression approach is a subset of the linear regression technique. \n",
    "    - Statisticians and econometricians employ quantile regression when linear regression requirements are not met or when the data contains outliers.\n",
    "\n",
    "- **Bayesian Linear Regression**\n",
    "    - Machine learning utilizes Bayesian linear regression, a form of regression analysis, to calculate the values of regression coefficients using Bayes’ theorem. \n",
    "    - Rather than determining the least-squares, this technique determines the features’ posterior distribution.\n",
    "    - The approach outperforms ordinary linear regression in terms of stability. \n",
    "\n",
    "- **Principal Components Regression**\n",
    "    - Multicollinear regression data is often evaluated using the principle components regression approach. \n",
    "    - The significant components regression approach, like ridge regression, reduces standard errors by biassing the regression estimates. \n",
    "    - First, principal component analysis (PCA) modifies the training data, and then the resulting transformed samples train the regressors.\n",
    "\n",
    "- **Partial Least Squares Regression**\n",
    "    - The partial least squares regression technique is a fast and efficient covariance-based regression analysis technique. \n",
    "    - It is advantageous for regression problems with many independent variables with a high probability of multicollinearity between the variables. \n",
    "    - The method reduces the number of variables to a manageable number of predictors, then uses them in regression.\n",
    "\n",
    "- **Elastic Net Regression**\n",
    "    - Elastic net regression combines ridge and lasso regression techniques that are particularly useful when dealing with strongly correlated data. \n",
    "    - It regularizes regression models by utilizing the penalties associated with the ridge and lasso regression methods.\n",
    "\n",
    "\n",
    "### Complete Workflow for Regression Modeling\n",
    "Steps of a regression modeling process, covering:\n",
    "- Exploratory Data Analysis (EDA), \n",
    "- assumption checking, \n",
    "- data transformations, \n",
    "- model fitting, and \n",
    "- interpretation.\n",
    "\n",
    "**Step 1: Problem Definition and Data Understanding**\n",
    "\n",
    "1. Define the Problem:\n",
    "- Identify the dependent (response) variable and independent (predictor) variables.\n",
    "- Clarify objectives\n",
    "    - prediction, \n",
    "    - inference,\n",
    "    - explanation.\n",
    "2. Understand the Data:\n",
    "- Review the dataset's structure, variable types, and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0069f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "def missing_values_table(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns\n",
    "\n",
    "train.head()\n",
    "train.info()\n",
    "train.shape\n",
    "\n",
    "missing_values_table(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37425f26",
   "metadata": {},
   "source": [
    "**Step 2: Exploratory Data Analysis (EDA)**\n",
    "\n",
    "1. Summary Statistics:\n",
    "Compute \n",
    "- mean, \n",
    "- median, \n",
    "- standard deviation, and \n",
    "- correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1eef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())\n",
    "print(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea9c95c",
   "metadata": {},
   "source": [
    "2. Visualization:\n",
    "- Histogram for distributions.\n",
    "- Scatter plots for relationships.\n",
    "- Box plots for detecting outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.pairplot(df, diag_kind=\"kde\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f3dfc1",
   "metadata": {},
   "source": [
    "3. Check Multicollinearity:\n",
    "\n",
    "- Compute the Variance Inflation Factor (VIF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13675dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X = df[[\"SquareFootage\", \"Bedrooms\", \"LocationIndex\"]]\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Features\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845874c3",
   "metadata": {},
   "source": [
    "**Step 3: Preprocessing and Transformations**\n",
    "\n",
    "1. Handle Missing Data:\n",
    "- Impute missing values or drop rows/columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50699106",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea404802",
   "metadata": {},
   "source": [
    "2. Encode Categorical Variables:\n",
    "- Use one-hot encoding or label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c677c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=[\"Location\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a60f544",
   "metadata": {},
   "source": [
    "3. Feature Scaling:\n",
    "- Standardize or normalize numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe495bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[[\"SquareFootage\", \"Bedrooms\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ada11e",
   "metadata": {},
   "source": [
    "4. Transform Non-linear Relationships:\n",
    "- Apply log, Box-Cox, or square root transformations for skewed variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08eeddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "df[\"LogPrice\"] = np.log(df[\"Price\"])\n",
    "df[\"BoxCoxPrice\"], _ = boxcox(df[\"Price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01549fc3",
   "metadata": {},
   "source": [
    "#### Common Data Cleaning Issues and How to Handle Them\n",
    "Data cleaning is a critical step in data preprocessing to ensure that datasets are accurate, consistent, and reliable before analysis or modeling. \n",
    "\n",
    "Data cleaning is an iterative process. It ensures the dataset is:\n",
    "- ✅ Complete (No missing values)\n",
    "- ✅ Accurate (No typos or incorrect entries)\n",
    "- ✅ Consistent (Standardized formats & categories)\n",
    "- ✅ Reliable (No data leakage or inconsistencies)\n",
    "\n",
    "##### **Missing Data**\n",
    "📌 Issue: Some values in the dataset are missing, which can affect analysis and machine learning models.\n",
    "\n",
    "🔹 Causes:\n",
    "- Data entry errors\n",
    "- Sensor failures\n",
    "- Non-response in surveys\n",
    "\n",
    "🔹 Solutions:\n",
    "- Remove rows/columns with excessive missing values (if the proportion is too high).\n",
    "- Impute missing values using:\n",
    "- Mean/Median (for numerical data)\n",
    "- Mode (for categorical data)\n",
    "- Forward/Backward Fill (for time-series data)\n",
    "- KNN or Regression Imputation (for advanced cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97bd6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Sample data\n",
    "data = {'Age': [25, 30, None, 40, 35], 'Salary': [50000, None, 60000, 65000, None]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Impute missing values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[['Age', 'Salary']] = imputer.fit_transform(df[['Age', 'Salary']])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d0d01d",
   "metadata": {},
   "source": [
    "##### **Duplicate Records**\n",
    "📌 Issue: Duplicate rows in the dataset can distort results and inflate counts.\n",
    "\n",
    "🔹 Causes:\n",
    "- Data entry errors\n",
    "- Merging multiple datasets\n",
    "- Web scraping issues\n",
    "\n",
    "🔹 Solutions:\n",
    "- Identify and remove exact duplicates using .drop_duplicates().\n",
    "- Use domain knowledge to define duplicates when small variations exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b5ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9a5cbf",
   "metadata": {},
   "source": [
    "##### **Inconsistent Data Formats**\n",
    "📌 Issue: Data is stored in different formats (e.g., date formats like MM-DD-YYYY vs. YYYY-MM-DD).\n",
    "\n",
    "🔹 Causes:\n",
    "- Different data sources\n",
    "- Human input errors\n",
    "\n",
    "🔹 Solutions:\n",
    "- Standardize formats (e.g., convert all dates to YYYY-MM-DD).\n",
    "- Use regex or string functions to clean inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff44ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0014dae3",
   "metadata": {},
   "source": [
    "##### **Outliers and Anomalies**\n",
    "📌 Issue: Some values are significantly different from the rest, which can distort statistical analysis.\n",
    "\n",
    "🔹 Causes:\n",
    "- Data entry errors\n",
    "- Genuine extreme events\n",
    "\n",
    "🔹 Solutions:\n",
    "- Detect outliers using:\n",
    "- Z-score (values beyond ±3 standard deviations)\n",
    "- IQR (Interquartile Range) method\n",
    "- Handle outliers by:\n",
    "- Capping values at a reasonable threshold\n",
    "- Using robust statistical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f300081",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['Age'].quantile(0.25)\n",
    "Q3 = df['Age'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "df = df[(df['Age'] >= (Q1 - 1.5 * IQR)) & (df['Age'] <= (Q3 + 1.5 * IQR))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f9260",
   "metadata": {},
   "source": [
    "##### **Incorrect or Inconsistent Categorical Values**\n",
    "📌 Issue: Categorical data may have misspellings or inconsistent labels (\"Male\", \"male\", \"M\")\n",
    "\n",
    "🔹 Causes:\n",
    "- Human entry errors\n",
    "- Multiple data sources\n",
    "\n",
    "🔹 Solutions:\n",
    "- Standardize text formats (e.g., lowercase everything).\n",
    "- Use fuzzy matching for minor spelling errors.\n",
    "- Manually map inconsistent categories to standardized ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'] = df['Gender'].str.lower().replace({'m': 'male', 'f': 'female'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6cabf3",
   "metadata": {},
   "source": [
    "##### **Structural Errors (Typos, Extra Spaces, Incorrect Encoding)**\n",
    "📌 Issue: Inconsistent spelling or encoding errors (\"New York \" vs. \"NewYork\")\n",
    "\n",
    "🔹 Causes:\n",
    "- Data entry errors\n",
    "- Copy-pasting issues\n",
    "\n",
    "🔹 Solutions:\n",
    "- Remove extra spaces using .strip().\n",
    "- Fix encoding issues with .encode().decode()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31522f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['City'] = df['City'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6f8323",
   "metadata": {},
   "source": [
    "##### **Data Leakage**\n",
    "📌 Issue: The dataset contains information that should not be available during model training.\n",
    "\n",
    "🔹 Causes:\n",
    "- Using future data in training\n",
    "- Including target-related variables as predictors\n",
    "\n",
    "🔹 Solutions:\n",
    "- Remove features that leak information (e.g., future sales data in a forecasting model).\n",
    "- Ensure a proper train-test split to prevent future data from influencing training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efa75db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e858f83b",
   "metadata": {},
   "source": [
    "##### **Data Type Mismatch**\n",
    "📌 Issue: Columns have incorrect data types (e.g., numeric values stored as strings).\n",
    "\n",
    "🔹 Causes:\n",
    "- Importing data from CSVs or Excel\n",
    "- Mixed data types in a column\n",
    "\n",
    "🔹 Solutions:\n",
    "- Convert data types explicitly using .astype().\n",
    "- Handle conversion errors with errors='coerce'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ef305",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Salary'] = pd.to_numeric(df['Salary'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ebc6c",
   "metadata": {},
   "source": [
    "##### **Imbalanced Data**\n",
    "📌 Issue: In classification tasks, one class dominates (90% fraud-free, 10% fraud cases).\n",
    "\n",
    "🔹 Causes:\n",
    "- Rare event classification (e.g., fraud detection)\n",
    "\n",
    "🔹 Solutions:\n",
    "- Oversample the minority class (e.g., SMOTE technique).\n",
    "- Undersample the majority class to balance the dataset.\n",
    "- Use weighted models that adjust for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb48dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c4bcd",
   "metadata": {},
   "source": [
    "**Step 4: Model Fitting and Assumption Checking**\n",
    "1. Fit the Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = sm.add_constant(df[[\"SquareFootage\", \"Bedrooms\"]])  # Add intercept\n",
    "Y = df[\"Price\"]\n",
    "model = sm.OLS(Y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860b0ea4",
   "metadata": {},
   "source": [
    "2. Check Model Assumptions:\n",
    "\n",
    "(a) Linearity: \n",
    "\n",
    "Residuals vs. Fitted Plot: Look for randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec39770",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(model.fittedvalues, model.resid)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.title(\"Residuals vs. Fitted\")\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809e320",
   "metadata": {},
   "source": [
    "(b) Normality of Residuals:\n",
    "\n",
    "Use a histogram and Q-Q plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41cb69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "stats.probplot(model.resid, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c4b1d",
   "metadata": {},
   "source": [
    "(c) Homoscedasticity:\n",
    "\n",
    "Breusch-Pagan test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a426d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "bp_test = het_breuschpagan(model.resid, X)\n",
    "print(f\"p-value: {bp_test[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8593c4",
   "metadata": {},
   "source": [
    "(d) Multicollinearity:\n",
    "\n",
    "Variance Inflation Factor (VIF) as shown earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa55a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X = df[[\"SquareFootage\", \"Bedrooms\", \"LocationIndex\"]]\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Features\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02246b",
   "metadata": {},
   "source": [
    "**Step 5: Address Issues and Refine the Model**\n",
    "\n",
    "1. Linearity:\n",
    "\n",
    "Transform variables if residual plots show non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc058419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SquareFootage_Sq\"] = df[\"SquareFootage\"] ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eadd8c",
   "metadata": {},
   "source": [
    "2. Non-Normal Residuals:\n",
    "\n",
    "Apply transformations to the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b372eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"LogPrice\"] = np.log(df[\"Price\"])\n",
    "model_log = sm.OLS(df[\"LogPrice\"], X).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce33f90",
   "metadata": {},
   "source": [
    "3. Heteroscedasticity:\n",
    "\n",
    "Use Weighted Least Squares (WLS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e853e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 1 / (model.resid ** 2)\n",
    "model_wls = sm.WLS(Y, X, weights=weights).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c67352",
   "metadata": {},
   "source": [
    "4. Multicollinearity:\n",
    "- Drop or combine highly correlated variables.\n",
    "- Use PCA, Ridge, or Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a65cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31127c0d",
   "metadata": {},
   "source": [
    "**Step 6: Evaluate Model Performance**\n",
    "- Metrics: \n",
    "    - $𝑅^2$\n",
    "    - Adjusted $𝑅^2$\n",
    "    - RMSE, \n",
    "    - MAE.\n",
    "\n",
    "- Residual Plots:\n",
    "    - Confirm residuals are normally distributed and homoscedastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad0522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(Y, model.predict(X)))\n",
    "mae = mean_absolute_error(Y, model.predict(X))\n",
    "print(f\"RMSE: {rmse}, MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1f7fc7",
   "metadata": {},
   "source": [
    "**Step 7: Interpretation and Communication**\n",
    "\n",
    "Coefficient Interpretation:\n",
    "- For each predictor, interpret its coefficient in terms of the dependent variable.\n",
    "\n",
    "Confidence Intervals:\n",
    "- Report 95% confidence intervals for coefficients.\n",
    "\n",
    "Visualize Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bacc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.regplot(x=\"SquareFootage\", y=\"Price\", data=df, line_kws={\"color\": \"red\"})\n",
    "plt.title(\"Regression Line: Square Footage vs. Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b548c4",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear Regression is a supervised learning algorithm used to model the relationship between a dependent variable (outcome) and one or more independent variables (predictors).\n",
    "- Predicts the relationship between two variables by assuming they have a straight-line connection. \n",
    "\n",
    "Linear Regression predicts a continuous target variable (e.g., the number of readmissions) by minimizing the residual sum of squares between observed and predicted values.\n",
    "- It finds the best line that minimizes the differences between predicted and actual values.\n",
    "\n",
    "## 1. Simple Linear Regression\n",
    "\n",
    "In a simple linear regression, there is \n",
    "- one independent variable and \n",
    "- one dependent variable. \n",
    "\n",
    "The model estimates the slope and intercept of the line of best fit, which represents the relationship between the variables. \n",
    "- The slope represents the change in the dependent variable for each unit change in the independent variable, while \n",
    "- The intercept represents the predicted value of the dependent variable when the independent variable is zero.\n",
    "\n",
    "What It Means: \n",
    "- Linear regression models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. \n",
    "- It assumes a straight-line relationship. \n",
    "- It shows the linear relationship between the independent(predictor) variable i.e. X-axis and the dependent (output) variable i.e. Y-axis, \n",
    "    - called linear regression.\n",
    "\n",
    "- It is employed to establish a link between a dependant variable and a single independent variable. \n",
    "    - A linear equation defines the relationship, with the \n",
    "        - slope and \n",
    "        - intercept \n",
    "    - of the line representing the effect of the independent variable on the dependant variable.\n",
    "        - An independent variable is the variable that is controlled in a scientific experiment to test the effects on the dependent variable.\n",
    "        - A dependent variable is the variable being measured in a scientific experiment.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each coefficient represents how much the dependent variable (outcome) changes when the predictor variable changes by one unit, keeping all else constant.\n",
    "\n",
    "**Assumptions of Linear Regression**\n",
    "\n",
    "Regression is a parametric approach, which means that it makes assumptions about the data\n",
    "\n",
    "For successful regression analysis, it’s essential to validate the following assumptions.\n",
    "\n",
    "- Linearity (Linear Relationship): The relationship between the predictors and the outcome is linear.\n",
    "    - Plot dependent variable and independent variable(s) and see linear relationship.\n",
    "- Independence of Errors: Residuals (errors) are independent of each other.\n",
    "    - The error terms should not be dependent on one another (like in time-series data wherein the next value is dependent on the previous one). \n",
    "    - There should be no correlation between the residual terms.\n",
    "    - The absence of this phenomenon is known as Autocorrelation.\n",
    "- No or Little Autocorrelation\n",
    "- Normality of Errors: Residuals are normally distributed.\n",
    "    - The mean of residuals should follow a normal distribution with a mean equal to zero or close to zero. \n",
    "    - This is done to check whether the selected line is the line of best fit or not. \n",
    "    - If the error terms are non-normally distributed, suggests that there are a few unusual data points that must be studied closely to make a better model.\n",
    "- Multivariate Normality\n",
    "- No or Little Multicollinearity\n",
    "- Homoscedasticity: Variance of residuals is constant across all levels of predictors.\n",
    "    - The error terms must have constant variance. \n",
    "    - The presence of non-constant variance in the error terms is referred to as Heteroscedasticity. \n",
    "\n",
    "Performance Measures:\n",
    "- R-squared: Indicates the proportion of the variance in the dependent variable explained by the independent variables. \n",
    "    - Values closer to 1 indicate a better fit.\n",
    "- Mean Squared Error (MSE): The average squared difference between observed and predicted values; lower values are better.\n",
    "\n",
    "Lay Explanation: \n",
    "- Think of linear regression like drawing a best-fit line through a scatterplot of data points, aiming to predict outcomes based on relationships in the data.\n",
    "- Finds a relationship between independent and dependent variables by finding a “best-fitted line” that has minimal distance from all the data points.\n",
    "- The algorithm explains the linear relationship between the dependent(output) variable y and the independent(predictor) variable X using a straight line\n",
    "\n",
    "Use Case: \n",
    "- When there is a linear relationship between the target and predictor variables.\n",
    "\n",
    "### Mathematics or Linear Regression\n",
    "\n",
    "- it is using the least square method finds a linear equation that minimizes the sum of squared residuals (SSR).\n",
    "- Cost Function:\n",
    "\n",
    "$ J(\\theta) = \\frac{1}{2m}\\sum^{m}_{i=1}(h_{\\theta}(x^{(i)})- y^{(i)})^{2}$\n",
    "\n",
    "Model Equation:\n",
    "$ 𝑦=𝛽_{0}+𝛽_{1}𝑥_{1}+…+𝛽_{𝑛}𝑥_{𝑛}+ 𝜖 $\n",
    "\n",
    "where:\n",
    "- $y$ = dependent variable\n",
    "- $𝛽_{0}$ = Y intercept / constant\n",
    "- $𝛽_{1}$ = Slope coefficient / intercept\n",
    "- $𝑥_{1}$ = independent variable\n",
    "- $𝜖 $ = error term\n",
    "\n",
    "**What is Cost Function ?**\n",
    "\n",
    "The goal of the linear regression algorithm is to get the best values for $𝛽_{0}+𝛽_{1}$ to find the **best-fit line**.\n",
    "- is a line that has the least error which means the error between predicted values and actual values should be minimum.\n",
    "\n",
    "A cost function, also referred to as a: \n",
    "- loss function : Used when we refer to the error for a single training example. \n",
    "- objective function : Used to refer to an average of the loss functions over an entire training dataset.\n",
    "It quantifies the difference between predicted and actual values, serving as a metric to evaluate the performance of a model.\n",
    "\n",
    "Objective \n",
    "- is to minimize the cost function, indicating better alignment between predicted and observed outcomes.\n",
    "- Guides the model towards optimal predictions by measuring its accuracy against the training data.\n",
    "\n",
    "AKA - Random Error (Residuals)\n",
    "- the difference between the observed value of the dependent variable($y_{i}$) and the predicted value(predicted) is called the residuals.\n",
    "    - $𝜖_{i}$ =  $y_{predicted}  –  y_{i}$\n",
    "\n",
    "where $𝑦_{predicted} = 𝛽_{0}+𝛽_{1}𝑥_{1}+…+𝛽_{𝑛}𝑥_{𝑛}+ 𝜖 $\n",
    "\n",
    "**Why to use a Cost function**\n",
    "\n",
    "Cost function helps us reach the optimal solution / work out the optimal values for $𝛽_{0}+𝛽_{1}$ . \n",
    "- How: It takes both predicted outputs by the model and actual outputs and calculates how much wrong the model was in its prediction.\n",
    "    - It basically measures the discrepancy between the model’s predictions and the true values it is attempting to predict. \n",
    "    - This variance is depicted as a lone numerical figure, enabling us to measure the model’s **precision**.\n",
    "- The cost function is the technique of evaluating “the performance of our algorithm/model”.\n",
    "\n",
    "Classifiers have very high accuracy but one solution (Classifier) is the best because it does not misclassify any point.\n",
    "- Reason why it classifies all the points perfectly is that the:\n",
    "    - line is almost exactly in between the two (n) groups, and not closer to any one of the groups.\n",
    "\n",
    "Explanation of the function of a cost function:\n",
    "\n",
    "- Error calculation: It determines the difference between the predicted outputs (what the model predicts as the answer) and the actual outputs (the true values we possess for the data).\n",
    "- Gives one value: This simplifies comparing the model’s performance on various datasets or training rounds.\n",
    "- Improving Guides: The objective is to reduce the cost function. \n",
    "    - How: Through modifying the internal parameters of the model such as weights and biases, we can aim to minimize the total error and enhance the accuracy / precision of the model.\n",
    "\n",
    "**Types of Cost function in machine learning**\n",
    "\n",
    "Its use cases depend on whether it is a regression problem or classification problem.\n",
    "- Regression cost Function\n",
    "- Binary Classification cost Functions\n",
    "- Multi-class Classification cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8576db",
   "metadata": {},
   "source": [
    "### Problem Context: Predicting Hospital Readmission Rates\n",
    "The aim to reduce hospital readmission rates. \n",
    "- High readmission rates can strain resources and negatively impact patient outcomes.\n",
    "- The goal is to predict the number of readmissions within 30 days of discharge for a particular condition, such as \n",
    "    - diabetes, based on \n",
    "        - patient demographic, \n",
    "        - clinical data, and \n",
    "        - treatment data.\n",
    "\n",
    "**Step 1. Define the Problem**\n",
    "\n",
    "We want to predict the number of readmissions ($𝑌$) using features ($𝑋$) such as:\n",
    "- Patient age\n",
    "- Length of hospital stay\n",
    "- Severity of condition\n",
    "- Medication adherence rate\n",
    "- Comorbidities (e.g., hypertension, kidney disease)\n",
    "- Number of follow-up visits scheduled\n",
    "\n",
    "**Step 2. Collect and Prepare Data**\n",
    "\n",
    "- Data Collection: Gather historical patient data from the hospital's database.\n",
    "- Understand the \n",
    "    - model description\n",
    "    - causality and \n",
    "    - directionality\n",
    "- Check the data\n",
    "    - categorical data, \n",
    "    - missing data and \n",
    "    - outliers\n",
    "- Data Cleaning: \n",
    "    - Dummy variable takes only the value 0 or 1 to indicate the effect for categorical variables.\n",
    "    - Handle missing values, \n",
    "    - remove duplicates, and \n",
    "    - correct errors.\n",
    "    - Outlier is a data point that differs significantly from other observations. \n",
    "        - use standard deviation method and \n",
    "        - interquartile range (IQR) method.\n",
    "- Feature Engineering: \n",
    "    - Encode categorical variables (e.g., age group), \n",
    "    - scale continuous variables (e.g., length of stay), and \n",
    "    - create interaction terms if necessary.\n",
    "\n",
    "**Step 3. Conduct a Simple Analysis**\n",
    "- Check the **effect** comparing between \n",
    "    - Dependent variable to independent variable and \n",
    "    - Independent variable to independent variable\n",
    "- Check the correlation.\n",
    "    - Use scatter plots\n",
    "- Check Multicollinearity \n",
    "    - This occurs when more than two independent variables are highly correlated. \n",
    "    - Use Variance Inflation Factor (VIF) \n",
    "        - if VIF > 5 there is highly correlated and \n",
    "        - if VIF > 10 there is certainly multicollinearity among the variables.\n",
    "- Interaction Term imply a change in the slope from one value to another value.\n",
    "\n",
    "`Show the relationship between the two variables using a scatter plot.`\n",
    "- We have our Y, our X, and time (months), but we're just trying to model ZAR/USD as a *function* of Exports. \n",
    "    - To see if we can see that there possibly exists a linear relationship between the two variables: Value of Exports and ZAR/USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0754546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['X'], df['Y'])\n",
    "plt.ylabel(\"ZAR/USD\")\n",
    "plt.xlabel(\"Value of Exports (ZAR, millions)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56d3e0",
   "metadata": {},
   "source": [
    "**Step 4. Formulate the Model (From Scratch)**\n",
    "- y in this equation stands for the predicted value, \n",
    "- x means the independent variable and \n",
    "- m & b are the **coefficients** we need to optimize in order to fit the regression line to our data.\n",
    "\n",
    "#### Finding the Best Fit Line\n",
    "Let's say we have estimated some values for $a$ and $b$. We could plug in all of our values of X to find the corresponding values of Y. These *new* values of Y could be compared to the *actual* values of Y to assess the fit of the line. This becomes tedious as the number of data points increases.\n",
    "   \n",
    "Looking at the data, we can make a guess at the values of the slope and intercept of the line. We'll use a rough estimate of the slope as $\\frac{rise}{run} = \\frac{16}{80000} = 0.0002$. For the intercept, we'll just take a guess and call it $-3$.   \n",
    "   \n",
    "Let's plot a line with values of $a = -3$, and $b = 0.0002$:   \n",
    "   \n",
    "First, we will need to generate some values of y using the following formula:\n",
    "   \n",
    "$$\\hat{y}_i = a + bx_i$$   \n",
    "\n",
    "\n",
    "\n",
    "Calculating coefficient of the equation:\n",
    "- To calculate the coefficients we need the formula for \n",
    "\n",
    "Covariance \n",
    "\n",
    "$Cov (X,Y) = \\frac{\\sum (X_{i}- X)(Y_{j} - Y)}{n}$\n",
    "\n",
    "Variance\n",
    "\n",
    "$var(x) = \\frac{\\sum^{n}_{i} (x_i -\\mu)^2}{N}$\n",
    "\n",
    "- To calculate the coefficient m\n",
    "    - m = cov(x, y) / var(x)\n",
    "    - b = mean(y) — m * mean(x)\n",
    "\n",
    "**Functions to calculate the Mean, Covariance, and Variance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c74157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean \n",
    "def get_mean(arr):\n",
    "    return np.sum(arr)/len(arr)\n",
    "\n",
    "# variance\n",
    "def get_variance(arr, mean):\n",
    "    return np.sum((arr-mean)**2)\n",
    "\n",
    "# covariance\n",
    "def get_covariance(arr_x, mean_x, arr_y, mean_y):\n",
    "    final_arr = (arr_x - mean_x)*(arr_y - mean_y)\n",
    "    return np.sum(final_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8932f",
   "metadata": {},
   "source": [
    "**Fuction to calculate the coefficients and the Linear Regression Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients \n",
    "# m = cov(x, y) / var(x)\n",
    "# b = y - m*x\n",
    "\n",
    "def get_coefficients(x, y):\n",
    "    x_mean = get_mean(x)\n",
    "    y_mean = get_mean(y)\n",
    "    m = get_covariance(x, x_mean, y, y_mean)/get_variance(x, x_mean)\n",
    "    b = y_mean - x_mean*m\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e179ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression \n",
    "# Train and Test\n",
    "# Train Split 80 % Test Split 20 %\n",
    "def linear_regression(x_train, y_train, x_test, y_test):\n",
    "    prediction = []\n",
    "    m, b = get_coefficients(x_train, y_train)\n",
    "    for x in x_test:\n",
    "        y = m*x + b\n",
    "        prediction.append(y)\n",
    "    \n",
    "    r2 = r2_score(prediction, y_test)\n",
    "    mse = mean_squared_error(prediction, y_test)\n",
    "    print(\"The R2 score of the model is: \", r2)\n",
    "    print(\"The MSE score of the model is: \", mse)\n",
    "    return prediction\n",
    "\n",
    "prediction = linear_regression(x[:80], y[:80], x[80:], y[80:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate values of y from a list of x, \n",
    "# Given parameters a and b\n",
    "\n",
    "def gen_y(x_list, a, b):\n",
    "    y_gen = []\n",
    "    for x_i in x_list:\n",
    "        y_i = a + b*x_i\n",
    "        y_gen.append(y_i)\n",
    "    \n",
    "    return(y_gen)\n",
    "\n",
    "# Generate the values by invoking the 'gen_y' function\n",
    "y_gen = gen_y(df.X, -3, 0.0002)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(df.X, df.Y)  # Plot the original data\n",
    "plt.plot(df.X, y_gen, color='red')  # Plot the line connecting the generated y-values\n",
    "plt.ylabel(\"ZAR/USD\")\n",
    "plt.xlabel(\"Value of Exports (ZAR, millions)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ec0333",
   "metadata": {},
   "source": [
    "**Visualize the regression line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb4088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reg_line(x, y):\n",
    "    # Calculate predictions for x ranging from 1 to 100\n",
    "    prediction = []\n",
    "    m, c = get_coefficients(x, y)\n",
    "    for x0 in range(1,100):\n",
    "        yhat = m*x0 + c\n",
    "        prediction.append(yhat)\n",
    "    \n",
    "    # Scatter plot without regression line\n",
    "    fig = plt.figure(figsize=(20,7))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.scatterplot(x=x, y=y)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Scatter Plot between X and Y')\n",
    "    \n",
    "    # Scatter plot with regression line\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.scatterplot(x=x, y=y, color = 'blue')\n",
    "    sns.lineplot(x = [i for i in range(1, 100)], y = prediction, color='red')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Regression Plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5bc7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression plot form seaborn\n",
    "# regplot is basically the combination of the scatter plot and the line plot\n",
    "sns.regplot(x, y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title(\"Regression Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aeea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reg_line(x, y):\n",
    "    # Calculate predictions for x ranging from 1 to 100\n",
    "    prediction = []\n",
    "    m, c = get_coefficients(x, y)\n",
    "    for x0 in range(1,100):\n",
    "        yhat = m*x0 + c\n",
    "        prediction.append(yhat)\n",
    "    \n",
    "    # Scatter plot without regression line\n",
    "    fig = plt.figure(figsize=(20,7))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.scatterplot(x=x, y=y)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Scatter Plot between X and Y')\n",
    "    \n",
    "    # Scatter plot with regression line\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.scatterplot(x=x, y=y, color = 'blue')\n",
    "    sns.lineplot(x = [i for i in range(1, 100)], y = prediction, color='red')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Regression Plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8247ed70",
   "metadata": {},
   "source": [
    "**Step 4. Formulate the model and Fit the Model (using library)**\n",
    "\n",
    "- Split the Data: Divide data into training and testing sets (e.g., 80% training, 20% testing).\n",
    "- Train the Model: Use a library like sklearn in Python to fit the regression model on the training data.\n",
    "- Evaluate the Model: Check metrics such as $𝑅^2$ (explained variance) and RMSE (Root Mean Squared Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Example\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create the dataset\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
    "y = np.array([2, 4, 5, 7, 8, 10, 11, 13, 14, 16])\n",
    "\n",
    "# Create the linear regression model\n",
    "model = LinearRegression().fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc758991",
   "metadata": {},
   "source": [
    "##### Calculate the Regression Coefficients\n",
    "\n",
    "Use the formulas for $𝛽_1$ (slope) and $𝛽_0$ (intercept):\n",
    "\n",
    "$𝛽_1 = \\frac{\\sum (x_{i}- \\bar{x})(y_{j} - \\bar{y})}{\\sum (x_{i}- \\bar{x})^2}$\n",
    "\n",
    "$𝛽_0 = \\bar{y} - 𝛽_1 \\bar{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d5d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of x and y from scratch \n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "\n",
    "# Calculating beta1 (slope)\n",
    "numerator = np.sum((x - x_mean) * (y - y_mean))\n",
    "denominator = np.sum((x - x_mean) ** 2)\n",
    "beta1 = numerator / denominator\n",
    "\n",
    "# Calculating beta0 (intercept)\n",
    "beta0 = y_mean - beta1 * x_mean\n",
    "\n",
    "print(f\"Beta0 (Intercept): {beta0:.3f}\")\n",
    "print(f\"Beta1 (Slope): {beta1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the slope and intercept of the line\n",
    "slope = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "# Plot the data points and the regression line\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, slope*X + intercept, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc4414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Example dataset\n",
    "X = data[['age', 'length_of_stay', 'severity', 'medication_adherence', 'comorbidities']]\n",
    "y = data['readmissions']\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse}, R^2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7183956e",
   "metadata": {},
   "source": [
    "**Let's check the calculted fit of the line** by measuring how far the true y-values of each point are from their corresponding y-value on the line.   \n",
    "   \n",
    "We'll use the equation below to calculate the error of each generated value of y:   \n",
    "   \n",
    "$$e_i = y_i - \\hat{y}_i$$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf61017",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.array(df.Y - y_gen)\n",
    "np.round(errors, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6a59c8",
   "metadata": {},
   "source": [
    "In addition to having some very large errors, we can also see that most of the errors are positive numbers. Ideally, we want our errors to be evenly distributed either side of zero - we want our line to best fit the data, i.e. no bias.\n",
    "   \n",
    "We can measure the overall error of the fit by calculating the **Residual Sum of Squares**:\n",
    "   \n",
    "$$RSS = \\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "##### Residual Sum of Squares (RSS)\n",
    "Definition: The Residual Sum of Squares (RSS) measures the discrepancy between the actual data points and the estimated values predicted by a regression model. It is calculated as the sum of the squared differences between actual ($𝑦_𝑖$) and predicted ($\\hat{y}_𝑖 $) values.\n",
    "\n",
    "The RSS finds the difference between the y-value of each data point and our estimated line (which may be either negative or positive), squares the difference, and then adds all the differences up. In other words, it's the sum of the squares of all the errors we calculated before.\n",
    "\n",
    "Here:\n",
    "\n",
    "- $𝑦_𝑖$ = Actual value of the dependent variable for observation 𝑖.\n",
    "- $\\hat{y}_𝑖 = 𝛽_0 + 𝛽_1 𝑥_𝑖$ , where:\n",
    "    - $𝛽_0$ is the intercept.\n",
    "    - $𝛽_1$ is the slope of the regression line.\n",
    "    - $𝑥_𝑖$ is the value of the independent variable for observation 𝑖.\n",
    "\n",
    "Substituting $\\hat{y}_𝑖$:\n",
    "\n",
    "$$RSS = \\sum_{i=1}^n(y_i-(𝛽_0 + 𝛽_1 𝑥_𝑖))^2$$\n",
    "\n",
    "The RSS quantifies the \"unexplained variance\" by the model.\n",
    "\n",
    "In a simple linear regression, minimizing RSS is equivalent to finding the best-fit line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e28d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual sum of squares from scratch\n",
    "rss = np.sum((y - y_pred) ** 2)\n",
    "print(f\"Residual Sum of Squares (RSS): {rss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb521f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Residual sum of squares:\", (errors ** 2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784bd8a",
   "metadata": {},
   "source": [
    "## Least Squares Method\n",
    "Least Squares is another method that allows us to find the line of best fit while enforcing the constraint of minimising the residuals. More specifically, the **Least Squares Criterion** states that the sum of the squares of the residuals should be minimized, i.e.   \n",
    "$$Q = \\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "The formulae for the intercept, $a$, and the slope, $b$, are determined by minimizing the equation for the sum of the squared prediction errors:   \n",
    "$$Q = \\sum_{i=1}^n(y_i-(a+bx_i))^2$$\n",
    "\n",
    "Optimal values for $a$ and $b$ are found by differentiating $Q$ with respect to $a$ and $b$, setting both equal to 0 and then solving for $a$ and $b$.   \n",
    "   \n",
    "We won't go into the [derivation process](http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_10.pdf) here, but the equations for $a$ and $b$ are:   \n",
    "   \n",
    "$$b = \\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$$   \n",
    "   \n",
    "and:   \n",
    "   \n",
    "$$a = \\bar{y} - b\\bar{x}$$\n",
    "\n",
    "where:\n",
    "- $ x_i$ Values of the independent variable.\n",
    "- $ y_i$ Values of the dependent variable.\n",
    "- $\\bar{y}$ are the mean values of $y$.\n",
    "- $\\bar{x}$ are the mean values of $x$ in our dataset, respectively.\n",
    "\n",
    "### Interpreting least-squares coefficients\n",
    "\n",
    "Interpreting the least-squares coefficients provides insights into the relationship between the independent variable (x) and the dependent variable (y) in a simple linear regression.\n",
    "\n",
    "#### The Slope ($𝛽_1$)\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- If $𝛽_1 > 0$: y increases as x increases (positive relationship).\n",
    "- If $𝛽_1 < 0$: y decreases as x increases (negative relationship).\n",
    "- If $𝛽_1 = 0$: No linear relationship exists between x and y.\n",
    "\n",
    "If $𝛽_1 = 0.28$ this means that for every one-unit increase in x, y is expected to increase by 0.28 units.\n",
    "\n",
    "Key Considerations:\n",
    "\n",
    "- The magnitude of $𝛽_1$ indicates the strength of the effect.\n",
    "- The direction (+/-) indicates the nature of the relationship.\n",
    "\n",
    "#### The Intercept ($𝛽_0$)\n",
    "\n",
    "Definition:\n",
    "The intercept ($𝛽_0$) represents the predicted value of the dependent variable (y) when the independent variable (x) is zero.\n",
    "\n",
    "Interpretation:\n",
    "- The intercept gives a baseline value of y when x=0.\n",
    "- It is meaningful only if 𝑥=0 is within the range of observed data. \n",
    "    - If not, the intercept might be extrapolated and have limited interpretive value.\n",
    "\n",
    "Limitations\n",
    "- Causation vs. Correlation: The coefficients indicate relationships, not causation, unless you have a well-controlled experimental design.\n",
    "- Range of x: The interpretation of $𝛽_0$ and $𝛽_1$ applies only within the range of observed x-values.\n",
    "- Other Factors: The model assumes that other variables do not influence 𝑦, which might not be the case in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add85037",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.X.values\n",
    "Y = df.Y.values\n",
    "\n",
    "# Calculate x bar, y bar\n",
    "x_bar = np.mean(X)\n",
    "y_bar = np.mean(Y)\n",
    "\n",
    "# Calculate slope\n",
    "b = sum( (X-x_bar)*(Y-y_bar) ) / sum( (X-x_bar)**2 )\n",
    "\n",
    "# Calculate intercept\n",
    "a = y_bar - b*x_bar\n",
    "\n",
    "print(\"Slope = \" + str(b))\n",
    "print(\"Intercept = \" + str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function we created earlier:\n",
    "# it generates y-values for given x-values based on parameters a, b\n",
    "y_gen2 = gen_y(df.X, a, b)\n",
    "\n",
    "plt.scatter(df.X, df.Y)\n",
    "plt.plot(df.X, y_gen2, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7483a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors2 = np.array(y_gen2 - df.Y)\n",
    "print(np.round(errors2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce02e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Residual sum of squares:\", (errors2 ** 2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13429034",
   "metadata": {},
   "source": [
    "Here we can see our RSS has improved from ~867 down to ~321.  \n",
    "Furthermore, if we calculate the sum of the errors we find that the value is close to 0.\n",
    "\n",
    "----\n",
    "Intuitively, this should make sense as it is an indication that the sum of the positive errors is equal to the sum of the negative errors. The line fits in the 'middle' of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round off to 11 decimal places\n",
    "np.round(errors2.sum(),11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8712bc8a",
   "metadata": {},
   "source": [
    "##### Recognise the Standard error of a statistic\n",
    "\n",
    "The standard error (SE) of a statistic in linear regression quantifies the variability of the estimated coefficients ($𝛽_0$ and $𝛽_1$) and other regression outputs. \n",
    "- It measures how much the coefficient estimates are expected to vary from sample to sample due to random noise in the data.\n",
    "\n",
    "**Standard Error of the Regression Coefficients**\n",
    "\n",
    "For a coefficient $𝛽_𝑗$ , the standard error (𝑆𝐸_𝛽_𝑗) is calculated as:\n",
    "\n",
    "$$𝑆𝐸_𝛽 = \\sqrt{\\frac{ \\sigma{^2}}{\\sum{}(x_i-\\bar{x})^2}}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\sigma{^2}$: The variance of the residuals, often estimated as the mean squared error (MSE):\n",
    "$$\\sigma{^2} = \\frac{RSS}{n−2}$$\n",
    "- n is the number of observations.\n",
    "\n",
    "$ \\sum{}(𝑥_𝑖 − \\bar{𝑥})^2$ : The total variation in the independent variable 𝑥\n",
    "\n",
    "**Standard Error of the Regression**\n",
    "\n",
    "The standard error of the regression (also called the residual standard error, $𝑅𝑆𝐸$ measures the average distance that the observed values fall from the regression line.\n",
    "\n",
    "$$RSE = \\frac{RSS}{n−2}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- RSS: Residual Sum of Squares.\n",
    "- $𝑛 − 2$ : Degrees of freedom for simple linear regression ($𝑛 − 𝑘 − 1$)\n",
    "    - with 𝑘 = 1 predictor.\n",
    "\n",
    "##### Role of Standard Errors in Linear Regression\n",
    "\n",
    "Done to evaluate the reliability and precision of your regression model.\n",
    "\n",
    "1.  Coefficient Standard Errors ($𝑆𝐸_{𝛽_0}$ and $𝑆𝐸_{𝛽_1}$\n",
    "- These are used to:\n",
    "    - Quantify Precision: Smaller standard errors indicate more precise estimates of the coefficients. \n",
    "    - Construct Confidence Intervals: The confidence interval for $𝛽_𝑗$ is:\n",
    "\n",
    "$$𝛽_𝑗 \\pm t \\cdot 𝑆𝐸_{𝛽_j}$$\n",
    "\n",
    "where 𝑡 is the critical value from the t-distribution for the desired confidence level.\n",
    " \n",
    "​- Perform Hypothesis Tests: To test if $𝛽_𝑗$ = 0, we calculate:\n",
    "\n",
    "$$t = \\frac{𝛽_𝑗}{SE_{𝛽_𝑗}}$$\n",
    "\n",
    "Compare t to the critical t-value to determine significance.\n",
    "\n",
    "2. Residual Standard Error (𝑅𝑆𝐸) \n",
    "- Indicates the average error in predictions.\n",
    "- Provides a baseline for assessing the fit of the model (smaller 𝑅𝑆𝐸 implies a better fit).\n",
    " \n",
    "**Intepretation**\n",
    "\n",
    "Residual Standard Error (RSE):\n",
    "- On average, the observed y-values deviate from the predicted  y-values by 0.147 units.\n",
    "\n",
    "Standard Error of Slope ($SE_{β_1}$):\n",
    "- The variability in the estimated slope is 0.065. This is used to assess the precision of $𝛽_1.\n",
    "\n",
    "Confidence in Coefficients:\n",
    "- Smaller standard errors indicate more confidence in the coefficient estimates.\n",
    "- Standard errors also allow hypothesis testing to determine if a predictor has a statistically significant impact on y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c4e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Define data\n",
    "x = np.array([1, 2, 3, 4, 5])  # Independent variable\n",
    "y = np.array([2.2, 2.8, 3.6, 4.5, 5.1])  # Dependent variable\n",
    "\n",
    "# Step 2: Calculate coefficients\n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "beta1 = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n",
    "beta0 = y_mean - beta1 * x_mean\n",
    "\n",
    "# Step 3: Calculate residuals and RSS\n",
    "y_pred = beta0 + beta1 * x  # Predicted values\n",
    "residuals = y - y_pred  # Residuals\n",
    "RSS = np.sum(residuals ** 2)  # Residual Sum of Squares\n",
    "\n",
    "# Step 4: Calculate standard error of the regression (RSE)\n",
    "n = len(x)  # Number of observations\n",
    "RSE = np.sqrt(RSS / (n - 2))  # Residual Standard Error\n",
    "\n",
    "# Step 5: Calculate standard error of the slope (SE_beta1)\n",
    "SE_beta1 = RSE / np.sqrt(np.sum((x - x_mean) ** 2))\n",
    "\n",
    "# Step 6: Print results\n",
    "print(f\"Residual Standard Error (RSE): {RSE:.3f}\")\n",
    "print(f\"Standard Error of Slope (SE_beta1): {SE_beta1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91173ea",
   "metadata": {},
   "source": [
    "### Applications of Standard Errors\n",
    "\n",
    "The standard errors (SEs) in linear regression are used to assess the precision and reliability of the estimated coefficients and model predictions. They serve as the foundation for key inferential techniques like \n",
    "- confidence intervals, \n",
    "- hypothesis testing, and \n",
    "- evaluating the overall fit of the regression model.\n",
    "\n",
    "1. Constructing Confidence Intervals\n",
    "\n",
    "Confidence intervals provide a range of plausible values for the regression coefficients.\n",
    "\n",
    "$$𝛽_𝑗 \\pm t \\cdot 𝑆𝐸_{𝛽_j}$$\n",
    "\n",
    "where:\n",
    "- $𝛽_𝑗$: Estimated coefficient.\n",
    "- $𝑆𝐸_{𝛽_j}$: Standard error of the coefficient.\n",
    "- t: Critical value from the t-distribution based on the desired confidence level and degrees of freedom ($𝑛 − 𝑘 − 1$).\n",
    "\n",
    "Interpretation: If the confidence interval for a coefficient does not include 0, it indicates that the predictor variable has a statistically significant relationship with the dependent variable at the given confidence level.\n",
    "\n",
    "**Calculate the 95% confidence interval for a regression coefficient, such as slope($𝛽_1$)**\n",
    "\n",
    "Use the following formula:\n",
    "\n",
    "Confidence Interval = $$𝛽_1 \\pm t \\cdot 𝑆𝐸_{𝛽_1}$$\n",
    "\n",
    "Steps to Calculate the 95% Confidence Interval\n",
    "\n",
    "1. Estimate the Slope Coefficient ($𝛽_1$)\n",
    "\n",
    "$$𝛽_1 = \\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$$\n",
    "\n",
    "2. Calculate the Standard Error of the Slope ($𝛽_1$)\n",
    "\n",
    "- The standard error of the slope is:\n",
    "\n",
    "$$𝑆𝐸_{𝛽_1} = \\frac{ RSE}{\\sqrt{\\sum{}(x_i-\\bar{x})^2}}$$\n",
    "\n",
    "- RSE (Residual Standard Error) is:\n",
    "\n",
    "$$RSE = \\frac{RSS}{n−2}$$\n",
    "where RSS = $\\sum{} (y_i-\\hat{y_1})^2$\n",
    "\n",
    "3. Find the Critical t-value ($𝑡_{critical}$):\n",
    "\n",
    "Use the t-distribution with 𝑛 − 2 degrees of freedom to find the critical value for the 95% confidence level ($𝑡_{critical}$).\n",
    "\n",
    "4. Apply the Confidence Interval Formula:\n",
    "\n",
    "- Combine the values:\n",
    "    - Confidence Interval = $$𝛽_1 \\pm 𝑡_{critical} \\cdot 𝑆𝐸_{𝛽_1}$$\n",
    "\n",
    "_______\n",
    "\n",
    "2. Hypothesis Testing\n",
    "\n",
    "Hypothesis testing in Linear Regression\n",
    "- Once you have fitted a straight line on the data, you need to ask, \n",
    "    - “Is this straight line a significant fit for the data?” Or \n",
    "    - “Is the beta coefficient explain the variance in the data plotted?” \n",
    "- Here comes the idea of hypothesis testing on the beta coefficient:\n",
    "\n",
    "$H_0 : B_1  = 0$\n",
    "    \n",
    "$H_A : B_1  ≠ 0$\n",
    "\n",
    "Interpret the Regression Equation\n",
    "- The coefficients ($𝛽$) indicate the magnitude and direction of the relationship between each predictor and readmissions.\n",
    "    - Example: A coefficient of -0.5 for medication_adherence means that for every 1% increase in medication adherence, readmissions decrease by 0.5.\n",
    "- The intercept ($𝛽_0$) represents the expected number of readmissions when all predictors are zero.\n",
    "\n",
    "Assessing the Model Fit\n",
    "- Other parameters to assess a model are:\n",
    "    - t statistic: It is used to determine the p-value and hence, helps in determining whether the coefficient is significant or not\n",
    "    - F statistic: It is used to assess whether the overall model fit is significant or not. \n",
    "        - the higher the value of the F-statistic, the more significant a model turns out to be.\n",
    "\n",
    "To determine whether a predictor variable has a significant impact on the dependent variable, use hypothesis testing.\n",
    "- Null Hypothesis ($𝐻_0): 𝛽_𝑗 =0$ (the predictor has no effect on response(𝑦) varaible).\n",
    "- Alternative Hypothesis $(𝐻_𝑎): 𝛽_𝑗 ≠ 0$ (the predictor has an effect/ there is a relationship).\n",
    "- t-statistic:\n",
    "\n",
    "**How to Calculate the t-statistic in Linear regression**\n",
    "The t-statistic in linear regression measures how many standard errors the estimated coefficient is away from zero. \n",
    "- It is used for hypothesis testing to determine if a predictor variable is statistically significant.\n",
    "\n",
    "The formula to calculate the t-statistic for a coefficient\n",
    "\n",
    "$$t = \\frac{𝛽_𝑗}{SE_{𝛽_𝑗}}$$\n",
    "\n",
    "Where:\n",
    "- $𝛽_𝑗$: Estimated coefficient (e.g., slope or intercept).\n",
    "- $SE_{𝛽_𝑗}$: Standard error of the estimated coefficient.\n",
    "\n",
    "If the t-statistic is large in magnitude, it indicates that $𝛽_j (or β_1 in this case) is far from zero, suggesting the predictor has a significant effect on the dependent variable.\n",
    "\n",
    "- P-Value: Compare the computed t-value to the critical value from the t-distribution, or calculate the p-value:\n",
    "    - If $𝑝 < 𝛼 (e.g., 0.05)$, reject $𝐻_0$ and conclude the predictor is statistically significant.\n",
    "_________\n",
    "\n",
    "3. Evaluating Model Fit\n",
    "\n",
    "The standard error of the regression (Residual Standard Error, 𝑅𝑆𝐸) assesses the accuracy of the model's predictions.\n",
    "\n",
    "$$RSE = \\sqrt{\\frac{RSS}{n−k-1}}$$\n",
    "\n",
    "**Degrees of Freedom**\n",
    "The t-statistic follows a t-distribution with $𝑛 − 𝑘 − 1 degrees of freedom,\n",
    "\n",
    "where:\n",
    "- RSS: Residual sum of squares.\n",
    "- n: Number of observations.\n",
    "- k: Number of predictors (excluding the intercept).\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- A smaller 𝑅𝑆𝐸 indicates better model fit.\n",
    "- Used as a baseline to evaluate other models.\n",
    "\n",
    "________\n",
    "\n",
    "4. Comparing Predictors\n",
    "\n",
    "Standard errors help compare the relative importance of different predictors by normalizing their coefficient estimates.\n",
    "- Predictors with smaller $𝑆𝐸{𝛽_𝑗}$ have more stable effects on 𝑦.\n",
    "- Variables with larger $𝑆𝐸{𝛽_𝑗}$ might need further investigation (e.g., multicollinearity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cbf5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Applications of Standard Errors\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Step 1: Define data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2.2, 2.8, 3.6, 4.5, 5.1])\n",
    "\n",
    "# Step 2: Calculate coefficients\n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "beta1 = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n",
    "beta0 = y_mean - beta1 * x_mean\n",
    "\n",
    "# Step 3: Calculate residuals and RSS\n",
    "y_pred = beta0 + beta1 * x\n",
    "residuals = y - y_pred\n",
    "RSS = np.sum(residuals ** 2)\n",
    "\n",
    "# Step 4: Calculate RSE\n",
    "n = len(x)\n",
    "RSE = np.sqrt(RSS / (n - 2))\n",
    "\n",
    "# Step 5: Calculate standard error of the slope (SE_beta1)\n",
    "SE_beta1 = RSE / np.sqrt(np.sum((x - x_mean) ** 2))\n",
    "\n",
    "# Step 6: Hypothesis Testing and Confidence Interval\n",
    "t_stat = beta1 / SE_beta1  # t-statistic\n",
    "p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-2))  # Two-tailed test\n",
    "\n",
    "# Confidence Interval for beta1\n",
    "t_critical = stats.t.ppf(0.975, df=n-2)  # 95% confidence level\n",
    "conf_interval = (beta1 - t_critical * SE_beta1, beta1 + t_critical * SE_beta1)\n",
    "\n",
    "# Step 7: Print results\n",
    "print(f\"Coefficient (beta1): {beta1:.3f}\")\n",
    "print(f\"Standard Error (SE_beta1): {SE_beta1:.3f}\")\n",
    "print(f\"t-Statistic: {t_stat:.3f}\")\n",
    "print(f\"p-Value: {p_value:.5f}\")\n",
    "print(f\"95% Confidence Interval for beta1: {conf_interval}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb0c96b",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "- Coefficient (𝛽_1): The slope is 0.7\n",
    "    - 0.7, indicating that y increases by 0.7 units for every one-unit increase in 𝑥 \n",
    "- Standard Error (𝑆𝐸_{𝛽_1): The slope estimate has a variability of 0.094, indicating precision.\n",
    "- t-Statistic and p-Value: The large t-statistic and small p-value indicate that 𝛽_1 is statistically significant.\n",
    "- Confidence Interval: We are 95% confident that the true value of 𝛽_1 lies between  0.467 and 0.933.\n",
    "\n",
    "Summary\n",
    "\n",
    "- Confidence Intervals: Quantify the uncertainty around coefficient estimates.\n",
    "- Hypothesis Testing: Assess the statistical significance of predictors.\n",
    "- Model Diagnostics: Evaluate and compare models using 𝑅𝑆𝐸.\n",
    "- Decision-Making: Use SEs to identify reliable predictors and improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d238e8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ calculate the 95% confidence interval for a regression coefficient, such as 𝛽1 (slope), you use the following formula:\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "# Step 1: Define data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2.2, 2.8, 3.6, 4.5, 5.1])\n",
    "\n",
    "# Step 2: Calculate the slope (beta1) and intercept (beta0)\n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "beta1 = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n",
    "beta0 = y_mean - beta1 * x_mean\n",
    "\n",
    "# Step 3: Calculate residuals, RSS, and RSE\n",
    "y_pred = beta0 + beta1 * x\n",
    "residuals = y - y_pred\n",
    "RSS = np.sum(residuals ** 2)\n",
    "n = len(x)\n",
    "RSE = np.sqrt(RSS / (n - 2))\n",
    "\n",
    "# Step 4: Calculate standard error of the slope (SE_beta1)\n",
    "SE_beta1 = RSE / np.sqrt(np.sum((x - x_mean) ** 2))\n",
    "\n",
    "# Step 5: Determine t-critical value for 95% confidence interval\n",
    "alpha = 0.05  # 95% confidence level\n",
    "df = n - 2  # Degrees of freedom\n",
    "t_critical = t.ppf(1 - alpha/2, df)\n",
    "\n",
    "# Step 6: Calculate confidence interval\n",
    "lower_bound = beta1 - t_critical * SE_beta1\n",
    "upper_bound = beta1 + t_critical * SE_beta1\n",
    "\n",
    "# Step 7: Print results\n",
    "print(f\"Slope (beta1): {beta1:.3f}\")\n",
    "print(f\"Standard Error (SE_beta1): {SE_beta1:.3f}\")\n",
    "print(f\"t-Critical: {t_critical:.3f}\")\n",
    "print(f\"95% Confidence Interval for beta1: ({lower_bound:.3f}, {upper_bound:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700d1d6",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "- The 95% confidence interval for $𝛽_1$ is (0.467, 0.933).\n",
    "- This means we are 95% confident that the true slope ($𝛽_1$) lies within this range.\n",
    "- Since the interval does not include 0, it indicates that the relationship between x and 𝑦 is statistically significant at the 5% significance level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d549922",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### calculate the t-statistic for a simple linear regression with one predictor (x) and one response variable (y).\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "# Step 1: Define data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2.2, 2.8, 3.6, 4.5, 5.1])\n",
    "\n",
    "# Step 2: Calculate coefficients\n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "beta1 = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n",
    "beta0 = y_mean - beta1 * x_mean\n",
    "\n",
    "# Step 3: Calculate residuals, RSS, and RSE\n",
    "y_pred = beta0 + beta1 * x\n",
    "residuals = y - y_pred\n",
    "RSS = np.sum(residuals ** 2)\n",
    "n = len(x)\n",
    "RSE = np.sqrt(RSS / (n - 2))\n",
    "\n",
    "# Step 4: Calculate standard error of the slope (SE_beta1)\n",
    "SE_beta1 = RSE / np.sqrt(np.sum((x - x_mean) ** 2))\n",
    "\n",
    "# Step 5: Calculate the t-statistic\n",
    "t_statistic = beta1 / SE_beta1\n",
    "\n",
    "# Step 6: Calculate p-value (two-tailed test)\n",
    "df = n - 2  # Degrees of freedom\n",
    "p_value = 2 * (1 - t.cdf(abs(t_statistic), df))\n",
    "\n",
    "# Step 7: Print results\n",
    "print(f\"Slope (beta1): {beta1:.3f}\")\n",
    "print(f\"Standard Error (SE_beta1): {SE_beta1:.3f}\")\n",
    "print(f\"t-Statistic: {t_statistic:.3f}\")\n",
    "print(f\"p-Value: {p_value:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80bdf33",
   "metadata": {},
   "source": [
    "##### Interpretation\n",
    "t-Statistic:\n",
    "- The t-statistic for the slope is 7.435, indicating the estimated coefficient is significantly far from zero.\n",
    "\n",
    "p-Value:\n",
    "- The small p-value (0.00231) suggests strong evidence against the null hypothesis (𝛽_1 = 0).\n",
    "- The predictor (x) is statistically significant at a 5% significance level.\n",
    "\n",
    "By comparing the t-statistic to critical t-values or using the p-value, you can conclude whether the predictor is significantly associated with the response variable.\n",
    "\n",
    "### Explaining the rules for rejecting the null hypothesis using p-values\n",
    "\n",
    "1. What is a p-value?\n",
    "\n",
    "The p-value is the probability of observing the data (or something more extreme) if the null hypothesis ($𝐻_0$) is true.\n",
    "- A low p-value indicates that the observed result is unlikely under the assumption of the null hypothesis.\n",
    "\n",
    "2. Decision Rule for Rejecting the Null Hypothesis\n",
    "\n",
    "The decision rule depends on the significance level (𝛼), which is the threshold for rejecting. \n",
    "- Common choices for α are 0.05 (5%) or 0.01 (1%).\n",
    "\n",
    "If p-value ≤ 𝛼: Reject the null hypothesis ($𝐻_0$).\n",
    "- The result is statistically significant.\n",
    "- There is strong evidence against the null hypothesis.\n",
    "\n",
    "If p-value > α: Fail to reject the null hypothesis ($𝐻_0$).\n",
    "- The result is not statistically significant.\n",
    "- There isn’t enough evidence to conclude that the null hypothesis is false.\n",
    "\n",
    "3. Interpretation Guidelines\n",
    "\n",
    "Small p-value (≤𝛼):\n",
    "- The observed effect is unlikely due to chance alone.\n",
    "Example: \n",
    "- p=0.03 suggests that there’s only a 3% chance of observing your data if $𝐻_0$ were true.\n",
    "\n",
    "Large p-value (>α):\n",
    "- The observed effect could plausibly occur due to chance.\n",
    "Example: \n",
    "- p=0.10 suggests that there’s a 10% chance of observing your data if $𝐻_0$ were true.\n",
    "\n",
    "Common Misinterpretations to Avoid\n",
    "\n",
    "1. The p-value is not the probability that $𝐻_0$ is true.\n",
    "- It reflects the likelihood of observing the data assuming $𝐻_0$ is true.\n",
    "2. Failing to reject $𝐻_0$ does not mean $𝐻_0$ is true.\n",
    "- It only means there isn’t enough evidence to conclude otherwise.\n",
    "3. A small p-value does not indicate a large effect size.\n",
    "- Statistical significance doesn’t always mean practical significance.\n",
    "\n",
    "Summary\n",
    "\n",
    "- The choice of significance level (α) determines the threshold for rejecting $𝐻_0$\n",
    "​- The p-value provides a way to quantify the strength of evidence against $𝐻_0$.\n",
    "- Always report both the p-value and 𝛼 for transparency in hypothesis testing.\n",
    "\n",
    "##### Practical Example\n",
    "Suppose you are testing whether a new marketing strategy improves sales:\n",
    "\n",
    "Null Hypothesis ($𝐻_0$): The new marketing strategy has no effect on sales (β=0).\n",
    "\n",
    "Alternative Hypothesis ($𝐻_a$): The new marketing strategy increases sales (β>0).\n",
    "\n",
    "If your analysis gives a p-value of 0.03, and you have set α=0.05:\n",
    "- Since p-value (0.03) < α (0.05), you reject ($𝐻_0$).\n",
    "- You conclude that the new marketing strategy likely increases sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f3e50",
   "metadata": {},
   "source": [
    "### Regression cost functions: Regression model evaluation metrics\n",
    "\n",
    "**loss function** is for a single training example. It is also sometimes called an error function. \n",
    "\n",
    "**cost function**, on the other hand, is the average loss over the entire training dataset. \n",
    "\n",
    "**Steps for Loss Functions**\n",
    "1. Define the predictor function f(X), and identify the parameters to find.\n",
    "2. Determine the loss for each training example.\n",
    "3. Derive the expression for the Cost Function, representing the average loss across all examples.\n",
    "4. Compute the gradient of the Cost Function concerning each unknown parameter.\n",
    "5. Select the learning rate and execute the weight update rule for a fixed number of iterations.\n",
    "\n",
    "These steps guide the optimization process, aiding in the determination of optimal model parameters.\n",
    "\n",
    "Regression model we generally use to evaluate the prediction error rates and model performance in regression analysis.\n",
    "\n",
    "1. **R-squared (Coefficient of determination)** \n",
    "- Indicates the proportion of variance in the dependent variable explained by the independent variables. \n",
    "- It represents the coefficient of how well the values fit compared to the original values. \n",
    "- It helps answer the question: \"How well does my model explain the variability in the dependent variable?\"\n",
    "- The value from 0 to 1 interpreted as percentages. \n",
    "    - where: \n",
    "        - $𝑅^2$ = 1 (close to 1): Perfect fit (all variability in y is explained by X).\n",
    "            - The model explains a large proportion of the variability in the data.\n",
    "            - A large proportion of the variance in the dependent variable is explained by the independent variables.\n",
    "            - Example: If $𝑅^2$ = 0.85\n",
    "                - Then 85% of the variability in y is explained by X. The remaining 15% is due to unexplained variability (e.g., noise, unobserved variables).\n",
    "        - $𝑅^2$ = 0 (close to 0): No relationship (the model does not explain any variability in y).\n",
    "            - The model fails to explain much of the variability or A small proportion of the variance is explained.\n",
    "            - Example: If $𝑅^2$ = 0.1\n",
    "                - only 10% of the variability is explained by the model. This suggests either:\n",
    "                    - The model lacks important predictors.\n",
    "                    - The relationship between X and y may not be linear.\n",
    "                    - There is high variability in y that cannot be captured effectively.\n",
    "    - The higher the value is, the better the model is / model fits the data better, but it does not necessarily mean the model is accurate in predictions. and does not imply causation or that the model is the best predictor.\n",
    "\n",
    "R-squared statistic is calculated as:\n",
    "\n",
    "$$𝑅^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$$\n",
    "\n",
    "Where:\n",
    "- $SS_{res}$ (Residual Sum of Squares): Sum of squared differences between actual and predicted values.\n",
    "\n",
    "$$SS_{res} = \\sum^{n}_{i = 1} (y_i - \\hat{y_i})^2$$\n",
    "\n",
    "- $SS_{tot}$ (Total Sum of Squares): Sum of squared differences between actual values and their mean.\n",
    "\n",
    "$$SS_{res} = \\sum^{n}_{i = 1} (y_i - \\bar{y_i})^2$$\n",
    "\n",
    "### Important Caveats\n",
    "1. Overfitting in Complex Models\n",
    "- High $𝑅^2$ may result from overfitting, especially in models with many predictors.\n",
    "2. Does Not Imply Causation\n",
    "- High $𝑅^2$ shows correlation, not causation. For example, a model may explain variability due to spurious relationships.\n",
    "3. Limited Applicability to Prediction\n",
    "- High $𝑅^2$ doesn’t guarantee that the model predicts new data well (check with metrics like RMSE on test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8171e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Step 1: Create synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10  # Independent variable\n",
    "y = 3 * X.squeeze() + 7 + np.random.randn(100) * 3  # Dependent variable with noise\n",
    "\n",
    "# Step 2: Fit a simple linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Step 3: Calculate R-squared manually\n",
    "y_mean = np.mean(y)\n",
    "SS_res = np.sum((y - y_pred) ** 2)  # Residual Sum of Squares\n",
    "SS_tot = np.sum((y - y_mean) ** 2)  # Total Sum of Squares\n",
    "R_squared = 1 - (SS_res / SS_tot)\n",
    "\n",
    "print(f\"Manual R-squared: {R_squared:.4f}\")\n",
    "\n",
    "# Step 4: Calculate R-squared using sklearn\n",
    "R_squared_sklearn = r2_score(y, y_pred)\n",
    "print(f\"Sklearn R-squared: {R_squared_sklearn:.4f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(X, y, label=\"Actual Data\", alpha=0.7)\n",
    "plt.plot(X, y_pred, color=\"red\", label=\"Fitted Line\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title(f\"Linear Regression (R-squared: {R_squared:.4f})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7df5a7",
   "metadata": {},
   "source": [
    "2. **Adjusted R-squared**\n",
    "- Adjusted version of $𝑅^2$ that accounts for the number of predictors in the model.\n",
    "\n",
    "$$𝑅^2_{adj} = 1 - \\frac{(1 - 𝑅^2)(n -1)}{n - p - 1}$$\n",
    "\n",
    "where:\n",
    "- n is the number of observations, and\n",
    "- p is the number of predictors.\n",
    "\n",
    "The Significance of R-squared is:\n",
    "    - if $R^2$ = 1 : Best-fit Line\n",
    "    - if $R^2$ = 0.5 : still some errors\n",
    "    - if $R^2$ = 0.05 : not performing well\n",
    "\n",
    "Usage: Useful when comparing models with a different number of predictors\n",
    "\n",
    "_____\n",
    "\n",
    "3. Mean Error (ME)\n",
    "- The error for each training data is calculated and then the mean value of all these errors is derived.\n",
    "- Errors can be both negative and positive. So they can cancel each other out during summation giving zero mean error for the model.\n",
    "- Not a recommended cost function but it does lay the foundation for other cost functions of regression models.\n",
    "\n",
    "Residual Analysis\n",
    "\n",
    "- Residuals: Differences between actual and predicted values ($𝑦_𝑖 − \\hat{𝑦_𝑖}$).\n",
    "- Analysis: Residual plots help diagnose issues like non-linearity, heteroscedasticity, and independence of errors.\n",
    "\n",
    "_____\n",
    "\n",
    "4. **MSE (Mean Squared Error)**\n",
    "- known as L2 loss.\n",
    "- represents the difference between the original and predicted values extracted by squared the average difference over the data set.\n",
    "- Here a square of the difference between the actual and predicted value is calculated to avoid any possibility of negative error(drawback cause).\n",
    "- It is measured as the average of the sum of squared differences between predictions and actual observations.\n",
    "$$MSE = \\frac{1}{n} \\sum^{n}_{i = 1} (y_i - \\hat{y_i})^2$$\n",
    "- Since each error is squared, it helps to penalize even small deviations in prediction when compared to MAE. \n",
    "    - But if our dataset has outliers that contribute to larger prediction errors, then squaring this error further will magnify the error many times more and also lead to higher MSE error.\n",
    "    - MSE loss function penalizes the model for making large errors by squaring them. Squaring a large quantity makes it even larger\n",
    "        - it is less robust to outliers\n",
    "        - not to be used if our data is prone to many outliers.\n",
    "\n",
    "Usage: Penalizes larger errors more heavily than smaller ones.\n",
    "\n",
    "Graphically\n",
    "- It is a positive quadratic function (of the form $ax^2 + bx + c$ where $a > 0$)\n",
    "- A quadratic function only has a global minimum. \n",
    "    - Since there are no local minima, we will never get stuck in one. \n",
    "- Hence, it is always guaranteed that Gradient Descent will converge (if it converges at all) to the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ff6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_MSE(m, b, X, Y, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    for i in range(N):\n",
    "        # Calculate partial derivatives\n",
    "        # -2x(y - (mx + b))\n",
    "        m_deriv += -2*X[i] * (Y[i] - (m*X[i] + b))\n",
    "\n",
    "        # -2(y - (mx + b))\n",
    "        b_deriv += -2*(Y[i] - (m*X[i] + b))\n",
    "\n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfde74f",
   "metadata": {},
   "source": [
    "**Advantages of R-squared**:\n",
    "\n",
    "- Interpretability: $𝑅^2$ is unitless and ranges between 0 and 1, making it easy to understand and compare across datasets.\n",
    "- Proportional Explanation: Quantifies the proportion of variance explained by the model, offering insights into model effectiveness.\n",
    "- Model Comparison: Useful for comparing the explanatory power of different models or regression equations.\n",
    "\n",
    "**Disadvantages of RSE Compared to R-squared**:\n",
    "\n",
    "- RSE depends on the scale of the dependent variable, making it hard to compare across datasets with different units.\n",
    "- RSE alone does not provide information on how much variance the model explains.\n",
    "\n",
    "_______\n",
    "\n",
    "5. **RMSE (Root Mean Squared Error)** \n",
    "- is the error rate by the square root of MSE.\n",
    "\n",
    "$$RMSE = \\sqrt{MSE}$$\n",
    "\n",
    "Usage: Commonly used because it is in the same units as the dependent variable and emphasizes larger errors.\n",
    "\n",
    "______\n",
    "\n",
    "6. **MAE (Mean absolute error)**\n",
    "- known as L1 Loss.\n",
    "- represents the difference between the original and predicted values extracted by averaged the absolute difference over the data set.\n",
    "- It is the average of the absolute differences between predicted and actual values.\n",
    "- Absolute Error for each training example is the distance between the predicted and the actual values, irrespective of the sign.\n",
    "    - it is the absolute difference between the actual and predicted values.\n",
    "- Here an absolute difference between the actual and predicted value is calculated to avoid any possibility of negative error.\n",
    "- It is measured as the average of the sum of absolute differences between predictions and actual observations.\n",
    "    - It is robust to outliers thus it will give better results even when our dataset has noise or outliers.\n",
    "    - MAE cost is more robust to outliers as compared to MSE\n",
    "-  The cost is the Mean of these Absolute Errors\n",
    "\n",
    "$$MAE = \\frac{1}{n} \\sum^{n}_{i = 1} |y_i - \\hat{y_i}|$$\n",
    "\n",
    "Usage: Provides an easily interpretable measure of error in the same units as the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34787f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_MAE(m, b, X, Y, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    for i in range(N):\n",
    "        # Calculate partial derivatives\n",
    "        # -x(y - (mx + b)) / |mx + b|\n",
    "        m_deriv += - X[i] * (Y[i] - (m*X[i] + b)) / abs(Y[i] - (m*X[i] + b))\n",
    "\n",
    "        # -(y - (mx + b)) / |mx + b|\n",
    "        b_deriv += -(Y[i] - (m*X[i] + b)) / abs(Y[i] - (m*X[i] + b))\n",
    "\n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ab3d2",
   "metadata": {},
   "source": [
    "7. **Mean Absolute Percentage Error (MAPE)**\n",
    "\n",
    "Definition: The mean of the absolute percentage differences between predicted and actual values.\n",
    "\n",
    "$$MAPE = \\frac{1}{n} \\sum^{n}_{i = 1} |\\frac{y_i - \\hat{y_i}}{y_i}| \\times 100$$\n",
    "\n",
    "Usage: Expresses error as a percentage, making it easier to interpret across datasets\n",
    "\n",
    "___________\n",
    "\n",
    "8. Huber Loss\n",
    "\n",
    "- The Huber loss combines the best properties of MSE and MAE.\n",
    "- It is quadratic for smaller errors and is linear otherwise (and similarly for its gradient). \n",
    "- It is identified by its delta parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b94332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_Huber(m, b, X, Y, delta, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    for i in range(N):\n",
    "        # derivative of quadratic for small values and of linear for large values\n",
    "        if abs(Y[i] - m*X[i] - b) <= delta:\n",
    "          m_deriv += -X[i] * (Y[i] - (m*X[i] + b))\n",
    "          b_deriv += - (Y[i] - (m*X[i] + b))\n",
    "        else:\n",
    "          m_deriv += delta * X[i] * ((m*X[i] + b) - Y[i]) / abs((m*X[i] + b) - Y[i])\n",
    "          b_deriv += delta * ((m*X[i] + b) - Y[i]) / abs((m*X[i] + b) - Y[i])\n",
    "    \n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ddd25a",
   "metadata": {},
   "source": [
    "##### Choosing the Right Measure\n",
    "- Use R-squared or Adjusted R-squared to evaluate the proportion of variance explained.\n",
    "- Use MAE, MSE, or RMSE for measuring prediction accuracy in units of the target variable.\n",
    "- Use MAPE for interpreting errors as percentages.\n",
    "\n",
    "**Step 5: Interpret the Results**\n",
    "\n",
    "Residual Analysis:\n",
    "- Check normal distribution and normality for the residuals.\n",
    "- Homoscedasticity describes a situation in which error term is the same across all values of the independent variables. \n",
    "    - means that the residuals are equal across the regression line.\n",
    "\n",
    "Interpretation of Regression Output\n",
    "- R-Squared : is a statistical measure of fit that indicates how much variation of a dependent variable is explained by the independent variables. \n",
    "    - Higher R-Squared value represents smaller differences between the observed data and fitted values.\n",
    "\n",
    "**Optimization technique/Strategy**\n",
    "\n",
    "We will use Gradient Descent as an optimization strategy to find the regression line.\n",
    "- Weight Update Rule\n",
    "\n",
    "NB: Perform optimization on the training data and check its performance on a new validation data.\n",
    "\n",
    "**Gradient Descent for Linear Regression**\n",
    "\n",
    "What is gradient descent?\n",
    "- lay man: \n",
    "    - It is a way of checking the ground near you and observe where the land tends to descend.\n",
    "    - It gives an idea in what direction you should take your steps.\n",
    "    - It helps models find the optimal set of parameters by iteratively adjusting them in the opposite direction of the gradient, aiming to find the optimal set of parameters.\n",
    "\n",
    "Mathematical terms:\n",
    "- find out the best parameters ($θ_1$) and ($θ_2$) for our learning algorithm.\n",
    "\n",
    "Cost space is how our algorithm would perform when we choose a particular value for a parameter.\n",
    "\n",
    "Cost Function is a function that measures the performance of a model for any given data. Cost Function quantifies the error between predicted values and expected values and presents it in the form of a single real number.\n",
    "\n",
    "1. Make a hypothesis with initial parameters\n",
    "- Hypothesis: $h_θ(x) = θ_0 + θ_1 x$\n",
    "- Parameters: $θ_o, θ_1$\n",
    "2. Calculate the Cost function\n",
    "- Cost Function: $J(θ_o, θ_1) = \\frac{1}{2m}\\sum^{m}_{i = 1} (h_θ (x^{(i)}) - y^{i})^2$\n",
    "3. The goal is to reduce the cost function, we modify the parameters by using the Gradient descent algorithm over the given data.\n",
    "- Goal: $minimize_{θ_o, θ_1} J(θ_o, θ_1)$\n",
    "\n",
    "**Gradient descent**\n",
    "\n",
    "- one of the optimization algorithms that optimize the cost function (objective function) to reach the optimal minimal solution.\n",
    "- aims to find the parameters that minimize this discrepancy and improve the model’s performance.\n",
    "    - Need to reduce the cost function (MSE) for all data points. \n",
    "    - This is done by updating the values of the slope coefficient and the constant coefficient iteratively until we get an optimal solution for the linear function.\n",
    "\n",
    "The algorithm operates by calculating the gradient of the cost function, \n",
    "- which indicates the direction and magnitude of the steepest ascent. \n",
    "\n",
    "However, since the goal is to minimize the cost function, gradient descent moves in the opposite direction of the gradient, \n",
    "- known as the negative gradient direction.\n",
    "\n",
    "Iteratively updating the model’s parameters in the negative gradient direction, gradient descent gradually converges towards the optimal set of parameters that yields the lowest cost.\n",
    "\n",
    "- Hyperparameter: learning rate, determines the step size taken in each iteration, influencing the speed and stability of convergence.\n",
    "\n",
    "Gradient descent can be applied to:\n",
    "- linear regression, \n",
    "- logistic regression, \n",
    "- neural networks, and \n",
    "- support vector machines.\n",
    "\n",
    "**Definition**: Gradient descent is an iterative optimization algorithm for finding the local minimum of a function.\n",
    "\n",
    "To find the local minimum of a function using gradient descent, we must take steps proportional to the negative of the gradient (move away from the gradient) of the function at the current point.\n",
    "- If we take steps proportional to the positive of the gradient (moving towards the gradient), we will approach a local maximum of the function, and the procedure is called Gradient Ascent.\n",
    "\n",
    "The goal of the gradient descent algorithm is to minimize the given function (say, cost function)\n",
    "- it performs two steps iteratively:\n",
    "1. Compute the gradient (slope), the first-order derivative of the function at that point\n",
    "2. Make a step (move) in the direction opposite to the gradient. The opposite direction of the slope increases from the current point by alpha times the gradient at that point.\n",
    "- number of steps you’re taking can be considered as the learning rate, and this decides how fast the algorithm converges to the minima.\n",
    "\n",
    "This code creates a function called gradient_descent, which requires the training data, learning rate, and number of iterations as parameters.\n",
    "\n",
    "Steps :\n",
    "1. Sets weights and bias to arbitrary values during initialization.\n",
    "2. Executes a set number of iterations for loops.\n",
    "3. Computes the estimated y values by utilizing the existing weights and bias.\n",
    "4. Calculates the discrepancy between expected and real y values.\n",
    "5. Determines the changes in the cost function based on weights and bias.\n",
    "6. Adjusts the weights and bias by incorporating the gradients and learning rate.\n",
    "7. Outputs the acquired weights and bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad28b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(X, y, learning_rate, num_iters):\n",
    "  \"\"\"\n",
    "  Performs gradient descent to find optimal weights and bias for linear regression.\n",
    "\n",
    "  Args:\n",
    "      X: A numpy array of shape (m, n) representing the training data features.\n",
    "      y: A numpy array of shape (m,) representing the training data target values.\n",
    "      learning_rate: The learning rate to control the step size during updates.\n",
    "      num_iters: The number of iterations to perform gradient descent.\n",
    "\n",
    "  Returns:\n",
    "      A tuple containing the learned weights and bias.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize weights and bias with random values\n",
    "  m, n = X.shape\n",
    "  weights = np.random.rand(n)\n",
    "  bias = 0\n",
    "\n",
    "  # Loop for the number of iterations\n",
    "  for i in range(num_iters):\n",
    "    # Predict y values using current weights and bias\n",
    "    y_predicted = np.dot(X, weights) + bias\n",
    "\n",
    "    # Calculate the error\n",
    "    error = y - y_predicted\n",
    "\n",
    "    # Calculate gradients for weights and bias\n",
    "    weights_gradient = -2/m * np.dot(X.T, error)\n",
    "    bias_gradient = -2/m * np.sum(error)\n",
    "\n",
    "    # Update weights and bias using learning rate\n",
    "    weights -= learning_rate * weights_gradient\n",
    "    bias -= learning_rate * bias_gradient\n",
    "\n",
    "  return weights, bias\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[1, 1], [2, 2], [3, 3]])\n",
    "y = np.array([2, 4, 5])\n",
    "learning_rate = 0.01\n",
    "num_iters = 100\n",
    "\n",
    "weights, bias = gradient_descent(X, y, learning_rate, num_iters)\n",
    "\n",
    "print(\"Learned weights:\", weights)\n",
    "print(\"Learned bias:\", bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af34d913",
   "metadata": {},
   "source": [
    "How Does Gradient Descent Work?\n",
    "1. The algorithm optimizes to minimize the model’s cost function.\n",
    "2. The cost function measures how well the model fits the training data and defines the difference between the predicted and actual values.\n",
    "3. The cost function’s gradient is the derivative with respect to the model’s parameters and points in the direction of the steepest ascent.\n",
    "4. The algorithm starts with an initial set of parameters and updates them in small steps to minimize the cost function.\n",
    "5. In each iteration of the algorithm, it computes the gradient of the cost function with respect to each parameter.\n",
    "6. The gradient tells us the direction of the steepest ascent, and by moving in the opposite direction, we can find the direction of the steepest descent.\n",
    "7. The learning rate controls the step size, which determines how quickly the algorithm moves towards the minimum.\n",
    "8. The process is repeated until the cost function converges to a minimum. Therefore indicating that the model has reached the optimal set of parameters.\n",
    "9. Different variations of gradient descent include batch gradient descent, stochastic gradient descent, and mini-batch gradient descent, each with advantages and limitations.\n",
    "10. Efficient implementation of gradient descent is essential for performing well in machine learning tasks. The choice of the learning rate and the number of iterations can significantly impact the algorithm’s performance.\n",
    "\n",
    "On the basis of differentiation techniques \n",
    "- Gradient descent requires Calculation of gradient by differentiation of cost function. We can either use first order differentiation or second order differentiation.\n",
    "    - First order Differentiation\n",
    "    - Second order Differentiation.\n",
    "\n",
    "To update B 0 and B 1, we take gradients from the cost function. To find these gradients, we take partial derivatives for $B_0$ and $B_1$.\n",
    "\n",
    "$J = \\frac{1}{n} \\sum^{n}_{i = 1} (𝛽_{0}+𝛽_{1} . x_i - y_i)^2$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial 𝛽_{0}} = \\frac{2}{n} \\sum^{n}_{i = 1} (𝛽_{0}+𝛽_{1} . x_i - y_i)$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial 𝛽_{1}} = \\frac{2}{n} \\sum^{n}_{i = 1} (𝛽_{0}+𝛽_{1} . x_i - y_i) . x_i$\n",
    "\n",
    "$𝛽_{0} = 𝛽_{0} - \\alpha . \\frac{2}{n} \\sum^{n}_{i = 1} ( y_{pred} - y_{i}) $\n",
    "\n",
    "$𝛽_{1} = 𝛽_{1} - \\alpha . \\frac{2}{n} \\sum^{n}_{i = 1} ( y_{pred} - y_{i}) . x_i $\n",
    "\n",
    "Where: \n",
    "- The partial derivates are the gradients, and they are used to update the values of $B_0$ and $B_1$. \n",
    "- Alpha is the learning rate.\n",
    "\n",
    "**Types of Gradient Descent**\n",
    "\n",
    "Classified by two methods mainly:\n",
    "- On the basis of data ingestion: choice of gradient descent algorithm depends on the problem at hand and the size of the dataset.\n",
    "\n",
    "**Full Batch Gradient Descent Algorithm**:\n",
    "- Batch gradient descent,\n",
    "    - also known as vanilla gradient descent, \n",
    "- full batch gradient descent algorithms, you use whole data at once to compute the gradient.\n",
    "    - It updates the model’s parameters using the gradient of the entire training set.\n",
    "- It calculates the average gradient of the cost function for all the training examples and updates the parameters in the opposite direction.\n",
    "    - calculates the error for each example within the training dataset.\n",
    "    - The model is not changed until every training sample has been assessed. \n",
    "        - The entire procedure is referred to as a **cycle and a training epoch**.\n",
    "- Batch gradient descent guarantees convergence to the global minimum but can be computationally expensive and slow for large datasets.\n",
    "    - Batch gradient descent is suitable for small datasets.\n",
    "    - Its computational efficiency, which produces a stable error gradient and a stable convergence.\n",
    "- Drawbacks are that the stable error gradient can sometimes result in a state of convergence that isn’t the best the model can achieve. \n",
    "    - It also requires the entire training dataset to be in memory and available to the algorithm.\n",
    "\n",
    "Advantages\n",
    "- Fewer model updates mean that this variant of the steepest descent method is more computationally efficient than the stochastic gradient descent method.\n",
    "- Reducing the update frequency provides a more stable error gradient and a more stable convergence for some problems.\n",
    "- Separating forecast error calculations and model updates provides a parallel processing-based algorithm implementation.\n",
    "\n",
    "Disadvantages\n",
    "- A more stable error gradient can cause the model to prematurely converge to a suboptimal set of parameters.\n",
    "- End-of-training epoch updates require the additional complexity of accumulating prediction errors across all training examples.\n",
    "- The batch gradient descent method typically requires the entire training dataset in memory and is implemented for use in the algorithm.\n",
    "- Large datasets can result in very slow model updates or training speeds.\n",
    "- Slow and require more computational power.\n",
    "\n",
    "#### Variants\n",
    "\n",
    "##### Vanilla Gradient Descent, \n",
    "\n",
    "Vanilla means pure / without any adulteration.\n",
    "- simplest form of gradient descent technique\n",
    "    - main feature is that we take small steps in the direction of the minima by taking gradient of the cost function.\n",
    "\n",
    "Pseudocode Vanilla Gradient Descent\n",
    "\n",
    "$ update = learning rate * gradient of parameters$\n",
    "\n",
    "$ parameters = parameters - update$\n",
    "\n",
    "- make an update to the parameters by taking gradient of the parameters. \n",
    "- And multiplying it by a learning rate, which is essentially a constant number suggesting how fast we want to go the minimum. 4\n",
    "**Learning rate** is a hyper-parameter and should be treated with care when choosing its value.\n",
    "\n",
    "##### Gradient Descent with Momentum\n",
    "\n",
    "Tweaks the above algorithm in such a way that we pay heed to the prior step before taking the next step.\n",
    "\n",
    "Pseudocode Gradient Descent with Momentum\n",
    "\n",
    "$ update = learning_rate * gradient$ \n",
    "\n",
    "$ velocity = previous_update * momentum$ \n",
    "\n",
    "$ parameter = parameter + velocity – update$ \n",
    "\n",
    "Introduces Velocity, which considers the previous update and a constant which is called momentum.\n",
    "\n",
    "##### ADAGRAD\n",
    "\n",
    "ADAGRAD uses adaptive technique for learning rate updation. In this algorithm, on the basis of how the gradient has been changing for all the previous iterations we try to change the learning rate.\n",
    "\n",
    "Pseudocode ADAGRAD\n",
    "\n",
    "$ grad_component = previous_grad_component + (gradient * gradient)$ \n",
    "\n",
    "$ rate_change = square_root(grad_component) + epsilon$\n",
    "\n",
    "$ adapted_learning_rate = learning_rate * rate_change$\n",
    "\n",
    "$update = adapted_learning_rate * gradient$\n",
    "\n",
    "$parameter = parameter – update$\n",
    "\n",
    "where:\n",
    "-  epsilon is a constant which is used to keep rate of change of learning rate in check.\n",
    "\n",
    "##### ADAM\n",
    "\n",
    "ADAM is one more adaptive technique which builds on adagrad and further reduces it downside.\n",
    "- consider this as momentum + ADAGRAD.\n",
    "\n",
    "Pseudocode.\n",
    "\n",
    "$ adapted_gradient = previous_gradient + ((gradient – previous_gradient) * (1 – beta1))$\n",
    "\n",
    "$ gradient_component = (gradient_change – previous_learning_rate)$\n",
    "\n",
    "$ adapted_learning_rate =  previous_learning_rate + (gradient_component * (1 – beta2))$\n",
    "\n",
    "$ update = adapted_learning_rate * adapted_gradient$\n",
    "\n",
    "$ parameter = parameter – update$\n",
    "\n",
    "where:\n",
    "- beta1 and beta2 are constants to keep changes in gradient and learning rate in check\n",
    "\n",
    "There are also second order differentiation method like **l-BFGS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ec0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDRegressor:\n",
    "    \n",
    "    def __init__(self,learning_rate=0.01,epochs=100):\n",
    "        \n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def fit(self,X_train,y_train):\n",
    "        # init your coefs\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            # update all the coef and the intercept\n",
    "            y_hat = np.dot(X_train,self.coef_) + self.intercept_\n",
    "            #print(\"Shape of y_hat\",y_hat.shape)\n",
    "            intercept_der = -2 * np.mean(y_train - y_hat)\n",
    "            self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "            \n",
    "            coef_der = -2 * np.dot((y_train - y_hat),X_train)/X_train.shape[0]\n",
    "            self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "        \n",
    "        print(self.intercept_,self.coef_)\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c381ad",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent Algorithm**\n",
    "- stochastic you take a sample while computing the gradient.\n",
    "    - It randomly selects a training dataset example, \n",
    "        - changes the parameters for each training sample one at a time for each training example in the dataset.\n",
    "            - The regular updates give us a fairly accurate idea of the rate of improvement. (benefit)\n",
    "    - computes the gradient of the cost function for that example, \n",
    "    - and updates the parameters in the opposite direction.\n",
    "- stochastic gradient descent algorithm is more suitable for large datasets.\n",
    "- It is computationally efficient and can converge faster than batch gradient descent. It can be noisy (produce noisy gradients), cause the error rate to fluctuate rather than gradually go down and may not converge to the global minimum.\n",
    "\n",
    "Advantages\n",
    "- You can instantly see your model’s performance and improvement rates with frequent updates.\n",
    "- This variant of the steepest descent method is probably the easiest to understand and implement, especially for beginners.\n",
    "- Increasing the frequency of model updates will allow you to learn more about some issues faster.\n",
    "- The noisy update process allows the model to avoid local minima (e.g., premature convergence).\n",
    "- Faster and require less computational power.\n",
    "- Suitable for the larger dataset.\n",
    "\n",
    "Disadvantages\n",
    "- Frequent model updates are more computationally intensive than other steepest descent configurations, and it takes considerable time to train the model with large datasets.\n",
    "- Frequent updates can result in noisy gradient signals. This can result in model parameters and cause errors to fly around (more variance across the training epoch).\n",
    "- A noisy learning process along the error gradient can also make it difficult for the algorithm to commit to the model’s minimum error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c4540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "clf.fit(X, y)\n",
    "SGDClassifier(max_iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b03ce9",
   "metadata": {},
   "source": [
    "**Mini-batch Gradient Descent**\n",
    "- Mini-batch is a good compromise between the two and is often used in practice.\n",
    "- updates the model’s parameters using the gradient of a small batch size of the training dataset, known as a mini-batch. \n",
    "- It calculates the average gradient of the cost function for the mini-batch and updates the parameters in the opposite direction.\n",
    "- It is the most commonly used method in practice because combines the ideas of batch gradient descent with SGD.\n",
    "        - strikes a balance between batch gradient descent’s effectiveness and stochastic gradient descent’s durability.\n",
    "- It is computationally efficient and less noisy than stochastic gradient descent while still being able to converge to a good solution.\n",
    "- Mini-batch sizes typically range from 50 to 256.\n",
    "\n",
    "Advantages\n",
    "- The model is updated more frequently than the stack gradient descent method, allowing for more robust convergence and avoiding local minima.\n",
    "- Batch updates provide a more computationally efficient process than stochastic gradient descent.\n",
    "- Batch processing allows for both the efficiency of not having all the training data in memory and implementing the algorithm.\n",
    "\n",
    "Disadvantages\n",
    "- Mini-batch requires additional hyperparameters “mini-batch size” to be set for the learning algorithm.\n",
    "- Error information should be accumulated over a mini-batch of training samples, such as batch gradient descent.\n",
    "- it will generate complex functions.\n",
    "\n",
    "Configure Mini-Batch Gradient Descent:\n",
    "\n",
    "- The mini-batch steepest descent method is a variant of the steepest descent method recommended for most applications, intense learning.\n",
    "- Mini-batch sizes, commonly called “batch sizes” for brevity, are often tailored to some aspect of the computing architecture in which the implementation is running. \n",
    "        - For example, a power of 2 that matches the memory requirements of the GPU or CPU hardware, such as 32, 64, 128, and 256.\n",
    "- The stack size is a slider for the learning process.\n",
    "- Smaller values ​​allow the learning process to converge quickly at the expense of noise in the training process. Larger values ​​result in a learning - process that slowly converges to an accurate estimate of the error gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8eaf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBGDRegressor:\n",
    "    \n",
    "    def __init__(self,batch_size,learning_rate=0.01,epochs=100):\n",
    "        \n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def fit(self,X_train,y_train):\n",
    "        # init your coefs\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            for j in range(int(X_train.shape[0]/self.batch_size)):\n",
    "                \n",
    "                idx = random.sample(range(X_train.shape[0]),self.batch_size)\n",
    "                \n",
    "                y_hat = np.dot(X_train[idx],self.coef_) + self.intercept_\n",
    "                #print(\"Shape of y_hat\",y_hat.shape)\n",
    "                intercept_der = -2 * np.mean(y_train[idx] - y_hat)\n",
    "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "\n",
    "                coef_der = -2 * np.dot((y_train[idx] - y_hat),X_train[idx])\n",
    "                self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "        \n",
    "        print(self.intercept_,self.coef_)\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c2ddb9",
   "metadata": {},
   "source": [
    "**Step 6: Use the Model for Decision-Making**\n",
    "\n",
    "Understanding which factors significantly influence readmissions,\n",
    "\n",
    "To do this, you need a systematic approach grounded in exploratory analysis, statistical rigor, and effective communication\n",
    "\n",
    "1. Thinking Approach: Identifying Significant Factors\n",
    "- Define the Business Objective\n",
    "    - Objective: Identify key drivers of hospital readmissions (to improve patient care and optimize resource allocation)\n",
    "    - Questions to Answer:\n",
    "        - What are the strongest predictors of readmissions?\n",
    "        - Which predictors can be influenced through policy or operational changes?\n",
    "        - How much can readmissions be reduced if certain factors are addressed?\n",
    "\n",
    "- Perform Exploratory Data Analysis (EDA)\n",
    "    - Inspect Data Distributions: Use histograms and boxplots to understand the spread of variables.\n",
    "    - Check Relationships:\n",
    "        - Pairwise correlations for numerical variables (e.g., length_of_stay vs. readmissions).\n",
    "        - Grouped summaries for categorical variables (e.g., readmissions across age groups).\n",
    "        - Example Insights:\n",
    "            - Patients with longer stays might have higher readmission risks.\n",
    "            - Non-adherence to medication might strongly correlate with readmissions.\n",
    "\n",
    "- Statistical Hypothesis Testing\n",
    "    - Use statistical tests to confirm relationships:\n",
    "        - T-tests for differences in means (e.g., medication adherence between high and low readmission groups).\n",
    "        - Chi-square tests for independence between categorical variables (e.g., age group vs. readmission rates).\n",
    "\n",
    "Example 1: Statistical Hypothesis Testing for Medication Adherence\n",
    "- Objective: Determine if medication adherence significantly differs between patients who are readmitted and those who are not.\n",
    "- Approach: Two-Sample t-Test\n",
    "- Hypotheses: \n",
    "    - $𝐻_0$ : The mean adherence rate is the same for both groups (readmitted and not readmitted).\n",
    "    - $𝐻_𝑎$ : The mean adherence rate differs between the groups.\n",
    "\n",
    "- Steps:\n",
    "    - Prepare the Data:\n",
    "    - Split patients into two groups: \"Readmitted\" and \"Not Readmitted.\"\n",
    "    - Collect medication adherence rates for each group.\n",
    "\n",
    "- Check Assumptions:\n",
    "    - Normality: Use a Shapiro-Wilk or Kolmogorov-Smirnov test to check if adherence rates are normally distributed.\n",
    "    - Equal Variance: Use Levene’s test or Bartlett’s test.\n",
    "\n",
    "- Perform the t-Test:\n",
    "    - If variances are equal, use a standard t-test. If not, use Welch’s t-test.\n",
    "\n",
    "- Interpret Results: \n",
    "    - If $𝑝 < 0.05$, reject $𝐻_0$\n",
    "    - Conclude that adherence rates differ significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21143b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Example data\n",
    "adherence_readmitted = [0.7, 0.65, 0.6, 0.75, 0.8]  # Adherence rates for readmitted\n",
    "adherence_not_readmitted = [0.9, 0.85, 0.88, 0.92, 0.89]  # Adherence rates for not readmitted\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = ttest_ind(adherence_readmitted, adherence_not_readmitted, equal_var=False)\n",
    "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3ddb9d",
   "metadata": {},
   "source": [
    "Example 2: Statistical Hypothesis Testing for Age Group vs. Readmission Rates\n",
    "- Objective: Test if age group (categorical variable) is independent of readmission status.\n",
    "- Approach: Chi-Square Test of Independence\n",
    "- Hypotheses:\n",
    "    - $𝐻_0$ : Age group is independent of readmission status.\n",
    "    - $𝐻_𝑎$ : Age group and readmission status are dependent.\n",
    "\n",
    "- Steps:\n",
    "    - Create a Contingency Table:\n",
    "        - Rows: Age groups (e.g., <40, 40–60, >60).\n",
    "        - Columns: Readmission status (e.g., Yes, No).\n",
    "\n",
    "- Perform the Chi-Square Test:\n",
    "\n",
    "- Interpret Results:\n",
    "    - If $ 𝑝< 0.05$, reject $𝐻_0$​\n",
    "    - Conclude that age group influences readmission rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aecd0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Contingency table\n",
    "table = np.array([[50, 200], [70, 230], [100, 300]])\n",
    "\n",
    "# Perform Chi-Square Test\n",
    "chi2, p_value, dof, expected = chi2_contingency(table)\n",
    "print(f\"Chi2 Statistic: {chi2}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d538a",
   "metadata": {},
   "source": [
    "Example 3: Statistical Hypothesis Testing for Length of Stay (LOS)\n",
    "- Objective: Compare Average LOS for Readmitted vs. Not Readmitted Patients\n",
    "- Approach: Two-Sample t-Test\n",
    "    - $𝐻_0$ : The mean LOS is the same for readmitted and non-readmitted patients.\n",
    "    - $𝐻_𝑎$ : The mean LOS differs.\n",
    "- Steps:\n",
    "    - Prepare the Data:\n",
    "    - Split patients into two groups: \"Readmitted\" and \"Not Readmitted.\"\n",
    "    - Collect medication Length of stay for each group.\n",
    "\n",
    "- Check Assumptions:\n",
    "    - Normality: Use a Shapiro-Wilk or Kolmogorov-Smirnov test to check if Lengths of stay are normally distributed.\n",
    "    - Equal Variance: Use Levene’s test or Bartlett’s test.\n",
    "\n",
    "- Perform the t-Test:\n",
    "    - If variances are equal, use a standard t-test. If not, use Welch’s t-test.\n",
    "\n",
    "- Interpret Results: \n",
    "    - If $𝑝 < 0.05$, reject $𝐻_0$\n",
    "    - Conclude that adherence rates differ significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a39723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe8fb84d",
   "metadata": {},
   "source": [
    "Example 4: Relationship Between LOS and Readmission Rate\n",
    "- Approach: ANOVA (Analysis of Variance)\n",
    "- Objective: Check if LOS groups (<3 days, 3–7 days, >7 days) have significantly different readmission rates.\n",
    "- Hypotheses: \n",
    "    - $𝐻_0$ : The mean readmission rate is the same across all LOS groups.\n",
    "    - $𝐻_𝑎$ : At least one group differs.\n",
    "- Steps:\n",
    "    - Group the Data:\n",
    "        - Divide LOS into groups.\n",
    "        - Calculate readmission rates for each group.\n",
    "- Perform ANOVA:\n",
    "- Interpret Results:\n",
    "    - If $𝑝 < 0.05$\n",
    "    - reject $𝐻_0$\n",
    "    - Conclude that LOS impacts readmission rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab649501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Example data\n",
    "readmission_short = [0.1, 0.12, 0.08, 0.15]  # Readmission rates for <3 days\n",
    "readmission_medium = [0.2, 0.22, 0.25, 0.18]  # Readmission rates for 3–7 days\n",
    "readmission_long = [0.35, 0.4, 0.38, 0.42]  # Readmission rates for >7 days\n",
    "\n",
    "# Perform ANOVA\n",
    "f_stat, p_value = f_oneway(readmission_short, readmission_medium, readmission_long)\n",
    "print(f\"F-statistic: {f_stat}, P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be03fb0",
   "metadata": {},
   "source": [
    "\n",
    "- Build and Interpret a Regression Model\n",
    "    - Fit the Linear Regression model to identify significant predictors:\n",
    "    - Check p-values of coefficients: Variables with p-values below a chosen threshold (e.g., 0.05) are statistically significant.\n",
    "    - Evaluate effect size: Large coefficients indicate strong influence on the target.\n",
    "    - Test for interaction effects, such as how length_of_stay and severity jointly influence readmissions.\n",
    "\n",
    "- Refine the Model\n",
    "    - Handle multicollinearity: Use Variance Inflation Factor (VIF) to remove or combine highly correlated predictors.\n",
    "    - Validate the model: Perform cross-validation to ensure robustness.\n",
    "\n",
    "This will help the institute to:\n",
    "- Improve medication adherence programs for high-risk patients.\n",
    "- Extend hospital stays for patients with severe conditions if needed.\n",
    "- Schedule follow-up visits more effectively to minimize readmission risks.\n",
    "\n",
    "Example 2: Predicting Readmissions Based on LOS\n",
    "- Approach: Linear Regression\n",
    "- Objective: Use regression to predict readmissions based on LOS and other predictors.\n",
    "\n",
    "##### Linear Regression Helps Solve This Problem\n",
    "- Quantifies Relationships: Identifies and quantifies the factors contributing to readmissions.\n",
    "- Predicts Outcomes: Provides actionable predictions to guide healthcare interventions.\n",
    "- Allocates Resources: Helps prioritize patients who need more attention post-discharge.\n",
    "- Supports Policy Changes: Enables data-driven policy improvements in patient care."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03949464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Example data\n",
    "X = [2, 4, 6, 8, 10]  # LOS\n",
    "y = [0, 1, 0, 1, 1]  # Readmission (0 = No, 1 = Yes)\n",
    "\n",
    "# Add constant for intercept\n",
    "X = sm.add_constant(X)\n",
    "model = sm.Logit(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d99cd9",
   "metadata": {},
   "source": [
    "2. Presenting Findings to Senior Management and Board\n",
    "- Tailor Communication to the Audience\n",
    "    - Senior management: Focus on actionable insights, resource implications, and patient care improvements.\n",
    "    - Board of directors: Emphasize high-level trends, financial impacts, and alignment with strategic goals.\n",
    "\n",
    "- Structure of Presentation\n",
    "    - Introduction\n",
    "        - Start with the context: \"Readmission rates are a critical indicator of hospital performance and patient care quality.\"\n",
    "        - Summarize the objective: \"This study identifies key factors driving readmissions and proposes targeted interventions.\"\n",
    "\n",
    "    - Key Findings\n",
    "        - Use visuals like \n",
    "            - bar charts, \n",
    "            - scatter plots, and \n",
    "            - regression coefficient tables:\n",
    "                - Example: \"Medication adherence has the strongest inverse relationship with readmissions. A 10% increase in adherence reduces readmissions by 5%.\"\n",
    "            - Highlight statistical significance:\n",
    "                - \"Length of stay and severity are significant at p < 0.05, confirming their importance.\"\n",
    "    \n",
    "    - Implications\n",
    "        - Show real-world impact: \"Addressing non-adherence could prevent ~300 readmissions annually, saving $1.2M in costs.\"\n",
    "        - Prioritize recommendations: \"Focus on medication adherence programs, especially for older patients with comorbidities.\"\n",
    "\n",
    "    - Actionable Recommendations\n",
    "        - Immediate Steps:\n",
    "            - Develop a post-discharge follow-up protocol for high-risk groups.\n",
    "            - Launch an adherence monitoring program.\n",
    "        - Future Research:\n",
    "            - Investigate additional factors like social determinants of health.\n",
    "\n",
    "    - Conclusion\n",
    "        - Reinforce value: \"By addressing these factors, we can improve patient outcomes, meet regulatory benchmarks, and reduce financial strain.\"\n",
    "\n",
    "- Tools for Communication\n",
    "    - Visual Dashboards: Create dashboards showing predicted readmissions, trends over time, and \"what-if\" scenarios.\n",
    "    - Executive Summaries: Provide concise summaries with high-impact visuals and key takeaways.\n",
    "    - Financial Impact Models: Quantify cost savings or ROI of proposed interventions.\n",
    "\n",
    "3. Example Insights and Visualizations\n",
    "Insight Example: Medication Adherence\n",
    "    - Insight: \"Medication adherence has a strong negative correlation with readmissions ($𝑅=−0.65$)\n",
    "        - A 10% increase in adherence is associated with a 5% reduction in readmissions.\"\n",
    "\n",
    "Visualization:\n",
    "    - A bar chart comparing adherence rates and average readmissions.\n",
    "    - Regression coefficient chart showing the magnitude of influence.\n",
    "\n",
    "Insight Example: Length of Stay\n",
    "    - Insight: \"Patients with hospital stays >7 days are 2x more likely to be readmitted within 30 days.\"\n",
    "\n",
    "Visualization:\n",
    "    - Scatter plot: length_of_stay vs. readmissions.\n",
    "    - Box plot: Readmission rates by length-of-stay categories.\n",
    "\n",
    "4. Implementation Plan\n",
    "Once the board approves, focus on operationalizing findings:\n",
    "\n",
    "- Deploy targeted interventions for high-risk patients.\n",
    "- Set KPIs to monitor the effectiveness of changes.\n",
    "- Continuously refine the model based on new data.\n",
    "\n",
    "##### Set KPIs to monitor the effectiveness of changes\n",
    "\n",
    "**KPI 1: 30-Day Readmission Rate**\n",
    "- Definition: Percentage of patients readmitted to the hospital within 30 days of discharge.\n",
    "- Why Important: This is the primary metric to assess whether interventions are reducing readmissions.\n",
    "- Formula: $Readmission Rate = \\frac{Number of patients readmitted within 30 days}{Total number of discharged patients} × 100$\n",
    "- Target: A reduction in the readmission rate over time indicates success.\n",
    "\n",
    "**KPI 2: Medication Adherence Rate**\n",
    "- Definition: Percentage of patients adhering to their prescribed medications post-discharge.\n",
    "- Why Important: Non-adherence is a leading cause of readmissions. Monitoring this ensures interventions like counseling and follow-ups are effective\n",
    "- Formula: $Medication Adherence Rate = \\frac{Number of patients adhering to medications}{Total number of patients} × 100$\n",
    "- Target: An increase in adherence correlates with better outcomes and fewer readmissions.\n",
    "\n",
    "**KPI 3: Follow-Up Appointment Compliance**\n",
    "- Definition: Percentage of discharged patients attending follow-up appointments within the recommended time frame.\n",
    "- Why Important: Follow-up visits can identify issues early and prevent readmissions.\n",
    "- Formula: $Compliance Rate= \\frac{Number of scheduled follow-ups}{Number of attended follow-ups} × 100$\n",
    "- Target: High compliance indicates improved patient engagement.\n",
    "\n",
    "**KPI 4: Average Length of Stay (LOS)**\n",
    "- Definition: Average number of days patients spend in the hospital.\n",
    "- Why Important: Shorter stays can indicate efficiency but might increase readmissions if patients are discharged prematurely.\n",
    "- Formula: $LOS= \\frac{Number of discharges}{Total inpatient days}$\n",
    "​- Target: Maintain an optimal LOS that balances cost and readmission prevention.\n",
    "\n",
    "**KPI 5: Percentage of High-Risk Patients Identified**\n",
    "- Definition: Proportion of discharged patients flagged as high-risk for readmission and targeted for interventions.\n",
    "- Why Important: Monitoring ensures that predictive models and risk stratification tools are working effectively.\n",
    "- Formula:$High-Risk Patients Identified = \\frac{Total number of discharged patients}{Number of flagged high-risk patients} × 100$\n",
    "- Target: Increase the identification rate while reducing actual readmissions.\n",
    "\n",
    "##### Presenting KPIs to Stakeholders\n",
    "\n",
    "**Visual Presentation**\n",
    "\n",
    "Use dashboards and visualizations:\n",
    "- Bar charts to compare readmission rates before and after interventions.\n",
    "- Line graphs showing trends over time for medication adherence and follow-up compliance.\n",
    "- Heatmaps for condition-specific readmission trends.\n",
    "\n",
    "Narrative\n",
    "- Highlight success: \"We reduced the 30-day readmission rate from 18% to 12%, saving $500,000 annually.\"\n",
    "- Focus on actionable insights: \"Medication adherence programs have been effective, with a 15% increase in adherence leading to a 5% drop in readmissions.\"\n",
    "\n",
    "Recommendations\n",
    "- Continue monitoring these KPIs for sustained improvements.\n",
    "- Scale successful interventions to other patient groups or hospitals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f51a4",
   "metadata": {},
   "source": [
    "## 2. Multiple Linear Regression:\n",
    "\n",
    "simple linear regression equation is as follows:\n",
    "\n",
    "$$Y = \\beta_{0} + \\beta_{1}X_1$$\n",
    "\n",
    "where:\n",
    "- $\\beta_{0}$ is the intercept, interpreted as the value of $Y$ when $X_1 = 0$;\n",
    "- $\\beta_{1}$ is the coefficient, interpreted as the effect on $Y$ for a one unit increase in $X_1$; and\n",
    "- $X_1$ is the single predictor variable.\n",
    "\n",
    "Extending that idea to multiple linear regression is as simple as adding an $X_{j}$ and corresponding $\\beta_{j}$ for each of the $p$ predictor variables, where $j$ is an element of the set $[1,p]$.\n",
    "   \n",
    "Hence in multiple linear regression, our regression equation becomes:   \n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $Y$ is the reponse variable which depends on the $p$ predictor variables;\n",
    "- $\\beta_0$ is the intercept, interpreted as the value of $Y$ when _all_ predictor variables are equal to zero;\n",
    "- $\\beta_j$ is the average effect on $Y$ of a one unit increase in $X_j$, assuming all other predictors are held fixed.\n",
    "\n",
    "Multiple linear regression is a technique to understand the relationship between a single dependent variable and multiple independent variables.\n",
    "\n",
    "$$ 𝑦=𝛽_{0}+𝛽_{1}𝑥_{1}+…+𝛽_{𝑛}𝑥_{𝑛}+ 𝜖 $$\n",
    "\n",
    "What it means:\n",
    "- It is used when two or more independent variables influence the dependant variable. \n",
    "\n",
    "- A linear equation defines the relationship, with the \n",
    "    - coefficients of the independent variables \n",
    "    \n",
    "- representing the effect of each variable on the dependant variable.\n",
    "\n",
    "# Assumptions of Multiple Linear Regression\n",
    "\n",
    "Regression is a parametric approach, which means that it makes assumptions about the data\n",
    "\n",
    "For successful regression analysis, it’s essential to validate the following assumptions.\n",
    "\n",
    "- Overfitting: When more and more variables are added to a model, the model may become far too complex and usually ends up memorizing all the data points in the training set\n",
    "    - This phenomenon is known as the overfitting of a model. \n",
    "    - This usually leads to high training accuracy and very low test accuracy.\n",
    "- Understanding of linearity and multicollinearity (predictors).\n",
    "    - It is the phenomenon where a model with several independent variables, may have some variables interrelated.\n",
    "- Understanding of independence, homoscedasticity, and normality (residuals).\n",
    "- Feature Selection: With more variables present, selecting the optimal set of predictors from the pool of given features (many of which might be redundant) becomes an important task for building a relevant and better model.\n",
    "\n",
    "We'll be moving through the following sections in order to achieve our objectives:\n",
    "\n",
    "- Investigating our predictor variables:\n",
    "    - Checking for linearity;\n",
    "    - Checking for multicollinearity;\n",
    "- Fitting a model with `statsmodels.OLS`;\n",
    "- Evaluating our fitted model:\n",
    "    - Checking for independence;\n",
    "    - Checking for homoscedasticity;\n",
    "    - Checking for normaility;\n",
    "    - Checking for outliers.\n",
    "\n",
    "# Checking for Linearity\n",
    "\n",
    "Linearity is a key assumption in multilinear regression. It states that the relationship between each predictor and the response variable should be linear. When this assumption is violated, the model's predictions may be biased or less effective.\n",
    "\n",
    "The first thing we need to check is the mathematical relationship between each predictor variable and the response variable. == linearity. \n",
    "- A linear relationship means that a change in the response *Y* due to a one-unit change in the predictor $X_j$ is constant, regardless of the value of $X_j$.\n",
    "\n",
    "If we fit a regression model to a dataset that is non-linear, \n",
    "- it will fail to adequately capture the relationship in the data - resulting in a mathematically inappropriate model. \n",
    "\n",
    "#### Non-Linearity\n",
    "Issue:\n",
    "- Linear regression assumes a linear relationship between the independent variables (𝑋) and the dependent variable (Y). \n",
    "- If the true relationship is nonlinear, the model may provide poor predictions and incorrect interpretations.\n",
    "\n",
    "Detection:\n",
    "- Residual Plots: If the residuals (errors) show a curved or non-random pattern, non-linearity is likely present.\n",
    "- Scatterplots: Plot 𝑋 vs. Y to check for a linear relationship.\n",
    "- Polynomial or Log Transformations: Fit higher-order models and compare performance.\n",
    "\n",
    "Solutions:\n",
    "- Apply transformations (e.g., log, square root, Box-Cox) to make the relationship more linear.\n",
    "- Use polynomial regression (e.g., adding $𝑋^2 terms).\n",
    "- Consider using non-linear models such as decision trees or splines.\n",
    "\n",
    "### Detecting Non-Linearity\n",
    "\n",
    "To check for linearity, \n",
    "- we can produce scatter plots of each individual predictor against the response variable. \n",
    "- The intuition here is that we are looking for obvious linear relationships.\n",
    "\n",
    "**Result**\n",
    "\n",
    "- State what appears of the variables that have an approximately linear relationship.\n",
    "- State that exhibits no linearity with resonse variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,5, figsize=(14,6),)\n",
    "fig.subplots_adjust(hspace = 0.5, wspace=.2)\n",
    "axs = axs.ravel()\n",
    "\n",
    "for index, column in enumerate(df.columns):\n",
    "    axs[index-1].set_title(\"{} vs. mpg\".format(column),fontsize=16)\n",
    "    axs[index-1].scatter(x=df[column],y=df['mpg'],color='blue',edgecolor='k')\n",
    "    \n",
    "fig.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17497ab2",
   "metadata": {},
   "source": [
    "Step 1: Diagnosing Non-Linearity\n",
    "\n",
    "Visual Inspection\n",
    "- Use scatter plots to visualize the relationship between predictors and the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20442d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plots for each predictor vs. response\n",
    "for predictor in [\"SquareFootage\", \"Bedrooms\", \"DistanceFromCityCenter\"]:\n",
    "    sns.scatterplot(x=df[predictor], y=df[\"HousePrice\"])\n",
    "    plt.title(f\"{predictor} vs. HousePrice\")\n",
    "    plt.xlabel(predictor)\n",
    "    plt.ylabel(\"HousePrice\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot to visualize relationships\n",
    "sns.pairplot(df, x_vars=[\"SquareFootage\", \"Bedrooms\", \"DistanceFromCityCenter\"], y_vars=\"HousePrice\", kind=\"reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bb6195",
   "metadata": {},
   "source": [
    "Residual Plots\n",
    "- Residual plots help check for linearity by plotting residuals against predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85a50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted values and residuals\n",
    "predicted = model.predict(X)\n",
    "residuals = Y - predicted\n",
    "\n",
    "# Residual plot\n",
    "plt.scatter(predicted, residuals)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb68e22",
   "metadata": {},
   "source": [
    "If you see a pattern (e.g., curves or increasing spread), it indicates non-linearity.\n",
    "\n",
    "If the relationship between variables is non-linear Applying Log Transformation:\n",
    "\n",
    "Step 2: Transforming Predictors or the Response Variable\n",
    "\n",
    "When to Transform\n",
    "- Predictors: Transform when individual predictors have non-linear relationships with the response.\n",
    "- Response Variable: Transform when the response itself shows a skewed distribution or non-linear relationship with predictors.\n",
    "\n",
    "|Transformation |Formula|Use Case|\n",
    "|---------------|-------|--------|\n",
    "|Log            |$log(x)$|Skewed data, multiplicative relationships, exponential growth.\n",
    "|Square Root\t|$\\sqrt{x}$ |Reduces spread while preserving the order of values.|\n",
    "|Polynomial\t    |$𝑥^2, x^3,...$|For non-linear relationships that resemble curves.|\n",
    "|Reciprocal\t\t|$\\frac{1}{x}$|When values decrease rapidly as the predictor increases.|\n",
    "|Box-Cox\t\t|$y^{𝜆}$|Optimal transformation for normalizing data or reducing variance.|\n",
    "\n",
    "Step 3: Applying Transformations in Python\n",
    "\n",
    "Example 1: Log Transformation\n",
    "- Suppose SquareFootage has a non-linear relationship with HousePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc9623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformation of SquareFootage\n",
    "df[\"Log_SquareFootage\"] = np.log(df[\"SquareFootage\"])\n",
    "\n",
    "# Fit the model again\n",
    "X_trans = df[[\"Log_SquareFootage\", \"Bedrooms\", \"DistanceFromCityCenter\"]]\n",
    "X_trans = sm.add_constant(X_trans)\n",
    "model_trans = sm.OLS(Y, X_trans).fit()\n",
    "\n",
    "print(model_trans.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36275b0",
   "metadata": {},
   "source": [
    "Example 2: Polynomial Transformation\n",
    "- Suppose DistanceFromCityCenter has a curved relationship with HousePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2da387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polynomial terms\n",
    "df[\"Distance_Squared\"] = df[\"DistanceFromCityCenter\"] ** 2\n",
    "\n",
    "# Fit model with polynomial term\n",
    "X_poly = df[[\"SquareFootage\", \"Bedrooms\", \"DistanceFromCityCenter\", \"Distance_Squared\"]]\n",
    "X_poly = sm.add_constant(X_poly)\n",
    "model_poly = sm.OLS(Y, X_poly).fit()\n",
    "\n",
    "print(model_poly.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930bf477",
   "metadata": {},
   "source": [
    "Example 3: Box-Cox Transformation for Response Variable\n",
    "- Normalize HousePrice if it's highly skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12520314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "# Box-Cox transformation\n",
    "Y_boxcox, lambda_boxcox = boxcox(Y)\n",
    "print(f\"Optimal lambda for Box-Cox: {lambda_boxcox}\")\n",
    "\n",
    "# Fit model with transformed response\n",
    "model_boxcox = sm.OLS(Y_boxcox, X).fit()\n",
    "print(model_boxcox.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6fdfd7",
   "metadata": {},
   "source": [
    "Step 4: Comparing Models\n",
    "Use metrics like Adjusted $𝑅^2$ , AIC, and BIC to compare the effectiveness of models before and after transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a398e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "print(\"Original Model AIC:\", model.aic)\n",
    "print(\"Log-Transformed Model AIC:\", model_log.aic)\n",
    "print(\"Polynomial Model AIC:\", model_poly.aic)\n",
    "print(\"Box-Cox Model AIC:\", model_boxcox.aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148429d4",
   "metadata": {},
   "source": [
    "Step 5: Visualizing and Validating Improvements\n",
    "- Visualizing Residuals After Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9597d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plot after transformation\n",
    "predicted_trans = model_log.predict(X_trans)\n",
    "residuals_trans = Y - predicted_trans\n",
    "\n",
    "plt.scatter(predicted_trans, residuals_trans)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.title(\"Residual Plot After Transformation\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32374997",
   "metadata": {},
   "source": [
    "Checking $𝑅^2$ and Adjusted $𝑅^2$ Compare values before and after applying transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0a58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original Model R^2: {model.rsquared}\")\n",
    "print(f\"Transformed Model R^2: {model_log.rsquared}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d75827",
   "metadata": {},
   "source": [
    "# Checking for Multicollinearity\n",
    "\n",
    "Multicollinearity occurs when predictors in a regression model are highly correlated. This can inflate standard errors, making it difficult to assess the individual impact of predictors on the response variable.\n",
    "- As multicollinearity makes it difficult to find out which variable is contributing towards the prediction of the response variable, it leads one to conclude incorrectly, the effects of a variable on the target variable.\n",
    "- Properly detect and deal with the multicollinearity present in the model, as random removal of any of these correlated variables from the model causes the coefficient values to swing wildly and even change signs.\n",
    "\n",
    "Multicollinearity refers to the presence of strong correlation among two or more of the predictor variables in the dataset. The presence of any correlation among predictors is detrimental to model quality for two reasons:\n",
    "\n",
    "- It tends to increase the standard error;\n",
    "\n",
    "- It becomes difficult to estimate the effect of any one predictor variable on the response variable.\n",
    "\n",
    "We will check for multicollinearity by generating \n",
    "- pairwise scatter plots among predictors\n",
    "- a correlation heatmap.\n",
    "\n",
    "#### Collinearity (Multicollinearity)\n",
    "Issue:\n",
    "- Multicollinearity occurs when two or more independent variables are highly correlated. \n",
    "- This makes it difficult to isolate their individual effects, leading to unstable coefficients and inflated standard errors.\n",
    "\n",
    "Detection:\n",
    "- Variance Inflation Factor (VIF): VIF > 10 suggests high multicollinearity.\n",
    "- Correlation Matrix: High pairwise correlations (> 0.8) indicate potential collinearity.\n",
    "- Condition Index: A high condition number (> 30) suggests multicollinearity.\n",
    "\n",
    "Solutions:\n",
    "- Remove redundant predictors.\n",
    "- Use Principal Component Analysis (PCA), Ridge Regression, or Lasso Regression to handle collinearity.\n",
    "- Combine correlated variables into a single composite variable (e.g., sum or average).\n",
    "\n",
    "\n",
    "Multicollinearity can be detected using the following methods.\n",
    "\n",
    "- Pairwise Correlations: Checking the pairwise correlations between different pairs of independent variables can throw useful insights into detecting multicollinearity.\n",
    "    - Pairwise correlations may not always be useful as it is possible that just one variable might not be able to completely explain some other variable but some of the variables combined could be ready to do this.  Thus, to check these sorts of relations between variables, one can use VIF:\n",
    "- Variance Inflation Factor (VIF): VIF explains the relationship of one independent variable with all the other independent variables. \n",
    "    - VIF is given by,\n",
    "\n",
    "$ VIF = \\frac{1}{1 - R^2}$\n",
    "\n",
    "where \n",
    "- $i$ refers to the $ith$ variable which is being represented as a linear combination of the rest of the independent variables.\n",
    "\n",
    "Heuristics\n",
    "- if VIF > 10 then the value is high and it should be dropped.\n",
    "- if the VIF=5 then it may be valid but should be inspected first.\n",
    "- if VIF < 5, then it is considered a good VIF value.\n",
    "\n",
    "**Step 1: Detecting Multicollinearity**\n",
    "\n",
    "(a) Pairwise scatter plots\n",
    "\n",
    "As can be inferred by the name, a pairwise scatter plot simply produces a visual $n \\times n$ matrix, where $n$ is the total number of variables compared, in which each cell represents the relationship between two variables. The diagonal cells of this visual represent the comparison of a variable with itself, and as such are substituted by a representation of the distribution of values taken by the visual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c028bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the number of visuals created, this codeblock takes about one minute to run.\n",
    "from seaborn import pairplot\n",
    "g = pairplot(df1.drop('mpg', axis='columns'))\n",
    "g.fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b9774",
   "metadata": {},
   "source": [
    "(b) Correlation Matrix\n",
    "- Use a correlation matrix to identify highly correlated predictors.\n",
    "\n",
    "Correlation heatmap\n",
    "\n",
    "Another way we can visually discover linearity between two or more variables within our dataset is through the use of a correlation heatmap. Similar to the pairwise scatter plot we produced above, this visual presents a matrix in which each row represents a distinct variable, with each colum representing the correlation between this variable and another one within the dataset.\n",
    "\n",
    "Result Interpretation\n",
    "- Look for correlations > 0.8 or < -0.8, which may indicate multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only compare the predictor variables, and thus drop the target `mpg` column.\n",
    "corr = df1.drop('mpg', axis='columns').corr()\n",
    "\n",
    "from statsmodels.graphics.correlation import plot_corr\n",
    "\n",
    "fig=plot_corr(corr,xnames=corr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = df[[\"SquareFootage\", \"Bedrooms\", \"DistanceFromCityCenter\"]].corr()\n",
    "\n",
    "# Display the heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166af992",
   "metadata": {},
   "source": [
    "(c) Variance Inflation Factor (VIF)\n",
    "- VIF quantifies how much the variance of a regression coefficient is inflated due to multicollinearity.\n",
    "\n",
    "Result Interpretation\n",
    "\n",
    "Rule of Thumb:\n",
    "- $VIF=1$: No multicollinearity.\n",
    "- $1<VIF<5$: Low multicollinearity.\n",
    "- $VIF>5$: High multicollinearity.\n",
    "- $VIF>10$: Severe multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac61546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Calculate VIF for each predictor\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7466f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Prepare data for VIF calculation\n",
    "X_vif = df[[\"SquareFootage\", \"Bedrooms\", \"DistanceFromCityCenter\"]]\n",
    "X_vif = sm.add_constant(X_vif)\n",
    "\n",
    "# Calculate VIF for each predictor\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed731934",
   "metadata": {},
   "source": [
    "**Step 2: Mitigating Multicollinearity**\n",
    "\n",
    "1. Drop Highly Correlated Predictors:\n",
    "- If two predictors are highly correlated, remove one to reduce redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876fcbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = X.drop(columns=[\"Bedrooms\"])  # Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad41427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Bedrooms' if it has high multicollinearity\n",
    "X_reduced = df[[\"SquareFootage\", \"DistanceFromCityCenter\"]]\n",
    "X_reduced = sm.add_constant(X_reduced)\n",
    "\n",
    "# Fit the model with reduced predictors\n",
    "model_reduced = sm.OLS(Y, X_reduced).fit()\n",
    "print(model_reduced.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819bb5c",
   "metadata": {},
   "source": [
    "2. Apply Ridge or Lasso Regression:\n",
    "\n",
    "- Ridge regression penalizes large coefficients to handle multicollinearity.\n",
    "- Lasso regression performs feature selection by shrinking some coefficients to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be237178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "ridge = Ridge(alpha=1.0)  # Regularization strength\n",
    "ridge.fit(X, Y)\n",
    "print(\"Ridge coefficients:\", ridge.coef_)\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, Y)\n",
    "print(\"Lasso coefficients:\", lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9fb022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ridge regression\n",
    "ridge = Ridge(alpha=1.0)  # Adjust alpha (regularization strength) as needed\n",
    "ridge.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate Ridge model\n",
    "ridge_predictions = ridge.predict(X_test)\n",
    "ridge_mse = mean_squared_error(Y_test, ridge_predictions)\n",
    "print(\"Ridge Regression MSE:\", ridge_mse)\n",
    "print(\"Ridge Coefficients:\", ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea2eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Lasso regression\n",
    "lasso = Lasso(alpha=0.1)  # Adjust alpha as needed\n",
    "lasso.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate Lasso model\n",
    "lasso_predictions = lasso.predict(X_test)\n",
    "lasso_mse = mean_squared_error(Y_test, lasso_predictions)\n",
    "print(\"Lasso Regression MSE:\", lasso_mse)\n",
    "print(\"Lasso Coefficients:\", lasso.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d20559",
   "metadata": {},
   "source": [
    "3. Principal Component Analysis (PCA):\n",
    "- PCA reduces dimensions by transforming correlated predictors into uncorrelated components.\n",
    "\n",
    "Interpreting PCA:\n",
    "- Principal components represent uncorrelated combinations of the original predictors.\n",
    "- The explained variance ratio tells you how much variance is captured by each componen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dd8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)  # Reduce dimensions\n",
    "X_pca = pca.fit_transform(X.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c0f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Scale predictors for PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X.iloc[:, 1:])  # Exclude constant term\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Choose number of components\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Fit model with PCA components\n",
    "model_pca = sm.OLS(Y, sm.add_constant(X_pca)).fit()\n",
    "print(model_pca.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a55d84",
   "metadata": {},
   "source": [
    "**Step 3: Comparing Models**\n",
    "\n",
    "Evaluate model performance before and after applying mitigation techniques using metrics such as:\n",
    "\n",
    "- Mean Squared Error (MSE)\n",
    "- Adjusted $𝑅^2$ \n",
    "- Akaike Information Criterion (AIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18bdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model AIC\n",
    "print(\"Original Model AIC:\", model.aic)\n",
    "print(\"Reduced Model AIC:\", model_reduced.aic)\n",
    "print(\"Ridge Model MSE:\", ridge_mse)\n",
    "print(\"Lasso Model MSE:\", lasso_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16da439",
   "metadata": {},
   "source": [
    "# Outliers in Data points\n",
    "Issue:\n",
    "- Outliers are extreme data points that can disproportionately affect regression estimates, leading to biased coefficients and distorted predictions.\n",
    "\n",
    "Detection:\n",
    "- Boxplots and scatterplots: Identify extreme values.\n",
    "- Standardized residuals: If residuals exceed ±3 standard deviations, they may be outliers.\n",
    "- Cook’s Distance: Values > 0.5–1 indicate influential points.\n",
    "\n",
    "Solutions:\n",
    "- Investigate the cause: Check for data entry errors.\n",
    "- Transform variables (log transformation can reduce outlier influence).\n",
    "- Use robust regression techniques (e.g., RANSAC, Huber regression).\n",
    "- Remove extreme outliers only if they are genuine data errors.\n",
    "\n",
    "### Impact of Outliers on Different Machine Learning Models\n",
    "Outliers can significantly impact certain models while others are more robust. Here's a breakdown of models that are least and most affected by outliers:\n",
    "\n",
    "##### **Most Affected Models (Sensitive to Outliers)**\n",
    "These models assume normally distributed data or use distance-based calculations, making them highly sensitive to extreme values.\n",
    "\n",
    "**Linear Regression**\n",
    "- Uses least squares optimization, which heavily penalizes outliers.\n",
    "- Outliers can skew the regression line, leading to biased coefficients.\n",
    "- Solution: Use Robust Regression or Log Transform data.\n",
    "\n",
    "**Logistic Regression**\n",
    "- While more resistant than linear regression, extreme values in features can affect decision boundaries.\n",
    "- Solution: Use Regularization (L1/L2 penalty) or Winsorization.\n",
    "\n",
    "**K-Nearest Neighbors (KNN)**\n",
    "- Distance-based, so outliers can pull the nearest neighbors incorrectly.\n",
    "- Solution: Use scaled data (StandardScaler) or Minkowski distance with low sensitivity.\n",
    "\n",
    "**Support Vector Machines (SVM)**\n",
    "- Tries to maximize the margin between classes, but outliers can change the margin dramatically.\n",
    "- Solution: Use soft-margin SVM (C parameter tuning).\n",
    "\n",
    "**Principal Component Analysis (PCA)**\n",
    "- Based on variance maximization, meaning outliers distort the principal components.\n",
    "- Solution: Use Robust PCA or detect outliers before applying PCA.\n",
    "\n",
    "**Neural Networks (Deep Learning Models)**\n",
    "- Can handle outliers if trained with enough data, but in small datasets, outliers cause unstable learning.\n",
    "- Solution: Use batch normalization or dropout layers.\n",
    "\n",
    "##### **Least Affected Models (Robust to Outliers)**\n",
    "These models ignore outliers naturally or are designed to handle them effectively.\n",
    "\n",
    "**Tree-Based Models (Decision Trees, Random Forest, XGBoost, LightGBM)**\n",
    "- Trees split data based on conditions, making them naturally robust to outliers.\n",
    "- Extreme values do not impact splits significantly.\n",
    "- Solution: No need for strict outlier handling.\n",
    "\n",
    "**Robust Regression (Huber Regression, Theil-Sen Estimator, RANSAC Regression)**\n",
    "- Modifies loss function to reduce the effect of large errors (outliers).\n",
    "- Best choice for handling outliers in regression.\n",
    "\n",
    "**Gradient Boosting Models (XGBoost, CatBoost, LightGBM)**\n",
    "- Uses tree-based structures, so outliers do not heavily impact performance.\n",
    "- Solution: Still benefits from log transformation if outliers exist.\n",
    "\n",
    "**K-Means with Outlier Detection (DBSCAN, K-Medoids, Gaussian Mixture Model)**\n",
    "- Traditional K-Means is affected by outliers, but DBSCAN and K-Medoids are robust.\n",
    "- Solution: Use DBSCAN instead of K-Means.\n",
    "\n",
    "Key Takeaways\n",
    "- Use Tree-Based Models (Random Forest, XGBoost, LightGBM) to avoid outlier issues.\n",
    "- For regression, use Robust Regression (Huber, Theil-Sen, RANSAC).\n",
    "- Use outlier-resistant clustering like DBSCAN instead of K-Means.\n",
    "- Preprocess data (log transform, Winsorization, trimming) for models sensitive to outliers.\n",
    "\n",
    "#### Outlier Detection & Handling in Python\n",
    "This script covers:\n",
    "- ✅ Identifying outliers using IQR, Z-score, and Isolation Forest\n",
    "- ✅ Handling outliers with Winsorization, transformation, and robust regression\n",
    "- ✅ Applying outlier-resistant models\n",
    "\n",
    "🔹 Step 1: Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a3b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Generate data with outliers\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(loc=50, scale=15, size=100).tolist()  # Normal data\n",
    "data += [150, 170, 200]  # Add extreme outliers\n",
    "df = pd.DataFrame({'Value': data})\n",
    "\n",
    "# Visualize outliers\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x=df['Value'])\n",
    "plt.title('Boxplot Showing Outliers')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e253d",
   "metadata": {},
   "source": [
    "🔹 Step 2: Detect Outliers\n",
    "\n",
    "1️⃣ IQR Method (Boxplot Approach)\n",
    "\n",
    "2️⃣ Z-Score Method\n",
    "\n",
    "3️⃣ Isolation Forest (Machine Learning Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e123ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "Q1 = df['Value'].quantile(0.25)\n",
    "Q3 = df['Value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df['Outlier_IQR'] = (df['Value'] < lower_bound) | (df['Value'] > upper_bound)\n",
    "print(df[df['Outlier_IQR']])\n",
    "\n",
    "# 2\n",
    "\n",
    "df['Z_Score'] = np.abs(stats.zscore(df['Value']))\n",
    "df['Outlier_Z'] = df['Z_Score'] > 3\n",
    "print(df[df['Outlier_Z']])\n",
    "\n",
    "# 3\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "df['Outlier_IF'] = iso_forest.fit_predict(df[['Value']])\n",
    "df['Outlier_IF'] = df['Outlier_IF'] == -1  # Convert -1 (outliers) to True\n",
    "print(df[df['Outlier_IF']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57570fd",
   "metadata": {},
   "source": [
    "🔹 Step 3: Handling Outliers\n",
    "\n",
    "1️⃣ Winsorization (Capping Outliers)\n",
    "\n",
    "2️⃣ Log Transformation (For Right-Skewed Data)\n",
    "\n",
    "3️⃣ Robust Regression (RANSAC for Outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2226b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 \n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "df['Value_Winsorized'] = winsorize(df['Value'], limits=[0.05, 0.05])  # Cap top/bottom 5%\n",
    "sns.boxplot(x=df['Value_Winsorized'])\n",
    "plt.title('Winsorized Data')\n",
    "plt.show()\n",
    "\n",
    "# 2 \n",
    "df['Value_Log'] = np.log(df['Value'].clip(lower=1))  # Avoid log(0)\n",
    "sns.histplot(df['Value_Log'], kde=True)\n",
    "plt.title('Log Transformed Data')\n",
    "plt.show()\n",
    "\n",
    "# 3\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "X = np.array(range(len(df))).reshape(-1, 1)  # Dummy feature\n",
    "y = df['Value']\n",
    "\n",
    "ransac = RANSACRegressor()\n",
    "ransac.fit(X, y)\n",
    "y_pred = ransac.predict(X)\n",
    "\n",
    "plt.scatter(X, y, label=\"Original Data\")\n",
    "plt.plot(X, y_pred, color='red', label=\"Robust Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc55c45",
   "metadata": {},
   "source": [
    "🔹 Step 4: Using Outlier-Resistant Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bbba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create dummy features\n",
    "df['Feature'] = np.random.normal(loc=10, scale=5, size=len(df))\n",
    "\n",
    "X = df[['Feature']]\n",
    "y = df['Value']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use Random Forest (Robust to Outliers)\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Model Trained Successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d860b93f",
   "metadata": {},
   "source": [
    "# High-Leverage Points\n",
    "High Leverage Points and the Leverage Statistic\n",
    "\n",
    "### What Are High Leverage Points?\n",
    "- High leverage points are observations with extreme predictor values (X-values) compared to the rest of the data.\n",
    "- They have a strong influence on the fitted regression model because they affect the estimated regression coefficients.\n",
    "- A high leverage point may or may not be an outlier in terms of Y (dependent variable), but it is extreme in X.\n",
    "\n",
    "Why Are High Leverage Points Important?\n",
    "- They can distort regression results, leading to biased coefficients.\n",
    "- If a high leverage point is also an outlier in Y, it has an outsized impact on model predictions.\n",
    "- Identifying and addressing high leverage points improves model robustness.\n",
    "\n",
    "Issue:\n",
    "- High-leverage points are observations with extreme values for predictor variables (𝑋). \n",
    "- They can unduly influence the regression model, even if they are not outliers in 𝑌.\n",
    "\n",
    "### Detecting High Leverage Points Using the Leverage Statistic\n",
    "The leverage of an observation i is measured by the hat value $ℎ_𝑖$, derived from the hat matrix 𝐻\n",
    "- helps detect high leverage points.\n",
    "$$ H = X(X^T X)^{-1} X^T$$\n",
    "\n",
    "Each observation has a leverage score:\n",
    "$$ h_i = X_i(X^T X)^{-1} X^T_i$$\n",
    "- Where:\n",
    "    - $X_i$ is the row vector of predictor values for observation i.\n",
    "    - $ℎ_𝑖$ measures how far $𝑋_𝑖$ is from the mean of all predictor values.\n",
    "\n",
    "Threshold for High Leverage\n",
    "\n",
    "High leverage points have predictor values that deviate significantly from the rest of the data.\n",
    "A common rule: $h_i > \\frac{2(k+1)}{n}$\n",
    "- Where:\n",
    "    - k = number of predictors,\n",
    "    - n = number of observations.\n",
    "\n",
    "High leverage points should be carefully examined, not automatically removed.\n",
    "\n",
    "Detection:\n",
    "- Leverage statistic (Hat values): High-leverage points typically have a hat value $h_i \\geq \\frac{2(p+1)}{n}$,  \n",
    "    - where \n",
    "        - p is the number of predictors and \n",
    "        - n is the sample size.\n",
    "- Cook’s Distance and DFFITS: Identify points with excessive influence.\n",
    "\n",
    "Implementing High Leverage Point Detection in Python\n",
    "- Fit a linear regression model.\n",
    "- Compute leverage scores.\n",
    "- Identify high leverage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded4193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.normal(10, 2, 100)  # Normally distributed predictor\n",
    "Y = 3 * X + np.random.normal(0, 3, 100)  # Linear relationship with noise\n",
    "\n",
    "# Introduce high leverage points\n",
    "X[95:] = X[95:] + 20  # Shift last 5 values far from the rest\n",
    "\n",
    "# Fit regression model\n",
    "X_const = sm.add_constant(X)  # Add intercept\n",
    "model = sm.OLS(Y, X_const).fit()\n",
    "\n",
    "# Compute leverage scores (hat values)\n",
    "influence = model.get_influence()\n",
    "leverage = influence.hat_matrix_diag  # Extract leverage values\n",
    "\n",
    "# Define threshold for high leverage\n",
    "k = X_const.shape[1] - 1  # Number of predictors\n",
    "n = len(X)\n",
    "threshold = 2 * (k + 1) / n\n",
    "\n",
    "# Identify high leverage points\n",
    "high_leverage_points = np.where(leverage > threshold)[0]\n",
    "\n",
    "# Plot leverage scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(range(n), leverage, label=\"Leverage Values\", alpha=0.7)\n",
    "plt.axhline(y=threshold, color='red', linestyle='dashed', label=\"Threshold\")\n",
    "plt.scatter(high_leverage_points, leverage[high_leverage_points], color='red', label=\"High Leverage Points\", s=100)\n",
    "plt.xlabel(\"Observation Index\")\n",
    "plt.ylabel(\"Leverage\")\n",
    "plt.title(\"Leverage Values and High Leverage Points\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print high leverage points\n",
    "print(\"High Leverage Points:\", high_leverage_points)\n",
    "print(\"Leverage Values of High Leverage Points:\", leverage[high_leverage_points])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21a25f",
   "metadata": {},
   "source": [
    "### How to Handle High Leverage Points?\n",
    "Solutions:\n",
    "- Assess model stability by running regression with and without the high-leverage points.\n",
    "\n",
    "✅ If they are valid observations:\n",
    "- Consider using robust regression (e.g., Ridge regression).\n",
    "    - Use robust regression methods that downweight influential points.\n",
    "- Verify if the observation provides valuable insight.\n",
    "\n",
    "❌ If they are errors or extreme influential points:\n",
    "- Investigate the cause.\n",
    "- Consider removing or transforming the data.\n",
    "    - Add interaction terms or transformations if high-leverage points reveal missing relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383bb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ba46285",
   "metadata": {},
   "source": [
    "# Overfitting and Underfitting in Linear Regression\n",
    "\n",
    "### Analyzing the Relationship Between Model Error and Model Complexity\n",
    "The relationship between model complexity and model error follows a well-known pattern often illustrated by the bias-variance tradeoff. As model complexity increases, error initially decreases but eventually rises again due to overfitting.\n",
    "\n",
    "##### **Types of Model Error**\n",
    "When model performs well on training data but not on the test data.\n",
    "\n",
    "Model error consists of two main components:\n",
    "\n",
    "**Bias** (Underfitting)\n",
    "\n",
    "Bias is a measure to determine how accurate a model’s predictions are likely to be on future unseen data.\n",
    "- Bias is errors made by training data.\n",
    "    - Represents error due to overly simplistic assumptions.\n",
    "        - Complex models, assuming there is enough training data available, can make accurate model predictions. \n",
    "        - Models that are too naive, are very likely to perform badly concerning model predictions.\n",
    "    - High-bias models do not learn enough patterns from the training data.\n",
    "- Linear algorithms have a high bias which makes them fast to learn and easier to understand but in general, are less flexible. \n",
    "    - Implying lower predictive performance on complex problems that fail to meet the expected outcomes.\n",
    "\n",
    "Bias\n",
    "\n",
    "📌 Definition: Bias refers to the error introduced by over-simplifying a machine learning model. A model with high bias makes strong assumptions about the data, leading to underfitting (poor performance on both training and test data).\n",
    "\n",
    "🔹 Characteristics of High-Bias Models:\n",
    "- Oversimplifies relationships in the data.\n",
    "- Misses key patterns, leading to high training and test error.\n",
    "- Performs poorly even on training data.\n",
    "\n",
    "🔹 Example:\n",
    "- Using linear regression to model a nonlinear relationship leads to high bias.\n",
    "- A decision tree with depth = 1 (stump) cannot capture complex patterns.\n",
    "\n",
    "**Variance** (Overfitting)\n",
    "\n",
    "Variance is the sensitivity of the model towards training data\n",
    "- it quantifies how much the model will react when input data is changed.\n",
    "    - Represents sensitivity to small fluctuations in the training data.\n",
    "    - model shouldn’t change too much from one training dataset to the next training data \n",
    "        - Whcih means that the algorithm is good at picking out the hidden underlying patterns between the inputs and the output variables.\n",
    "    - model should have lower variance which means that the model doesn’t change drastically after changing the training data(it is generalizable). \n",
    "        - Having higher variance will make a model change drastically even on a small change in the training dataset.\n",
    "    - High-variance models learn noise along with actual patterns.\n",
    "\n",
    "Variance\n",
    "\n",
    "📌 Definition: Variance refers to the error introduced by a model’s sensitivity to small variations in the training data. A model with high variance is too complex, capturing noise along with the actual patterns, leading to overfitting (low training error but high test error).\n",
    "\n",
    "🔹 Characteristics of High-Variance Models:\n",
    "- Memorizes training data, leading to low training error.\n",
    "- Performs poorly on test data due to lack of generalization.\n",
    "- Sensitive to small changes in the dataset.\n",
    "\n",
    "🔹 Example:\n",
    "- A deep decision tree (very high depth) memorizes training data but fails on test data.\n",
    "- Polynomial regression of very high degree overfits noise in the data.\n",
    "\n",
    "**Irreducible Error**\n",
    "\n",
    "This is noise inherent in the data that no model can eliminate.\n",
    "- Example: Measurement errors, random fluctuations in real-world processes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d0fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "### Python Example: High Bias in Linear Regression\n",
    "\n",
    "# Generate a non-linear dataset\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y = X**2 + np.random.normal(0, 1, X.shape)\n",
    "\n",
    "# Fit a linear regression model (high bias)\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Plot results\n",
    "plt.scatter(X, y, label=\"True Data\")\n",
    "plt.plot(X, y_pred, color=\"red\", label=\"Linear Model (High Bias)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "### Python Example: High Variance in Overfitting\n",
    "\n",
    "# Fit a high-degree polynomial model (high variance)\n",
    "poly = PolynomialFeatures(degree=10)\n",
    "X_poly = poly.fit_transform(X)\n",
    "high_var_model = LinearRegression()\n",
    "high_var_model.fit(X_poly, y)\n",
    "y_pred_high_var = high_var_model.predict(X_poly)\n",
    "\n",
    "# Plot results\n",
    "plt.scatter(X, y, label=\"True Data\")\n",
    "plt.plot(X, y_pred_high_var, color=\"red\", label=\"High Variance Model\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b240a7",
   "metadata": {},
   "source": [
    "##### **Model Complexity vs. Error Curve (Bias Variance Tradeoff)**\n",
    "The relationship follows a U-shaped curve:\n",
    "\n",
    "A supervised machine learning algorithm seeks to strike a balance between low bias and low variance for increased robustness.\n",
    "\n",
    "The relationship between bias and variance is characterized by an inverse correlation.\n",
    "- Increased bias leads to reduced variance.\n",
    "- Conversely, heightened variance results in diminished bias.\n",
    "Finding an equilibrium between bias and variance is crucial, and algorithms must navigate this trade-off for optimal outcomes.\n",
    "\n",
    "**Overfitting**\n",
    "\n",
    "When a model learns every pattern and noise in the data to such an extent that it affects the performance of the model on the unseen future dataset.\n",
    "- model fits the data so well that it interprets noise as patterns in the data.\n",
    "\n",
    "Caused when a model has **low bias and higher variance** it ends up memorizing the data.\n",
    "\n",
    "Overfitting causes the model to become specific rather than generic.\n",
    "- Model captures noise in training data. \n",
    "\n",
    "This usually leads to: \n",
    "- high training accuracy and very low test accuracy.\n",
    "    - Training error is low, but test error increases.\n",
    "\n",
    "There are several ways to prevent overfitting:\n",
    "- Cross-validation\n",
    "- If the training data is too small to train add more relevant and clean data.\n",
    "- If the training data is too large, do some feature selection and remove unnecessary features.\n",
    "- Regularization\n",
    "\n",
    "**Underfitting**\n",
    "\n",
    "When the model fails to learn from the training dataset and is also not able to generalize the test dataset.\n",
    "- Detected by the performance metrics.\n",
    "\n",
    "When a model has **high bias and low variance** it ends up not generalizing the data and causing underfitting.\n",
    "- Model is too simple and fails to capture patterns.\n",
    "- It is unable to find the hidden underlying patterns in the data. \n",
    "- This usually leads to low training accuracy and very low test accuracy.\n",
    "    - Training and test errors are both high.\n",
    "\n",
    "Ways to prevent underfitting:\n",
    "- Increase the model complexity\n",
    "- Increase the number of features in the training data\n",
    "- Remove noise from the data.\n",
    "\n",
    "Ways to Preventing Overfitting in Machine Learning\n",
    "\n",
    "Overfitting happens when a model learns not just the underlying patterns in the data but also the noise. This reduces its ability to generalize to new, unseen data. Below are effective techniques to prevent overfitting and improve model generalization.\n",
    "\n",
    "1. Train with More Data (If Possible)\n",
    "\n",
    "📌 Why? More data helps the model learn general patterns rather than noise.\n",
    "\n",
    "🔹 Example: In fraud detection, adding more transaction data reduces bias from rare cases.\n",
    "\n",
    "- How to check?\n",
    "    - If test error is much higher than training error, your model might be overfitting.\n",
    "    - Use learning curves (plot error vs. training size) to check if more data is needed.\n",
    "\n",
    "2. Cross-Validation (K-Fold CV)\n",
    "\n",
    "📌 Why? Ensures the model performs well on different subsets of the data.\n",
    "\n",
    "🔹 How? Instead of a single train-test split, K-Fold CV splits data into K groups and trains the model K times, each time using a different fold for validation.\n",
    "\n",
    "3. Regularization (L1, L2, Dropout)\n",
    "\n",
    "📌 Why? Regularization adds a penalty to overly complex models, reducing overfitting.\n",
    "\n",
    "- L1 (Lasso) and L2 (Ridge) Regularization for Regression\n",
    "    - L1 (Lasso): Shrinks some coefficients to zero, performing feature selection.\n",
    "    - L2 (Ridge): Reduces the magnitude of all coefficients, preventing extreme values.\n",
    "    - Elastic Net: Combines L1 and L2 regularization\n",
    "    - Dropout Regularization for Deep Learning\n",
    "        - 📌 Why? Prevents a neural network from relying too much on certain neurons by randomly \"dropping\" them during training.\n",
    "\n",
    "4. Feature Selection (Remove Irrelevant Features)\n",
    "\n",
    "📌 Why? Irrelevant or highly correlated features increase model complexity unnecessarily.\n",
    "\n",
    "🔹 How? Use techniques like correlation analysis, Lasso regression, or mutual information to select only the most important features.\n",
    "\n",
    "5. Early Stopping (For Deep Learning)\n",
    "\n",
    "📌 Why? Stops training when the validation error starts increasing, preventing overfitting.\n",
    "\n",
    "🔹 How? Monitor the validation loss and stop training when it stops improving.\n",
    "\n",
    "6. Data Augmentation (For Image Data)\n",
    "\n",
    "📌 Why? Increases dataset diversity, helping models generalize better.\n",
    "\n",
    "🔹 Example: In image classification, augmenting images by rotating, flipping, or changing brightness.\n",
    "\n",
    "7. Ensemble Methods (Bagging & Boosting)\n",
    "\n",
    "📌 Why? Combining multiple models reduces the impact of overfitting by balancing high variance and high bias.\n",
    "\n",
    "Bagging (Bootstrap Aggregating)\n",
    "- Uses random sampling to train multiple models and average their predictions.\n",
    "- Example: Random Forest (aggregates multiple decision trees).\n",
    "\n",
    "Boosting\n",
    "- Trains models sequentially, where each model corrects errors of the previous one.\n",
    "- Example: Gradient Boosting, XGBoost, LightGBM, AdaBoost.\n",
    "\n",
    "**Optimal Complexity:**\n",
    "- Balanced bias and variance.\n",
    "- Model generalizes well to unseen data.\n",
    "- Test error is minimized.\n",
    "\n",
    "The ideal model must balance bias and variance to minimize total error.\n",
    "\n",
    "| Model Complexity | Bias   | \tVariance       |\tGeneralization Error    |\n",
    "|---------------|--------------------|----------------------------------| ---------------------|   \n",
    "|Too Simple (e.g., Linear Regression for Nonlinear Data)|\tHigh |\tLow | High (Underfitting)  |\n",
    "|Too Complex (e.g., High-Depth Decision Tree)|\tLow |\tHigh | High (Overfitting)  |\n",
    "|Optimal Model (e.g., Regularized Model, Ensemble Learning)|\tModerate | Moderate |\tLow|\n",
    "\n",
    "Key Observations:\n",
    "- Increasing model complexity reduces bias but increases variance.\n",
    "- Simplifying a model reduces variance but increases bias.\n",
    "- The sweet spot balances both to achieve low generalization error.\n",
    "\n",
    "###  Strategies to Balance Bias and Variance\n",
    "\n",
    "| Strategy  | Reduces Bias?  | \tReduces Variance?      |\tBest for    |\n",
    "|---------------|--------------------|----------------------------------| ---------------------|   \n",
    "| Regularization (L1, L2, Dropout)  |\t❌ |\t✅ | Overfitting (High Variance) |\n",
    "| Ensemble Methods (Bagging & Boosting)|\t❌ |\t✅ | Overfitting (High Variance) |\n",
    "| Feature Selection (Remove Irrelevant Features)|\t❌ |\t✅ | Overfitting (High Variance) |\n",
    "| Training Data Increased | \t✅ |\t✅ | Generalization Improvement |\n",
    "| Cross-Validation (K-Fold CV) | \t✅ |\t✅ | Model Evaluation |\n",
    "| Simpler Model (Fewer Features, Lower Degree)|\t\t✅ |\t❌ | Underfitting (High Bias) |\n",
    "\n",
    "##### Bias-Variance Tradeoff in Polynomial Regression\n",
    "\n",
    "This example demonstrates how increasing model complexity affects error.\n",
    "\n",
    "Key Observations from the Code\n",
    "- Low-degree polynomial models (e.g., degree=1) → High training and test errors (underfitting).\n",
    "- Medium-degree models (e.g., degree=3 to 5) → Low test error (optimal complexity).\n",
    "- High-degree models (e.g., degree=10) → Very low training error but high test error (overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db55cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic dataset\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = np.sin(X).ravel() + np.random.normal(scale=0.3, size=X.shape[0])\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test = X[:80], X[80:]\n",
    "y_train, y_test = y[:80], y[80:]\n",
    "\n",
    "# Train models with different complexity levels\n",
    "degrees = [1, 3, 5, 10]  # Increasing complexity\n",
    "train_errors, test_errors = [], []\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for d in degrees:\n",
    "    model = make_pipeline(PolynomialFeatures(d), LinearRegression())\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate errors\n",
    "    train_error = mean_squared_error(y_train, y_train_pred)\n",
    "    test_error = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    train_errors.append(train_error)\n",
    "    test_errors.append(test_error)\n",
    "\n",
    "    # Plot model fits\n",
    "    plt.scatter(X_test, y_test, color=\"black\", label=\"True data\" if d == 1 else \"\")\n",
    "    plt.plot(X_test, y_test_pred, label=f\"Degree {d}\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Effect of Model Complexity on Fit\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot error vs complexity\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(degrees, train_errors, label=\"Training Error\", marker=\"o\")\n",
    "plt.plot(degrees, test_errors, label=\"Test Error\", marker=\"o\")\n",
    "plt.xlabel(\"Model Complexity (Polynomial Degree)\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Bias-Variance Tradeoff\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4853faca",
   "metadata": {},
   "source": [
    "##### How to Control Model Complexity?\n",
    "To find the right balance:\n",
    "- Use cross-validation to estimate test error.\n",
    "- Apply regularization techniques (Lasso, Ridge, Dropout for deep learning).\n",
    "- Compare simpler vs. more complex models to select the best fit.\n",
    "\n",
    "##### Techniques to Prevent Overfitting\n",
    "\n",
    "| Technique | How It Helps   | Best Use Case           |\n",
    "|---------------|--------------------|----------------------------------|  \n",
    "|More Data\t|Reduces variance by learning general patterns |\tWhen data is limited|\n",
    "|Cross-Validation\t| Ensures model is robust to unseen data | Model selection. |\n",
    "|Regularization\t|Penalizes complexity (Lasso, Ridge, Dropout). |\tRegression & Deep Learning.|\n",
    "|Feature Selection\t|Removes irrelevant or redundant features |\tHigh-dimensional data|\n",
    "|Early Stopping\t| Stops training before overfitting occurs | Deep learning. |\n",
    "|Data Augmentation\t|Creates synthetic data to increase diversity |\tImage processing|\n",
    "|Ensemble Methods|Combines multiple models to reduce variance |Tree-based models|\n",
    "\n",
    "\n",
    "Python Example: K-Fold Cross-Validation\n",
    "\n",
    "🔹 Result: A more reliable performance estimate compared to a single train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80c1188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate sample dataset\n",
    "X, y = make_regression(n_samples=500, n_features=5, noise=10, random_state=42)\n",
    "\n",
    "# Train model with 5-Fold Cross-Validation\n",
    "model = LinearRegression()\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean CV Score:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf69241",
   "metadata": {},
   "source": [
    "Python Example: Ridge and Lasso Regression\n",
    "\n",
    "🔹 Effect: Helps reduce model complexity while maintaining good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3dc626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train models with regularization\n",
    "ridge = Ridge(alpha=1.0).fit(X_train, y_train)\n",
    "lasso = Lasso(alpha=0.1).fit(X_train, y_train)\n",
    "\n",
    "print(\"Ridge Coefficients:\", ridge.coef_)\n",
    "print(\"Lasso Coefficients:\", lasso.coef_)  # Some coefficients will be zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f47994",
   "metadata": {},
   "source": [
    "Dropout Regularization for Deep Learning\n",
    "\n",
    "🔹 Effect: Reduces overfitting by forcing the model to learn redundant representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee00f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Define a simple neural network with dropout\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(10,)),\n",
    "    Dropout(0.5),  # Drops 50% of neurons randomly\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef5611a",
   "metadata": {},
   "source": [
    "Python Example: Feature Importance with Random Forest\n",
    "\n",
    "🔹 Effect: Helps in eliminating less useful features, improving generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6250992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "\n",
    "# Train Random Forest model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = pd.Series(model.feature_importances_, index=[\"Feature1\", \"Feature2\", \"Feature3\", \"Feature4\", \"Feature5\"])\n",
    "print(feature_importances.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ed314",
   "metadata": {},
   "source": [
    "Python Example: Early Stopping in Neural Networks\n",
    "\n",
    "🔹 Effect: Saves computational time and improves generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96135ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216a1be",
   "metadata": {},
   "source": [
    "Python Example: Image Augmentation with Keras\n",
    "\n",
    "🔹 Effect: Reduces overfitting by exposing the model to different variations of the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ebbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Augment training data\n",
    "augmented_images = datagen.flow(training_images, training_labels, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b87a14",
   "metadata": {},
   "source": [
    "Python Example: Random Forest vs. Gradient Boosting\n",
    "\n",
    "🔹 Effect: Reduces variance and improves stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f85483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100).fit(X_train, y_train)\n",
    "gb = GradientBoostingRegressor(n_estimators=100).fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest Test Score:\", rf.score(X_test, y_test))\n",
    "print(\"Gradient Boosting Test Score:\", gb.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bd70ae",
   "metadata": {},
   "source": [
    "### Fitting the model using `statsmodels.OLS`\n",
    "\n",
    "`sklearn` is limited in terms of metrics and tools available to evaluate the appropriateness of the regression models we fit.\n",
    "-As a means to expland our analysis, we import the `statsmodels` library which has a rich set of statistical tools to help us. \n",
    "\n",
    "##### Generating the regression string\n",
    "\n",
    "Those of you familiar with the R language will know that fitting a machine learning model requires a sort of string of the form:\n",
    "\n",
    "`y ~ X`\n",
    "\n",
    "which is read as follows: \"Regress y on X\". The `statsmodels` library works in a similar way, so we need to generate an appropriate string to feed to the method when we wish to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de013b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48989e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress target variable on all of the predictors.\n",
    "formula_str = df.columns[0]+' ~ '+'+'.join(df.columns[1:]); formula_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing seaborn library for visualizations\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# To plot all the scatterplots in a single plot\n",
    "sns.pairplot(df, x_vars=[ 'TV', ' Newspaper','Radio' ], y_vars = 'Sales', size = 4, kind = 'scatter' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f890be",
   "metadata": {},
   "source": [
    "##### Plotting 3D plot for multiple Linear regression\n",
    "\n",
    "To get a better idea of what a multi-dimensional dataset looks like, we'll generate a 3D scatter plot showing the `mpg` on the _z_-axis (height), with two predictor variables, `cyl` and `disp` on the _x_- and _y_-axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and 3d axes\n",
    "fig = plt.figure(figsize=(8,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# set axis labels\n",
    "ax.set_zlabel('MPG')\n",
    "ax.set_xlabel('No. of Cylinders')\n",
    "ax.set_ylabel('Weight (1000 lbs)')\n",
    "\n",
    "# scatter plot with response variable and 2 predictors\n",
    "ax.scatter(df['cyl'], df['wt'], df['mpg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e9a14",
   "metadata": {},
   "source": [
    "We know that in simple linear regression (2D), any model that we fit to data manifests in the form of a straight line. Extending this idea to 3D, the line becomes a plane - a flat surface which is chosen to minimise the squared vertical distances between each observation (red dots), and the plane, as shown in the figure below from ISLR.\n",
    "\n",
    "<img src=\"https://github.com/Explore-AI/Public-Data/raw/master/3D%20regression%20ISLR.jpg\" alt=\"plane\" style=\"width: 450px\"/>\n",
    "\n",
    "The result of a multivariate linear regression in higher dimensionality is known as a _hyperplane_ - similar to the flat surface in the figure above, but in a _p_-dimensional space, where $p>3$. Unfortunately, humans lack the ability to visualise any number of dimensions greater than three - so we have to be content with the idea that a hyperplane in _p_-dimensional space is effectively like a flat surface in 3-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e91c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot heatmap to find out correlations\n",
    "sns.heamap(df.corr(), cmap = 'YlGnBl', annot = True )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b531842",
   "metadata": {},
   "source": [
    "### Fitting the Multivariate Regression Model\n",
    "\n",
    "In `sklearn`, fitting a multiple linear regression model is much the same as fitting a simple linear regression. This time, of course, our $X$ contains multiple columns, where it only contained one before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb634e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, train_size = 0.7, test_size = 0.3, random_state = 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4ebbe",
   "metadata": {},
   "source": [
    "### Construct and fit the model\n",
    "\n",
    "We now go ahead and fit our model.\n",
    "- use the `ols` or Ordinary Least Squares regression model from the `statsmodels` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923aa12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to get an intercept\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "# Fit the resgression line using 'OLS'\n",
    "lr = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# OR\n",
    "\n",
    "model=sm.ols(formula=formula_str, data=df1)\n",
    "fitted = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a4053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the parameters,i.e. intercept and slope of the regression line obtained\n",
    "lr.params\n",
    "\n",
    "# extract model intercept\n",
    "beta_0 = float(lr.intercept_)\n",
    "\n",
    "# extract model coeffs\n",
    "beta_js = pd.DataFrame(lr.coef_, X.columns, columns=['Coefficient'])\n",
    "beta_js"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb2ae1",
   "metadata": {},
   "source": [
    "### Interpreting Coefficients of Multilinear Regression\n",
    "\n",
    "In a multilinear regression model, the coefficients represent the relationship between \n",
    "- each predictor (independent variable) and \n",
    "- the response (dependent variable), \n",
    "while controlling for the effects of other predictors in the model.\n",
    "\n",
    "Intercept ($𝛽_0$ | `beta_0`):\n",
    "- This is the predicted value of the response variable when all predictors are set to zero.\n",
    "- It is meaningful only if all predictors can realistically take a value of zero.\n",
    "\n",
    "Slope Coefficients ($𝛽_𝑖$ | `beta_js`):\n",
    "- Each $𝛽_𝑖$ measures the change in the response variable for a one-unit increase in predictor $𝑋_𝑖$, assuming all other predictors remain constant.\n",
    "- A positive $𝛽_𝑖$: Indicates that an increase in $𝑋_𝑖$ is associated with an increase in the response.\n",
    "- A negative $𝛽_𝑖$: Indicates that an increase in $𝑋_𝑖$ is associated with a decrease in the response.\n",
    "\n",
    "P-Values:\n",
    "- A p-value tests the null hypothesis that the coefficient $𝛽_{1}$ is zero (no effect). \n",
    "    - If the p-value is small (typically <0.05), the predictor is considered statistically significant in explaining the response variable.\n",
    "\n",
    "Standardized Coefficients:\n",
    "- If predictors are measured in different units, their coefficients can't be directly compared. Standardized coefficients (beta weights) are used to determine the relative importance of predictors.\n",
    "\n",
    "##### Explaining Multilinear Regression equation\n",
    "\n",
    "$$ 𝑦=𝛽_{0}+𝛽_{1}𝑥_{1}+…+𝛽_{2}𝑥_{2}+ 𝜖 $$\n",
    "\n",
    "- $𝛽_{1}$: if $𝛽_{1}$ = 2, then a one-unit increase in $𝑥_{1}$ is associated with an average increase of 2 units in 𝑌, holding $𝑥_{2}$ constant.\n",
    "- $𝛽_{2}$: if $𝛽_{2}$ = -3, then a one-unit increase in $𝑥_{2}$ is associated with an average decrease of 3 units in 𝑌, holding $𝑥_{1}$ constant.\n",
    "\n",
    "### Testing Relationships Between Response and Predictors\n",
    "\n",
    "Multilinear regression tests the relationship between the response variable (𝑌) and the predictors ($𝑥_{1}$,$𝑥_{2}$,…,$𝑥_{p}$) by modeling 𝑌 as a linear combination of the predictors:\n",
    "\n",
    "$$ 𝑦=𝛽_{0}+𝛽_{1}𝑥_{1}+ +𝛽_{2}𝑥_{2}+…+𝛽_{p}𝑥_{p}+ 𝜖 $$\n",
    "\n",
    "1. Hypothesis Testing:\n",
    "- For each predictor $𝑥_{p}$, Null Hypothesis ($𝐻_0): 𝛽_p =0$ (the predictor has no effect on response(𝑦) varaible | no relationship between predictor(x) response(𝑦)).\n",
    "- Alternative Hypothesis $(𝐻_𝑎): 𝛽_𝑗 ≠ 0$ (the predictor has an effect/ there is a relationship).\n",
    "\n",
    "2. **t-statistic: test is performed for each coefficient**\n",
    "\n",
    "How to Calculate the t-statistic in Linear regression\n",
    "\n",
    "The t-statistic in linear regression measures how many standard errors the estimated coefficient is away from zero. \n",
    "- It is used for hypothesis testing to determine if a predictor variable is statistically significant.\n",
    "\n",
    "The formula to calculate the t-statistic for a coefficient\n",
    "\n",
    "$$t = \\frac{\\hat{𝛽_p}}{SE_{\\hat{𝛽_p}}}$$\n",
    "\n",
    "Where:\n",
    "$𝛽_p$: Estimated coefficient (e.g., slope or intercept).\n",
    "$SE_{\\hat{𝛽_p}}$: Standard error of the estimated coefficient $\\hat{𝛽_p}$.\n",
    "\n",
    "### t-statistic maybe a misleading variable importance indicator:\n",
    "\n",
    "In multiple linear regression, the t-statistic evaluates the significance of individual predictor variables by testing the null hypothesis that a predictor's coefficient is zero ($𝐻_0): 𝛽_p =0$.\n",
    "\n",
    "It can be misleading as an indicator of variable importance in multilinear regression for the following reasons:\n",
    "\n",
    "- Multicollinearity\n",
    "    - When predictor variables are highly correlated, the variance of the coefficient estimates increases.\n",
    "    - This can lead to inflated standard errors and reduced t-statistics, causing variables to appear insignificant even if they are important.\n",
    "        - Conversely, some variables might have significant t-statistics due to correlation with other predictors rather than their actual contribution to the response variable.\n",
    "\n",
    "- Dependency on Units of Measurement\n",
    "    - The t-statistic depends on the scale of the predictor variables. \n",
    "        - For example, variables with larger numerical ranges can dominate, making direct comparisons between t-statistics across variables inappropriate without standardization.\n",
    "\n",
    "- Context of the Model\n",
    "- The importance of a variable depends on the context of other predictors in the model. \n",
    "    - Adding or removing predictors can change the coefficients and t-statistics, leading to different conclusions about importance.\n",
    "\n",
    "- Does Not Reflect Contribution to $R^2$\n",
    "    - The t-statistic evaluates the statistical significance of a single variable, but it does not measure its contribution to the model's overall explanatory power ($R^2$).\n",
    "    - A variable may be statistically significant (high t-statistic) yet contribute little to the variance explained.\n",
    "\n",
    "- Focuses on Statistical Significance Over Practical Significance\n",
    "    - A high t-statistic indicates statistical significance but does not imply that the variable is practically meaningful or contributes substantially to predictions.\n",
    "\n",
    "Best Practices to Assess Variable Importance\n",
    "- Use metrics like standardized coefficients to account for differences in units.\n",
    "- Evaluate variable importance metrics, such as partial $R^2$ , Shapley values, or permutation importance, especially in models with multicollinearity.\n",
    "- Perform model comparison using adjusted $R^2$ or the Akaike Information Criterion (AIC) to assess the model’s explanatory power with and without specific variables.\n",
    "\n",
    "##### Implementation of best practices for assessing variable importance in multilinear regression:\n",
    "\n",
    "- Standardized Coefficients: Calculates coefficients on a standardized scale for comparison.\n",
    "- Partial $R^2$: Measures the contribution of each variable to the overall $R^2$.\n",
    "- Permutation Importance: Evaluates the change in model performance when a variable's values are randomly shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8717cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Example data\n",
    "np.random.seed(42)\n",
    "X = pd.DataFrame({\n",
    "    'Variable_A': np.random.rand(100) * 100,\n",
    "    'Variable_B': np.random.rand(100) * 50,\n",
    "    'Variable_C': np.random.rand(100) * 10\n",
    "})\n",
    "y = 2 * X['Variable_A'] + 0.5 * X['Variable_B'] + 0.1 * X['Variable_C'] + np.random.randn(100) * 5\n",
    "\n",
    "# Step 1: Fit a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Step 2: Assess importance using standardized coefficients\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "model_scaled = LinearRegression()\n",
    "model_scaled.fit(X_scaled, y)\n",
    "standardized_coefficients = model_scaled.coef_\n",
    "\n",
    "# Step 3: Compute partial R-squared for each variable\n",
    "def partial_r2(X, y, variable):\n",
    "    X_partial = X.drop(columns=[variable])\n",
    "    model_partial = LinearRegression().fit(X_partial, y)\n",
    "    residuals = y - model_partial.predict(X_partial)\n",
    "    total_rss = np.sum((y - y.mean()) ** 2)\n",
    "    partial_rss = np.sum(residuals ** 2)\n",
    "    return 1 - (partial_rss / total_rss)\n",
    "\n",
    "partial_r2_values = {var: partial_r2(X, y, var) for var in X.columns}\n",
    "\n",
    "# Step 4: Compute permutation importance\n",
    "perm_importance = permutation_importance(model, X, y, n_repeats=30, random_state=42)\n",
    "\n",
    "# Step 5: Display results\n",
    "print(\"Standardized Coefficients:\")\n",
    "for var, coef in zip(X.columns, standardized_coefficients):\n",
    "    print(f\"{var}: {coef:.4f}\")\n",
    "\n",
    "print(\"\\nPartial R-squared Values:\")\n",
    "for var, r2 in partial_r2_values.items():\n",
    "    print(f\"{var}: {r2:.4f}\")\n",
    "\n",
    "print(\"\\nPermutation Importance:\")\n",
    "for var, importance in zip(X.columns, perm_importance.importances_mean):\n",
    "    print(f\"{var}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f4ceb",
   "metadata": {},
   "source": [
    "2. **F-Test for Overall Model Significance**:\n",
    "\n",
    "The F-statistic is used in hypothesis testing to evaluate the overall significance of a multiple linear regression model. Specifically, it tests whether at least one of the predictor variables in the model significantly explains variation in the dependent variable.\n",
    "- Tests the null hypothesis that all coefficients are zero ($𝛽_{1} = 𝛽_{2} = ... = 𝛽_{p} = 0$).\n",
    "    - If the F-statistic is significant, at least one predictor has a relationship with 𝑌.\n",
    "\n",
    "formula for the F-statistic is:\n",
    "\n",
    "$$ F= \\frac{Explained Mean Square (MSR)}{Residual Mean Square (MSE)}$$\n",
    "$$ F= \\frac{\\frac{TSS−RSS}{p}}{\\frac{RSS}{n−p−1}}$$\n",
    "\n",
    "Where:\n",
    "- TSS: Total Sum of Squares\n",
    "- RSS: Residual Sum of Squares\n",
    "- n: Number of observations\n",
    "- p: Number of predictors (excluding the intercept)\n",
    "- Mean Square Regression (MSR): $\\frac{TSS−RSS}{p}$\n",
    "- Mean Square Error (MSE): $frac{RSS}{n−p−1}$\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Calculate the degrees of freedom:\n",
    "- For regression: 𝑝\n",
    "- For error: 𝑛−𝑝−1\n",
    "\n",
    "2. Compute the explained variance: TSS−RSS\n",
    "\n",
    "3. Calculate Mean Square Regression (MSR) and Mean Square Error (MSE)\n",
    "\n",
    "4. Compute the F-statistic:\n",
    "$$F = \\frac{Explained Mean Square (MSR)}{Residual Mean Square (MSE)}$$\n",
    "\n",
    "When to Perform the F-Test?\n",
    "- Perform the F-test whenever you have a regression model and want to evaluate its overall significance. \n",
    "- It is especially relevant in multiple linear regression with several predictors.\n",
    "\n",
    "Why Perform the F-Test?\n",
    "- To determine if the model as a whole is useful for predicting the dependent variable.\n",
    "- It helps decide whether further analysis (e.g., testing individual predictors or refining the model) is warranted.\n",
    "\n",
    "##### Practical Steps in Hypothesis Testing:\n",
    "\n",
    "i.  Formulate the Hypotheses\n",
    "- Null Hypothesis ($𝐻_0$): All regression coefficients (except the intercept) are equal to zero, i.e., the predictors do not explain the variability in the dependent variable.\n",
    "$$ 𝐻_0: 𝛽_{1} = 𝛽_{2} = ... = 𝛽_{p} = 0$$\n",
    "- Alternative Hypothesis($𝐻_a$): At least one of the regression coefficients is not zero, i.e., at least one predictor contributes to explaining the variability.\n",
    "$$ 𝐻_a: at least one𝛽_{j}\\neq 0, for j = 1,2,...,p$$\n",
    "\n",
    "ii. Calculate the F-Statistic\n",
    "\n",
    "$$ F= \\frac{Explained Mean Square (MSR)}{Residual Mean Square (MSE)}$$\n",
    "$$ F= \\frac{\\frac{TSS−RSS}{p}}{\\frac{RSS}{n−p−1}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$MSR = \\frac{Explained Variance}{Degrees of Freedom for Regression (df_{reg})}$$\n",
    "\n",
    "$$MSE = \\frac{Residual Sum of Squares (RSS)}{Degrees of Freedom for Error (df_{error})}$$\n",
    "\n",
    "iii. Determine the Degrees of Freedom\n",
    "\n",
    "- $df_{reg}$ =p: Number of predictors.\n",
    "- $df_{error}$ =n−p−1: Residual degrees of freedom, where 𝑛 is the number of observations.\n",
    "\n",
    "iv. Find the Critical Value\n",
    "\n",
    "- Use the F-distribution table or Python to find the critical value for the given 𝛼 (commonly 0.05), $df_{reg}$ and $df_{error}$\n",
    "\n",
    "v. Compare F-Statistic with the Critical Value\n",
    "- If $𝐹 > 𝐹_{𝑐𝑟𝑖𝑡𝑖𝑐𝑎𝑙}$, reject $𝐻_0$. \n",
    "    - This implies that at least one predictor is significant.\n",
    "- If $𝐹 ≤ 𝐹_{𝑐𝑟𝑖𝑡𝑖𝑐𝑎𝑙}$, fail to reject $𝐻_0$. \n",
    "    - This implies the predictors do not collectively explain the variability better than random chance.\n",
    "\n",
    "vi. Use the p-Value (Optional)\n",
    "- Instead of using a critical value, you can calculate the p-value associated with the F-statistic:\n",
    "    - If p-value < α, reject $𝐻_0$.\n",
    "    - If p-value ≥ α, fail to reject $𝐻_0$.\n",
    "\n",
    "Interpreting Results\n",
    "- Significant F-statistic: Indicates the model has predictive power and at least one predictor is meaningful.\n",
    "- Non-significant F-statistic: Suggests the model does not explain variability better than a simple mean-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8deec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example parameters\n",
    "TSS = 1200  # Total Sum of Squares\n",
    "RSS = 300   # Residual Sum of Squares\n",
    "n = 50      # Number of observations\n",
    "p = 3       # Number of predictors (excluding the intercept)\n",
    "\n",
    "# Step 1: Degrees of freedom\n",
    "df_regression = p               # Degrees of freedom for regression\n",
    "df_error = n - p - 1           # Degrees of freedom for error\n",
    "\n",
    "# Step 2: Explained variance\n",
    "explained_variance = TSS - RSS\n",
    "\n",
    "# Step 3: Calculate MSR and MSE\n",
    "MSR = explained_variance / df_regression  # Mean Square Regression\n",
    "MSE = RSS / df_error                     # Mean Square Error\n",
    "\n",
    "# Step 4: Calculate the F-statistic\n",
    "F_statistic = MSR / MSE\n",
    "\n",
    "# Step 5: Perform hypothesis testing\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Calculate the critical value for the F-distribution\n",
    "alpha = 0.05  # Significance level\n",
    "F_critical = stats.f.ppf(1 - alpha, df_regression, df_error)\n",
    "\n",
    "# Calculate the p-value for the F-statistic\n",
    "p_value = 1 - stats.f.cdf(F_statistic, df_regression, df_error)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Degrees of Freedom (Regression): {df_regression}\")\n",
    "print(f\"Degrees of Freedom (Error): {df_error}\")\n",
    "print(f\"Explained Variance: {explained_variance}\")\n",
    "print(f\"Mean Square Regression (MSR): {MSR}\")\n",
    "print(f\"Mean Square Error (MSE): {MSE}\")\n",
    "print(f\"F-Statistic: {F_statistic}\")\n",
    "print(f\"Critical F-Value: {F_critical}\")\n",
    "print(f\"P-Value: {p_value}\")\n",
    "\n",
    "# Decision based on F-statistic\n",
    "if F_statistic > F_critical:\n",
    "    print(\"Reject the null hypothesis: At least one predictor is significant.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: The model is not significant.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d7e8e",
   "metadata": {},
   "source": [
    "3. Assessing Fit:\n",
    "- Coefficient of Determination $R^2$ : \n",
    "    - Proportion of variance in 𝑌 explained by the predictors.\n",
    "    - Purpose: Measures the proportion of variance in the dependent variable explained by the independent variables.\n",
    "    - When to Use: Always, as a baseline measure of model fit.\n",
    "- Adjusted $R^2$: \n",
    "    - Adjusts $R^2$ for the number of predictors, penalizing the inclusion of irrelevant predictors.\n",
    "    - Key Consideration: Adjusted $R^2$ accounts for the number of predictors, providing a better measure for models with multiple variables.\n",
    "- Residual Analysis\n",
    "    - Purpose: Examines the residuals (differences between observed and predicted values) to check assumptions of the regression model.\n",
    "    - How to Use:\n",
    "        - Plot residuals vs. predicted values to check for patterns (should appear random).\n",
    "        - Use a histogram or Q-Q plot of residuals to check normality.\n",
    "        - Examine residuals vs. independent variables to check for independence.\n",
    "    - When to Use: Always, to validate assumptions like linearity, homoscedasticity, and normality.\n",
    "- Mean Squared Error (MSE)\n",
    "    - Purpose: Measures the average squared difference between observed and predicted values.\n",
    "    - When to Use: To quantify model error; lower MSE indicates better fit.\n",
    "- F-Statistic\n",
    "    - Purpose: Tests the overall significance of the model by comparing explained variance to unexplained variance.\n",
    "    - When to Use: To test whether at least one predictor is significant in explaining the variance of the dependent variable.\n",
    "- Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)\n",
    "    - Purpose: Compare models, penalizing for model complexity.\n",
    "    - When to Use: When comparing multiple models with different numbers of predictors or structures.\n",
    "- Cross-Validation\n",
    "    - Purpose: Evaluates the model’s performance on unseen data.\n",
    "    - How to Use:\n",
    "        - Use k-fold cross-validation to divide data into training and test sets.\n",
    "        - Calculate metrics (e.g., $R^2$ , MSE) on test sets.\n",
    "    - When to Use: To assess model generalizability.\n",
    "- Variance Inflation Factor (VIF)\n",
    "    - Purpose: Detects multicollinearity among predictors.\n",
    "    - How to Use: Compute VIF for each predictor; values > 10 indicate high multicollinearity.\n",
    "    - When to Use: To assess stability of coefficient estimates.\n",
    "- Cook’s Distance and Leverage\n",
    "    - Purpose: Identifies influential observations that disproportionately affect the regression results.\n",
    "    - How to Use:\n",
    "        - Cook’s Distance: Observations with values > 1 are considered influential.\n",
    "        - Leverage: High-leverage points have significant potential to influence the model.\n",
    "    - When to Use: To identify outliers and influential data points.\n",
    "- Normalized Residual Standard Error (NRSE)\n",
    "    - Purpose: Provides a standardized measure of error in the model.\n",
    "    - When to Use: To compare models with different dependent variable scales.\n",
    "- Predictive Metrics (e.g., RMSE, MAE)\n",
    "    - Purpose: Evaluate model accuracy in predicting outcomes.\n",
    "    - When to Use: For regression models focused on prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348fade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.api import OLS, add_constant\n",
    "\n",
    "# Example dataset\n",
    "np.random.seed(42)\n",
    "X = pd.DataFrame({\n",
    "    'Variable_A': np.random.rand(100) * 100,\n",
    "    'Variable_B': np.random.rand(100) * 50,\n",
    "    'Variable_C': np.random.rand(100) * 10\n",
    "})\n",
    "y = 2 * X['Variable_A'] + 0.5 * X['Variable_B'] + 0.1 * X['Variable_C'] + np.random.randn(100) * 5\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 1. Coefficient of Determination (R^2)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "adjusted_r2 = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
    "print(f\"R^2: {r2:.4f}, Adjusted R^2: {adjusted_r2:.4f}\")\n",
    "\n",
    "# 2. Residual Analysis\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x=y_pred, y=residuals)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Predicted')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.xlabel('Residuals')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.show()\n",
    "\n",
    "# 3. Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "# 4. F-Statistic (using statsmodels)\n",
    "X_train_const = add_constant(X_train)\n",
    "ols_model = OLS(y_train, X_train_const).fit()\n",
    "print(ols_model.summary())\n",
    "\n",
    "# 5. Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)\n",
    "print(f\"AIC: {ols_model.aic:.4f}, BIC: {ols_model.bic:.4f}\")\n",
    "\n",
    "# 6. Cross-Validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "print(f\"Cross-Validation R^2 Scores: {cv_scores}\")\n",
    "print(f\"Mean CV R^2: {np.mean(cv_scores):.4f}\")\n",
    "\n",
    "# 7. Variance Inflation Factor (VIF)\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Variable'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(\"Variance Inflation Factor (VIF):\")\n",
    "print(vif_data)\n",
    "\n",
    "# 8. Cook's Distance and Leverage\n",
    "influence = ols_model.get_influence()\n",
    "cooks_d = influence.cooks_distance[0]\n",
    "high_influence_points = np.where(cooks_d > 4 / len(X_train))[0]\n",
    "print(f\"High Influence Points (Cook's Distance > 4/n): {high_influence_points}\")\n",
    "\n",
    "# 9. Residual Standard Error (RSE)\n",
    "rse = np.sqrt(mse)\n",
    "print(f\"Residual Standard Error (RSE): {rse:.4f}\")\n",
    "\n",
    "# 10. Predictive Metrics (RMSE and MAE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}, Mean Absolute Error (MAE): {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8217773",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Step 1: Import Libraries and Load Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate example dataset\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    \"SquareFootage\": np.random.uniform(500, 4000, 100),\n",
    "    \"Bedrooms\": np.random.randint(1, 6, 100),\n",
    "    \"DistanceFromCityCenter\": np.random.uniform(1, 20, 100),\n",
    "    \"HousePrice\": np.random.uniform(50000, 500000, 100),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print sample data\n",
    "print(df.head())\n",
    "\n",
    "############# Step 2: Fit the Multilinear Regression Model\n",
    "\n",
    "# Define predictors (X) and response (Y)\n",
    "X = df[[\"SquareFootage\", \"Bedrooms\", \"DistanceFromCityCenter\"]]\n",
    "Y = df[\"HousePrice\"]\n",
    "\n",
    "# Add a constant for the intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the model\n",
    "model = sm.OLS(Y, X).fit()\n",
    "\n",
    "# Summary of the model\n",
    "print(model.summary())\n",
    "\n",
    "############## Step 3: Interpret the Output\n",
    "\n",
    "# Performing a summary operation lists out all different parameters of the regression line fitted\n",
    "print(lr.summary())\n",
    "\n",
    "# OR\n",
    "\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5de94d",
   "metadata": {},
   "source": [
    "##### Interpretation of `summary`\n",
    "\n",
    "The model.summary() provides:\n",
    "\n",
    "Coefficients:\n",
    "- $𝛽_{0}$: The intercept.\n",
    "- $𝛽_{1}$,$𝛽_{2}$,$𝛽_{3}$: Coefficients for predictors.\n",
    "\n",
    "P-values:\n",
    "- Assess the significance of each predictor.\n",
    "  - If 𝑝 < 0.05, the predictor significantly explains variations in the response variable.\n",
    "\n",
    "$R^{2}$ and Adjusted $R^{2}$ :\n",
    "- Measure how much variance in the response is explained by the predictors.\n",
    "\n",
    "**F-statistic**:\n",
    "- Tests the overall significance of the model.\n",
    "- It tests whether at least one predictor variable in the model has a non-zero coefficient, meaning it contributes significantly to explaining the variance in the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example parameters\n",
    "TSS = 1200  # Total Sum of Squares\n",
    "RSS = 300   # Residual Sum of Squares\n",
    "n = 50      # Number of observations\n",
    "p = 3       # Number of predictors (excluding the intercept)\n",
    "\n",
    "# Step 1: Degrees of freedom\n",
    "df_regression = p               # Degrees of freedom for regression\n",
    "df_error = n - p - 1           # Degrees of freedom for error\n",
    "\n",
    "# Step 2: Explained variance\n",
    "explained_variance = TSS - RSS\n",
    "\n",
    "# Step 3: Calculate MSR and MSE\n",
    "MSR = explained_variance / df_regression  # Mean Square Regression\n",
    "MSE = RSS / df_error                     # Mean Square Error\n",
    "\n",
    "# Step 4: Calculate the F-statistic\n",
    "F_statistic = MSR / MSE\n",
    "\n",
    "# Print the results\n",
    "print(f\"Degrees of Freedom (Regression): {df_regression}\")\n",
    "print(f\"Degrees of Freedom (Error): {df_error}\")\n",
    "print(f\"Explained Variance: {explained_variance}\")\n",
    "print(f\"Mean Square Regression (MSR): {MSR}\")\n",
    "print(f\"Mean Square Error (MSE): {MSE}\")\n",
    "print(f\"F-Statistic: {F_statistic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34baee6c",
   "metadata": {},
   "source": [
    "4. Assumptions:\n",
    "- Linearity: The relationship between predictors and response is linear.\n",
    "- Independence of Errors: Errors are independent of each other.\n",
    "- Homoscedasticity: Constant variance of errors.\n",
    "- Normality of Errors: Errors are normally distributed.\n",
    "\n",
    "##### Practical Steps:\n",
    "1. Plot residuals to check assumptions.\n",
    "2. Use statistical tests (e.g., Shapiro-Wilk for normality, Breusch-Pagan for homoscedasticity).\n",
    "3. Apply transformations or alternative models if assumptions are violated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb5ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Step 4: Visualize Residuals to Check Assumptions\n",
    "\n",
    "# Linearity and Homoscedasticity\n",
    "# Plot predicted vs actual values\n",
    "predicted = model.predict(X)\n",
    "residuals = Y - predicted\n",
    "\n",
    "plt.scatter(predicted, residuals)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Predicted Values\")\n",
    "plt.show()\n",
    "\n",
    "# Normality of Errors\n",
    "# Plot residual distribution\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title(\"Residual Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Perform Shapiro-Wilk test for normality\n",
    "from scipy.stats import shapiro\n",
    "shapiro_test = shapiro(residuals)\n",
    "print(f\"Shapiro-Wilk test p-value: {shapiro_test.pvalue}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad84692b",
   "metadata": {},
   "source": [
    "Step 5: Interpretation\n",
    "\n",
    "Coefficient Interpretation\n",
    "- if coefficient of , $𝛽_{1}$ = 50 it means that for every additional square foot, the house price increases by $50, assuming other predictors are held constant.\n",
    "\n",
    "Model Fit\n",
    "- if $R^{2}$ = 0.85, it means 85% of the variance in house prices is explained by the predictors.\n",
    "- Check adjusted $R^{2}$ to ensure added predictors improve the model meaningfully.\n",
    "\n",
    "Assumptions\n",
    "- A residual plot with no pattern confirms linearity.\n",
    "- Homoscedasticity: Residuals should have constant variance (scatter evenly around zero).\n",
    "- Normality: Residuals should approximately follow a normal distribution.\n",
    "___________\n",
    "\n",
    "few 2-dimensional plots; plotting `wt`, `disp`, `cyl`, and `hp` vs. `mpg`, respectively (top-left to bottom-right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c99e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(9,7))\n",
    "\n",
    "axs[0,0].scatter(df['wt'], df['mpg'])\n",
    "axs[0,0].plot(df['wt'], lm.intercept_ + lm.coef_[4]*df['wt'], color='red')\n",
    "axs[0,0].title.set_text('Weight (wt) vs. mpg')\n",
    "\n",
    "axs[0,1].scatter(df['disp'], df['mpg'])\n",
    "axs[0,1].plot(df['disp'], lm.intercept_ + lm.coef_[1]*df['disp'], color='red')\n",
    "axs[0,1].title.set_text('Engine displacement (disp) vs. mpg')\n",
    "\n",
    "axs[1,0].scatter(df['cyl'], df['mpg'])\n",
    "axs[1,0].plot(df['cyl'], lm.intercept_ + lm.coef_[0]*df['cyl'], color='red')\n",
    "axs[1,0].title.set_text('Number of cylinders (cyl) vs. mpg')\n",
    "\n",
    "axs[1,1].scatter(df['hp'], df['mpg'])\n",
    "axs[1,1].plot(df['hp'], lm.intercept_ + lm.coef_[2]*df['hp'], color='red')\n",
    "axs[1,1].title.set_text('Horsepower (hp) vs. mpg')\n",
    "\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af29b5fd",
   "metadata": {},
   "source": [
    "### Assessing Model Accuracy\n",
    "\n",
    "Let's assess the fit of our multivariate model. For the purpose of a rudimentary comparison, let's measure model accuracy aginst a simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18155fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to X_test\n",
    "X_test_sm = sm.add_constant(X_test)\n",
    "# Predict the y values corresponding to X_test_sm\n",
    "y_pred = lr.predict(X_test_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9e55c",
   "metadata": {},
   "source": [
    "We have included a column *Test RMSE*, which is simply the square root of the *Test MSE*.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "RMSE & = \\sqrt{MSE} \\\\\n",
    "     & = \\sqrt{\\frac{1}{N}\\sum^{N} (\\hat{y_i} - y_i)^{2}}\n",
    "\\end{align}\n",
    "\n",
    "Where $y_i$ are the actual target values for a dataset with $N$ datapoints, and $\\hat{y_i}$ represent our corresponding predictions. RMSE is a more intuitive metric to use than MSE because it is in the same units as the underlying variable being predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec0210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import math\n",
    "\n",
    "# Imporitng libraries\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# dictionary of results\n",
    "results_dict = {'Training MSE':\n",
    "                    {\n",
    "                        \"SLR\": metrics.mean_squared_error(y_train, slr.predict(X_train[['disp']])),\n",
    "                        \"MLR\": metrics.mean_squared_error(y_train, lm.predict(X_train))\n",
    "                    },\n",
    "                'Test MSE':\n",
    "                    {\n",
    "                        \"SLR\": metrics.mean_squared_error(y_test, slr.predict(X_test[['disp']])),\n",
    "                        \"MLR\": metrics.mean_squared_error(y_test, lm.predict(X_test))\n",
    "                    },\n",
    "                'Test RMSE':\n",
    "                    {\n",
    "                        \"SLR\": math.sqrt(metrics.mean_squared_error(y_test, slr.predict(X_test[['disp']]))),\n",
    "                        \"MLR\": math.sqrt(metrics.mean_squared_error(y_test, lm.predict(X_test)))\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d2ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE value\n",
    "print(\"RMSE: \",np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "#R-squared value\n",
    "print(\"R-squared: \",r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c3b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lm = X_train_lm.values.reshape(-1,1)\n",
    "X_test_lm = X_test_lm.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa82d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_lm.shape)\n",
    "print(X_train_lm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ffa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "#Representing LinearRegression as lr (creating LinearRegression object)\n",
    "lr = LinearRegression()\n",
    "#Fit the model using lr.fit()\n",
    "lr.fit(X_train_lm,y_train_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba41d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get intercept\n",
    "print(lr.intercept_)\n",
    "#get slope\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b73f2",
   "metadata": {},
   "source": [
    "# Addressing Assumptions in Multilinear Regression\n",
    "\n",
    "### **When using Linear Regression, several potential problems can arise that violate its assumptions**\n",
    "This leads to biased or inefficient estimates. Below are key issues related to \n",
    "- non-linearity, \n",
    "- correlation of error terms, \n",
    "- non-constant variances of error terms, \n",
    "- outliers, \n",
    "- high-leverage points, and \n",
    "- collinearity—along with how to detect and address them.\n",
    "\n",
    "Initial Diagnostics:\n",
    "- Examine scatter plots and residual plots.\n",
    "- Test assumptions (e.g., Breusch-Pagan for heteroscedasticity, Shapiro-Wilk for normality).\n",
    "\n",
    "Transform Data if Necessary:\n",
    "- Use log, Box-Cox, or polynomial transformations to address issues like non-linearity and heteroscedasticity.\n",
    "\n",
    "Refit and Compare Models:\n",
    "- Use metrics like Adjusted $𝑅^2$ , Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC) to compare models.\n",
    "\n",
    "Document Interpretations:\n",
    "- Explain coefficients in the context of transformed variables.\n",
    "- Discuss any trade-offs made during model selection.\n",
    "\n",
    "# Checking for Independence\n",
    "\n",
    "Independence of Errors: Errors should be independent (important for time series or clustered data).\n",
    "\n",
    "We have done checks for linearity and multicollinearity, which both referred to the predictor variables. \n",
    "\n",
    "To checking some of the artefacts of the fitted model for three more statistical phenomena which further help us determine its quality.\n",
    "\n",
    "#### Residuals vs. Predictor Variables Plots \n",
    "\n",
    "The first check we do involves plotting the residuals (vertical distances between each data point and the regression hyperplane). \n",
    "- We are looking to confirm the independence assumption here, i.e.: the residuals should be independent. \n",
    "\n",
    "If they are we will see:\n",
    "- Residuals approximately uniformly randomly distributed about the zero x-axes;\n",
    "- Residuals not forming specific clusters.\n",
    "\n",
    "Observing the plots two things should be relatively clear:\n",
    "\n",
    "- Residuals are slightly to skewed to the positive or negative (reaching +5 but only about -3);\n",
    "\n",
    "- check for clustering, \n",
    "    - Check which may present a cluster on the value 6.\n",
    "\n",
    "Conclusion: is the residuals are largely independent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,5, figsize=(14,6),sharey=True)\n",
    "fig.subplots_adjust(hspace = 0.5, wspace=.2)\n",
    "fig.suptitle('Predictor variables vs. model residuals', fontsize=16)\n",
    "axs = axs.ravel()\n",
    "\n",
    "for index, column in enumerate(df.columns):\n",
    "    axs[index-1].set_title(\"{}\".format(column),fontsize=12)\n",
    "    axs[index-1].scatter(x=df[column],y=fitted.resid,color='blue',edgecolor='k')\n",
    "    axs[index-1].grid(True)\n",
    "    xmin = min(df[column])\n",
    "    xmax = max(df[column])\n",
    "    axs[index-1].hlines(y=0,xmin=xmin*0.9,xmax=xmax*1.1,color='red',linestyle='--',lw=3)\n",
    "    if index == 1 or index == 6:\n",
    "        axs[index-1].set_ylabel('Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4845571",
   "metadata": {},
   "source": [
    "# Correlation of Error Terms (Autocorrelation)\n",
    "Issue:\n",
    "- Errors (residuals) in a regression model should be independent. \n",
    "    - If errors are correlated (autocorrelation), the model violates the assumption of independent errors, leading to underestimated standard errors and inflated R² values.\n",
    "\n",
    "Detection:\n",
    "- Durbin-Watson Test: Values close to 2 suggest no autocorrelation, while values near 0 or 4 indicate positive or negative autocorrelation.\n",
    "- Residual Plots: Plot residuals over time—patterns or trends suggest autocorrelation.\n",
    "\n",
    "Solutions:\n",
    "- Use lag variables to capture dependencies in time series data.\n",
    "- Apply Generalized Least Squares (GLS) or use a Newey-West estimator to correct for autocorrelation.\n",
    "-  Consider ARIMA models for time-dependent data.\n",
    "\n",
    "### Effect of Correlated Errors on Standard Errors and Confidence Intervals\n",
    "When the error terms in a regression model are correlated, the assumptions of ordinary least squares (OLS) regression are violated. \n",
    "- This can lead to biased standard errors and misleading confidence intervals, which in turn affect hypothesis testing and model interpretation.\n",
    "\n",
    "#### 1. How Correlated Errors Affect Standard Errors\n",
    "- Standard errors estimate the variability of the regression coefficients.\n",
    "- When errors are correlated (also known as autocorrelation in time series or spatial correlation in spatial data), standard errors are underestimated or overestimated.\n",
    "- Underestimated standard errors: The model falsely appears more precise than it actually is, increasing the likelihood of Type I errors (incorrectly rejecting the null hypothesis).\n",
    "- Overestimated standard errors: The model appears less precise, making it harder to detect significant relationships.\n",
    "\n",
    "Key Impact: The formula for standard errors assumes independence. \n",
    "- If errors are correlated, the estimated variance is incorrect, leading to misleading inferences.\n",
    "\n",
    "Effect on Confidence Intervals\n",
    "- Confidence intervals depend on standard errors:\n",
    "$$ CI = \\hat{\\beta} \\pm t_{\\frac{\\alpha}{2}} \\cdot SE(\\hat{\\beta}) $$\n",
    "- When standard errors are underestimated, confidence intervals are too narrow, making the model falsely appear more certain.\n",
    "- When standard errors are overestimated, confidence intervals are too wide, reducing the power to detect real effects.\n",
    "\n",
    "Key Impact: The true confidence level differs from what is reported. \n",
    "- A 95% confidence interval might not actually contain the true parameter 95% of the time.\n",
    "\n",
    "Visualizing Correlated Errors in Python\n",
    "We will:\n",
    "- Simulate a dataset with uncorrelated errors (ideal case).\n",
    "- Introduce correlated errors (violating OLS assumptions).\n",
    "- Compare standard errors and confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate predictor variable\n",
    "X = np.linspace(1, 100, 100)\n",
    "\n",
    "# Generate **uncorrelated errors** (white noise)\n",
    "errors_uncorrelated = np.random.normal(0, 10, 100)\n",
    "y_uncorrelated = 2.5 * X + errors_uncorrelated\n",
    "\n",
    "# Generate **correlated errors** (introducing autocorrelation)\n",
    "errors_correlated = np.zeros(100)\n",
    "rho = 0.8  # Degree of correlation\n",
    "\n",
    "for t in range(1, 100):\n",
    "    errors_correlated[t] = rho * errors_correlated[t-1] + np.random.normal(0, 10)\n",
    "\n",
    "y_correlated = 2.5 * X + errors_correlated\n",
    "\n",
    "# Fit OLS regression models\n",
    "X_const = sm.add_constant(X)\n",
    "model_uncorrelated = sm.OLS(y_uncorrelated, X_const).fit()\n",
    "model_correlated = sm.OLS(y_correlated, X_const).fit()\n",
    "\n",
    "# Compare standard errors of coefficients\n",
    "print(\"Standard Error of Coefficients:\")\n",
    "print(\"Uncorrelated Errors:\", model_uncorrelated.bse)\n",
    "print(\"Correlated Errors:\", model_correlated.bse)\n",
    "\n",
    "# Compare confidence intervals\n",
    "print(\"\\nConfidence Intervals:\")\n",
    "print(\"Uncorrelated Errors:\\n\", model_uncorrelated.conf_int())\n",
    "print(\"Correlated Errors:\\n\", model_correlated.conf_int())\n",
    "\n",
    "# Plot residuals to visualize correlation\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(errors_uncorrelated, label=\"Uncorrelated Errors\", color=\"blue\")\n",
    "plt.title(\"Uncorrelated Errors\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(errors_correlated, label=\"Correlated Errors\", color=\"red\")\n",
    "plt.title(\"Correlated Errors\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd03cbf",
   "metadata": {},
   "source": [
    "Interpretation of Results\n",
    "\n",
    "Standard Errors Comparison\n",
    "- The standard error is larger for correlated errors because variance is underestimated in OLS.\n",
    "- If errors are correlated, we get false precision (low SE) or inflated uncertainty (high SE).\n",
    "\n",
    "Confidence Interval Comparison\n",
    "- With correlated errors, the confidence interval is incorrect (either too narrow or too wide).\n",
    "- The interval might suggest false confidence in parameter estimates, leading to incorrect conclusions.\n",
    "\n",
    "Solutions for Correlated Errors\n",
    "1. Check for correlation using Durbin-Watson test:\n",
    "    - DW ≈ 2 → No correlation.\n",
    "    - DW < 1 or > 3 → Strong correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd80fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "dw_stat = durbin_watson(model_correlated.resid)\n",
    "print(f\"Durbin-Watson Statistic: {dw_stat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727f97b6",
   "metadata": {},
   "source": [
    "2. Use Generalized Least Squares (GLS): Handles correlated errors better than OLS.\n",
    "\n",
    "3. Use HAC (Heteroskedasticity and Autocorrelation Consistent) Standard Errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f3762",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hac = model_correlated.get_robustcov_results(cov_type=\"HAC\", maxlags=1)\n",
    "print(model_hac.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86f5750",
   "metadata": {},
   "source": [
    "# Checking for Homoscedasticity\n",
    "\n",
    "Homoscedasticity is an important assumption in linear regression. It implies that the variance of the residuals (errors) is constant across all levels of the independent variables. When this assumption is violated (heteroscedasticity), the model's standard errors, and p-values can become unreliable, potentially leading to incorrect inferences.\n",
    "\n",
    "Homoscedasticity: Residuals should have constant variance.\n",
    "\n",
    "What is Heteroscedasticity?\n",
    "- Heteroscedasticity occurs when the variance of the error terms (residuals) is not constant across all levels of an independent variable in a regression model. \n",
    "- This violates a key assumption of Ordinary Least Squares (OLS) regression, which assumes that residuals have constant variance (homoscedasticity).\n",
    "\n",
    "Why is Heteroscedasticity a Problem?\n",
    "- Biased Standard Errors → Leads to incorrect hypothesis testing.\n",
    "- Unreliable Confidence Intervals → False statistical inferences.\n",
    "- Inefficient Estimates → OLS no longer provides the best linear unbiased estimators (BLUE).\n",
    "\n",
    "What needs to be done: Check whether the variance of the residuals (the error terms) is constant as the fitted values increase. \n",
    "\n",
    "#### Nonconstant Variance of Error Terms (Homoscedasticity vs. Heteroscedasticity)\n",
    "Issue:\n",
    "- Linear regression assumes homoscedasticity—that the variance of errors remains constant across all levels of the independent variables. \n",
    "- If error variance changes, the model becomes inefficient, affecting confidence intervals and hypothesis tests.\n",
    "\n",
    "Detection:\n",
    "- Residual vs. Fitted Value Plot: A funnel shape suggests heteroscedasticity.\n",
    "- Breusch-Pagan or White’s Test: Formal statistical tests for nonconstant variance.\n",
    "\n",
    "Solutions:\n",
    "- Use log or square root transformations on the dependent variable.\n",
    "- Apply Weighted Least Squares (WLS) regression.\n",
    "- Use robust standard errors to adjust for heteroscedasticity.\n",
    "\n",
    "#### Fitted vs. Residuals\n",
    "\n",
    "Determine this by plotting the magnitude of the fitted values (i.e.: `mpg`) against the residuals. \n",
    "- What we are looking for is the plotted points to approximately form a rectangle.\n",
    "- The magnitude of the residuals should not increase as the fitted values increase (if that is the case, the data will form the shape of a cone on its side).\n",
    "\n",
    "**Observation**\n",
    "- If the variance is constant, we have observed _homoscedasticity_. \n",
    "- If the variance is not constant, we have observed _heteroscedasticity_. \n",
    "\n",
    "Use the same plot to check for outliers: any plotted points that are visibly seperate from the random pattern of the rest of the residuals.\n",
    "\n",
    "**Observation**\n",
    "- Look at data point on particular side of the plot and observe the scatteredness/ density.\n",
    "    - Points towards the right-hand side of the plot tend to be scattered slightly less densely, indicating the presence of heteroscedasticity.\n",
    "    - This violates our assumption of homoscedasticity. \n",
    "- Look at the presesnce of outliers\n",
    "    - The presence of these outliers means that those values are weighted too heavily in the prediction process, disproportionately influencing the model's performance. \n",
    "    - This in turn can lead to the confidence interval for out of sample predictions (unseen data) being unrealistically wide or narrow.\n",
    "\n",
    "if Heteroscedasticity, \n",
    "- Solution: Use transformations (log, Box-Cox) or weighted least squares regression.\n",
    "\n",
    "**Step 1: Diagnosing Heteroscedasticity**\n",
    "\n",
    "Detecting Heteroscedasticity\n",
    "- Before applying transformations, confirm the presence of heteroscedasticity:\n",
    "    - Residual plots: Plot residuals vs. fitted values.\n",
    "    - Breusch-Pagan test: A statistical test for heteroscedasticity.\n",
    "    - White test: A general test for heteroscedasticity.\n",
    "\n",
    "(a) Residual Plot\n",
    "- Plot the residuals against the predicted values to check for patterns.\n",
    "\n",
    "Interpretation:\n",
    "- If the points are randomly scattered, homoscedasticity is likely satisfied.\n",
    "- A funnel-shaped or other pattern suggests heteroscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "p=plt.scatter(x=fitted.fittedvalues,y=fitted.resid,edgecolor='k')\n",
    "xmin = min(fitted.fittedvalues)\n",
    "xmax = max(fitted.fittedvalues)\n",
    "plt.hlines(y=0,xmin=xmin*0.9,xmax=xmax*1.1,color='red',linestyle='--',lw=3)\n",
    "plt.xlabel(\"Fitted values\",fontsize=15)\n",
    "plt.ylabel(\"Residuals\",fontsize=15)\n",
    "plt.title(\"Fitted vs. residuals plot\",fontsize=18)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e086dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predicted values and residuals\n",
    "predicted = model.predict(X)\n",
    "residuals = Y - predicted\n",
    "\n",
    "# Residual plot\n",
    "plt.scatter(predicted, residuals)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da4c7c2",
   "metadata": {},
   "source": [
    "(b) Breusch-Pagan Test\n",
    "- This statistical test explicitly checks for heteroscedasticity.\n",
    "\n",
    "Interpretation:\n",
    "- Null Hypothesis: Homoscedasticity is present.\n",
    "- If p-value < 0.05, reject the null hypothesis, indicating heteroscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a3270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "# Breusch-Pagan test\n",
    "bp_test = het_breuschpagan(residuals, X)\n",
    "print(\"Breusch-Pagan Test Results:\")\n",
    "print(f\"LM Statistic: {bp_test[0]}\")\n",
    "print(f\"p-value: {bp_test[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fb49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breusch-Pagan test\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "bp_test = het_breuschpagan(residuals, X)\n",
    "print(f\"Breusch-Pagan test p-value: {bp_test[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6046fa9",
   "metadata": {},
   "source": [
    "(c) White Test\n",
    "- Another test for heteroscedasticity, more flexible than Breusch-Pagan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3446d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_white\n",
    "\n",
    "# White test\n",
    "white_test = het_white(residuals, X)\n",
    "print(\"White Test Results:\")\n",
    "print(f\"LM Statistic: {white_test[0]}\")\n",
    "print(f\"p-value: {white_test[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3904a128",
   "metadata": {},
   "source": [
    "**Step 2: Addressing Heteroscedasticity/ Handling Homoscedasticity**\n",
    "\n",
    "1. Mitigating Heteroscedasticity Using Data Transformation: Transforming the Response Variable \n",
    "- Apply transformations to stabilize variance.\n",
    "- Applying transformations to the dependent (Y) or independent (X) variables can stabilize variance and improve model accuracy.\n",
    "\n",
    "Common Data Transformations for Heteroscedasticity:\n",
    "| Transformation | When to Use | Breed_Bulldog |\n",
    "|---------|----------------|---------------|\n",
    "| Log Transformation | When variance increases with larger values | $Y^* = log(Y)$      |\n",
    "| Square Root Transformation | When variance grows moderately  | $Y^* = \\sqrt{Y}$       |\n",
    "| Box-Cox Transformation| Generalized approach for different degrees of heteroscedasticity | $Y^* = \\frac{Y^{\\lambda} - 1}{\\lambda}$ |\n",
    "| Inverse Transformation| When larger values have very high variance | $Y^* = \\frac{1}{Y}$ |\n",
    "\n",
    "(a) Log Transformation: Use when variance increases with the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Log_HousePrice\"] = np.log(df[\"HousePrice\"])\n",
    "model_log = sm.OLS(df[\"Log_HousePrice\"], X).fit()\n",
    "print(model_log.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce56cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_log = np.log(Y)\n",
    "\n",
    "# Fit new model\n",
    "model_log = sm.OLS(Y_log, X_const).fit()\n",
    "\n",
    "# Plot residuals\n",
    "plt.scatter(model_log.fittedvalues, model_log.resid, alpha=0.7)\n",
    "plt.axhline(y=0, color='red', linestyle='dashed')\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot (Log Transformation)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192027e1",
   "metadata": {},
   "source": [
    "(b) Box-Cox Transformation: Automatically finds the best transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ceb289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "Y_transformed, lambda_boxcox = boxcox(Y)\n",
    "print(f\"Optimal lambda for Box-Cox: {lambda_boxcox}\")\n",
    "\n",
    "model_boxcox = sm.OLS(Y_transformed, X).fit()\n",
    "print(model_boxcox.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ce964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box-Cox transformation (requires strictly positive Y)\n",
    "Y_bc, lambda_bc = boxcox(Y - np.min(Y) + 1)  # Shift Y to be positive\n",
    "\n",
    "# Fit new model\n",
    "model_bc = sm.OLS(Y_bc, X_const).fit()\n",
    "\n",
    "# Plot residuals\n",
    "plt.scatter(model_bc.fittedvalues, model_bc.resid, alpha=0.7)\n",
    "plt.axhline(y=0, color='red', linestyle='dashed')\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(f\"Residual Plot (Box-Cox Transformation, λ={lambda_bc:.2f})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b5b6c8",
   "metadata": {},
   "source": [
    "(c) Square Root Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac53bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_sqrt = np.sqrt(Y)\n",
    "\n",
    "# Fit new model\n",
    "model_sqrt = sm.OLS(Y_sqrt, X_const).fit()\n",
    "\n",
    "# Plot residuals\n",
    "plt.scatter(model_sqrt.fittedvalues, model_sqrt.resid, alpha=0.7)\n",
    "plt.axhline(y=0, color='red', linestyle='dashed')\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot (Square Root Transformation)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afa0111",
   "metadata": {},
   "source": [
    "Implementing Transformations in Python\n",
    "\n",
    "In summarry, we will:\n",
    "- Generate heteroscedastic data.\n",
    "- Apply log, square root, and Box-Cox transformations.\n",
    "- Compare residual plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e768506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import boxcox\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate independent variable\n",
    "X = np.linspace(1, 100, 100)\n",
    "\n",
    "# Generate heteroscedastic errors (variance increases with X)\n",
    "errors = np.random.normal(0, X, 100)  \n",
    "\n",
    "# Create heteroscedastic response variable\n",
    "Y = 5 * X + errors  \n",
    "\n",
    "# Fit OLS model\n",
    "X_const = sm.add_constant(X)\n",
    "model = sm.OLS(Y, X_const).fit()\n",
    "\n",
    "# Check for heteroscedasticity (Breusch-Pagan test)\n",
    "bp_test = het_breuschpagan(model.resid, X_const)\n",
    "print(\"Breusch-Pagan Test p-value:\", bp_test[1])\n",
    "\n",
    "# Plot original residuals\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(model.fittedvalues, model.resid, alpha=0.7)\n",
    "plt.axhline(y=0, color='red', linestyle='dashed')\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot (Before Transformation)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80012913",
   "metadata": {},
   "source": [
    "2. Applying Weighted Least Squares (WLS)\n",
    "\n",
    "If heteroscedasticity is detected:\n",
    "- Use WLS to assign weights inversely proportional to the variance of residuals.\n",
    "\n",
    "When to use:\n",
    "- When residual patterns vary predictably with certain predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b252f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "# Calculate weights as inverse of squared residuals\n",
    "weights = 1 / (residuals**2)\n",
    "\n",
    "# Fit WLS model\n",
    "model_wls = sm.WLS(Y, X, weights=weights).fit()\n",
    "print(model_wls.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b08694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a weighted least squares model\n",
    "weights = 1 / (residuals**2)\n",
    "model_wls = sm.WLS(Y, X, weights=weights).fit()\n",
    "\n",
    "print(model_wls.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4323e179",
   "metadata": {},
   "source": [
    "3. Heteroscedasticity-Robust Standard Errors\n",
    "- Use robust standard errors to correct inference without changing the model structure.\n",
    "\n",
    "Types of Robust Covariance:\n",
    "- \"HC0\": Basic robust variance.\n",
    "- \"HC1\", \"HC2\", \"HC3\": Variants of robust variance, with \"HC3\" being stricter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47eb07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit OLS model with robust standard errors\n",
    "model_robust = sm.OLS(Y, X).fit(cov_type=\"HC3\")\n",
    "print(model_robust.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c152c3",
   "metadata": {},
   "source": [
    "Check Residual Plots After Mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986274d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals of WLS model\n",
    "predicted_wls = model_wls.predict(X)\n",
    "residuals_wls = Y - predicted_wls\n",
    "\n",
    "plt.scatter(predicted_wls, residuals_wls)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.title(\"Residual Plot After WLS\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7451a7",
   "metadata": {},
   "source": [
    "**Step 3: Comparing Models**\n",
    "\n",
    "Evaluate and Compare Performance\n",
    "- Residual plots before and after adjustments.\n",
    "- Metrics like $𝑅^2$, AIC, and BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare AIC and BIC\n",
    "print(f\"Original Model AIC: {model.aic}\")\n",
    "print(f\"Log-Transformed Model AIC: {model_log.aic}\")\n",
    "print(f\"Box-Cox Model AIC: {model_boxcox.aic}\")\n",
    "print(f\"WLS Model AIC: {model_wls.aic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a586ca1f",
   "metadata": {},
   "source": [
    "Best Practices and Considerations\n",
    "\n",
    "Diagnosis:\n",
    "- Always check residual plots and use tests like Breusch-Pagan or White.\n",
    "\n",
    "Correction:\n",
    "- Start with transformations if patterns suggest non-linearity or skewed responses.\n",
    "- Use WLS or robust standard errors for complex variance structures.\n",
    "\n",
    "Validation:\n",
    "- Ensure improvements in residual plots and metrics.\n",
    "- Balance interpretability and complexity when applying advanced techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a82de17",
   "metadata": {},
   "source": [
    "# Checking for Normality\n",
    "\n",
    "The normality of residuals is a key assumption in linear regression, especially for inference. It ensures that t-tests and F-tests for significance are valid. If residuals are not normally distributed, it can lead to unreliable p-values and confidence intervals.\n",
    "\n",
    "To confirm our assumption of normality amongst the residuals. \n",
    "- If the residuals are non-normally distributed, confidence intervals can become too wide or too narrow, \n",
    "    - which leads to difficulty in estimating coefficients based on the minimisation of ordinary least squares.\n",
    "\n",
    "Check for violation of the normality assumption in two different ways:\n",
    "1. Plotting a histogram of the normalised residuals;\n",
    "2. Generating a Q-Q plot of the residuals.\n",
    "\n",
    "**Step 1: Testing for Normality**\n",
    "\n",
    "(a) Visual Inspection: Histogram and Q-Q Plot\n",
    "\n",
    "1. Histogram: Examine the residuals' distribution.\n",
    "\n",
    "Plot a histogram of the residuals to take a look at their distribution. \n",
    "- It is fairly easy to pick up when a distribution looks similar to the classic _bell curve_ shape of the normal distribution.\n",
    "\n",
    "Interpretation:\n",
    "- Histogram: A bell-shaped curve suggests normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(fitted.resid_pearson,bins=8,edgecolor='k')\n",
    "plt.ylabel('Count',fontsize=15)\n",
    "plt.xlabel('Normalized residuals',fontsize=15)\n",
    "plt.title(\"Histogram of normalized residuals\",fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f9d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = Y - model.predict(X)\n",
    "\n",
    "# Histogram\n",
    "plt.hist(residuals, bins=20, edgecolor='k', alpha=0.7)\n",
    "plt.title(\"Residual Histogram\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9660ce8",
   "metadata": {},
   "source": [
    "2. Q-Q plot of the residuals\n",
    "\n",
    "Compare residuals to a normal distribution.\n",
    "- A Q-Q plot, A.K.A quantile-quantile plot, attempts to plot the theoretical quantiles of the standard normal distribution against the quantiles of the residuals. \n",
    "- The one-to-one line, indicated in red below, is the ideal line indicating normality. \n",
    "- The closer the plotted points are to the red line, the closer the residual distribution is to the standard normal distribution.\n",
    "\n",
    "Quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities or dividing the observations in a sample in the same way.\n",
    "- 2 quantile is known as the Median\n",
    "- 4 quantile is known as the Quartile\n",
    "- 10 quantile is known as the Decile\n",
    "- 100 quantile is known as the Percentile\n",
    "\n",
    "Interpretation:\n",
    "- Q-Q Plot: Points should lie close to the 45° line for normality.\n",
    "\n",
    "10 quantile will divide the Normal Distribution into 10 parts each having 10 % of the data points. The Q-Q plot or quantile-quantile plot is a scatter plot created by plotting two sets of quantiles against one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5653d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We once again use the statsmodel library to assist us in producing our qqplot visualisation. \n",
    "from statsmodels.graphics.gofplots import qqplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8361da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "fig=qqplot(fitted.resid_pearson,line='45',fit='True')\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.xlabel(\"Theoretical quantiles\",fontsize=15)\n",
    "plt.ylabel(\"Sample quantiles\",fontsize=15)\n",
    "plt.title(\"Q-Q plot of normalized residuals\",fontsize=18)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6053ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = Y - model.predict(X)\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q Plot of Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afa905b",
   "metadata": {},
   "source": [
    "(b) Shapiro-Wilk Test\n",
    "- Statistical test for normality.\n",
    "\n",
    "Interpretation\n",
    "\n",
    "Null Hypothesis: Residuals follow a normal distribution.\n",
    "- If p-value < 0.05, reject the null hypothesis, indicating non-normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "shapiro_test = shapiro(residuals)\n",
    "print(f\"Shapiro-Wilk Test Statistic: {shapiro_test.statistic}, p-value: {shapiro_test.pvalue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426fb5e1",
   "metadata": {},
   "source": [
    "(c) Kolmogorov-Smirnov Test\n",
    "- Another test for normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29455622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest\n",
    "\n",
    "ks_test = kstest(residuals, 'norm', args=(residuals.mean(), residuals.std()))\n",
    "print(f\"KS Test Statistic: {ks_test.statistic}, p-value: {ks_test.pvalue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0215a753",
   "metadata": {},
   "source": [
    "(d) Anderson-Darling Test\n",
    "- Tests for how well data fits a specific distribution.\n",
    "\n",
    "Compare the statistic to critical values. \n",
    "- If the statistic exceeds the critical value for a given significance level, residuals deviate from normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189c667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import anderson\n",
    "\n",
    "anderson_test = anderson(residuals, dist=\"norm\")\n",
    "print(\"Anderson-Darling Test Results:\")\n",
    "print(f\"Statistic: {anderson_test.statistic}\")\n",
    "print(\"Critical Values:\", anderson_test.critical_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b093a117",
   "metadata": {},
   "source": [
    "**Step 2: Addressing Non-Normal Residuals/ Handling Normality of Errors**\n",
    "\n",
    "(a) Transform the Response Variable\n",
    "\n",
    "1. Log Transformation: Use if residuals are right-skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7599aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df[\"Log_HousePrice\"] = np.log(df[\"HousePrice\"])\n",
    "model_log = sm.OLS(df[\"Log_HousePrice\"], X).fit()\n",
    "print(model_log.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4c3d4b",
   "metadata": {},
   "source": [
    "2. Applying Box-Cox Transformation\n",
    "- Finds the best transformation parameter (𝜆)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a0c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "# Apply Box-Cox to the response variable\n",
    "Y_boxcox, lambda_boxcox = boxcox(Y)\n",
    "print(f\"Optimal lambda for Box-Cox: {lambda_boxcox}\")\n",
    "\n",
    "# Fit the model again\n",
    "model_boxcox = sm.OLS(Y_boxcox, X).fit()\n",
    "print(model_boxcox.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "Y_transformed, lambda_boxcox = boxcox(Y)\n",
    "print(f\"Optimal lambda for Box-Cox: {lambda_boxcox}\")\n",
    "\n",
    "model_boxcox = sm.OLS(Y_transformed, X).fit()\n",
    "print(model_boxcox.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a92e7",
   "metadata": {},
   "source": [
    "3. Square Root Transformation: Helps stabilize variance and normalize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bec93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sqrt_HousePrice\"] = np.sqrt(df[\"HousePrice\"])\n",
    "model_sqrt = sm.OLS(df[\"Sqrt_HousePrice\"], X).fit()\n",
    "print(model_sqrt.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3871b7",
   "metadata": {},
   "source": [
    "(b) Using Robust Regression\n",
    "\n",
    "If normality cannot be achieved:\n",
    "- Robust regression minimizes the influence of outliers and non-normal errors.\n",
    "\n",
    "1. Huber Regression: Combines linear regression with robustness to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f243552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "huber = HuberRegressor()\n",
    "huber.fit(X, Y)\n",
    "print(\"Huber Coefficients:\", huber.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102a6cdb",
   "metadata": {},
   "source": [
    "2. Quantile Regression: Models conditional medians instead of means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "model_quantile = sm.QuantReg(Y, X).fit(q=0.5)  # Median regression\n",
    "print(model_quantile.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06009f97",
   "metadata": {},
   "source": [
    "3. Robust linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463892e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.robust.robust_linear_model import RLM\n",
    "\n",
    "# Fit a robust linear model\n",
    "model_robust = sm.RLM(Y, X).fit()\n",
    "print(model_robust.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5573d3",
   "metadata": {},
   "source": [
    "(c) Bootstrap for Non-Normal Residuals\n",
    "- Bootstrapping creates confidence intervals without assuming normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f813857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Bootstrap residuals\n",
    "bootstrap_samples = 1000\n",
    "boot_means = []\n",
    "\n",
    "for _ in range(bootstrap_samples):\n",
    "    Y_boot, X_boot = resample(Y, X)\n",
    "    model_boot = sm.OLS(Y_boot, X_boot).fit()\n",
    "    boot_means.append(model_boot.params)\n",
    "\n",
    "boot_means = np.array(boot_means)\n",
    "print(\"Bootstrap Confidence Intervals:\")\n",
    "print(np.percentile(boot_means, [2.5, 97.5], axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0500c7a6",
   "metadata": {},
   "source": [
    "**Step 3: Evaluating Adjustments**\n",
    "\n",
    "Evaluate and Compare Performance\n",
    "- Residual plots before and after adjustments.\n",
    "- Normality tests on new residuals.\n",
    "- Performance Metrics like $𝑅^2$, AIC, and BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3cc496",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original Model AIC: {model.aic}\")\n",
    "print(f\"Log-Transformed Model AIC: {model_log.aic}\")\n",
    "print(f\"Box-Cox Model AIC: {model_boxcox.aic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd82f9c3",
   "metadata": {},
   "source": [
    "Plot Residuals After Adjustments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc79817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals from Box-Cox model\n",
    "residuals_boxcox = Y_transformed - model_boxcox.predict(X)\n",
    "\n",
    "plt.hist(residuals_boxcox, bins=20, edgecolor='k', alpha=0.7)\n",
    "plt.title(\"Residual Histogram After Box-Cox\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7a294",
   "metadata": {},
   "source": [
    "Testing:\n",
    "- Use visual methods like histograms and Q-Q plots.\n",
    "- Statistical tests (Shapiro-Wilk, Anderson-Darling) confirm non-normality.\n",
    "\n",
    "Correction:\n",
    "- Start with transformations like log or Box-Cox.\n",
    "- Use robust regression if transformations fail or residuals deviate significantly.\n",
    "\n",
    "Validation:\n",
    "- Reassess residual plots and metrics post-adjustment.\n",
    "- Ensure the model aligns with assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ee434",
   "metadata": {},
   "source": [
    "# Checking for Outliers in Residuals\n",
    "\n",
    "Check for outliers amongst the residuals.\n",
    "\n",
    "#### Plotting Cook's Distance\n",
    "\n",
    "Cook's distance is a calculation which measures the effect of deleting an observation from the data. \n",
    "- Observations with large Cook's distances should be earmarked for closer examination in the analysis due to their disproportionate impact on the model.\n",
    "\n",
    "**Observation**\n",
    "\n",
    "Check values with much higher Cook's distances than the rest. \n",
    "- A rule of thumb for determining whether a Cook's distance is too large is whether it is greater than four times the mean Cook's distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import OLSInfluence as influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b57b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf=influence(fitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f81425",
   "metadata": {},
   "outputs": [],
   "source": [
    "(c, p) = inf.cooks_distance\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Cook's distance plot for the residuals\",fontsize=16)\n",
    "plt.stem(np.arange(len(c)), c, markerfmt=\",\", use_line_collection=True)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d04294",
   "metadata": {},
   "source": [
    "#### Calculate the mean Cooks Distance\n",
    "\n",
    "Check which observation are 4 X higher the the average\n",
    "\n",
    "Implications: Highly influential in this dataset\n",
    "- warrant closer examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Cook\\'s distance: ', c.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6228e",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression\n",
    "\n",
    "Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).\n",
    "\n",
    "It is used to describe data and to explain the relationship between one dependent binary variable and one or more \n",
    "- nominal, \n",
    "- ordinal, \n",
    "- interval or \n",
    "- ratio-level independent variables.\n",
    "\n",
    "Logistic regression is a statistical model used for binary classification tasks.\n",
    "- The outcome variable is categorical with two possible values (e.g., 1/0, Yes/No, Positive/Negative).\n",
    "- Used to predict the Probabilities for classification problems.\n",
    "\n",
    "It predicts the probability of an event occurring, transforming the linear combination of predictors through a logistic function (sigmoid function) to ensure the predicted probabilities lie between 0 and 1.\n",
    "\n",
    "Model Equation: \n",
    "$ 𝑃(𝑦=1)= \\frac{1}{1+𝑒^{−(𝛽_{0}+𝛽_{1}𝑥_{1}+…+𝛽_{𝑛}𝑥_{𝑛}})}$\n",
    "\n",
    "**What It Means:** \n",
    "- Logistic regression estimates the probability of a binary outcome (e.g., yes/no, success/failure) based on predictor variables. \n",
    "    - It uses a logistic function to map predictions to probabilities between 0 and 1.\n",
    "\n",
    "- It is a statistical technique for investigating the relationship between a binary dependent variable (outcome) and one or more independent variables (predictors). \n",
    "\n",
    "- The goal of logistic regression is to find the best-fitting model to describe the relationship between the dependent variable and the independent variables and then use that model to predict the outcome variable.\n",
    "\n",
    "**Lay Explanation:**\n",
    "- Logistic regression is like a yes-or-no decision helper. It estimates the chances of an event happening (e.g., a customer buying a product) based on known factors.\n",
    "- It tries to find the best-fitted curve for the data\n",
    "\n",
    "**Why use Logistic Regression rather than Linear Regression?**\n",
    "\n",
    "Outlier Influence:\n",
    "- best fit line in linear regression shifts to fit that point.\n",
    "\n",
    "Predicted outcome out of range:\n",
    "- In linear regression, the predicted values may be out of range.\n",
    "\n",
    "Response Variable:\n",
    "- Linear regression is used when dependent variable is continuous\n",
    "- Logistic Regression is used when our dependent variable is binary.\n",
    "\n",
    "Logistic regression is ideal for this problem because:\n",
    "- Binary Outcome: The target variable is binary: Readmitted (1) or Not Readmitted (0).\n",
    "- Interpretability: It provides coefficients (log odds) that indicate how changes in predictors affect the likelihood of the event (readmission).\n",
    "- Insights: It helps identify the significant factors influencing readmissions.\n",
    "\n",
    "### Outcome Interpretation: \n",
    "- The model outputs probabilities that can be converted to binary outcomes. \n",
    "- Coefficients show how each predictor variable influences the likelihood of the outcome.\n",
    "\n",
    "### Performance Measures:\n",
    "- Accuracy: Proportion of correct predictions.\n",
    "- AUC-ROC: Measures the model's ability to distinguish between classes; values closer to 1 indicate a better model.\n",
    "\n",
    "### Types of Logistic Regression\n",
    "\n",
    "#### Binary Logistic Regression\n",
    "Binary logistic regression is used to predict the probability of a binary outcome, such as \n",
    "- yes or no, \n",
    "- true or false, or \n",
    "- 0 or 1. \n",
    "\n",
    "For example, it could be used to:\n",
    "- predict whether a customer will churn or not, \n",
    "- predict whether a patient has a disease or not, or \n",
    "- predict whether a loan will be repaid or not.\n",
    "\n",
    "#### Multinomial Logistic Regression\n",
    "Multinomial logistic regression is used to predict the probability of one of three or more possible outcomes, such as \n",
    "- the type of product a customer will buy, \n",
    "- the rating a customer will give a product, or \n",
    "- the political party a person will vote for.\n",
    "\n",
    "#### Ordinal Logistic Regression\n",
    "Used to predict the probability of an outcome that falls into a predetermined order, such as \n",
    "- the level of customer satisfaction, \n",
    "- the severity of a disease, or \n",
    "- the stage of cancer.\n",
    "\n",
    "#### Least-Squares Regression \n",
    "Is a foundational method in statistics and data science for modeling relationships between variables, particularly for continuous dependent variables. \n",
    "- It does so by finding the line (or hyperplane in higher dimensions) that minimizes the sum of the squared differences (residuals) between the observed and predicted values of the dependent variable.\n",
    "\n",
    "Application:\n",
    "- Continuous Outcomes: Least-squares regression is most commonly used for problems where the dependent variable is continuous, such as \n",
    "    - predicting house prices, \n",
    "    - stock prices, or \n",
    "    - blood pressure.\n",
    "- Exploratory Analysis: Identifying potential relationships between variables.\n",
    "\n",
    "##### Drawback of least-squares regression\n",
    "When applied to classification tasks like logistic regression, is that it assumes linearity and can lead to problems when modeling binary or categorical outcomes.\n",
    "\n",
    "Key Issue:\n",
    "\n",
    "1. Inappropriate Predictions\n",
    "- Least-squares regression is designed for continuous outcomes and does not restrict predictions to the range [0, 1], which is required for probabilities in classification problems.\n",
    "- For binary classification, it can result in predictions outside the valid probability range, such as negative values or values greater than 1, which are meaningless.\n",
    "\n",
    "2. Violation of Assumptions\n",
    "- The error terms (residuals) in least-squares regression are assumed to be normally distributed and homoscedastic (constant variance). \n",
    "    - However, in classification problems, these assumptions are violated because:\n",
    "        - The dependent variable is not continuous but binary.\n",
    "        - The variance of the binary response variable is a function of the mean (heteroscedasticity), not constant.\n",
    "\n",
    "3. Inefficient Parameter Estimation (relationship between the predictors and the binary outcome)\n",
    "- Linear least squares does not model this relationship (non-linear relationship between predictors and the outcome) correctly .\n",
    "    - As a result, least squares is inefficient in estimating parameters and may lead to biased coefficients.\n",
    "- In classification tasks, the relationship between the predictors and the binary outcome is often non-linear (sigmoid-shaped in logistic regression). \n",
    "\n",
    "4. Poor Performance for Separation\n",
    "- Least-squares regression does not inherently maximize the separation between the two classes in binary classification problems. \n",
    "- Logistic regression, on the other hand, maximizes the likelihood of the observed data, providing a more suitable objective for classification tasks.\n",
    "\n",
    "5. Susceptibility to Outliers\n",
    "- Least-squares regression is sensitive to outliers, as it minimizes the squared residuals. \n",
    "- In a classification context, outliers in the feature space can have a disproportionately large influence on the model, leading to poor generalization.\n",
    "\n",
    "##### Why Logistic Regression Instead of Least Squares?\n",
    "Logistic regression overcomes these drawbacks by:\n",
    "- Modeling the probability of the binary outcome using the logit function (log-odds), ensuring probabilities stay within [0, 1].\n",
    "- Using maximum likelihood estimation (MLE) to fit the model, which aligns with the probabilistic nature of classification problems.\n",
    "- Making no assumptions about normally distributed errors, as it focuses on the Bernoulli distribution of binary outcomes.\n",
    "\n",
    "\n",
    "##### Differences Between Linear and Logistic Regression\n",
    "The core difference lies in their target predictions.\n",
    "- Linear regression excels at predicting continuous values along a spectrum. \n",
    "    - resulting output would be a specific amount, a continuous value on the amount scale.\n",
    "- Linear regression answers “how much” questions, providing a specific value on a continuous scale.\n",
    "\n",
    "- Logistic regression deals with categories. \n",
    "    - It doesn’t predict a specific value but rather the likelihood of something belonging to a particular class.\n",
    "    - output here would be a probability between 0 (not likely spam) and 1 (very likely spam). \n",
    "    - This probability is then used to assign an email to a definitive category (spam or not spam) based on a chosen threshold.\n",
    "- Logistic regression tackles “yes or no” scenarios, giving the probability of something belonging to a certain category.\n",
    "\n",
    "### Problem Statement\n",
    "Objective:\n",
    "- The medical institute, we want to identify the likelihood of patients being readmitted within 30 days of discharge based on patient \n",
    "    - demographics, \n",
    "    - medical history, \n",
    "    - length of stay (LOS), and \n",
    "    - clinical metrics such as blood pressure, \n",
    "    - blood glucose levels, and \n",
    "    - medication adherence.\n",
    "\n",
    "**Key Assumptions of Logistic Regression**\n",
    "\n",
    "Data Specific\n",
    "- Binary Outcome: The dependent variable is binary.\n",
    "    - Logistic regression is designed for binary dependent variables. \n",
    "    - If your outcome has more than two categories, you might need a multinomial logistic regression or other classification techniques.\n",
    "- Independence of Observations: Observations are independent of each other.\n",
    "    -  This means no repeated measurements or clustering within the data.\n",
    "\n",
    "Relationship Between Variables\n",
    "- Linearity of Log-Odds: There is a linear relationship between the log-odds of the outcome and the independent variables.\n",
    "    - Outcome itself has a relationship with log-odds.\n",
    "    - Outcome does not have linear relationship with the independent variables.\n",
    "- No Multicollinearity: Independent variables are not highly correlated.\n",
    "    - Multicollinearity can cause instability in the model and make it difficult to interpret the coefficients.\n",
    "\n",
    "Other\n",
    "- Large Sample Size: Logistic regression performs well with larger datasets.\n",
    "    - To ensure reliable parameter estimates.\n",
    "- Absence of Outliers: outliers can significantly influence the model. \n",
    "    - It’s important to check for and address any outliers that might distort the results.\n",
    "\n",
    "**Step 1: Define the Problem**\n",
    "- Target Variable: Readmission within 30 days (1 = Yes, 0 = No).\n",
    "- Predictors:\n",
    "    - Patient Demographics: Age, gender, insurance status.\n",
    "    - Clinical Metrics: Blood glucose levels, blood pressure, medication adherence.\n",
    "    - Hospital Metrics: Length of Stay (LOS), number of previous visits.\n",
    "\n",
    "**Step 2: Collect and Prepare Data**\n",
    "- Gather historical patient data and ensure it's clean and consistent.\n",
    "    - Check for Missing Data:\n",
    "    - Impute missing values for predictors like glucose levels using median or mean.\n",
    "    - Standardize Continuous Variables:\n",
    "    - Standardize LOS, glucose levels, and blood pressure for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed07b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    'age': [45, 60, 50, 40, 70],\n",
    "    'los': [3, 7, 4, 2, 10],\n",
    "    'glucose': [150, 200, 180, 140, 220],\n",
    "    'med_adherence': [0.8, 0.6, 0.75, 0.9, 0.5],\n",
    "    'readmitted': [1, 1, 0, 0, 1]\n",
    "})\n",
    "\n",
    "# Features and target\n",
    "X = data[['age', 'los', 'glucose', 'med_adherence']]\n",
    "y = data['readmitted']\n",
    "\n",
    "# Add constant for intercept\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8ef54",
   "metadata": {},
   "source": [
    "**Step 3: Exploratory Data Analysis**\n",
    "- Univariate Analysis: Examine distributions of continuous variables.\n",
    "- Bivariate Analysis: Analyze relationships between predictors and the target variable.\n",
    "- Correlation Matrix: Identify multicollinearity among predictors.\n",
    "\n",
    "**Step 4: Perform Logistic Regression**\n",
    "\n",
    "How logistic regression squeezes the output of linear regression between 0 and 1.\n",
    "\n",
    "Best Fit Equation in Linear Regression\n",
    "\n",
    "$ y = 𝛽_{0}+𝛽_{1}𝑥_{1}$\n",
    "\n",
    "Now we want to take probabilities (P) instead of y.\n",
    "\n",
    "**Issue**: \n",
    "the value of (P) will exceed 1 or go below 0 and we know that range of Probability is (0-1)\n",
    "\n",
    "Odds and log-odds are central to understanding the relationship between predictors and the probability of an event occurring.\n",
    "\n",
    "**Overcome issue of $0 < P < 1$**\n",
    "\n",
    "by taking “odds” of P:\n",
    "\n",
    "Odds: The odds represent the ratio of the probability of an event occurring (P) to the probability of it not occurring (1−P).\n",
    "\n",
    "$$ Odds =  \\frac{P}{1-P}$$\n",
    "\n",
    "Log-Odds (Logit): The natural logarithm of the odds.\n",
    "\n",
    "$$ Log-Odds =  \\log(\\frac{P}{1-P})$$\n",
    "\n",
    "In logistic regression, the log-odds are modeled as a linear function of the predictors:\n",
    "\n",
    "$$ P = 𝛽_{0}+𝛽_{1}𝑥_{1}$$\n",
    "$$ \\frac{P}{1-P} = 𝛽_{0}+𝛽_{1}𝑥_{1}$$\n",
    "\n",
    "Odds can always be positive which means the range will always be ($0,+∞ $).\n",
    "- Odds are the ratio of the probability of success and probability of failure.\n",
    "\n",
    "Why ‘odds’?\n",
    "- odds are probably the easiest way to do this.\n",
    "\n",
    "Problem: is that the range is restricted and we don’t want a restricted range because if we do so then our correlation will decrease.\n",
    "- By restricting the range we are actually decreasing the number of data points and if we decrease our data points, our correlation will decrease.\n",
    "- Making it difficult to model a variable that has a restricted range.\n",
    "\n",
    "Control:\n",
    "- Control this we take the log of odds which has a range from (-∞,+∞)\n",
    "\n",
    "$ \\log(\\frac{P}{1-P}) = 𝛽_{0}+𝛽_{1}𝑥_{1}$\n",
    "\n",
    "Now we just want a function of P because we want to predict probability not log of odds. To do so we will \n",
    "- multiply by exponent on both sides and then solve for P.\n",
    "\n",
    "$ \\exp[\\log(\\frac{P}{1-P})] = \\exp(𝛽_{0}+𝛽_{1}𝑥_{1})$\n",
    "\n",
    "$ \\exp^{\\ln[\\frac{P}{1-P})} = \\exp^{(𝛽_{0}+𝛽_{1}𝑥_{1})} $\n",
    "\n",
    "$ \\frac{P}{1-P} = \\exp^{(𝛽_{0}+𝛽_{1}𝑥_{1})} $\n",
    "\n",
    "$ p = \\exp^{(𝛽_{0}+𝛽_{1}𝑥_{1})}  - p\\exp^{(𝛽_{0}+𝛽_{1}𝑥_{1})}$\n",
    "\n",
    "Now we have sigmoid function.\n",
    "\n",
    "Model Equation: \n",
    "$ 𝑃(𝑦=1)= \\frac{1}{1+𝑒^{−(𝛽_{0}+𝛽_{1}𝑥_{1}+…+𝛽_{𝑛}𝑥_{𝑛}})}$\n",
    "\n",
    "It squeezes a straight line into an S-curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2893d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the sigmoid function\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    The sigmoid function maps log-odds to probabilities between 0 and 1.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Function to calculate odds and log-odds\n",
    "def logistic_regression_predict(X, coefficients):\n",
    "    \"\"\"\n",
    "    Predict probabilities, odds, and log-odds using logistic regression.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Feature matrix (numpy array of shape [n_samples, n_features])\n",
    "    - coefficients: Coefficients including intercept (numpy array of shape [n_features + 1])\n",
    "    \n",
    "    Returns:\n",
    "    - probabilities: Predicted probabilities (numpy array of shape [n_samples])\n",
    "    - odds: Odds of event occurring (numpy array of shape [n_samples])\n",
    "    - log_odds: Log-Odds (numpy array of shape [n_samples])\n",
    "    \"\"\"\n",
    "    # Add intercept to the feature matrix\n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add a column of ones for the intercept\n",
    "    \n",
    "    # Calculate log-odds (z = X * coefficients)\n",
    "    log_odds = np.dot(X, coefficients)\n",
    "    \n",
    "    # Calculate probabilities using the sigmoid function\n",
    "    probabilities = sigmoid(log_odds)\n",
    "    \n",
    "    # Calculate odds\n",
    "    #  Derived from probabilities using the formula\n",
    "    odds = probabilities / (1 - probabilities)\n",
    "    \n",
    "    return probabilities, odds, log_odds\n",
    "\n",
    "# Example usage\n",
    "# Example dataset: X contains two features, and coefficients include intercept and weights\n",
    "X = np.array([[2, 3], [1, 0], [4, 5]])  # Feature matrix\n",
    "coefficients = np.array([-3, 0.5, 1])  # Coefficients (intercept + weights for features)\n",
    "\n",
    "# Predict probabilities, odds, and log-odds\n",
    "probabilities, odds, log_odds = logistic_regression_predict(X, coefficients)\n",
    "\n",
    "# Predicted Probabilities: Likelihood of the event occurring.\n",
    "# Odds: Ratio of the probability of success to failure.\n",
    "# Log-Odds: Linear transformation of the predictors.\n",
    "\n",
    "# Print results\n",
    "print(\"Predicted Probabilities:\", probabilities)\n",
    "print(\"Odds:\", odds)\n",
    "print(\"Log-Odds:\", log_odds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79dd7e3",
   "metadata": {},
   "source": [
    "**log-odds linear function**\n",
    "\n",
    "The log-odds linear function is a core concept in logistic regression and represents the relationship between the independent variables (predictors) and the log-odds of the dependent variable (outcome).\n",
    "\n",
    "$$ \\log(\\frac{P}{1-P}) = 𝛽_{0}+𝛽_{1}𝑥_{1}+𝛽_{2}𝑥_{2}+...+ +𝛽_{p}𝑥_{p}$$\n",
    "\n",
    "Where:\n",
    "- $𝛽_{0}$: Intercept (bias term).\n",
    "- $𝛽_{1}, 𝛽_{2},..., 𝛽_{p}$: Coefficients of the predictors $x_{1}, x_{2},..., x_{p}$\n",
    "- $x_{1}, x_{2},..., x_{p}$: Values of the independent variables.\n",
    "= $P$: Predicted probability of the event occurring.\n",
    "\n",
    "Steps to Calculate Log-Odds\n",
    "1. Start with the linear combination: Compute a weighted sum of the predictors and the intercept:\n",
    "\n",
    "$$ z = 𝛽_{0}+𝛽_{1}𝑥_{1}+𝛽_{2}𝑥_{2}+...+ +𝛽_{p}𝑥_{p}$$\n",
    "\n",
    "2. Interpret z as the log-odds: The value z is the log-odds, which can be converted to:\n",
    "- Odds using: \n",
    "$$ odds = e^z $$\n",
    "- Probability using the sigmoid function:\n",
    "$$ P= \\frac{1}{1+e^z}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1384637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate log-odds\n",
    "def calculate_log_odds(intercept, coefficients, predictors):\n",
    "    \"\"\"\n",
    "    Calculate log-odds for logistic regression.\n",
    "\n",
    "    Parameters:\n",
    "    - intercept: Intercept term (beta_0)\n",
    "    - coefficients: Coefficients for the predictors (list or array)\n",
    "    - predictors: Values of the predictors (list or array)\n",
    "\n",
    "    Returns:\n",
    "    - log_odds: Computed log-odds\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays\n",
    "    coefficients = np.array(coefficients)\n",
    "    predictors = np.array(predictors)\n",
    "    \n",
    "    # Compute log-odds\n",
    "    log_odds = intercept + np.dot(coefficients, predictors)\n",
    "    return log_odds\n",
    "\n",
    "# Example inputs\n",
    "intercept = -2\n",
    "coefficients = [0.8, -1.2]  # Beta coefficients\n",
    "predictors = [3, 5]         # Predictor values (x_1, x_2)\n",
    "\n",
    "# Calculate log-odds\n",
    "log_odds = calculate_log_odds(intercept, coefficients, predictors)\n",
    "print(\"Log-Odds:\", log_odds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6cf053",
   "metadata": {},
   "source": [
    "**Calculate class probabilities in logistic regression**\n",
    "\n",
    "the logistic (sigmoid) function is used to transform the log-odds into probabilities. \n",
    "- The logistic function ensures the probabilities range between 0 and 1, making it suitable for classification problems\n",
    "\n",
    "Logistic Function for Probability\n",
    "\n",
    "$$ P= \\frac{1}{1+e^z}$$\n",
    "\n",
    "Where:\n",
    "- P: Probability of the positive class (class 1).\n",
    "- z: Log-odds, calculated as:\n",
    "\n",
    "$$ z = 𝛽_{0}+𝛽_{1}𝑥_{1}+𝛽_{2}𝑥_{2}+...+ +𝛽_{p}𝑥_{p}$$\n",
    "\n",
    "- z is the weighted sum of the predictors and the intercept.\n",
    "\n",
    "The logistic function outputs:\n",
    "- P: Probability of the positive class (class 1).\n",
    "- 1−P: Probability of the negative class (class 0).\n",
    "\n",
    "Steps to Calculate Class Probability\n",
    "1. Calculate Log-Odds (z): Compute the linear combination of the intercept ($𝛽_0$) and the predictor variables.\n",
    "2. Apply the Logistic Function: \n",
    "Use the formula: \n",
    "$$ P= \\frac{1}{1+e^z}$$\n",
    "\n",
    "3. Interpret the Result:\n",
    "- If P≥0.5, classify the observation as the positive class (class 1).\n",
    "- If P<0.5, classify the observation as the negative class (class 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid (logistic) function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Function to calculate probability\n",
    "def calculate_probability(intercept, coefficients, predictors):\n",
    "    \"\"\"\n",
    "    Calculate class probability using the logistic function.\n",
    "\n",
    "    Parameters:\n",
    "    - intercept: Intercept term (beta_0)\n",
    "    - coefficients: Coefficients for predictors (list or array)\n",
    "    - predictors: Values of predictors (list or array)\n",
    "\n",
    "    Returns:\n",
    "    - probability: Probability of the positive class (class 1)\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays\n",
    "    coefficients = np.array(coefficients)\n",
    "    predictors = np.array(predictors)\n",
    "    \n",
    "    # Calculate log-odds\n",
    "    log_odds = intercept + np.dot(coefficients, predictors)\n",
    "    \n",
    "    # Apply sigmoid function to get probability\n",
    "    probability = sigmoid(log_odds)\n",
    "    return probability\n",
    "\n",
    "# Example inputs\n",
    "intercept = -2\n",
    "coefficients = [0.8, -1.2]  # Beta coefficients\n",
    "predictors = [3, 5]         # Predictor values (x_1, x_2)\n",
    "\n",
    "# Calculate class probability\n",
    "probability = calculate_probability(intercept, coefficients, predictors)\n",
    "print(\"Class Probability (P for class 1):\", probability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176fc5fc",
   "metadata": {},
   "source": [
    "### Decision boundary in logistic regression\n",
    "\n",
    "The decision boundary in logistic regression is the threshold at which the model predicts one class over the other. It represents the dividing line (or surface in higher dimensions) between the predicted classes in the feature space.\n",
    "\n",
    "Key Points about Decision Boundary in Logistic Regression\n",
    "1. Sigmoid Function and Threshold:\n",
    "- Logistic regression uses the sigmoid function to output probabilities between 0 and 1.\n",
    "- A commonly used threshold is 0.5\n",
    "    - If P≥0.5, classify as class 1 (positive class).\n",
    "    - If P<0.5, classify as class 0 (negative class).\n",
    "\n",
    "2. Log-Odds and Decision Boundary:\n",
    "- The decision boundary corresponds to where the log-odds (z) equals zero.\n",
    "- At z=0:\n",
    "\n",
    "$$ P= \\frac{1}{1+e^z}$$\n",
    "$$ P= \\frac{1}{1+e^0}$$\n",
    "$$ P= \\frac{1}{2}$$\n",
    "$$ P= 0.5$$\n",
    "\n",
    "- Thus, the decision boundary is the set of points where z=0, or equivalently:\n",
    "\n",
    "$$ z = 𝛽_{0}+𝛽_{1}𝑥_{1}+𝛽_{2}𝑥_{2}+...+ +𝛽_{p}𝑥_{p} = 0$$\n",
    "\n",
    "3. Geometric Interpretation:\n",
    "- In 2D (one predictor): The decision boundary is a line.\n",
    "- In 3D (two predictors): The decision boundary is a plane.\n",
    "- In higher dimensions: The decision boundary is a hyperplane.\n",
    "\n",
    "4. Linear Nature of Decision Boundary:\n",
    "- Logistic regression assumes a linear relationship between the predictors and the log-odds.\n",
    "- The decision boundary is linear unless the model is extended with non-linear transformations of the predictors (e.g., polynomial features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3791eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Coefficients\n",
    "beta_0 = -2      # Intercept\n",
    "beta_1 = 0.8     # Coefficient for x1\n",
    "beta_2 = -1.2    # Coefficient for x2\n",
    "\n",
    "# Generate a range of x1 values\n",
    "x1 = np.linspace(-10, 10, 100)\n",
    "\n",
    "# Calculate x2 for the decision boundary\n",
    "x2 = (-beta_0 - beta_1 * x1) / beta_2\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x1, x2, label=\"Decision Boundary\", color=\"red\")\n",
    "\n",
    "# Add some random points for class 0 and class 1\n",
    "np.random.seed(42)\n",
    "class_0 = np.random.multivariate_normal([3, 3], [[2, 1], [1, 2]], size=50)\n",
    "class_1 = np.random.multivariate_normal([-3, -3], [[2, 1], [1, 2]], size=50)\n",
    "\n",
    "plt.scatter(class_0[:, 0], class_0[:, 1], label=\"Class 0\", color=\"blue\", alpha=0.7)\n",
    "plt.scatter(class_1[:, 0], class_1[:, 1], label=\"Class 1\", color=\"green\", alpha=0.7)\n",
    "\n",
    "# Formatting\n",
    "plt.axhline(0, color=\"black\", linewidth=0.5, linestyle=\"--\")\n",
    "plt.axvline(0, color=\"black\", linewidth=0.5, linestyle=\"--\")\n",
    "plt.title(\"Decision Boundary of Logistic Regression\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018e34e",
   "metadata": {},
   "source": [
    "### Key properties of the logistic regression equation\n",
    "\n",
    "Expalin the Logistic regression model\n",
    "\n",
    "Sigmoid Function:\n",
    "- uses a special “S” shaped curve to predict probabilities. It ensures that the predicted probabilities stay between 0 and 1.\n",
    "\n",
    "Straightforward Relationship:\n",
    "- relationship between our inputs and the outcome is like drwing a straight line but a curve is there instead.\n",
    "\n",
    "Coefficients / parameters:\n",
    "- numbers that tell us how much each input affects the outcome in the logistic regression model.\n",
    "- coefficient tells us how much the outcome changes for every one unit increase in predictor variable.\n",
    "\n",
    "Best Guess: \n",
    "- Figure out the best coefficients for the logistic regression model by looking at the data we have and tweaking them until our predictions match the real outcomes as closely as possible.\n",
    "\n",
    "Basic Assumptions:\n",
    "- We assume that our observations are independent, meaning one doesn’t affect the other. \n",
    "- We assume that there’s not too much overlap between our predictors (like age and height), \n",
    "- We assume the relationship between our predictors and the outcome is kind of like a straight line.\n",
    "\n",
    "Probabilities, Not Certainties:\n",
    "- Logistic regression gives us probabilities.\n",
    "- Then decide on a cutoff point to make our final decision.\n",
    "\n",
    "Checking Our Work:\n",
    "- We make sure our predictions are good, like \n",
    "    - accuracy, \n",
    "    - precision, \n",
    "    - recall,\n",
    "    - ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "data = pd.read_csv('data.csv') # read data from csv file\n",
    "X = data[['Independent_Var_1', 'Independent_Var_2', 'Independent_Var_3']] # select independent variables\n",
    "Y = data['Dependent_Var'] # select dependent variable\n",
    "\n",
    "# Add a constant to the independent variable set\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = sm.Logit(Y, X).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3daca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train = data[:800]\n",
    "test = data[800:]\n",
    "\n",
    "# Define the independent variables\n",
    "X_train = train[['age', 'gender', 'income']]\n",
    "X_test = test[['age', 'gender', 'income']]\n",
    "\n",
    "# Define the dependent variable\n",
    "y_train = train['buy_product']\n",
    "y_test = test['buy_product']\n",
    "\n",
    "# Fit the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the outcomes for the test data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa885e4",
   "metadata": {},
   "source": [
    "**Step 5: Interpret Coefficients and Evaluate the Model**\n",
    "\n",
    "- Log Odds: Each coefficient represents the change in log odds of readmission for a unit increase in the predictor.\n",
    "- Odds Ratios: Use np.exp(model.params) to convert coefficients to odds ratios.\n",
    "\n",
    "1. Accuracy\n",
    "2. Confusion Matrix\n",
    "3. ROC Curve and AUC\n",
    "\n",
    "**The Role of Deviance in a Logistic Regression Model**\n",
    "- In logistic regression, deviance measures how well the model fits the data. It is similar to the Residual Sum of Squares (RSS)\n",
    "    - but is used for models where the dependent variable is categorical (e.g., binary classification).\n",
    "- In linear regression It is similar to the Residual Sum of Squares (RSS).\n",
    "\n",
    "What is Deviance?\n",
    "- Deviance is derived from the log-likelihood function and quantifies how much error remains after fitting the model. It is given by:\n",
    "$$Deviance=−2×log(Likelihood)$$\n",
    "- A lower deviance means the model fits the data better.\n",
    "- A higher deviance indicates poor model fit.\n",
    "\n",
    "In logistic regression, we use two types of deviance:\n",
    "- Null Deviance (Baseline Model):\n",
    "    - The deviance of a model with no predictors (only an intercept).\n",
    "    - It assumes that all observations are predicted using the mean response.\n",
    "\n",
    "- Residual Deviance (Full Model):\n",
    "    - The deviance of the fitted model with predictors.\n",
    "    - Lower residual deviance means the model explains more of the variability in the data.\n",
    "\n",
    "Deviance Reduction:\n",
    "$$Deviance Reduction=Null Deviance−Residual Deviance$$\n",
    "- The bigger this difference, the better the model explains the data.\n",
    "\n",
    "\n",
    "Deviance as a Goodness-of-Fit Measure\n",
    "- In logistic regression, deviance plays a role similar to RSS in linear regression:\n",
    "\n",
    "| Regression Type | Error Metric   | Purpose |\n",
    "|---------------|--------|----------------------------------|  \n",
    "|Linear Regression\t| Residual Sum of Squares (RSS) |Measures total squared errors|\n",
    "|Logistic Regression\t| Deviance |Measures log-likelihood (fit quality)  |\n",
    "\n",
    "- A large difference between null and residual deviance suggests a good model.\n",
    "- A small difference means the predictors add little value to the model.\n",
    "\n",
    "Using Deviance for Model Comparison\n",
    "Likelihood Ratio Test (LRT) Using Deviance\n",
    "To compare two nested models (one with fewer predictors, one with more), we use:\n",
    "$$𝐺^2 = Deviance_{simpler model} − Deviance_{full model}$$\n",
    "- If $𝐺^2$ is large, the extra predictors significantly improve the model.\n",
    "- If $𝐺^2$ is small, the added predictors may be unnecessary.\n",
    "- A Chi-square test can determine statistical significance.\n",
    "\n",
    "✅ Key Insight: Deviance helps evaluate whether additional predictors significantly improve the logistic regression model.\n",
    "\n",
    "- Null Deviance: Deviance of a model with only the intercept (baseline model).\n",
    "- Residual Deviance: Deviance of the fitted model (with predictors).\n",
    "- Lower Deviance: Means a better fit to the data.\n",
    "- Deviance Difference: Helps measure how much the predictors improve the model.\n",
    "- Likelihood Ratio Test: Compares models to determine if added predictors are useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c803f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Select relevant features\n",
    "df = df[['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
    "\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Define predictors (X) and target (y)\n",
    "X = df[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
    "y = df['Survived']\n",
    "\n",
    "# Add intercept term (required for statsmodels logistic regression)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Step 2: Fit Logistic Regression Model & Compute Deviance\n",
    "# compare null deviance (baseline model) and residual deviance (fitted model).\n",
    "# Fit logistic regression model\n",
    "model = sm.Logit(y, X).fit()\n",
    "\n",
    "# Print model summary (includes deviance values)\n",
    "print(model.summary())\n",
    "\n",
    "# Extract null deviance & residual deviance\n",
    "null_deviance = model.llnull * -2  # Deviance of model with only intercept\n",
    "residual_deviance = model.llf * -2  # Deviance of fitted model\n",
    "\n",
    "# Compute deviance reduction\n",
    "deviance_reduction = null_deviance - residual_deviance\n",
    "\n",
    "# Print results\n",
    "print(f\"Null Deviance: {null_deviance:.4f}\")\n",
    "print(f\"Residual Deviance: {residual_deviance:.4f}\")\n",
    "print(f\"Deviance Reduction: {deviance_reduction:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185891ab",
   "metadata": {},
   "source": [
    "Step 3: Interpret Results\n",
    "- Null Deviance → Deviance of the model with only the intercept.\n",
    "- Residual Deviance → Deviance after adding predictors.\n",
    "- Deviance Reduction → Improvement in model fit.\n",
    "    - Higher reduction → Predictors add significant value.\n",
    "    - Small reduction → Predictors may not improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1321556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Likelihood Ratio Test for Model Comparison\n",
    "# Compare the full model with a reduced model (fewer predictors).\n",
    "# Reduced model (fewer predictors)\n",
    "X_reduced = df[['Pclass', 'Age']]  # Use only Pclass & Age as predictors\n",
    "X_reduced = sm.add_constant(X_reduced)\n",
    "\n",
    "# Fit reduced model\n",
    "reduced_model = sm.Logit(y, X_reduced).fit()\n",
    "\n",
    "# Compute likelihood ratio test statistic\n",
    "G2 = (reduced_model.llf - model.llf) * -2\n",
    "p_value = 1 - sm.stats.chisqprob(G2, df=X.shape[1] - X_reduced.shape[1])\n",
    "\n",
    "# Print results\n",
    "print(f\"Likelihood Ratio Test Statistic (G2): {G2:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Interpretation:\n",
    "# If G2 is large & p-value < 0.05, the additional predictors significantly improve the model.\n",
    "# If G2 is small & p-value > 0.05, the additional predictors may not be useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c936983",
   "metadata": {},
   "source": [
    "**Step 6: Optimisation**\n",
    "\n",
    "### Cost Function in Logistic Regression\n",
    "\n",
    "Linear regression, uses the Mean squared error which was the difference between y_predicted and y_actual\n",
    "- this is derived from the **maximum likelihood estimator**.\n",
    "\n",
    "logistic regression $Yi$ is a non-linear function ($ Ŷ= \\frac{1}​{1+ e-z}$).\n",
    "- If we use this in the above MSE equation then it will give a non-convex graph with many local minima.\n",
    "\n",
    "Problem: cost function will give results with local minima\n",
    "- End up miss out on our global minima and our error will increase.\n",
    "\n",
    "Solution: derive a different cost function for logistic regression\n",
    "- **log loss** which is also derived from the **maximum likelihood estimation method**.\n",
    "\n",
    "$ Log Loss = \\frac{1}{N} \\sum^{N}_{i = 1} - ( y_i * \\log(Y_i) + (1 - y_i) * log (1 - Y_i))$\n",
    "\n",
    "#### Maximum likelihood estimator\n",
    "\n",
    "Primary Objctive:\n",
    "- is to identify parameter values that maximize the likelihood function.\n",
    "- it represents the joint probability density function (pdf) of our sample observations.\n",
    "- it involves multiplying the conditional probabilities for observing each example given the distribution parameters.\n",
    "- this process aims to discover parameter values such that, when plugged into the model for P(x), it produces a value close to one for individuals with a predicted outcome and close to zero for those with a predicted outcome.\n",
    "\n",
    "Start by defining our likelihood function. \n",
    "- We now know that the labels are binary\n",
    "- we have two outcomes success and failure. \n",
    "- This means we can interpret each label as Bernoulli random variable.\n",
    "\n",
    "**Random experiment** whose outcomes are of two types, success S and failure F, occurring with probabilities p and q respectively is called a Bernoulli trial. If for this experiment a random variable X is defined such that it takes value 1 when S occurs and 0 if F occurs, then X follows a Bernoulli Distribution.\n",
    "\n",
    "#### Math behind this log loss function\n",
    "\n",
    "$ Y ~ Ber(P)$\n",
    "\n",
    "Where P is our sigmoid function\n",
    "\n",
    "$ P[Y=y | X=x] = \\sigma ( \\theta^{T} x^i)^y (1 - \\sigma(\\theta^{T} x^i))^{1-y} $\n",
    "\n",
    "where σ(θ^T*x^i) is the sigmoid function. Now for n observations\n",
    "\n",
    "$ L(\\theta) = \\prod^{n}_{1} \\sigma ( \\theta^{T} x^i)^y (1 - \\sigma(\\theta^{T} x^i))^{1-y} $\n",
    "\n",
    "We need a value for theta which will maximize this likelihood function. \n",
    "\n",
    "To make our calculations easier\n",
    "- we multiply the log on both sides. \n",
    "\n",
    "The function we get is also called the \n",
    "- log-likelihood function or \n",
    "- sum of the log conditional probability\n",
    "\n",
    "$ \\log(L(\\theta)) = \\sum^{n}_{1} * \\log[\\sigma ( \\theta^{T} x^i)] + (1-y) * \\log(1 - \\sigma(\\theta^{T} x^i)] $\n",
    "\n",
    "In ML, it is conventional to minimize a loss(error) function via gradient descent, rather than maximize an objective function via gradient ascent. \n",
    "- If we maximize this above function then we’ll have to deal with gradient ascent to avoid this we take negative of this log so that we use gradient descent.\n",
    "\n",
    "$ max[log(x)] = min[-log(x)] $\n",
    "\n",
    "The negative of this function is our cost function and what do we want with our cost function? That it should have a minimum value. \n",
    "It is common practice to minimize a cost function for optimization problems; therefore, we can invert the function so that we minimize the negative log-likelihood (NLL).\n",
    "\n",
    "$ - \\log(L(\\theta)) =  -\\sum^{n}_{1} * \\log[\\sigma ( \\theta^{T} x^i)] + (1-y) * \\log(1 - \\sigma(\\theta^{T} x^i)] $\n",
    "\n",
    "where \n",
    "- y represents the actual class and \n",
    "    - p(y) is the probability of 1.\n",
    "- log(σ(θ^T*x^i) ) is the probability of that class.\n",
    "    - 1-p(y) is the probability of 0.\n",
    "\n",
    "Get graph of cost function when y=1 and y=0.\n",
    "- By getting a convex graph with only 1 local minimum and now it’ll be easy to use gradient descent.\n",
    "    - red line here represents the 1 class (y=1), the right term of cost function will vanish. Now if the predicted probability is close to 1 then our loss will be less and when probability approaches 0, our loss function reaches infinity.\n",
    "    - black line represents 0 class (y=0), the left term will vanish in our cost function and if the predicted probability is close to 0 then our loss function will be less but if our probability approaches 1 then our loss function reaches infinity.\n",
    "\n",
    "$ Cost(h_{\\Theta}(x),y) = \\left\\{ \\begin{array}{rcl} - \\log(h_{\\Theta}(x)) if y = 1\\\\ - \\log(1 - h_{\\Theta}(x)) if y = 0 \\end{array}\\right.$\n",
    "\n",
    "Cost function is also called **log loss**\n",
    "\n",
    "It also ensures that as the\n",
    "- probability of the correct answer is maximized, \n",
    "- probability of the incorrect answer is minimized. \n",
    "    - Lower the value of this cost function higher will be the accuracy.\n",
    "\n",
    "### Gradient Descent Optimization\n",
    "\n",
    "How to use Gradient Descent to compute the minimum cost.\n",
    "\n",
    "- Gradient descent changes the value of our weights in such a way that it always converges to minimum point\n",
    "    - it aims at finding the optimal weights which minimize the loss function of our model.\n",
    "Gradient descent is an iterative method that finds the minimum of a function by figuring out the slope at a random point and then moving in the opposite direction.\n",
    "\n",
    "At first \n",
    "- gradient descent takes a random value of our parameters from our function. \n",
    "- need an algorithm that will tell us whether at the next iteration we should move left or right to reach the minimum point.\n",
    "    - The gradient descent algorithm \n",
    "        - finds the slope of the loss function at that particular point and then \n",
    "In the next iteration, \n",
    "- it moves in the opposite direction to reach the minima.\n",
    "\n",
    "Since we have a convex graph now we don’t need to worry about local minima. \n",
    "    - A convex curve will always have only 1 minima.\n",
    "\n",
    "Gradient descent algorithm\n",
    "\n",
    "$ \\theta_{new} = \\theta_{old} - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j} $\n",
    "\n",
    "where alpha is known as the learning rate. \n",
    "- It determines the step size at each iteration while moving towards the minimum point. \n",
    "    - a lower value of “alpha” is preferred, because if the learning rate is a big number then we may miss the minimum point and keep on oscillating in the convex curve.\n",
    "\n",
    "#### Derivation of Cost Function\n",
    "Derive this cost function w.r.t our parameters.\n",
    "\n",
    "$\n",
    "\\frac{d\\sigma(x)}{dx} = \\frac{d}{dx} \\left( \\frac{1}{1+e^{-x}} \\right) = \\frac{d}{dx} \\left( 1 + e^{-x} \\right)^{-1} $\n",
    "\n",
    "$\\Rightarrow -\\left(1 + e^{-x}\\right)^{-2} \\times \\frac{d}{dx} \\left(1 + e^{-x}\\right)$\n",
    "\n",
    "$\\Rightarrow -\\left(1 + e^{-x}\\right)^{-2} \\times \\left[ 0 + \\frac{d}{dx} \\left(e^{-x}\\right) \\right]$\n",
    "\n",
    "$\\Rightarrow -\\left(1 + e^{-x}\\right)^{-2} \\times \\left[e^{-x} \\times \\frac{d}{dx}(-x) \\right]$\n",
    "\n",
    "$\\Rightarrow -\\left(1 + e^{-x}\\right)^{-2} \\times \\left[e^{-x} \\times (-1) \\right]$\n",
    "\n",
    "$\\Rightarrow e^{-x} \\left(1 + e^{-x}\\right)^{-2}$\n",
    "\n",
    "$\\Rightarrow \\frac{e^{-x}}{(1+e^{-x})^2} = \\frac{e^{-x} + 1 - 1}{(1+e^{-x})(1+e^{-x})}$\n",
    "\n",
    "$\\Rightarrow \\frac{(1+e^{-x}) - 1}{(1+e^{-x})(1+e^{-x})} = \\frac{1}{(1+e^{-x})} \\left[ \\frac{(1+e^{-x})}{(1+e^{-x})} - \\frac{1}{(1+e^{-x})} \\right]$\n",
    "\n",
    "$\\Rightarrow \\frac{1}{(1+e^{-x})} \\left[ 1 - \\frac{1}{(1+e^{-x})} \\right]$\n",
    "\n",
    "Derive the cost function with the help of the chain rule as it allows us to calculate complex partial derivatives by breaking them down.\n",
    "\n",
    "**Step-1: Use chain rule and break the partial derivative of log-likelihood**\n",
    "\n",
    "$-\\frac{\\partial LL(\\theta)}{\\partial \\theta_j} = -\\frac{\\partial LL(\\theta)}{\\partial p} \\cdot \\frac{\\partial p}{\\partial \\theta} \\quad$\n",
    "$\\text{where } p= \\sigma\\left[\\theta^\\top x\\right]$\n",
    "\n",
    "$= -\\frac{\\partial LL(\\theta)}{\\partial p} \\cdot \\frac{\\partial p}{\\partial z} \\cdot \\frac{\\partial z}{\\partial \\theta_j} \\quad $\n",
    "$\\text{where } z =\\theta^\\top x$\n",
    "\n",
    "**Step-2: Find derivative of log-likelihood w.r.t p**\n",
    "\n",
    "We know,\n",
    "\n",
    "$LL(\\theta) = y \\log(p) + (1-y)\\log(1-p) \\quad \\text{where } p = \\sigma\\left[\\theta^\\top x\\right]$\n",
    "\n",
    "$\\frac{\\partial LL(\\theta)}{\\partial p} = \\frac{y}{p} + \\frac{(1-y)}{(1-p)}$\n",
    "\n",
    "**Step-3: Find derivative of ‘p’ w.r.t ‘z’**\n",
    "\n",
    "$ p= \\sigma(z)$\n",
    "\n",
    "$\\frac{\\partial p}{\\partial z} = \\frac{\\partial[ \\sigma (z)]}{\\partial z}$\n",
    "\n",
    "We know the derivative of sigmoid function is $\\sigma[\\theta^\\top x][1 - \\sigma(\\theta^\\top x)]$\n",
    "\n",
    "$\\Rightarrow \\frac{\\partial p}{\\partial z} =  \\sigma [z][1 - \\sigma(z)]$\n",
    "\n",
    "**Step-4: Find derivate of z w.r.t θ**\n",
    "\n",
    "$ z=\\theta^\\top x$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial \\theta_j} = x_j$\n",
    "\n",
    "**Step-5: Put all the derivatives in equation 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X) > 0.5\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc749e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a4e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y, model.predict(X))\n",
    "auc = roc_auc_score(y, model.predict(X))\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda6f67",
   "metadata": {},
   "source": [
    "**Understanding Factors Significantly Influencing Readmission**\n",
    "\n",
    "1. Use p-values from the logistic regression summary:\n",
    "- Predictors with $𝑝< 0.05$ are statistically significant.\n",
    "2. Assess the odds ratios:\n",
    "- For example, if the odds ratio for LOS is 2.0, each additional day in the hospital doubles the odds of readmission.\n",
    "3. Visualize relationships:\n",
    "- Plot odds ratios for key predictors to present to stakeholders.\n",
    "\n",
    "**Statistical Hypothesis Testing**\n",
    "\n",
    "Example 1: Relationship Between LOS and Readmission\n",
    "- Hypotheses:\n",
    "    - $𝐻_0$: LOS has no effect on readmission.\n",
    "    - $𝐻_𝑎$: LOS has a significant effect on readmission.\n",
    "- Approach: Perform a logistic regression test and check the p-value for LOS.\n",
    "\n",
    "Example 2: Age Group vs. Readmission\n",
    "- Hypotheses:\n",
    "    - $𝐻_0$: Age group is independent of readmission.\n",
    "    - $𝐻_𝑎$: Age group and readmission are dependent.\n",
    "- Approach: Use a Chi-Square test of independence (see previous example).\n",
    "\n",
    "**Actionable Insights**\n",
    "- Highlight key factors significantly influencing readmission (e.g., LOS, medication adherence).\n",
    "- Use odds ratios to explain how much each factor increases or decreases the likelihood of readmission.\n",
    "- Present findings visually (e.g., bar charts for odds ratios, ROC curves for model performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3375f1b8",
   "metadata": {},
   "source": [
    "# Variables and Variable Selection\n",
    "\n",
    "Learn how to:\n",
    "\n",
    "- Differentiate between Variable Types and Dummy Variables;\n",
    "- Select features based on correlation;\n",
    "- Select features based on variance thresholds.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "**Variables** are the basic building blocks of datasets. \n",
    "- The quality of the variables present within your dataset has a direct impact on the intuition and overall outcome of your machine learning model. \n",
    "\n",
    "**Variable selection** and an in-depth knowledge of the domain you're building your model in remains essential when developing a predictive model.\n",
    "\n",
    "The purpose of regression is essentially to build associations between multiple variables. \n",
    "- Variable selection involves the \n",
    "    - elimination of input variables which may in turn reduce the computational cost of modeling \n",
    "    - improve the performance of the model. \n",
    "\n",
    "The model is structured around the belief that one of the variables in our dataset is a dependent variable (DV), that is explained or predicted in some way by the other independent variables (IVs). In this sense we work with: \n",
    "\n",
    "**Input variables** - are referred to as the independent variables (IVs) and used to explain or predict the target variable\n",
    "\n",
    "**Target variable** - are referred to as the dependent variable (DV) and is the target variable you want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f6c343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns have white space that we want to replace with an underscore (to avoid using the column names as variable names later on)\n",
    "df.columns = [col.replace(\" \",\"_\") for col in df.columns] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5d5b3f",
   "metadata": {},
   "source": [
    "##### Perfom preliminary data preprocessing\n",
    "\n",
    "to build some relationship between variables that are likely to indicate the dependent variable outcome once someone has taken a positive outcome (taken a loan), we really only want to consider instances (customers) who actually are on the positive predictive outcome (took personal loan) to build this relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e9549",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Personal_Loan'] == 1]\n",
    "df = df.drop(['Personal_Loan'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd704a80",
   "metadata": {},
   "source": [
    "##### Varaible types\n",
    "\n",
    "`df.info()` specifically outputs the number of non-null entries in each column. \n",
    "- We can be certain that our data has missing values if columns have a varying number of non-null entries.\n",
    "\n",
    "`df.describe()` show the summary statistics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4723f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6935c709",
   "metadata": {},
   "source": [
    "### Dummy Variables in Regression Models\n",
    "Dummy variables are artificial variables created to represent categorical data (data that can take on a limited number of distinct values) in a numerical format that a regression model can process. \n",
    "\n",
    "Regression models work with numerical data, dummy variables allow us to include categorical variables in the analysis by converting categories into binary values (0 or 1).\n",
    "\n",
    "Why Use Dummy Variables?\n",
    "- Regression models require numerical inputs to calculate relationships between variables. \n",
    "- Dummy variables serve as a bridge between categorical predictors (e.g., gender, region, or treatment group) and the mathematical structure of regression models. \n",
    "    - By using dummy variables, you can incorporate the effects of categorical predictors into the model while preserving interpretability.\n",
    "\n",
    "#### Dummy Variable Encoding\n",
    "From summary statistics of our numerical categorical data ('Online', 'CD_Account', 'Securities_Account') , Little to No information gotten.\n",
    "\n",
    "NB, All input data for regression model building purposes needs to be numerical. \n",
    "\n",
    "Transform the text data (found within columns such as 'Education','Gender', and 'Area') into numbers before we can train our machine learning model.\n",
    "\n",
    "##### **Using qualitative variables with more than two levels in multiple regression**\n",
    "Requires transforming them into a numerical format that the regression model can understand. This is typically achieved through dummy coding, which creates binary variables for each category.\n",
    "\n",
    "Qualitative Variables with More than Two Levels:\n",
    "- Examples:\n",
    "    - Smoking Status: [\"Non-Smoker\", \"Current Smoker\", \"Former Smoker\"]\n",
    "    - Region: [\"Urban\", \"Suburban\", \"Rural\"]\n",
    "    - Blood Type: [\"A\", \"B\", \"AB\", \"O\"]\n",
    "- These are categorical variables that don’t have a natural numeric relationship or order.\n",
    "\n",
    "Dummy Variables:\n",
    "- For a categorical variable with k levels, we create k−1 dummy variables.\n",
    "- One category is used as the reference category (baseline), and the others are compared against it.\n",
    "\n",
    "Including Dummy Variables in Regression:\n",
    "- The regression equation incorporates these dummy variables as predictors.\n",
    "- The coefficients for the dummy variables represent the difference in the dependent variable (outcome) between each category and the reference category.\n",
    "\n",
    "##### Create Dummy Variables\n",
    "The process involves:\n",
    "1. Identify the Categorical Variable: \n",
    "- Select the categorical variable you want to include in your model (e.g., \"Region\" with categories: \"North,\" \"South,\" \"East,\" \"West\").\n",
    "2. Choose a Reference Category: \n",
    "- Pick one of the categories to act as the reference group. \n",
    "- The reference group will not have a separate dummy variable; its effect is captured in the intercept.\n",
    "3. Create Dummy Variables: \n",
    "- Create one binary variable for each of the remaining categories. Each variable will take the value:\n",
    "    - 1: if the observation belongs to that category.\n",
    "    - 0: otherwise.\n",
    "- To facilitate this transformation from textual-categorical data to numerical equivalents, \n",
    "    - use a pandas method called `get_dummies`. \n",
    "- The text data are categorical variables, and get_dummies will transform all the categorical text data into numbers by adding a column for each distinct category. \n",
    "    - The new column has a \n",
    "        - 1 for observations which were in this category, and a \n",
    "        - 0 for observations that were not.\n",
    "\n",
    "For example, the dataframe:\n",
    "\n",
    "| Dog Age | Breed      |\n",
    "|---------|------------|\n",
    "| 15      | \"Bulldog\"  |\n",
    "| 12      | \"Labrador\" |\n",
    "| 10      | \"Labrador\" |\n",
    "| 22      | \"Beagle\"   |\n",
    "| 9       | \"Labrador\" |\n",
    "\n",
    "\n",
    "After `pd.dummies` becomes:\n",
    "\n",
    "| Dog Age | Breed_Labrador | Breed_Bulldog | Breed_Beagle |\n",
    "|---------|----------------|---------------|--------------|\n",
    "| 15      | 1              | 0             | 0            |\n",
    "| 12      | 0              | 1             | 0            |\n",
    "| 10      | 1              | 0             | 0            |\n",
    "| 22      | 0              | 0             | 1            |\n",
    "| 9       | 1              | 0             | 0            |\n",
    "\n",
    "This is a process known as [Dummy Variable Encoding]\n",
    "- important step in preprocessing data for regression analysis\n",
    "\n",
    "##### Application in Regression\n",
    "Example 1: A Simple Model\n",
    "- Scenario:\n",
    "- We want to model BMI as a function of Smoking Status (categorical) and Age (continuous).\n",
    "    - Smoking Status has three levels: [\"Non-Smoker\", \"Current Smoker\", \"Former Smoker\"].\n",
    "- Imagine you are studying the relationship between patient BMI (body mass index) and a categorical variable, Smoking Status (\"Non-Smoker,\" \"Current Smoker,\" \"Former Smoker\").\n",
    "\n",
    "1. Define categories: \"Non-Smoker,\" \"Current Smoker,\" \"Former Smoker.\"\n",
    "2. Choose a reference group: \"Non-Smoker.\"\n",
    "3. Create dummy variables:\n",
    "- Create k−1= 3−1= 2 dummy variables:\n",
    "    - CurrentSmoker: 1 if \"Current Smoker,\" 0 otherwise.\n",
    "    - FormerSmoker: 1 if \"Former Smoker,\" 0 otherwise.\n",
    "\n",
    "Now the regression equation becomes:\n",
    "\n",
    "$$BMI = 𝛽_0 + 𝛽_1 \\cdot CurrentSmoker + 𝛽_2 \\cdot FormerSmoker + \\epsilon $$\n",
    "\n",
    "$$BMI = 𝛽_0 + 𝛽_1 \\cdot Age + 𝛽_2 \\cdot CurrentSmoker + 𝛽_3 \\cdot FormerSmoker + \\epsilon $$\n",
    "\n",
    "- Where:\n",
    "    - $𝛽_0$ is the average BMI for non-smokers (the reference group). / Predicted BMI for a non-smoker (reference category) when age = 0.\n",
    "    - $𝛽_1$: Effect of age on BMI, holding smoking status constant.\n",
    "    - $𝛽_2$ is the difference in BMI between current smokers and non-smokers, holding age constant.\n",
    "    - $𝛽_3$ is the difference in BMI between former smokers and non-smokers, holding age constant.\n",
    "\n",
    "Interpret Coefficients:\n",
    "- $𝛽_2$ > 0: Current smokers have higher BMI than non-smokers.\n",
    "- $𝛽_3$ < 0: Former smokers have lower BMI than non-smokers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee3ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Age': [25, 40, 35, 50, 60, 45],\n",
    "    'SmokingStatus': ['Non-Smoker', 'Current Smoker', 'Former Smoker', \n",
    "                      'Non-Smoker', 'Current Smoker', 'Former Smoker'],\n",
    "    'BMI': [22.5, 27.5, 24.0, 23.0, 28.0, 25.0]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Create dummy variables\n",
    "df = pd.get_dummies(df, columns=['SmokingStatus'], drop_first=True)\n",
    "\n",
    "# Inspect the transformed dataset\n",
    "print(df)\n",
    "\n",
    "# Step 2: Define predictors and target variable\n",
    "X = df[['Age', 'SmokingStatus_Current Smoker', 'SmokingStatus_Former Smoker']]\n",
    "y = df['BMI']\n",
    "\n",
    "# Add constant (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Step 3: Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Step 4: Summary of regression results\n",
    "print(model.summary())\n",
    "\n",
    "# ==============================================================================\n",
    "#                  Coef.    Std Err    t      P>|t|    [95% Conf. Interval]\n",
    "# ------------------------------------------------------------------------------\n",
    "# const           21.500    2.000     10.750  0.001    [16.000, 27.000]\n",
    "# Age              0.200    0.050     4.000   0.015    [ 0.075,  0.325]\n",
    "# SmokingStatus_CurrentSmoker    2.500    0.900     2.778   0.043    [ 0.050,  4.950]\n",
    "# SmokingStatus_FormerSmoker     1.800    0.950     1.895   0.080    [-0.400,  4.000]\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72205455",
   "metadata": {},
   "source": [
    "Intercept ($𝛽_0$)\n",
    "- The predicted BMI for a non-smoker when age = 0 (reference group). Here, 21.5.\n",
    "\n",
    "Age $𝛽_1$\n",
    "- The change in BMI for each additional year of age, holding smoking status constant\n",
    "- For each additional year of age, BMI increases by 0.2 units, regardless of smoking status.\n",
    "\n",
    "Smoking Status Variables ($𝛽_2$,$𝛽_3$)\n",
    "- SmokingStatus_CurrentSmoker $𝛽_2$ The average difference in BMI between current smokers and non-smokers (when Age = 0).\n",
    "    - Current smokers, on average, have a BMI 2.5 units higher than non-smokers, holding age constant.\n",
    "- SmokingStatus_FormerSmoker $𝛽_3$ The average difference in BMI between former smokers and non-smokers (when Age = 0).\n",
    "    - Former smokers, on average, have a BMI 1.8 units higher than non-smokers, but this difference is not statistically significant (p>0.05).\n",
    "\n",
    "Important Considerations\n",
    "- Choosing the Reference Category:\n",
    "    - Choose a category that makes interpretation easiest or is clinically relevant (e.g., \"Non-Smoker\").\n",
    "    - Changing the reference category will change the coefficients, but the overall model fit stays the same.\n",
    "- Collinearity:\n",
    "    - Dummy coding ensures that the sum of dummy variables doesn't equal 1 to avoid multicollinearity.\n",
    "- Interaction Terms:\n",
    "    - If you suspect that the relationship between age and BMI depends on smoking status, add interaction terms (e.g., Age × SmokingStatus).\n",
    "\n",
    "Applications in Healthcare\n",
    "- Treatment Type: [\"Placebo\", \"Drug A\", \"Drug B\"]\n",
    "- Disease Stage: [\"Mild\", \"Moderate\", \"Severe\"]\n",
    "- Hospital Region: [\"North\", \"South\", \"East\", \"West\"]\n",
    "\n",
    "Dummy variables allow you to measure the effects of these categorical predictors while controlling for other factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20502ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Age': [45, 34, 50, 29],\n",
    "    'Region': ['North', 'South', 'East', 'West'],\n",
    "    'BMI': [25.0, 27.5, 24.3, 22.8]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create dummy variables for the 'Region' column\n",
    "df_dummies = pd.get_dummies(df, columns=['Region'], drop_first=True)\n",
    "\n",
    "# Inspect the data\n",
    "print(df_dummies)\n",
    "\n",
    "# Regression model\n",
    "X = df_dummies[['Age', 'Region_South', 'Region_East', 'Region_West']]  # Predictors\n",
    "y = df_dummies['BMI']  # Target variable\n",
    "\n",
    "# Add constant for intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Summary of regression results\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f26ba8",
   "metadata": {},
   "source": [
    "### Linear regression models rely on several key assumptions, including additivity and linearity\n",
    "These ensure that the model captures relationships between variables appropriately.\n",
    "\n",
    "##### **1. Additive Assumption**\n",
    "The additive assumption means that the effect of each predictor variable on the dependent variable (outcome) is independent of the other predictors. In other words:\n",
    "- The predictors’ effects combine additively to determine the outcome.\n",
    "- There are no interactions or dependencies between predictor variables unless explicitly modeled.\n",
    "\n",
    "Mathematical Representation\n",
    "\n",
    "In a multiple linear regression model:\n",
    "$$ Y = 𝛽_0 + 𝛽_1 X_1 + 𝛽_2 X_2 + 𝛽_n X_n +\\epsilon $$\n",
    "- Each predictor ($X_i$) contributes independently to the dependent variable (Y).\n",
    "- The change in Y due to $X_1$ is unaffected by the values of other predictors ($X_1 + X_2 + X_n$).\n",
    "\n",
    "Violation of Additivity\n",
    "- If the predictors interact with each other, the additive assumption is violated.\n",
    "$$ y = 𝛽_0 + 𝛽_1 X_1 + 𝛽_2 X_2 + 𝛽_3 X_1 X_2 +\\epsilon $$\n",
    "- Here,\n",
    "    - the `interaction term` $X_1 X_2$ indicates that the effect of  $X_1$ on Y depends on the value of $X_2$, breaking the additivity assumption.\n",
    "\n",
    "##### **2. Linearity Assumption**\n",
    "The linearity assumption means that the relationship between the dependent variable (Y) and each predictor ($X_i$) is linear. This implies that:\n",
    "- The change in Y is proportional to the change in $X_i$, holding other predictors constant.\n",
    "- The regression equation captures this relationship as a straight line (or hyperplane in higher dimensions).\n",
    "\n",
    "Mathematical Representation\n",
    "\n",
    "For a single predictor:\n",
    "$$ Y = 𝛽_0 + 𝛽_1 X_1 +\\epsilon $$\n",
    "- The dependent variable Y is a linear function of $X_1$.\n",
    "- $β_1$ is the slope, indicating the constant rate of change in Y for a one-unit change in $X_1$.\n",
    "\n",
    "For multiple predictors:\n",
    "$$ Y = 𝛽_0 + 𝛽_1 X_1 + 𝛽_2 X_2 + 𝛽_n X_n +\\epsilon $$\n",
    "- Each predictor has a linear relationship with Y.\n",
    "\n",
    "##### Checking Additivity and Linearity in Practice\n",
    "Additivity:\n",
    "- Use interaction terms to test for non-additive effects.\n",
    "    - Example: Add $X_1 \\times X_2$ to the model to test whether the relationship between $X_1$ and Y depends on $X_2$.\n",
    "- Plot residuals against predictor variables to detect patterns that might indicate interaction effects.\n",
    "\n",
    "Linearity:\n",
    "- Residual plots:\n",
    "    - Plot residuals vs. predicted values or predictor variables.\n",
    "    - If the residuals show a non-random pattern (e.g., curvature), the linearity assumption might be violated.\n",
    "- Polynomial regression or non-linear transformations:\n",
    "    - Add quadratic or higher-order terms ($X^2, X^3$) to account for non-linear relationships.\n",
    "- Scatterplots:\n",
    "    - Visualize Y vs. $X_i$ to check if the relationship appears linear.\n",
    "\n",
    "##### Consequences of Violating Additivity or Linearity\n",
    "Violating Additivity:\n",
    "- The model may produce biased coefficients because it doesn’t account for interactions between variables.\n",
    "- Predictions will be less accurate since the relationship between predictors isn’t properly captured.\n",
    "\n",
    "Violating Linearity:\n",
    "- The model cannot fit the data well, leading to high residual variance and low predictive accuracy.\n",
    "- Inference (e.g., significance of predictors) becomes unreliable.\n",
    "\n",
    "Key Considerations\n",
    "- Avoid the Dummy Variable Trap: The dummy variable trap occurs when all categories are included as dummy variables, resulting in perfect multicollinearity. To avoid this, always exclude one category (the reference group).\n",
    "- Interpretation: Regression coefficients of dummy variables represent the change in the dependent variable (e.g., BMI) relative to the reference group.\n",
    "- Interaction Terms: You can include interaction terms between dummy variables and continuous variables (e.g., Age × Region) if you suspect the relationship between the independent variables and the dependent variable varies across groups.\n",
    "- Standardization: While dummy variables themselves don’t need to be standardized, you may want to standardize continuous variables in the presence of dummy variables to ensure all predictors are on comparable scales.\n",
    "\n",
    "Examples\n",
    "\n",
    "Additivity Violation:\n",
    "- Imagine you’re studying the effect of exercise and diet on weight loss:\n",
    "- Additive assumption: The effects of exercise and diet are independent\n",
    "$$ WeightLoss = 𝛽_0 + 𝛽_1 (Exercise) + 𝛽_2 (Diet) + \\epsilon $$\n",
    "- If the effectiveness of exercise depends on the type of diet, an interaction term (Exercise × Diet) should be added:\n",
    "$$ WeightLoss = 𝛽_0 + 𝛽_1 (Exercise) + 𝛽_2 (Diet) +  𝛽_3 (Exercise \\times Diet) + \\epsilon $$\n",
    "\n",
    "Linearity Violation:\n",
    "- Suppose you’re modeling blood pressure as a function of age:\n",
    "- Linear relationship: \n",
    "$$ BP = 𝛽_0 + 𝛽_1 (Age) +  \\epsilon $$\n",
    "- If blood pressure increases non-linearly with age (e.g., faster in older individuals), use a transformation:\n",
    "$$ BP = 𝛽_0 + 𝛽_1 (Age) + 𝛽_2 (Age^2) + \\epsilon $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83489a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "# Simulated data\n",
    "np.random.seed(42)\n",
    "X = np.random.uniform(0, 10, 100)\n",
    "Y = 2 * X + 5 + np.random.normal(0, 1, 100)\n",
    "\n",
    "# Scatter plot to check linearity\n",
    "sns.scatterplot(x=X, y=Y)\n",
    "plt.title(\"Scatterplot of Y vs. X\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()\n",
    "\n",
    "# Fit a linear regression model\n",
    "X_with_const = sm.add_constant(X)\n",
    "model = sm.OLS(Y, X_with_const).fit()\n",
    "\n",
    "# Residual plot\n",
    "residuals = model.resid\n",
    "sns.residplot(x=X, y=residuals, lowess=True)\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa1ec37",
   "metadata": {},
   "source": [
    "#####  **Understanding Interaction Terms**\n",
    "Let look into interaction terms and how to implement them.\n",
    "\n",
    "An interaction term in regression captures the combined effect of two variables on the dependent variable. \n",
    "It’s useful when you suspect that the relationship between a predictor (e.g., age) and the outcome (e.g., BMI) depends on the level of another variable (e.g., gender or region).\n",
    "\n",
    "For example:\n",
    "- In healthcare, the effect of age on BMI might differ by region (urban vs. rural).\n",
    "\n",
    "Interaction terms allow the regression model to capture this varying effect.\n",
    "\n",
    "Mathematically, an interaction term for two predictors $X_1$ and $X_2$ is the product $X_1 \\times X_2$\n",
    "- The model equation becomes:\n",
    "$$ y = 𝛽_0 + 𝛽_1 X_1 + 𝛽_2 X_2 + 𝛽_3 (X_1 \\cdot X_2) +\\epsilon $$\n",
    "\n",
    "- Here \n",
    "    - $𝛽_3$ measures how the effect of $X_1$ on y changes as $X_2$ changes. \n",
    "\n",
    "**When an interaction term should be added to a multiple regression model?**\n",
    "- when the effect of one predictor variable on the dependent variable depends on the value of another predictor variable. \n",
    "    - This allows the model to capture relationships between variables that are non-additive.\n",
    "\n",
    "1. Suspected Interaction Between Variables\n",
    "- If there’s theoretical or domain knowledge suggesting that the relationship between one predictor and the outcome is influenced by another predictor.\n",
    "- Example: In a study on weight loss:\n",
    "    - Exercise might have a stronger effect on weight loss when combined with a healthy diet than when diet is poor.\n",
    "\n",
    "2. Observed Non-Additive Patterns\n",
    "- If exploratory data analysis reveals that the effect of one variable changes at different levels of another variable.\n",
    "- Example: A scatterplot or heatmap shows that the effect of marketing spend on sales depends on product price.\n",
    "\n",
    "3. Model Diagnostics\n",
    "- If residual plots or other diagnostics indicate that the model fails to capture the relationship properly, interaction terms might be necessary.\n",
    "\n",
    "4. Testing for Moderating Effects\n",
    "- If you want to test whether a predictor moderates the relationship between another predictor and the outcome.\n",
    "    - Example: Does the effect of education on income vary by gender?\n",
    "\n",
    "Mathematical Formulation\n",
    "- An interaction term is a product of two predictors in a regression model.\n",
    "    - For predictors $X_1$ and $X_2$, the interaction term is $X_1 \\times X_2$. The regression equation becomes:\n",
    "$$ y = 𝛽_0 + 𝛽_1 X_1 + 𝛽_2 X_2 + 𝛽_3 (X_1 \\cdot X_2) +\\epsilon $$\n",
    "- $𝛽_3$: The coefficient of the interaction term, which quantifies how the effect of $X_1$ on Y changes for different values of $X_2$.\n",
    "\n",
    "**Interpreting Interaction Terms**\n",
    "\n",
    "Without Interaction Term:\n",
    "- The effect of $X_1$ on Y is constant, regardless of $X_2$.\n",
    "$$ y = 𝛽_0 + 𝛽_1 X_1 + 𝛽_2 X_2  +\\epsilon $$\n",
    "\n",
    "- With Interaction Term:\n",
    "\n",
    "The effect of $𝑋_1$ on Y depends on $X_2$.\n",
    "$$ y = 𝛽_0 + 𝛽_1 X_1 + 𝛽_2 X_2 + 𝛽_3 (X_1 \\cdot X_2) +\\epsilon $$\n",
    "- The effect of $X_1$ on Y: $\\frac{\\partial Y}{\\partial X_1} = 𝛽_1 + 𝛽_3 X_2 $.\n",
    "- The effect of $X_2$ on Y: $\\frac{\\partial Y}{\\partial X_2} = 𝛽_2 + 𝛽_3 X_1 $.\n",
    "\n",
    "Example - Healthcare Outcomes:\n",
    "- Predictors: Age ($X_1$)) and Treatment Type ($X_2$).\n",
    "- Outcome: Recovery Rate (Y).\n",
    "- Interaction: The effect of treatment might vary by age.\n",
    "    - Certain treatments could work better for younger patients.\n",
    "\n",
    "**When NOT to Add Interaction Terms**\n",
    "\n",
    "No Theoretical Basis:\n",
    "- Avoid adding interaction terms just because you can. Ensure there’s a plausible reason for the interaction.\n",
    "\n",
    "Small Dataset:\n",
    "- Adding interaction terms increases the number of parameters, which can lead to overfitting in small datasets.\n",
    "\n",
    "Multicollinearity:\n",
    "- Interaction terms are often correlated with their constituent predictors, potentially increasing multicollinearity. Use VIF (Variance Inflation Factor) to check for multicollinearity.\n",
    "\n",
    "___________\n",
    "\n",
    "Example Scenario in a Medical Dataset\n",
    "\n",
    "Problem Statement:\n",
    "- We want to study BMI based on:\n",
    "    - Age (continuous variable),\n",
    "    - Smoking Status (categorical: \"Non-Smoker,\" \"Current Smoker,\" \"Former Smoker\"),\n",
    "    - Interaction between Age and Smoking Status.\n",
    "\n",
    "We suspect that the effect of age on BMI is different for smokers compared to non-smokers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b034c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Age': [45, 34, 50, 29, 60, 40],\n",
    "    'SmokingStatus': ['Non-Smoker', 'Current Smoker', 'Former Smoker', \n",
    "                      'Non-Smoker', 'Current Smoker', 'Former Smoker'],\n",
    "    'BMI': [25.0, 27.5, 24.3, 22.8, 28.7, 26.4]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Create dummy variables for 'SmokingStatus'\n",
    "df = pd.get_dummies(df, columns=['SmokingStatus'], drop_first=True)\n",
    "\n",
    "# Step 2: Create interaction terms (Age × SmokingStatus)\n",
    "df['Age_CurrentSmoker'] = df['Age'] * df['SmokingStatus_Current Smoker']\n",
    "df['Age_FormerSmoker'] = df['Age'] * df['SmokingStatus_Former Smoker']\n",
    "\n",
    "# Inspect the dataset\n",
    "print(df)\n",
    "\n",
    "# Step 3: Define predictors and target variable\n",
    "X = df[['Age', 'SmokingStatus_Current Smoker', 'SmokingStatus_Former Smoker', \n",
    "        'Age_CurrentSmoker', 'Age_FormerSmoker']]\n",
    "y = df['BMI']\n",
    "\n",
    "# Add constant (intercept) to the model\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Step 4: Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Step 5: Summary of regression results\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "#==============================================================================\n",
    "#                 Coef.    Std Err    t      P>|t|    [95% Conf. Interval]\n",
    "#------------------------------------------------------------------------------\n",
    "#const           22.500    2.500     9.000   0.001    [16.000, 29.000]\n",
    "#Age              0.200    0.050     4.000   0.015    [ 0.075,  0.325]\n",
    "#SmokingStatus_CurrentSmoker   2.500    1.000     2.500   0.050    [ 0.000,  5.000]\n",
    "#SmokingStatus_FormerSmoker    1.800    1.200     1.500   0.180    [-1.000,  4.600]\n",
    "#Age_CurrentSmoker             0.050    0.020     2.500   0.050    [ 0.000,  0.100]\n",
    "#Age_FormerSmoker              0.030    0.025     1.200   0.300    [-0.020,  0.080]\n",
    "#=============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8ab90",
   "metadata": {},
   "source": [
    "\n",
    "##### Last Step: Interpreting the Results\n",
    "\n",
    "Intercept ($𝛽_0$)\n",
    "- The baseline BMI for a 0-year-old non-smoker (reference group).\n",
    "\n",
    "Age $𝛽_1$\n",
    "- The change in BMI for each additional year of age, holding smoking status constant\n",
    "\n",
    "Smoking Status Variables ($𝛽_2$,$𝛽_3$)\n",
    "- $𝛽_2$ The average difference in BMI between current smokers and non-smokers (when Age = 0).\n",
    "- $𝛽_3$ The average difference in BMI between former smokers and non-smokers (when Age = 0).\n",
    "\n",
    "Interaction Terms ($𝛽_4$, $𝛽_5$)\n",
    "- $𝛽_4$ The additional change in BMI per year of age for current smokers compared to non-smokers.\n",
    "- $𝛽_5$ The additional change in BMI per year of age for former smokers compared to non-smokers.\n",
    "\n",
    "Example Insights:\n",
    "- If the Age × SmokingStatus_Current Smoker interaction term ($𝛽_4$) is statistically significant (e.g., $p<0.05$), \n",
    "    - it indicates that the effect of age on BMI is significantly different for current smokers compared to non-smokers.\n",
    "- If $𝛽_4$ > 0, BMI increases more steeply with age for current smokers than for non-smokers.\n",
    "- If $𝛽_4$ < 0, BMI increases less steeply with age for current smokers than for non-smokers.\n",
    "\n",
    "Note\n",
    "- Visualization: Use visual tools (e.g., line plots) to illustrate how BMI changes with age across smoking groups.\n",
    "- Statistical Significance: Focus on p-values to assess whether interactions are meaningful.\n",
    "- Real-World Use Case: Interaction terms are vital in clinical studies where the combined effect of variables (e.g., medication × age) is of interest.\n",
    "\n",
    "\n",
    "##### Adding Interaction Terms in Python\n",
    "- how you can include interaction terms in a regression model using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6265fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Simulated data\n",
    "data = pd.DataFrame({\n",
    "    'MarketingSpend': [10, 20, 30, 40, 50],\n",
    "    'Price': [1, 2, 3, 4, 5],\n",
    "    'Sales': [15, 25, 35, 50, 60]\n",
    "})\n",
    "\n",
    "# Add interaction term (manual)\n",
    "data['Interaction'] = data['MarketingSpend'] * data['Price']\n",
    "\n",
    "# Fit regression model with interaction\n",
    "model = ols('Sales ~ MarketingSpend + Price + MarketingSpend:Price', data=data).fit()\n",
    "\n",
    "# Summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d5828e",
   "metadata": {},
   "source": [
    "Using Patsy Syntax\n",
    "\n",
    "You can also use the formula-based approach to automatically add interaction terms:\n",
    "- The * operator automatically includes the main effects and the interaction term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587eb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('Sales ~ MarketingSpend * Price', data=data).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74855dfc",
   "metadata": {},
   "source": [
    "##### Detecting Interaction Effects\n",
    "1. Visual Inspection:\n",
    "- Use scatterplots, heatmaps, or line plots to observe how the relationship between $𝑋_1$ and 𝑌 changes at different levels of $𝑋_2$.\n",
    "\n",
    "2. Statistical Testing:\n",
    "- Include interaction terms in your regression model and check their significance (p-value of the interaction term).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.lmplot(data=data, x='MarketingSpend', y='Sales', hue='Price', palette='coolwarm')\n",
    "plt.title(\"Interaction Effect\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e1199",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Example Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff403e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continueing from example\n",
    "df_dummies = pd.get_dummies(df)\n",
    "\n",
    "# Again we make sure that all the column names have underscores instead of whitespaces\n",
    "df_dummies.columns = [col.replace(\" \",\"_\") for col in df_dummies.columns] \n",
    "\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3294b442",
   "metadata": {},
   "source": [
    "Correlations and Model Structure\n",
    "\n",
    "Now, we can build a model that predicts `Loan_Size` (our dependent variable) as a function of 43 different independent variables (IVs)\n",
    "\n",
    "1. reorder columns so that our dependent variable is the last column of the dataframe. \n",
    "- making a heatmap visualisation representing a correlation matrix of our data easier to interpret.\n",
    "\n",
    "2. Run correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db898f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_titles = [col for col in df_dummies.columns if col!= 'Loan_Size'] + ['Loan_Size']\n",
    "df_dummies=df_dummies.reindex(columns=column_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f5a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run corr matrix\n",
    "df_dummies.corr()\n",
    "\n",
    "from statsmodels.graphics.correlation import plot_corr\n",
    "\n",
    "fig = plt.figure(figsize=(15,15));\n",
    "ax = fig.add_subplot(111);\n",
    "plot_corr(df_dummies.corr(), xnames = df_dummies.corr().columns, ax = ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fc998e",
   "metadata": {},
   "source": [
    "#### Rerun your Model: Fitting the model using `statsmodels.OLS`\n",
    "\n",
    "##### Generating the regression string\n",
    "\n",
    "Importing the statsmodels library which has a rich set of statistical tools to help us. \n",
    "\n",
    "Those of you familiar with the R language will know that fitting a machine learning model requires a sort of string of the form:\n",
    "\n",
    "`y ~ X`\n",
    "\n",
    "- which is read as follows: \"Regress y on X\". \n",
    "\n",
    "`statsmodels` works in a similar way, so we need to generate an appropriate string to feed to the method when we wish to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Model DataFrame with all of the columns:\n",
    "dfm = df_dummies.copy()\n",
    "\n",
    "# The dependent variable:\n",
    "y_name = 'Loan_Size'\n",
    "# The independent variable\n",
    "# (let's first try all of the columns in the model DataFrame)\n",
    "X_names = [col for col in dfm.columns if col != y_name]\n",
    "\n",
    "# Build the OLS formula string \" y ~ X \"\n",
    "formula_str = y_name+\" ~ \"+\" + \".join(X_names);\n",
    "print('Formula:\\n\\t {}'.format(formula_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b08a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using the model dataframe\n",
    "model=ols(formula=formula_str, data=dfm)\n",
    "fitted = model.fit()\n",
    "\n",
    "# Output the fitted summary\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the OLS Regression Summary\n",
    "\n",
    "**Model Performance**\n",
    "\n",
    "|Measure           |Value             |\n",
    "|------------------|------------------|\n",
    "| Dep. Variable:   |        Loan_Size | \n",
    "| Model:           |              OLS | \n",
    "| Method:          |    Least Squares | \n",
    "| Date:            | Sat, 02 May 2020 |\n",
    "| Time:            |         13:21:01 |\n",
    "| No. Observations:|              471 |\n",
    "| Df Residuals:    |              430 |\n",
    "| Df Model:        |               40 |\n",
    "| Covariance Type: |        nonrobust |\n",
    "| R-squared:       |             0.777|\n",
    "| Adj. R-squared:  |             0.757|\n",
    "| F-statistic:     |             37.56|\n",
    "| Prob (F-statistic): |      1.71e-115|\n",
    "| Log-Likelihood:  |           -1387.0|\n",
    "| AIC:             |             2856.|\n",
    "| BIC:             |             3026.|\n",
    "\n",
    "Dependent Variable: Loan_Size\n",
    "- The target variable being modeled, indicating the size of loans in this context.\n",
    "\n",
    "R-squared: 0.777\n",
    "- Meaning: 77.7% of the variation in Loan_Size is explained by the independent variables in the model.\n",
    "- Thresholds: Higher values (closer to 1) indicate better model fit. However, 77.7% is a strong fit for real-world data.\n",
    "- Stakeholder Message: The model is effective at explaining the variability in loan sizes based on the input variables.\n",
    "\n",
    "Adj. R-squared: 0.757\n",
    "- Meaning: 75.7% of the variation in Loan_Size is explained by the independent variables in the model. but adjusts for the number of predictors to avoid overfitting. \n",
    "- A slight drop from R-squared suggests that some variables may add limited value to the model.\n",
    "\n",
    "**Statistical Significance of the Model**\n",
    "\n",
    "F-statistic: 37.56\n",
    "- looks at Statistical Significance of the Model\n",
    "- Meaning: The F-test checks if at least one of the predictors is statistically significant.\n",
    "\n",
    "Prob (F-statistic): 1.71e-115 (extremely small, close to 0)\n",
    "- Stakeholder Message: The overall model is statistically significant, indicating that the predictors together effectively explain variations in loan size.\n",
    "\n",
    "__________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "**Coefficients and Their Interpretation**\n",
    "\n",
    "|                          |   coef   | std err    |      t     | P>t    | [0.025      0.975]|\n",
    "|--------------------------|----------|------------|------------|-----------|-------------------|\n",
    "|Intercept                 | 6.4496   |   2.696    |  2.392     | 0.017     |  1.150      11.749|\n",
    "|Age                       |-0.3140   |   0.194    | -1.620     | 0.106     | -0.695       0.067|\n",
    "|Experience                | 0.2226   |   0.195    |  1.142     | 0.254     | -0.160       0.605|\n",
    "|Income                    | 0.1777   |   0.008    | 23.319     | 0.000     |  0.163       0.193|\n",
    "|Family                    | 1.3289   |   0.219    |  6.060     | 0.000     |  0.898       1.760|\n",
    "|CCAvg                     | 1.4333   |   0.114    | 12.521     | 0.000     |  1.208       1.658|\n",
    "|Mortgage                  |-0.0370   |   0.001    |-24.962     | 0.000     | -0.040      -0.034|\n",
    "|Securities_Account        | 1.5816   |   0.798    |  1.982     | 0.048     |  0.013       3.150|\n",
    "|CD_Account                |-0.6828   |   0.634    | -1.078     | 0.282     | -1.928       0.563|\n",
    "|Online                    | 0.1235   |   0.513    |  0.241     | 0.810     | -0.886       1.133|\n",
    "|Education_Postgrad        | 2.3492   |   0.941    |  2.496     | 0.013     |  0.499       4.199|\n",
    "|Education_Professional    | 1.9695   |   0.968    |  2.034     | 0.043     |  0.066       3.873|\n",
    "|Education_Undergrad       | 2.1309   |   0.988    |  2.156     | 0.032     |  0.188       4.074|\n",
    "|Gender_Female             | 3.6759   |   1.383    |  2.658     | 0.008     |  0.958       6.394|\n",
    "|Gender_Male               | 2.7737   |   1.352    |  2.052     | 0.041     |  0.117       5.431|\n",
    "|Area_Alameda              |-0.0350   |   0.854    | -0.041     | 0.967     | -1.714       1.644|\n",
    "|Area_Butte                |-2.9267   |   3.371    | -0.868     | 0.386     | -9.553       3.700|\n",
    "|Area_Contra_Costa         |-0.1349   |   1.435    | -0.094     | 0.925     | -2.956       2.686|\n",
    "|Area_Fresno               | 2.0428   |   3.397    |  0.601     | 0.548     | -4.634       8.719|\n",
    "|Area_Humboldt             | 0.0294   |   3.371    |  0.009     | 0.993     | -6.596       6.655|\n",
    "|Area_Kern                 | 1.1313   |   1.830    |  0.618     | 0.537     | -2.465       4.727|\n",
    "|Area_Los_Angeles          |-0.2556   |   0.653    | -0.392     | 0.696     | -1.538       1.027|\n",
    "|Area_Marin                | 0.2734   |   1.969    |  0.139     | 0.890     | -3.596       4.143|\n",
    "|Area_Mendocino            | 4.0507   |   4.756    |  0.852     | 0.395     | -5.297      13.398|\n",
    "|Area_Monterey             |-2.3811   |   1.289    | -1.847     | 0.065     | -4.914       0.152|\n",
    "|Area_Orange               | 0.5804   |   1.005    |  0.578     | 0.564     | -1.395       2.556|\n",
    "|Area_Placer               |-0.1183   |   3.351    | -0.035     | 0.972     | -6.706       6.469|\n",
    "|Area_Riverside            |-0.4246   |   1.991    | -0.213     | 0.831     | -4.339       3.489|\n",
    "|Area_Sacramento           | 0.9005   |   1.310    |  0.687     | 0.492     | -1.675       3.476|\n",
    "|Area_San_Bernardino       | 2.3827   |   2.770    |  0.860     | 0.390     | -3.062       7.827|\n",
    "|Area_San_Diego            | 0.4737   |   0.767    |  0.618     | 0.537     | -1.034       1.981|\n",
    "|Area_San_Francisco        |-1.4785   |   1.173    | -1.260     | 0.208     | -3.785       0.828|\n",
    "|Area_San_Joaquin          | 1.1931   |   4.742    |  0.252     | 0.801     | -8.128      10.514|\n",
    "|Area_San_Luis_Obispo      |-0.2345   |   2.408    | -0.097     | 0.922     | -4.968       4.499|\n",
    "|Area_San_Mateo            | 0.6569   |   1.559    |  0.421     | 0.674     | -2.408       3.722|\n",
    "|Area_Santa_Barbara        |-0.0998   |   1.498    | -0.067     | 0.947     |  -3.044       2.845|\n",
    "|Area_Santa_Clara          | 0.1681   |   0.729    |  0.231     | 0.818     |  -1.265       1.601|\n",
    "|Area_Santa_Cruz           |-0.3529   |   1.827    | -0.193     | 0.847     |  -3.944       3.238|\n",
    "|Area_Shasta               |-0.6051   |   2.779    | -0.218     | 0.828     |  -6.068       4.858|\n",
    "|Area_Solano               |-2.0356   |   2.749    | -0.740     | 0.459     |  -7.440       3.368|\n",
    "|Area_Sonoma               | 0.4197   |   1.987    |  0.211     | 0.833     |  -3.485       4.325|\n",
    "|Area_Stanislaus           |-0.9779   |   4.726    | -0.207     | 0.836     | -10.268       8.312|\n",
    "|Area_Ventura              | 1.7134   |   1.487    |   1.152    |  0.250    |   -1.210       4.636|\n",
    "|Area_Yolo                 | 2.4941   |   1.719    |  1.451     | 0.148     |  -0.885       5.873|\n",
    "\n",
    "\n",
    "The coef values represent the average change in Loan_Size for a one-unit change in each predictor, holding other variables constant.\n",
    "\n",
    "Significant Predictors:\n",
    "\n",
    "Income (coef = 0.1777, p < 0.001):\n",
    "- A one-unit increase in income is associated with an increase of 0.1777 in loan size, on average.\n",
    "- Stakeholder Message: Higher income levels significantly increase loan size, suggesting income is a major determinant of loan allocation.\n",
    "\n",
    "Family (coef = 1.3289, p < 0.001):\n",
    "- Loan size increases by 1.33 units for each additional family member.\n",
    "- Stakeholder Message: Family size positively influences loan size, which may reflect financial responsibilities influencing loan demand.\n",
    "\n",
    "CCAvg (coef = 1.4333, p < 0.001):\n",
    "- Average monthly credit card spending significantly increases loan size.\n",
    "- Stakeholder Message: High credit card spending is a strong indicator of higher loan eligibility or need.\n",
    "\n",
    "Mortgage (coef = -0.0370, p < 0.001):\n",
    "- A negative coefficient implies that larger mortgages slightly reduce loan size.\n",
    "- Stakeholder Message: Customers with higher mortgage liabilities may receive lower loans, possibly reflecting risk concerns.\n",
    "\n",
    "Non-Significant Predictors:\n",
    "\n",
    "Age (p = 0.106), Experience (p = 0.254), Online (p = 0.810), many Area variables:\n",
    "- These variables do not have a statistically significant relationship with loan size as p > 0.05.\n",
    "- Stakeholder Message: These factors might be excluded in future models unless they align with business insights or strategies.\n",
    "\n",
    "Categorical Predictors:\n",
    "\n",
    "Education:\n",
    "- Postgraduates, professionals, and undergraduates receive larger loans compared to the reference category (likely \"No Education\").\n",
    "- Stakeholder Message: Educational qualifications influence loan size, aligning with the idea that higher education may imply better creditworthiness.\n",
    "\n",
    "Gender:\n",
    "- Women receive loans that are on average larger by 3.68 units compared to men.\n",
    "- Stakeholder Message: Gender differences in loan sizes could reflect underlying demographic or financial patterns.\n",
    "\n",
    "___________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "**Diagnostic Measures**\n",
    "| Measure                  |  Value      |\n",
    "|--------------------------|-------------|\n",
    "|Omnibus:                  |  17.650 |\n",
    "|Durbin-Watson:            |     2.004  |\n",
    "|Prob(Omnibus):            |    0.000   |\n",
    "|Jarque-Bera (JB):         |    19.137|\n",
    "|Skew:                     |   -0.431   |\n",
    "|Prob(JB):                 |  6.99e-05|\n",
    "|Kurtosis:                 |    3.482 |\n",
    "|Cond. No.                 |  7.37e+16|\n",
    "\n",
    "Omnibus and Jarque-Bera Tests (p < 0.001):\n",
    "- Indicate that residuals (errors) may not be perfectly normally distributed.\n",
    "- Stakeholder Message: While the model is strong, residual non-normality could be further investigated to refine the model.\n",
    "\n",
    "Durbin-Watson Statistic (2.004):\n",
    "- A value close to 2 suggests no significant autocorrelation in residuals, meaning errors are independent.\n",
    "- Stakeholder Message: The model meets the independence of errors assumption.\n",
    "\n",
    "Condition Number (7.37e+16):\n",
    "- High values suggest multicollinearity issues (predictors are highly correlated).\n",
    "- Stakeholder Message: Some predictors may overlap in their explanatory power. This could be addressed through techniques like variable selection or regularization (e.g., Ridge/Lasso regression).\n",
    "\n",
    "Warnings:\n",
    "\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "[2] The smallest eigenvalue is 4.16e-27. This might indicate that there are\n",
    "strong multicollinearity problems or that the design matrix is singular.\n",
    "- likely as a result of the incorrect filtering of one hot encoded dummy variables\n",
    "- to ensure that we don't assume an underlying relationship between the categories\n",
    "    - call `pd.get_dummies` with the argument `drop_first=True` so that we only create n-1 columns for each variable with n categories\n",
    "        - (i.e. one variable/column with five categories will be transformed into four columns of 0's and 1's)\n",
    "\n",
    "_______________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "**Actionable Insights for Stakeholders**\n",
    "- Focus efforts on variables like Income, CCAvg, and Family, which are key drivers of loan sizes.\n",
    "- Investigate non-significant variables for possible removal to simplify the model and enhance interpretability.\n",
    "- Address potential multicollinearity by refining the input variables.\n",
    "- Consider segmentation by Education and Gender to tailor loan products effectively.\n",
    "- Reassess area-specific variables since many are non-significant; geographical targeting may not substantially impact loan size decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a7c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Again make sure that all the column names have underscores instead of whitespaces\n",
    "df_dummies.columns = [col.replace(\" \", \"_\") for col in df_dummies.columns]\n",
    "\n",
    "# Reorder columns with the dependent variable (claim_amount) the last column\n",
    "column_titles = [col for col in df_dummies.columns if col !=\n",
    "                 'Loan_Size'] + ['Loan_Size']\n",
    "df_dummies = df_dummies.reindex(columns=column_titles)\n",
    "\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545d4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll keep the model DataFrame, but only specify the columns we want to fit this time\n",
    "X_names = [col for col in df_dummies.columns if col != y_name]\n",
    "\n",
    "# Build the OLS formula string \" y ~ X \"\n",
    "formula_str = y_name+' ~ '+'+'.join(X_names)\n",
    "\n",
    "# Fit the model using the model dataframe\n",
    "model = ols(formula=formula_str, data=dfm)\n",
    "fitted = model.fit()\n",
    "\n",
    "# Output the fitted summary\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3b0ae2",
   "metadata": {},
   "source": [
    "# Comparison of Feature Selection Methods with Evaluation Metrics\n",
    "Feature selection methods help improve model \n",
    "- accuracy, \n",
    "- interpretability, and \n",
    "- efficiency. \n",
    "\n",
    "## Feature Selection Techniques: Categories\n",
    "Feature selection methods can be broadly classified into three categories:\n",
    "\n",
    "\n",
    "### **Filter Methods (Statistical Tests)**\n",
    "These methods evaluate the relevance of each feature independently based on statistical properties. They do not involve any machine learning models.\n",
    "- Making further selections on the variables now using their significance.\n",
    "\n",
    "✅ Pros: Fast, scalable, works for high-dimensional data.\n",
    "\n",
    "❌ Cons: Ignores interactions between features.\n",
    "\n",
    "Examples:\n",
    "- Mutual Information: Measures dependency between features and the target variable.\n",
    "- Correlation Coefficients (Pearson, Spearman): Measures linear or monotonic relationships.\n",
    "- Chi-Square Test: Used for categorical variables.\n",
    "- Variance Threshold: Removes features with low variance.\n",
    "\n",
    "### 1. Variable Selection by Correlation and Significance\n",
    "\n",
    "We need to choose the best ones to be our predictors. \n",
    "\n",
    "One way is to \n",
    "- look at the correlations between the `Loan Size` and each variables in our DataFrame\n",
    "    - and select those with the strongest correlations (both positive and negative).\n",
    "- consider how significant those features are. \n",
    "\n",
    "Create a new DataFrame and store the correlation coefficents and p-values in that DataFrame for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae123c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations between predictor variables and the response variable\n",
    "corrs = df_dummies.corr()['Loan_Size'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece4d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Build a dictionary of correlation coefficients and p-values\n",
    "dict_cp = {}\n",
    "\n",
    "column_titles = [col for col in corrs.index if col!= 'Loan_Size']\n",
    "for col in column_titles:\n",
    "    p_val = round(pearsonr(df_dummies[col], df_dummies['Loan_Size'])[1],6)\n",
    "    dict_cp[col] = {'Correlation_Coefficient':corrs[col],\n",
    "                    'P_Value':p_val}\n",
    "    \n",
    "df_cp = pd.DataFrame(dict_cp).T\n",
    "df_cp_sorted = df_cp.sort_values('P_Value')\n",
    "df_cp_sorted[df_cp_sorted['P_Value']<0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda6f97a",
   "metadata": {},
   "source": [
    "Get a sorted list of the p-values and correlation coefficients for each of the features, when considered on their own.  \n",
    "\n",
    "If we were to use a logic test with a significance value of 5% (p-value < 0.05), \n",
    "- we could infer that the following features are statistically significant:\n",
    "    - List features\n",
    "\n",
    "Keep only the variables that have a significant correlation with the dependent variable. \n",
    "- Put them into an independent variable DataFrame `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ead79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dependent variable remains the same:\n",
    "y_data = df_dummies[y_name]  # y_name = 'Loan_Size'\n",
    "\n",
    "# Model building - Independent Variable (IV) DataFrame\n",
    "X_names = list(df_cp[df_cp['P_Value'] < 0.05].index)\n",
    "X_data = df_dummies[X_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63eb840",
   "metadata": {},
   "source": [
    "Also, look for predictor variable pairs which have a high correlation with each other to avoid autocorrelation.\n",
    "\n",
    "Easier to isolate the sections of the correlation matrix to where the off-diagonal correlations are high:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the correlation matrix\n",
    "corr = X_data.corr()\n",
    "\n",
    "# Find rows and columnd where correlation coefficients > 0.9 or <-0.9\n",
    "corr[np.abs(corr) > 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9701436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, we create the correlation matrix\n",
    "# and find rows and columnd where correlation coefficients > 0.9 or <-0.9\n",
    "corr = X_data.corr()\n",
    "r, c = np.where(np.abs(corr) > 0.9)\n",
    "\n",
    "# We are only interested in the off diagonal entries:\n",
    "off_diagonal = np.where(r != c)\n",
    "\n",
    "# Show the correlation matrix rows and columns where we have highly correlated off diagonal entries:\n",
    "corr.iloc[r[off_diagonal], c[off_diagonal]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603091c5",
   "metadata": {},
   "source": [
    "##### Resulting OLS fit summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf352120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a new subset of our potential independent variables\n",
    "X_remove = ['Age']\n",
    "X_corr_names = [col for col in X_names if col not in X_remove]\n",
    "\n",
    "# Create our new OLS formula based-upon our smaller subset\n",
    "formula_str = y_name+' ~ '+' + '.join(X_corr_names);\n",
    "print('Formula:\\n\\t{}'.format(formula_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f838918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the OLS model using the model dataframe\n",
    "model=ols(formula=formula_str, data=dfm)\n",
    "fitted = model.fit()\n",
    "\n",
    "# Display the fitted summary\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99313db",
   "metadata": {},
   "source": [
    "### 2. Variable Selection by Variance Thresholds\n",
    "\n",
    "Variance Thresholds remove features whose values don't change much from observation to observation. \n",
    "\n",
    "The objective here is to remove all features that have a variance lower than the selected threshold.\n",
    "- Suppose that in our loans dataset 97% of observations were for 40-year-old women, then the *Age* and *Gender* features can be removed without a great loss in information.\n",
    "\n",
    "It is important to note that variance is dependent on scale, so the features will have to be normalized before implementing variance thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into independent (X) and independent (y) variables\n",
    "X_names = list(df_dummies.columns)\n",
    "X_names.remove(y_name)\n",
    "X_data = df_dummies[X_names]\n",
    "y_data = df_dummies[y_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1816ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "X_normalize = pd.DataFrame(X_scaled, columns=X_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f558fa07",
   "metadata": {},
   "source": [
    "#### Variance Threshold in Scikit Learn\n",
    "\n",
    "To implement Variance Threshold in Scikit Learn we have to do the following:\n",
    "\n",
    "Import and create an instance of the VarianceThreshold class;\n",
    "- Use the .fit() method to select subset of features based on the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create VarianceThreshold object\n",
    "selector = VarianceThreshold(threshold=0.03)\n",
    "\n",
    "# Use the object to apply the threshold on data\n",
    "selector.fit(X_normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100777d",
   "metadata": {},
   "source": [
    "##### Calculated variance for each predictive variable.\n",
    "\n",
    "Show the variances of the individual columns before any threshold is applied. \n",
    "\n",
    "It allows us to revise our initial variance threshold if we feel that we might exclude important variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f71b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column variances\n",
    "column_variances = selector.variances_\n",
    "\n",
    "vars_dict = {}\n",
    "vars_dict = [{\"Variable_Name\": c_name, \"Variance\": c_var}\n",
    "             for c_name, c_var in zip(X_normalize.columns, column_variances)]\n",
    "df_vars = pd.DataFrame(vars_dict)\n",
    "df_vars.sort_values(by='Variance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1318dc81",
   "metadata": {},
   "source": [
    "#### Extract the results and use them to select our new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select new columns\n",
    "X_new = X_normalize[X_normalize.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "# Save variable names for later\n",
    "X_var_names = X_new.columns\n",
    "\n",
    "# View first few entries\n",
    "X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Variance Threshold objects\n",
    "selector_1 = VarianceThreshold(threshold=0.05)\n",
    "selector_2 = VarianceThreshold(threshold=0.1)\n",
    "selector_3 = VarianceThreshold(threshold=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ee00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_1.fit(X_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2fad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_2.fit(X_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae070ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_3.fit(X_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select subset of columns\n",
    "X_1 = X_normalize[X_normalize.columns[selector_1.get_support(indices=True)]]\n",
    "X_2 = X_normalize[X_normalize.columns[selector_2.get_support(indices=True)]]\n",
    "X_3 = X_normalize[X_normalize.columns[selector_3.get_support(indices=True)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49658433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axes\n",
    "f, ax = plt.subplots(figsize=(8, 3), nrows=1, ncols=1)\n",
    "\n",
    "# Create list of titles and predictions to use in for loop\n",
    "subset_preds = [X_1.shape[1], X_2.shape[1], X_3.shape[1]]\n",
    "thresholds = ['0.05', '0.1', '0.15']\n",
    "\n",
    "# Plot graph\n",
    "ax.set_title('# of Predictors vs Thresholds')\n",
    "ax.set_ylabel('# of Predictors')\n",
    "ax.set_xlabel('Threshold')\n",
    "sns.barplot(x=thresholds, y=subset_preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e9bf69",
   "metadata": {},
   "source": [
    "\n",
    "##### Extract the predictor names of the 3 different datasets above?\n",
    "\n",
    "Results OLS fit summary for a threshold of 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is our new OLS formula?\n",
    "formula_str = y_name+' ~ '+' + '.join(X_new.columns)\n",
    "print('Formula:\\n\\t{}'.format(formula_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d94f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using the model dataframe\n",
    "model = ols(formula=formula_str, data=df_dummies)\n",
    "fitted = model.fit()\n",
    "\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b8e7e5",
   "metadata": {},
   "source": [
    "#### Advantages & Disadvantages of Variance Thresholds\n",
    "\n",
    "Let's consider some trade-offs associated with using variance thresholds for variable selection: \n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* Applying variance thresholds is based on solid intuition: features that don't change much also don't add much information;\n",
    "* Easy and relatively safe way to reduce dimensionality (i.e. number of features) at the start of the modeling process.\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "* Not the ideal algorithm if dimensionality reduction is not really required;\n",
    "* The threshold must be manually tuned, which can be a fickle process requiring domain/problem expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45d0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68895683",
   "metadata": {},
   "source": [
    "Preprocess the data\n",
    "\n",
    "make sure that all models are trained and tested on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6796317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data,\n",
    "                                                    y_data,\n",
    "                                                    test_size=0.20,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9919e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and testing data for variance threshold model\n",
    "X_var_train = X_train[X_var_names]\n",
    "X_var_test = X_test[X_var_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a036d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and testing data for correlation threshold model\n",
    "X_corr_train = X_train[X_corr_names]\n",
    "X_corr_test = X_test[X_corr_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44630c3b",
   "metadata": {},
   "source": [
    "##### Fit models\n",
    "\n",
    "instantiate and fit our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a3eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm_corr = LinearRegression()\n",
    "lm_var = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff51e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(X_train, y_train);\n",
    "lm_corr.fit(X_corr_train,y_train);\n",
    "lm_var.fit(X_var_train,y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ea6ba",
   "metadata": {},
   "source": [
    "##### Assess model accuracy \n",
    "Let's see how our linear models performed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb5467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create figure and axes\n",
    "f, ax = plt.subplots(figsize=(15, 5), nrows=1, ncols=3, sharey=True)\n",
    "\n",
    "# Create list of titles and predictions to use in for loop\n",
    "train_pred = [lm.predict(X_train),\n",
    "              lm_corr.predict(X_corr_train),\n",
    "              lm_var.predict(X_var_train)]\n",
    "test_pred = [lm.predict(X_test),\n",
    "             lm_corr.predict(X_corr_test),\n",
    "             lm_var.predict(X_var_test)]\n",
    "title = ['No threshold', 'Corr threshold', 'Var threshold']\n",
    "\n",
    "# Key:\n",
    "# No threshold - linear regression with all predictive variables\n",
    "# Corr threshold - linear regression with correlation thresholded predictive variables\n",
    "# Var threshold - linear regression with variance thresholded predictive variables\n",
    "\n",
    "\n",
    "# Loop through all axes to plot each model's results\n",
    "for i in range(3):\n",
    "    test_mse = round(mean_squared_error(test_pred[i], y_test), 4)\n",
    "    test_r2 = round(r2_score(test_pred[i], y_test), 4)\n",
    "    train_mse = round(mean_squared_error(train_pred[i], y_train), 4)\n",
    "    train_r2 = round(r2_score(train_pred[i], y_train), 4)\n",
    "    title_str = f\"Linear Regression({title[i]}) \\n train MSE = {train_mse} \\n \" + \\\n",
    "                f\"test MSE = {test_mse} \\n training $R^{2}$ = {train_r2} \\n \" + \\\n",
    "                f\"test $R^{2}$ = {test_r2}\"\n",
    "    ax[i].set_title(title_str)\n",
    "    ax[i].set_xlabel('Actual')\n",
    "    ax[i].set_ylabel('Predicted')\n",
    "    ax[i].plot(y_test, y_test, 'r')\n",
    "    ax[i].scatter(y_test, test_pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698c7fea",
   "metadata": {},
   "source": [
    "### **Wrapper Methods (Model-Based Selection)**\n",
    "These methods train models and evaluate feature subsets based on performance metrics.\n",
    "\n",
    "✅ Pros: Accounts for feature interactions, gives best subset.\n",
    "\n",
    "❌ Cons: Computationally expensive.\n",
    "\n",
    "Examples:\n",
    "- Best Subset Selection: Evaluates all feature combinations (very slow).\n",
    "- Recursive Feature Elimination (RFE): Removes least important features iteratively.\n",
    "- Forward/Backward Selection: Adds/removes features one at a time.\n",
    "\n",
    "### 1. Best Subset Selection in Machine Learning\n",
    "Definition:\n",
    "\n",
    "Best subset selection is a feature selection technique used in regression and machine learning to identify the optimal subset of predictors (independent variables) that best explain the dependent variable. This method evaluates all possible combinations of features and selects the one that minimizes a chosen error metric\n",
    "- AIC, \n",
    "- BIC, \n",
    "- adjusted $𝑅^2$, \n",
    "- cross-validation error. \n",
    "\n",
    "It is particularly useful when dealing with a large number of predictors but is computationally expensive for large datasets.\n",
    "\n",
    "### Steps in Best Subset Selection\n",
    "Prepare the Dataset:\n",
    "- Ensure the dataset is clean, with no missing values or irrelevant variables.\n",
    "- Identify the dependent variable and all possible independent variables.\n",
    "\n",
    "Generate All Possible Subsets of Predictors:\n",
    "- Consider all combinations of predictors, ranging from using no predictors to using all predictors.\n",
    "    - If there are p predictors, there will be $2^𝑝$ possible subsets.\n",
    "- **NB - Best Subset Selection exhaustively evaluates all feature subsets.**\n",
    "\n",
    "Train a Model for Each Subset:\n",
    "- Fit a regression model (e.g., linear regression) for each subset of predictors.\n",
    "- Compute performance metrics for each model.\n",
    "\n",
    "Evaluate Models Using a Selection Criterion:\n",
    "- Compare models using criteria like:\n",
    "    - Adjusted $𝑅^2$ – Rewards models that explain more variance while penalizing unnecessary predictors.\n",
    "        - **NB - It ensures we balance performance with model complexity.**\n",
    "    - Akaike Information Criterion (AIC) – Balances model fit and complexity.\n",
    "    - Bayesian Information Criterion (BIC) – Similar to AIC but penalizes complexity more.\n",
    "    - Cross-validation error (e.g., mean squared error on validation data).\n",
    "        - **NB - Cross-validation prevents overfitting.**\n",
    "\n",
    "Select the Best Model:\n",
    "- Choose the model with the best evaluation metric, e.g:\n",
    "    - highest adjusted $𝑅^2$\n",
    "    - or lowest AIC/BIC.\n",
    "\n",
    "Validate the Model:\n",
    "- Test the selected model on unseen data (e.g., using cross-validation) to check its generalization ability.\n",
    "\n",
    "**Two common approaches to selecting the best model concerning test error**\n",
    "1. Cross-Validation (CV) Approach\n",
    "\n",
    "✅ What It Does:\n",
    "- Splits the data into training and validation sets multiple times.\n",
    "- The model is trained on different subsets and tested on the remaining data.\n",
    "- The average test error across all validation sets is used to select the best model.\n",
    "\n",
    "✅ Why It Works:\n",
    "- Helps estimate out-of-sample performance.\n",
    "- Reduces the risk of overfitting by evaluating the model on unseen data.\n",
    "- Common types:\n",
    "    - k-Fold Cross-Validation (splits data into k parts, trains on k−1, tests on the remaining part).\n",
    "    - Leave-One-Out Cross-Validation (LOOCV) (uses one observation for testing, rest for training).\n",
    "\n",
    "2. Information Criteria Approach (AIC/BIC)\n",
    "\n",
    "✅ What It Does:\n",
    "- Uses statistical metrics to balance model fit and complexity.\n",
    "- Penalizes models with too many predictors to avoid overfitting.\n",
    "- Two commonly used criteria:\n",
    "    - Akaike Information Criterion (AIC): Focuses on minimizing information loss.\n",
    "    - Bayesian Information Criterion (BIC): Stronger penalty for model complexity than AIC.\n",
    "\n",
    "✅ Why It Works:\n",
    "- Selects a model with the best trade-off between accuracy and simplicity.\n",
    "- Lower AIC/BIC values indicate a better model.\n",
    "\n",
    "| Method | Best for   | Drawback                     |\n",
    "|---------------|--------|----------------------------------| \n",
    "|Cross-Validation (CV)|General model evaluation & prediction performance |\tComputationally expensive for large datasets|\n",
    "|AIC/BIC\t|Selecting best statistical model with trade-off between fit & complexity | Assumes correct model form and doesn’t directly measure test error |\n",
    "\n",
    "##### **Model Selection Criteria in Subset Selection**\n",
    "When performing subset selection in regression, the goal is to find the best combination of predictors that results in the most accurate and interpretable model. The key challenge is balancing model fit and complexity—\n",
    "- a model with too many predictors might overfit, while \n",
    "- a model with too few predictors might underfit.\n",
    "When performing subset selection in regression (choosing the best subset of predictors), we need evaluation metrics to compare models and select the best one. The commonly used criteria are:\n",
    "- Mallows' $𝐶_p$\n",
    "- Akaike Information Criterion (AIC)\n",
    "- Bayesian Information Criterion (BIC)\n",
    "- Adjusted $𝑅^2$\n",
    "Each metric balances model fit and complexity to prevent overfitting or underfitting.\n",
    "\n",
    "_____________________\n",
    "\n",
    "**Mallows' $𝐶_𝑝$ Criterion**\n",
    "- It estimates how well a subset model predicts new data while considering bias and variance.\n",
    "- helps identify models with low prediction error.\n",
    "- It balances model fit (RSS) and complexity (number of predictors d).\n",
    "\n",
    "Equation for  $𝐶_𝑝$\n",
    "$$ C_p = \\frac{1}{\\sigma^2} (RSS + 2d\\sigma^2)$$ \n",
    "\n",
    "or, in prctice:\n",
    "$$ C_p = \\frac{RSS}{\\hat{\\sigma^2}} + 2d - n$$\n",
    "\n",
    "- where:\n",
    "    - RSS = Residual Sum of Squares of the model\n",
    "    - $\\hat{\\sigma^2}$\n",
    "    - d = number of predictors in the model (including intercept)\n",
    "    - n = number of observations\n",
    "\n",
    "Interpretation\n",
    "- Lower $C_p$ means a better model.\n",
    "- A model with $C_p$ ≈ d suggests good predictive performance.\n",
    "    - If $C_p$ is much larger than d, the model is overfitting.\n",
    "    - If $C_p$ is too small, the model might be underfitting.\n",
    "\n",
    "When to Use?\n",
    "- When selecting among different regression models.\n",
    "- Useful when comparing models with different numbers of predictors.\n",
    "\n",
    "Decision Rule:\n",
    "- Select the model where $C_p$ ≈ d\n",
    "- If two models have similar $C_p$ choose the simpler one (fewer predictors).\n",
    "___________________________________\n",
    "\n",
    "**Akaike Information Criterion (AIC)**\n",
    "- AIC measures model quality based on likelihood and penalizes complexity. \n",
    "- Measures how well a model explains the data while penalizing complexity.\n",
    "- Based on likelihood estimation (probability of data given model).\n",
    "- It is widely used in linear and logistic regression.\n",
    "\n",
    "Equation for AIC\n",
    "$$ AIC = -2log L + 2d$$\n",
    "\n",
    "- where:\n",
    "    - L = likelihood of the model\n",
    "    - d = number of predictors (including intercept)\n",
    "    - n = number of observations\n",
    "\n",
    "For linear regression:\n",
    "$$ AIC = n log (\\frac{RSS}{n}) + 2d$$\n",
    "\n",
    "Interpretation\n",
    "- Lower AIC is better.\n",
    "- Balances model fit (log-likelihood) and complexity (penalty term).\n",
    "- AIC prefers models that generalize well (not too complex, not too simple).\n",
    "- Can be used for any model (linear, logistic, time series, etc.).\n",
    "    - AIC works for any statistical model, including logistic regression and time series. (Unlike $C_p$)\n",
    "\n",
    "When to Use?\n",
    "- When comparing models in regression, time series, and machine learning.\n",
    "- Works well when the true model is unknown.\n",
    "\n",
    "Decision Rule:\n",
    "Select the model with the lowest AIC.\n",
    "If two models have similar AIC, prefer the simpler model.\n",
    "\n",
    "_______________________________\n",
    "\n",
    "**Bayesian Information Criterion (BIC)**\n",
    "- BIC is similar to AIC but imposes a harsher penalty for complexity. \n",
    "- Similar to AIC but with a stronger penalty for complex models.\n",
    "- Used when trying to find the best explanatory model (not just predictive performance).\n",
    "- It is based on Bayesian probability.\n",
    "\n",
    "Equation for BIC\n",
    "$$ BIC = -2log L + d log n $$\n",
    "\n",
    "For linear regression:\n",
    "$$ BIC = n log (\\frac{RSS}{n}) + dlog n$$\n",
    "- where:\n",
    "    - dlogn imposes a heavier penalty on models with more predictors than AIC.\n",
    "\n",
    "Interpretation\n",
    "- Lower BIC is better.\n",
    "- Stronger penalty for more parameters compared to AIC.\n",
    "- More likely to select simpler models.\n",
    "- If AIC and BIC select different models, BIC tends to be more conservative.\n",
    "\n",
    "When to Use BIC?\n",
    "- When selecting a model for inference (finding the best explanatory variables).\n",
    "- Works well when sample size n is large.\n",
    "\n",
    "Decision Rule:\n",
    "- Select the model with the lowest BIC.\n",
    "- If models have close BIC values, pick the simpler one.\n",
    "\n",
    "\n",
    "AIC vs. BIC\n",
    "\n",
    "| Criterion | Focus   | Penalty for Complexity          | When to Use?  |\n",
    "|---------------|--------|----------------------------------| -----------------------|  \n",
    "|AIC\t|Model Fit & Complexity |\tMild (2d) | If the goal is prediction     |\n",
    "|BIC\t|Parsimony (Simplicity) | Stronger (d log n)    |If the goal is finding the true model  |\n",
    "\n",
    "_________________\n",
    "\n",
    "**Adjusted $R^2**\n",
    "- Adjusted $R^2$ improves on $R^2$ by adjusting for the number of predictors / to account for the number of predictors.\n",
    "- it does not always increase when adding predictors.\n",
    "\n",
    "Equation for Adjusted $R^2$\n",
    "$$ R^2_{adj} = 1 - (\\frac{(1 - R^2)(n - 1)}{n - d - 1}))$$\n",
    "\n",
    "- where:\n",
    "    - $R^2$ = coefficient of determination\n",
    "    - n = number of observations\n",
    "    - d = number of predictors (excluding intercept)\n",
    "\n",
    "Interpretation\n",
    "- Higher $R^2_{adj}$ is better.\n",
    "- Unlike $R^2$,  it does not always increase when adding predictors.\n",
    "    - If a predictor does not add value, $R^2_{adj}$ will decrease.\n",
    "    - If adding a predictor improves the model $R^2_{adj}$ increases.\n",
    "\n",
    "When to Use?\n",
    "- When comparing models with different numbers of predictors.\n",
    "- If the goal is to maximize explained variance while controlling for complexity.\n",
    "\n",
    "Decision Rule:\n",
    "- Select the model with the highest Adjusted $R^2$\n",
    "- Do not just maximize $R^2$,\n",
    "- prefer simpler models with high $R^2_{adj}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809968f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries & Load Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset (Boston Housing dataset as example)\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['MEDV'] = boston.target  # Target variable\n",
    "\n",
    "# Define response variable (Y) and predictor variables (X)\n",
    "Y = df['MEDV']\n",
    "X = df.drop(columns=['MEDV'])\n",
    "\n",
    "# Add constant term for intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Step 2: Compute Subset Selection Metrics\n",
    "# Function to compute AIC, BIC, Cp, Adjusted R^2 for a given subset\n",
    "def compute_metrics(X_subset, Y):\n",
    "    model = sm.OLS(Y, X_subset).fit()\n",
    "    n, d = X_subset.shape\n",
    "    \n",
    "    # Compute metrics\n",
    "    RSS = np.sum(model.resid ** 2)\n",
    "    sigma_hat2 = RSS / (n - d)  # Estimate of variance\n",
    "    AIC = n * np.log(RSS / n) + 2 * d\n",
    "    BIC = n * np.log(RSS / n) + d * np.log(n)\n",
    "    Cp = (RSS / sigma_hat2) + 2 * d - n\n",
    "    Adj_R2 = 1 - ((1 - model.rsquared) * (n - 1) / (n - d - 1))\n",
    "    \n",
    "    return AIC, BIC, Cp, Adj_R2\n",
    "\n",
    "# Compute for different subsets of predictors\n",
    "results = []\n",
    "for d in range(1, X.shape[1] + 1):\n",
    "    for subset in combinations(X.columns, d):\n",
    "        X_subset = X[list(subset)]\n",
    "        AIC, BIC, Cp, Adj_R2 = compute_metrics(X_subset, Y)\n",
    "        results.append((subset, AIC, BIC, Cp, Adj_R2))\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['Predictors', 'AIC', 'BIC', 'Cp', 'Adj_R2'])\n",
    "\n",
    "# Step 3: Select Best Model\n",
    "# Select best model based on lowest AIC\n",
    "best_aic_model = results_df.loc[results_df['AIC'].idxmin()]\n",
    "print(\"Best Model (AIC):\", best_aic_model)\n",
    "\n",
    "# Select best model based on lowest BIC\n",
    "best_bic_model = results_df.loc[results_df['BIC'].idxmin()]\n",
    "print(\"Best Model (BIC):\", best_bic_model)\n",
    "\n",
    "# Select best model based on Cp ≈ d\n",
    "best_cp_model = results_df.iloc[(results_df['Cp'] - results_df.index).abs().argsort()[:1]]\n",
    "print(\"Best Model (Cp):\", best_cp_model)\n",
    "\n",
    "# Select best model based on highest Adjusted R2\n",
    "best_adj_r2_model = results_df.loc[results_df['Adj_R2'].idxmax()]\n",
    "print(\"Best Model (Adjusted R2):\", best_adj_r2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06407308",
   "metadata": {},
   "source": [
    "\n",
    "| Criterion | Purpose   | Penalty for Complexity          | Decision Rule  | Best When         |\n",
    "|---------------|--------|----------------------------------| -----------------------|----------------------------------|\n",
    "|AIC\t|Model Fit & Complexity |\tmoderate (2d) | Choose model with lowest AIC     | Lower is better |\n",
    "|BIC\t|Parsimony (Simplicity) | Stronger (d log n)    |Choose model with lowest BIC (more conservative) | Lower is better |\n",
    "|$𝐶_p$\t|Predictive accuracy |  Mild (2d)  | $𝐶_p$ ≈d | $𝐶_p$ ≈d |\n",
    "|$R^2_{adj}$\t|Maximize variance explained |  None  | Higher is better  | Higher is better  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a337ee",
   "metadata": {},
   "source": [
    "### **Analysisng RSS and $𝑅^2$**\n",
    "In multiple linear regression, the number of predictor variables plays a crucial role in how well the model fits the data. Two common metrics for evaluating this fit are the Residual Sum of Squares (RSS) and the Coefficient of Determination ($R^2$).\n",
    "\n",
    "##### **Relationship between RSS and number of Vaiables**\n",
    "\n",
    "$$ RSS = \\sum^n_{i = 1} (y_i - \\hat{y_i})^2 $$\n",
    "- How RSS Changes as the Number of Predictors Increases\n",
    "    - Adding More Predictors Always Reduces RSS\n",
    "        - Each new variable introduces additional flexibility, allowing the model to better fit the data.\n",
    "        - Even if the new variable is not actually useful, RSS still decreases (or remains unchanged).\n",
    "        - The reason is that the least-squares estimation adjusts the coefficients to minimize RSS, so more variables allow a better (or at least equivalent) fit.\n",
    "    - Downside: Overfitting\n",
    "        - A model that includes too many variables may capture random noise rather than true relationships.\n",
    "        - This results in a model that performs well on training data but poorly on new, unseen data (low generalizability).\n",
    "\n",
    "Key Insight\n",
    "- Adding more predictors always decreases RSS but does not necessarily improve the model’s predictive power.\n",
    "\n",
    "##### **Relationship between $𝑅^2$ and number of Vaiables**\n",
    "\n",
    "$$ 𝑅^2 = 1 - \\frac{RSS}{TSS} $$\n",
    "- Where:\n",
    "    - $𝑅^2$ represents the proportion of variance explained by the model.\n",
    "    - TSS (Total Sum of Squares) measures total variance in the outcome variable.\n",
    "\n",
    "How $𝑅^2$ Changes as the Number of Predictors Increases\n",
    "- $𝑅^2$ lways Increases or Remains the Same\n",
    "    - Since RSS never increases when adding variables, $𝑅^2$ either increases or stays the same.\n",
    "    - It never decreases, even if the additional predictor is completely irrelevant.\n",
    "- Downside: $𝑅^2$ Overestimates Model Quality\n",
    "    - A higher $𝑅^2$ does not necessarily mean the model is better.\n",
    "    - If a predictor has no real relationship with Y, it still reduces RSS slightly, artificially increasing $𝑅^2$\n",
    "    - This makes $𝑅^2$ a misleading metric for model comparison when different numbers of predictors are used.\n",
    "\n",
    "Key Insight\n",
    "Adding more predictors always increases $𝑅^2$ (or keeps it unchanged), even if the new variables have no real predictive power.\n",
    "\n",
    "##### **Addressing the Issue: Adjusted $𝑅^2$**\n",
    "- To fix the overestimation issue of $𝑅^2$ , we use Adjusted $𝑅^2$\n",
    "$$ R^2_{adj} = 1 - (\\frac{RSS/ (n - d - 1)}{TSS/(n - 1)})$$\n",
    "- Where:\n",
    "    - n = number of observations\n",
    "    - d = number of predictors\n",
    "\n",
    "How Adjusted $𝑅^2$ Helps\n",
    "- Introduces a penalty for adding more predictors.\n",
    "- Only increases if the new variable improves the model beyond chance.\n",
    "- Decreases if an added predictor is useless, preventing overfitting.\n",
    "\n",
    "Key Insight\n",
    "- Unlike $𝑅^2$ , Adjusted $𝑅^2$  does not always increase with more variables—it penalizes unnecessary complexity.\n",
    "\n",
    "Implementation of the Interaction between the number of predictors and RSS, $𝑅^2$ , and Adjusted $𝑅^2$ in multiple linear regression.\n",
    "We will:\n",
    "\n",
    "- Generate synthetic data with a known structure.\n",
    "- Fit multiple regression models by adding predictors step by step.\n",
    "- Track RSS, $𝑅^2$, and Adjusted $𝑅^2$ to observe their behavior.\n",
    "- Visualize the interaction between these metrics and the number of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15fd35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Generate Synthetic Data\n",
    "# We'll create a dataset where only two predictors 𝑋1 and 𝑋2  are truly related to the target 𝑌 but we'll introduce irrelevant predictors \n",
    "# 𝑋3,𝑋4,…,𝑋8 to observe their effect.\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 100 observations\n",
    "n = 100  \n",
    "\n",
    "# True predictors\n",
    "X1 = np.random.normal(0, 1, n)\n",
    "X2 = np.random.normal(0, 1, n)\n",
    "\n",
    "# Noise\n",
    "epsilon = np.random.normal(0, 1, n)\n",
    "\n",
    "# True relationship\n",
    "Y = 3 + 2*X1 - 1.5*X2 + epsilon\n",
    "\n",
    "# Irrelevant predictors\n",
    "X3 = np.random.normal(0, 1, n)\n",
    "X4 = np.random.normal(0, 1, n)\n",
    "X5 = np.random.normal(0, 1, n)\n",
    "X6 = np.random.normal(0, 1, n)\n",
    "X7 = np.random.normal(0, 1, n)\n",
    "X8 = np.random.normal(0, 1, n)\n",
    "\n",
    "# Combine into a DataFrame\n",
    "df = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'X4': X4, 'X5': X5, 'X6': X6, 'X7': X7, 'X8': X8, 'Y': Y})\n",
    "\n",
    "# Display first 5 rows\n",
    "df.head()\n",
    "\n",
    "# Step 3: Define a Function to Compute Regression Metrics\n",
    "# This function will: \n",
    "# 1. Fit a linear regression model with a given set of predictors.\n",
    "# 2. Compute RSS, 𝑅2, R2, and Adjusted 𝑅2.\n",
    "# 3. Return these metrics for analysis.\n",
    "\n",
    "def compute_metrics(predictors):\n",
    "    X = df[predictors]  # Select given predictors\n",
    "    X = sm.add_constant(X)  # Add intercept term\n",
    "    y = df['Y']\n",
    "    \n",
    "    model = sm.OLS(y, X).fit()  # Fit OLS regression\n",
    "    \n",
    "    rss = np.sum(model.resid ** 2)  # Compute RSS\n",
    "    r2 = model.rsquared  # Compute R^2\n",
    "    adj_r2 = model.rsquared_adj  # Compute Adjusted R^2\n",
    "    \n",
    "    return rss, r2, adj_r2\n",
    "\n",
    "# Step 4: Evaluate Different Models\n",
    "# We'll start with no predictors, then incrementally add variables to observe how RSS, 𝑅2, and Adjusted 𝑅2 change.\n",
    "predictor_sets = [\n",
    "    [],                    # No predictors\n",
    "    ['X1'],                # One true predictor\n",
    "    ['X1', 'X2'],          # Both true predictors\n",
    "    ['X1', 'X2', 'X3'],    # Adding an irrelevant predictor\n",
    "    ['X1', 'X2', 'X3', 'X4'],\n",
    "    ['X1', 'X2', 'X3', 'X4', 'X5'],\n",
    "    ['X1', 'X2', 'X3', 'X4', 'X5', 'X6'],\n",
    "    ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7'],\n",
    "    ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8'],  # All predictors\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Compute metrics for each model\n",
    "for predictors in predictor_sets:\n",
    "    rss, r2, adj_r2 = compute_metrics(predictors)\n",
    "    results.append((len(predictors), rss, r2, adj_r2))\n",
    "\n",
    "# Convert results to DataFrame for visualization\n",
    "results_df = pd.DataFrame(results, columns=['Num_Predictors', 'RSS', 'R2', 'Adjusted_R2'])\n",
    "\n",
    "# Display results\n",
    "results_df\n",
    "\n",
    "# Step 5: Visualizing the Interaction\n",
    "# Now, let's plot RSS, 𝑅2, and Adjusted 𝑅2 as functions of the number of predictors.\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot RSS\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(results_df['Num_Predictors'], results_df['RSS'], marker='o', color='red', label='RSS')\n",
    "plt.xlabel('Number of Predictors')\n",
    "plt.ylabel('RSS')\n",
    "plt.title('RSS vs Number of Predictors')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot R^2\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(results_df['Num_Predictors'], results_df['R2'], marker='o', color='blue', label='R^2')\n",
    "plt.xlabel('Number of Predictors')\n",
    "plt.ylabel('R^2')\n",
    "plt.title('R^2 vs Number of Predictors')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Adjusted R^2\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(results_df['Num_Predictors'], results_df['Adjusted_R2'], marker='o', color='green', label='Adjusted R^2')\n",
    "plt.xlabel('Number of Predictors')\n",
    "plt.ylabel('Adjusted R^2')\n",
    "plt.title('Adjusted R^2 vs Number of Predictors')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea31a83",
   "metadata": {},
   "source": [
    "Interpretation of Results\n",
    "\n",
    "- RSS Always Decreases\n",
    "    - As more predictors are added, RSS continuously declines.\n",
    "    - However, this does not mean the model improves—it could be overfitting.\n",
    "- $𝑅^2$ Always Increases or Remains Constant\n",
    "    - Since RSS decreases, $𝑅^2$ keeps increasing, even when adding irrelevant predictors.\n",
    "- Adjusted $𝑅^2$  Initially Increases, Then Decreases\n",
    "    - When adding useful predictors (e.g., $𝑋_1,𝑋_2$), Adjusted $𝑅^2$ increases.\n",
    "    - When adding irrelevant predictors (e.g.,  $𝑋_3,𝑋_4,… $), Adjusted $𝑅^2$ starts decreasing.\n",
    "        - This decline signals that we are adding unnecessary complexity.\n",
    "\n",
    "Key Takeaways\n",
    "\n",
    "✅ RSS is unreliable for selecting the best model because it always decreases as predictors are added.\n",
    "\n",
    "✅ $𝑅^2$ is misleading because it always increases with more predictors, even if they don’t improve prediction.\n",
    "\n",
    "✅ Adjusted $𝑅^2$ is more useful as it penalizes complexity and peaks at the optimal number of predictors.\n",
    "\n",
    "NB: add cross-validation to confirm these results on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b18f481",
   "metadata": {},
   "source": [
    "##### **Effects of Low RSS (or High $𝑅^2$) on Training and Test Error**\n",
    "- Residual Sum of Squares (RSS) and $𝑅^2$ are key metrics in regression analysis, but their impact on training and test error depends on the complexity of the model.\n",
    "\n",
    "Training Error: Low RSS (High $𝑅^2$) Always Improves\n",
    "- Lower RSS means that the model fits the training data more closely, leading to lower training error.\n",
    "- Higher $𝑅^2$ (closer to 1) suggests the model explains more of the variance in the training data.\n",
    "- Adding more predictors always decreases RSS and increases $𝑅^2$ (even if the predictors are irrelevant).\n",
    "\n",
    "✅ Effect:\n",
    "- Training error keeps decreasing as we add more predictors.\n",
    "- This can lead to overfitting, where the model memorizes training data instead of learning general patterns.\n",
    "\n",
    "Test Error: Low RSS (High $𝑅^2$ ) Can Be Misleading\n",
    "- If the model is too complex, it captures noise in the training data, which leads to poor generalization.\n",
    "- As a result, the test error may increase even though the training error remains low.\n",
    "- The bias-variance tradeoff explains this effect:\n",
    "    - Underfitting (high bias): Model is too simple → both training and test error are high.\n",
    "    - Good fit (optimal complexity): Model balances fit and generalization → test error is minimized.\n",
    "    - Overfitting (high variance): Model is too complex → training error is low, but test error increases.\n",
    "\n",
    "✅ Effect:\n",
    "- When RSS is too low (or $𝑅^2$ is too high), the model is likely overfitting the training data.\n",
    "- The model may perform poorly on new, unseen data (high test error).\n",
    "\n",
    "| Scenario | Training Error   | Test Error | Model Complexity |\n",
    "|---------------|--------|----------------------------------| -----------------------|  \n",
    "|Underfitting (High Bias)\t|High |\tHigh| Too Simple  |\n",
    "|Good Fit (Balanced)\t|Moderate |Low (Optimal)  |Just Right  |\n",
    "|Overfitting (High Variance)        |\tVery Low|\tHigh      | Too Complex| \n",
    "\n",
    "🔹 Low RSS / High $𝑅^2$ in training is not always good! If the test error increases, the model is overfitting. The best model minimizes test error, not just training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb8989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y = 2 * X**2 + np.random.normal(0, 2, size=X.shape)  # Quadratic relationship + noise\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Train models with different polynomial degrees\n",
    "degrees = [1, 2, 5, 10]  # Increasing model complexity\n",
    "train_errors, test_errors = [], []\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for d in degrees:\n",
    "    poly = PolynomialFeatures(degree=d)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train_poly)\n",
    "    y_test_pred = model.predict(X_test_poly)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    train_errors.append(train_mse)\n",
    "    test_errors.append(test_mse)\n",
    "    \n",
    "    # Plot the fitted model\n",
    "    X_range = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "    X_range_poly = poly.transform(X_range)\n",
    "    y_range_pred = model.predict(X_range_poly)\n",
    "    \n",
    "    plt.subplot(2, 2, degrees.index(d) + 1)\n",
    "    plt.scatter(X_train, y_train, color='blue', label='Train Data')\n",
    "    plt.scatter(X_test, y_test, color='red', label='Test Data')\n",
    "    plt.plot(X_range, y_range_pred, color='green', label=f'Degree {d} Fit')\n",
    "    plt.legend()\n",
    "    plt.title(f'Polynomial Degree {d}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Plot Training vs Test Errors\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(degrees, train_errors, marker='o', label='Training Error', color='blue')\n",
    "plt.plot(degrees, test_errors, marker='o', label='Test Error', color='red')\n",
    "plt.xlabel('Model Complexity (Polynomial Degree)')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.title('Training vs Test Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400727c7",
   "metadata": {},
   "source": [
    "Explanation of Results\n",
    "\n",
    "Polynomial Degree 1 (Underfitting)\n",
    "- The model is too simple (linear), leading to high training and test error (underfitting).\n",
    "\n",
    "Polynomial Degree 2 (Good Fit)\n",
    "- The model closely matches the true quadratic relationship, resulting in low training and test error (optimal model).\n",
    "\n",
    "Polynomial Degree 5 & 10 (Overfitting)\n",
    "- The model is overly complex and starts capturing noise in the training data.\n",
    "- Training error decreases, but test error increases, showing poor generalization.\n",
    "\n",
    "___________________\n",
    "\n",
    "**Why Not Use Training RSS and Training $𝑅^2$ to Select the Best Model?**\n",
    "\n",
    "When selecting the best model from a set of models with different predictors, relying on Training Residual Sum of Squares (RSS) or Training $𝑅^2$ can be misleading due to the following reasons:\n",
    "\n",
    "Overfitting Risk\n",
    "- Training RSS always decreases when more predictors are added, even if those predictors are irrelevant.\n",
    "- Training $𝑅^2$ always increases when more predictors are included, but this does not mean the model generalizes well.\n",
    "- A model with too many predictors might fit the noise in the data, leading to poor test performance.\n",
    "\n",
    "🔹 Example:\n",
    "- Imagine you add random noise variables to a regression model. The Training RSS will decrease and Training $𝑅^2$ will increase, but the model's real predictive power remains the same or worsens on unseen data.\n",
    "\n",
    "Does Not Reflect Out-of-Sample Performance\n",
    "- The goal of model selection is to find a model that generalizes well to new, unseen data.\n",
    "- Training RSS and Training $𝑅^2$ only measure how well the model fits the training data, not how well it will perform on future data.\n",
    "A good model should minimize test error, not just training error.\n",
    "\n",
    "🔹 Example:\n",
    "- A complex model with many predictors can memorize the training data (low RSS, high $𝑅^2$), but when tested on new data, its test error increases due to overfitting.\n",
    "\n",
    "Model Complexity Bias\n",
    "- More predictors = more parameters to estimate, which can lead to unnecessarily complex models.\n",
    "- Training RSS and $𝑅^2$ do not penalize models for complexity, making them poor criteria for selecting the best model.\n",
    "\n",
    "✅ Solution: Use Model Selection Methods That Penalize Complexity\n",
    "- To avoid selecting an overfit model, use:\n",
    "    - ✔ Cross-Validation (CV) – evaluates performance on unseen data.\n",
    "    - ✔ AIC/BIC – penalize overly complex models.\n",
    "    - ✔ Adjusted $𝑅^2$ – corrects for the number of predictors.\n",
    "\n",
    "Interpret and Use the Final Model:\n",
    "- Examine the selected features and their impact on predictions.\n",
    "- Use the model for inference or prediction in a real-world application.\n",
    "\n",
    "How to Interpret Results\n",
    "- Lower AIC/BIC values are better\n",
    "    - Model with bmi, bp, s5 has the lowest AIC/BIC → best trade-off between fit and complexity.\n",
    "- Lower CV error indicates better generalization\n",
    "    - Model with bmi, bp, s5 also has the lowest CV error → best out-of-sample performance.\n",
    "\n",
    "Python implementation comparing Cross-Validation (CV) and Information Criteria (AIC/BIC) for selecting the best regression model. \n",
    "- Use linear regression with different subsets of features and evaluate them using \n",
    "    - CV error, \n",
    "    - AIC, and \n",
    "    - BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff50e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load dataset\n",
    "diabetes = load_diabetes()\n",
    "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y = diabetes.target  # Target variable\n",
    "\n",
    "# Define function to calculate AIC & BIC\n",
    "def compute_aic_bic(model, X, y):\n",
    "    \"\"\"Calculate AIC and BIC for a given regression model\"\"\"\n",
    "    n = X.shape[0]  # Number of samples\n",
    "    p = X.shape[1]  # Number of predictors\n",
    "    residuals = model.resid\n",
    "    sse = np.sum(residuals**2)  # Sum of Squared Errors\n",
    "    sigma2 = sse / n  # Estimate of variance\n",
    "\n",
    "    # Calculate Log-Likelihood\n",
    "    log_likelihood = -0.5 * n * np.log(2 * np.pi * sigma2) - (sse / (2 * sigma2))\n",
    "\n",
    "    # AIC and BIC formulas\n",
    "    aic = -2 * log_likelihood + 2 * p\n",
    "    bic = -2 * log_likelihood + p * np.log(n)\n",
    "\n",
    "    return aic, bic\n",
    "\n",
    "# Define function to perform model selection using CV, AIC, and BIC\n",
    "def compare_models(X, y):\n",
    "    results = []\n",
    "\n",
    "    # Loop through subsets of features (we'll try 1 to 5 features for demonstration)\n",
    "    feature_subsets = [\n",
    "        ['bmi'],\n",
    "        ['bmi', 'bp'],\n",
    "        ['bmi', 'bp', 's5'],\n",
    "        ['bmi', 'bp', 's5', 'sex'],\n",
    "        ['bmi', 'bp', 's5', 'sex', 's3']\n",
    "    ]\n",
    "\n",
    "    for features in feature_subsets:\n",
    "        X_subset = X[features]\n",
    "\n",
    "        # Fit Linear Regression Model\n",
    "        model = sm.OLS(y, sm.add_constant(X_subset)).fit()\n",
    "\n",
    "        # Compute AIC and BIC\n",
    "        aic, bic = compute_aic_bic(model, X_subset, y)\n",
    "\n",
    "        # Perform 5-Fold Cross-Validation\n",
    "        lr = LinearRegression()\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_error = -np.mean(cross_val_score(lr, X_subset, y, cv=cv, scoring='neg_mean_squared_error'))\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Features': features,\n",
    "            'AIC': round(aic, 2),\n",
    "            'BIC': round(bic, 2),\n",
    "            'Cross-Validation Error': round(cv_error, 2)\n",
    "        })\n",
    "\n",
    "    # Convert results to DataFrame and display\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Run model comparison\n",
    "comparison_results = compare_models(X, y)\n",
    "print(comparison_results)\n",
    "\n",
    "#                  Features    AIC      BIC  Cross-Validation Error\n",
    "# 0                 [bmi]  5164.32  5171.24                  3883.45\n",
    "# 1            [bmi, bp]  5142.78  5154.63                  3832.27\n",
    "# 2       [bmi, bp, s5]  5102.46  5119.24                  3568.51\n",
    "# 3  [bmi, bp, s5, sex]  5104.93  5126.65                  3574.68\n",
    "# 4  [bmi, bp, s5, sex, s3]  5109.23  5135.88                  3603.12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5290e525",
   "metadata": {},
   "source": [
    "##### **Analyzing the Interaction Between RSS and the Penalty Terms in $𝐶_𝑝$, AIC, and BIC**\n",
    "- Model selection criteria like Mallows', Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC) aim to balance model fit (measured by RSS) and model complexity (penalized by the number of predictors). \n",
    "- Understanding how RSS interacts with the penalty terms in each criterion helps us make better model selection decisions.\n",
    "\n",
    "**Residual Sum of Squares (RSS) and Model Complexity**\n",
    "- Residual Sum of Squares (RSS) is a measure of how well the model fits the data:\n",
    "$$RSS = \\sum^n_{i = 1} (y_i - \\hat{y_i})^2$$\n",
    "- Lower RSS → Better fit to the training data.\n",
    "- Higher RSS → Poorer fit, meaning the model does not capture the data well.\n",
    "\n",
    "However, RSS always decreases when we add more predictors because the model gets more flexibility. This can lead to overfitting, where the model fits the training data too well but generalizes poorly to new data.\n",
    "\n",
    "Thus, we need penalty terms to prevent overfitting.\n",
    "\n",
    "**Mallows’ $C_p$:  Interaction Between RSS and Penalty**\n",
    "$$ C_p = \\frac{RSS}{\\hat{\\sigma^2}} + 2d - n$$\n",
    "\n",
    "How RSS Affects $C_p$\n",
    "- Lower RSS → Lower $C_p$ (suggesting a better model).\n",
    "- Adding more predictors decreases RSS, but the penalty term 2d prevents too many predictors.\n",
    "- If $C_p$ is much larger than d, it suggests overfitting.\n",
    "\n",
    "Effect of the Penalty Term (2d)\n",
    "- The penalty grows linearly with the number of predictors.\n",
    "- If the penalty is too low, we risk overfitting.\n",
    "- If the penalty is too high, we risk underfitting.\n",
    "\n",
    "Key Insight\n",
    "- We want to choose a model where $C_p$ ≈d, meaning the reduction in RSS is justified by the model complexity.\n",
    "\n",
    "**Akaike Information Criterion (AIC): Interaction Between RSS and Penalty**\n",
    "$$ AIC = n log (\\frac{RSS}{n}) + 2d$$\n",
    "\n",
    "How RSS Affects AIC\n",
    "- Lower RSS → Lower AIC, meaning a better model.\n",
    "- Adding more predictors decreases RSS, but the penalty term 2d prevents too much complexity.\n",
    "\n",
    "Effect of the Penalty Term (2d)\n",
    "- The penalty grows linearly with the number of predictors.\n",
    "- Unlike $𝐶_𝑝$, AIC is based on likelihood estimation, so it applies to any statistical model.\n",
    "- AIC favors models that explain the data well but penalizes excessive predictors.\n",
    "\n",
    "Key Insight\n",
    "- AIC does not penalize complexity as strongly as BIC, making it more tolerant of slightly complex models.\n",
    "\n",
    "\n",
    "**Bayesian Information Criterion (BIC): Interaction Between RSS and Penalty**\n",
    "$$ BIC = n log (\\frac{RSS}{n}) + dlog n$$\n",
    "\n",
    "How RSS Affects BIC\n",
    "- Lower RSS → Lower BIC, suggesting a better model.\n",
    "- BIC behaves similarly to AIC but applies a stronger penalty for additional predictors.\n",
    "\n",
    "Effect of the Penalty Term (𝑑 log 𝑛)\n",
    "- The penalty grows faster than AIC because it includes log 𝑛\n",
    "- The larger the sample size n, the stronger the penalty for adding predictors.\n",
    "- BIC is more conservative than AIC, meaning it favors simpler models.\n",
    "\n",
    "Key Insight\n",
    "= BIC penalizes additional predictors more aggressively than AIC. When AIC and BIC select different models, BIC usually selects the simpler one.\n",
    "\n",
    "Final Takeaways\n",
    "- RSS always decreases as we add predictors, so we need penalty terms to avoid overfitting.\n",
    "- $C_p$, AIC, and BIC balance RSS and model complexity differently:\n",
    "    - $C_p$ is good for predictive models but assumes the full model is unbiased.\n",
    "    - AIC is more flexible, allowing slightly complex models.\n",
    "    - BIC is conservative, preferring simpler models when in doubt.\n",
    "- AIC is preferred for prediction, while BIC is better for finding the \"true\" model.\n",
    "\n",
    "Demonstrate the interaction between RSS (Residual Sum of Squares) and the penalty terms in model selection criteria:\n",
    "- Cp (Mallows' Cp)\n",
    "- AIC (Akaike Information Criterion)\n",
    "- BIC (Bayesian Information Criterion)\n",
    "\n",
    "- We will:\n",
    "    - ✅ Generate synthetic data with a known structure.\n",
    "    - ✅ Fit multiple regression models with an increasing number of predictors.\n",
    "    - ✅ Calculate RSS, Cp, AIC, and BIC for each model.\n",
    "    - ✅ Visualize how the penalty terms interact with RSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Generate Synthetic Data\n",
    "# We'll create a dataset where only two predictors 𝑋1 and 𝑋2  are truly related to the target 𝑌 but we'll introduce irrelevant predictors \n",
    "# 𝑋3,𝑋4,…,𝑋8 to observe their effect.\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 100 observations\n",
    "n = 100  \n",
    "\n",
    "# True predictors\n",
    "X1 = np.random.normal(0, 1, n)\n",
    "X2 = np.random.normal(0, 1, n)\n",
    "\n",
    "# Noise\n",
    "epsilon = np.random.normal(0, 1, n)\n",
    "\n",
    "# True relationship\n",
    "Y = 3 + 2*X1 - 1.5*X2 + epsilon\n",
    "\n",
    "# Irrelevant predictors\n",
    "X3 = np.random.normal(0, 1, n)\n",
    "X4 = np.random.normal(0, 1, n)\n",
    "X5 = np.random.normal(0, 1, n)\n",
    "X6 = np.random.normal(0, 1, n)\n",
    "X7 = np.random.normal(0, 1, n)\n",
    "X8 = np.random.normal(0, 1, n)\n",
    "\n",
    "# Combine into a DataFrame\n",
    "df = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'X4': X4, 'X5': X5, 'X6': X6, 'X7': X7, 'X8': X8, 'Y': Y})\n",
    "\n",
    "# Display first 5 rows\n",
    "df.head()\n",
    "\n",
    "# Step 3: Define Functions to Compute Cp, AIC, and BIC\n",
    "def compute_metrics(predictors):\n",
    "    X = df[predictors]  # Select given predictors\n",
    "    X = sm.add_constant(X)  # Add intercept\n",
    "    y = df['Y']\n",
    "    \n",
    "    model = sm.OLS(y, X).fit()  # Fit OLS regression\n",
    "    \n",
    "    rss = np.sum(model.resid ** 2)  # Compute RSS\n",
    "    p = len(predictors) + 1  # Number of parameters (predictors + intercept)\n",
    "    \n",
    "    # Estimate variance from full model\n",
    "    full_model = sm.OLS(y, sm.add_constant(df.iloc[:, :-1])).fit()\n",
    "    sigma2 = np.sum(full_model.resid ** 2) / (n - len(df.columns[:-1]) - 1)\n",
    "    \n",
    "    # Compute Cp, AIC, and BIC\n",
    "    cp = (rss / sigma2) - n + 2 * p\n",
    "    aic = n * np.log(rss / n) + 2 * p\n",
    "    bic = n * np.log(rss / n) + p * np.log(n)\n",
    "    \n",
    "    return rss, cp, aic, bic\n",
    "\n",
    "\n",
    "# Step 4: Evaluate Different Models\n",
    "# We'll compute RSS, Cp, AIC, and BIC for different models, starting from no predictors and progressively adding variables.\n",
    "predictor_sets = [\n",
    "    [],                    # No predictors\n",
    "    ['X1'],                # One true predictor\n",
    "    ['X1', 'X2'],          # Both true predictors\n",
    "    ['X1', 'X2', 'X3'],    # Adding an irrelevant predictor\n",
    "    ['X1', 'X2', 'X3', 'X4'],\n",
    "    ['X1', 'X2', 'X3', 'X4', 'X5'],\n",
    "    ['X1', 'X2', 'X3', 'X4', 'X5', 'X6'],\n",
    "    ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7'],\n",
    "    ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8'],  # All predictors\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "#  Compute metrics for each model\n",
    "for predictors in predictor_sets:\n",
    "    rss, cp, aic, bic = compute_metrics(predictors)\n",
    "    results.append((len(predictors), rss, cp, aic, bic))\n",
    "\n",
    "# Convert results to DataFrame for visualization\n",
    "results_df = pd.DataFrame(results, columns=['Num_Predictors', 'RSS', 'Cp', 'AIC', 'BIC'])\n",
    "\n",
    "# Display results\n",
    "results_df\n",
    "\n",
    "# Step 5: Visualizing the Interaction\n",
    "# plot RSS, Cp, AIC, and BIC as functions of the number of predictors.\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot RSS\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(results_df['Num_Predictors'], results_df['RSS'], marker='o', color='red')\n",
    "plt.xlabel('Number of Predictors')\n",
    "plt.ylabel('RSS')\n",
    "plt.title('RSS vs Number of Predictors')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Cp\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(results_df['Num_Predictors'], results_df['Cp'], marker='o', color='blue')\n",
    "plt.xlabel('Number of Predictors')\n",
    "plt.ylabel('Cp')\n",
    "plt.title('Cp vs Number of Predictors')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot AIC\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(results_df['Num_Predictors'], results_df['AIC'], marker='o', color='green')\n",
    "plt.xlabel('Number of Predictors')\n",
    "plt.ylabel('AIC')\n",
    "plt.title('AIC vs Number of Predictors')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot BIC\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(results_df['Num_Predictors'], results_df['BIC'], marker='o', color='purple')\n",
    "plt.xlabel('Number of Predictors')\n",
    "plt.ylabel('BIC')\n",
    "plt.title('BIC vs Number of Predictors')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef73def",
   "metadata": {},
   "source": [
    "Interpretation of Results\n",
    "- RSS Always Decreases\n",
    "    - As more predictors are added, RSS continuously declines, but this does not mean a better model.\n",
    "- Cp, AIC, and BIC Capture Complexity\n",
    "    - Cp, AIC, and BIC first decrease, then increase after a certain number of predictors.\n",
    "    - This reflects overfitting, where adding irrelevant predictors increases model complexity without improving fit.\n",
    "- BIC Penalizes Complexity More Than AIC\n",
    "    - Since BIC has a stronger penalty term (plog(n)), it often selects a simpler model than AIC.\n",
    "\n",
    "Key Takeaways\n",
    "\n",
    "✅ Penalty terms counterbalance the decreasing RSS, preventing overfitting.\n",
    "\n",
    "✅ The \"best\" model is found where Cp, AIC, or BIC is minimized.\n",
    "\n",
    "✅ BIC tends to select fewer predictors than AIC due to its stronger penalty.\n",
    "\n",
    "NB: Extend this with cross-validation to confirm model selection on test data\n",
    "_________________\n",
    "\n",
    "Incorporating cross-validation to evaluate model selection.\n",
    "\n",
    "What We Will Do:\n",
    "-  Split the dataset into training and test sets.\n",
    "- Train multiple models with an increasing number of predictors.\n",
    "- Compute RSS, Cp, AIC, and BIC on the training set.\n",
    "- Use Cross-Validation (CV) to evaluate test error.\n",
    "- Compare model selection criteria (AIC, BIC, Cp) with actual test error.\n",
    "- Visualize how penalty terms affect model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa6df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 2: Generate Synthetic Data\n",
    "# We'll create a dataset where only two predictors 𝑋1 and 𝑋2  are truly related to the target 𝑌 but we'll introduce irrelevant predictors \n",
    "# 𝑋3,𝑋4,…,𝑋8 to observe their effect.\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 100 observations\n",
    "n = 100  \n",
    "\n",
    "# True predictors\n",
    "X1 = np.random.normal(0, 1, n)\n",
    "X2 = np.random.normal(0, 1, n)\n",
    "\n",
    "# Noise\n",
    "epsilon = np.random.normal(0, 1, n)\n",
    "\n",
    "# True relationship\n",
    "Y = 3 + 2*X1 - 1.5*X2 + epsilon\n",
    "\n",
    "# Irrelevant predictors\n",
    "X3 = np.random.normal(0, 1, n)\n",
    "X4 = np.random.normal(0, 1, n)\n",
    "X5 = np.random.normal(0, 1, n)\n",
    "X6 = np.random.normal(0, 1, n)\n",
    "X7 = np.random.normal(0, 1, n)\n",
    "X8 = np.random.normal(0, 1, n)\n",
    "\n",
    "# Combine into a DataFrame\n",
    "df = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'X4': X4, 'X5': X5, 'X6': X6, 'X7': X7, 'X8': X8, 'Y': Y})\n",
    "\n",
    "# Display first 5 rows\n",
    "df.head()\n",
    "\n",
    "# Step 3: Split into Training & Test Set\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train = train_df.drop(columns=['Y'])\n",
    "y_train = train_df['Y']\n",
    "X_test = test_df.drop(columns=['Y'])\n",
    "y_test = test_df['Y']\n",
    "\n",
    "\n",
    "# Step 4: Define Functions to Compute Cp, AIC, and BIC\n",
    "def compute_metrics(predictors, X_train, y_train):\n",
    "    X = X_train[predictors]\n",
    "    X = sm.add_constant(X)  # Add intercept\n",
    "    model = sm.OLS(y_train, X).fit()\n",
    "\n",
    "    rss = np.sum(model.resid ** 2)\n",
    "    p = len(predictors) + 1  # Number of predictors + intercept\n",
    "    n = len(y_train)\n",
    "\n",
    "    # Estimate variance from full model\n",
    "    full_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "    sigma2 = np.sum(full_model.resid ** 2) / (n - len(X_train.columns) - 1)\n",
    "\n",
    "    cp = (rss / sigma2) - n + 2 * p\n",
    "    aic = n * np.log(rss / n) + 2 * p\n",
    "    bic = n * np.log(rss / n) + p * np.log(n)\n",
    "\n",
    "    return rss, cp, aic, bic, model\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Evaluate Different Models\n",
    "# Evaluate Models Using CV and Compute Test Error\n",
    "predictor_sets = [\n",
    "    [], ['X1'], ['X1', 'X2'], ['X1', 'X2', 'X3'], ['X1', 'X2', 'X3', 'X4'],\n",
    "    ['X1', 'X2', 'X3', 'X4', 'X5'], ['X1', 'X2', 'X3', 'X4', 'X5', 'X6'],\n",
    "    ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7'], ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8']\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for predictors in predictor_sets:\n",
    "    rss, cp, aic, bic, model = compute_metrics(predictors, X_train, y_train)\n",
    "\n",
    "    # Compute test error (MSE)\n",
    "    if predictors:\n",
    "        X_test_subset = sm.add_constant(X_test[predictors])\n",
    "        y_pred = model.predict(X_test_subset)\n",
    "        test_mse = mean_squared_error(y_test, y_pred)\n",
    "    else:\n",
    "        test_mse = np.mean(y_test ** 2)\n",
    "\n",
    "    # Compute Cross-Validation Score (5-fold CV)\n",
    "    if predictors:\n",
    "        lm = LinearRegression()\n",
    "        cv_score = -np.mean(cross_val_score(lm, X_train[predictors], y_train, scoring='neg_mean_squared_error', cv=5))\n",
    "    else:\n",
    "        cv_score = np.mean(y_train ** 2)\n",
    "\n",
    "    results.append((len(predictors), rss, cp, aic, bic, test_mse, cv_score))\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['Num_Predictors', 'RSS', 'Cp', 'AIC', 'BIC', 'Test_MSE', 'CV_MSE'])\n",
    "\n",
    "# Display results\n",
    "results_df\n",
    "\n",
    "\n",
    "# Step 6: Visualizing the Model Selection Criteria\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# RSS Plot\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(results_df['Num_Predictors'], results_df['RSS'], marker='o', color='red')\n",
    "plt.xlabel('Number of Predictors')\n",
    "plt.ylabel('RSS')\n",
    "plt.title('RSS vs Number of Predictors')\n",
    "plt.grid(True)\n",
    "\n",
    "# Cp Plot\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.plot(results_df['Num_Predictors'], results_df['Cp'], marker='o', color='blue')\n",
    "plt.xlabel('Number of Predictors')\n",
    "plt.ylabel('Cp')\n",
    "plt.title('Cp vs Number of Predictors')\n",
    "plt.grid(True)\n",
    "\n",
    "# AIC Plot\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.plot(results_df['Num_Predictors'], results_df['AIC'], marker='o', color='green')\n",
    "plt.xlabel('Number of Predictors')\n",
    "plt.ylabel('AIC')\n",
    "plt.title('AIC vs Number of Predictors')\n",
    "plt.grid(True)\n",
    "\n",
    "# BIC Plot\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(results_df['Num_Predictors'], results_df['BIC'], marker='o', color='purple')\n",
    "plt.xlabel('Number of Predictors')\n",
    "plt.ylabel('BIC')\n",
    "plt.title('BIC vs Number of Predictors')\n",
    "plt.grid(True)\n",
    "\n",
    "# Test MSE Plot\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.plot(results_df['Num_Predictors'], results_df['Test_MSE'], marker='o', color='brown')\n",
    "plt.xlabel('Number of Predictors')\n",
    "plt.ylabel('Test MSE')\n",
    "plt.title('Test MSE vs Number of Predictors')\n",
    "plt.grid(True)\n",
    "\n",
    "# CV MSE Plot\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.plot(results_df['Num_Predictors'], results_df['CV_MSE'], marker='o', color='black')\n",
    "plt.xlabel('Number of Predictors')\n",
    "plt.ylabel('Cross-Validation MSE')\n",
    "plt.title('Cross-Validation MSE vs Number of Predictors')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e8d5d",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "- RSS Decreases as predictors increase, but this does not mean a better model.\n",
    "- AIC and Cp First Decrease, Then Increase\n",
    "- This happens due to overfitting, where additional variables increase model complexity.\n",
    "- BIC Penalizes Complexity More than AIC and selects fewer predictors.\n",
    "- Test MSE & CV MSE Show U-Shaped Pattern\n",
    "- The test error initially decreases but rises when adding too many predictors.\n",
    "- The best model minimizes Test MSE & CV MSE, aligning with AIC/BIC recommendations.\n",
    "\n",
    "Key Takeaways\n",
    "\n",
    "✅ AIC & Cp favor slightly complex models, while BIC prefers simpler models.\n",
    "\n",
    "✅ Test MSE confirms that the best model is not always the most complex one.\n",
    "\n",
    "✅ Cross-validation helps validate model selection in real-world applications.\n",
    "\n",
    "NB: Use Ridge & Lasso regression for automatic feature selection\n",
    "_____________________________\n",
    "\n",
    "### Pros and Cons of Best Subset Selection\n",
    "✅ Pros:\n",
    "- Finds the best performing model by considering all combinations.\n",
    "- Provides a clear interpretation of feature importance.\n",
    "\n",
    "❌ Cons:\n",
    "- Computationally expensive for large datasets ($𝑂(2^𝑝))$ complexity).\n",
    "    - Can lead to overfitting if not validated properly.\n",
    "\n",
    "**Drawback of Best Subset Selection**\n",
    "\n",
    "Best Subset Selection evaluates all possible combinations of predictor variables to determine the best model. However, it has two major drawbacks:\n",
    "\n",
    "1️⃣ Computationally Expensive 🚀\n",
    "- If there are p predictors, we need to evaluate $2^𝑝$ models (including all possible subsets).\n",
    "    - For p = 10, this is $2^10 = 1,024 models—manageable.\n",
    "    - For p = 20, this becomes $2^20 =1,048,576 models—computationally infeasible!\n",
    "        - For large datasets, this method is impractical.\n",
    "\n",
    "2️⃣ Risk of Overfitting 🎭\n",
    "- Since it searches for the \"best\" subset by evaluating many models, it may find patterns in noise rather than meaningful relationships.\n",
    "- This leads to overfitting, meaning poor performance on unseen data.\n",
    "\n",
    "##### 💡 Alternative? Stepwise Selection! 🎯\n",
    "To handle computational challenges, alternative methods like \n",
    "- Stepwise Selection (Forward or Backward Selection), \n",
    "    - builds (or reduces) a model iteratively, making it computationally more efficient.\n",
    "- Lasso Regression, and \n",
    "- Feature Selection with Tree-based Methods are often preferred.\n",
    "\n",
    "##### Step-by-Step Implementation of Best Subset Selection in Python\n",
    "- Load Data\n",
    "- Generate All Possible Feature Subsets\n",
    "- Fit Models for Each Subset\n",
    "- Evaluate Performance (Using Adjusted $𝑅^2 and Cross-Validation Error)\n",
    "- Select the Best Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load Example Dataset (Using Boston Housing Data)\n",
    "from sklearn.datasets import load_diabetes\n",
    "data = load_diabetes()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target  # Add target column\n",
    "\n",
    "# Define the Feature Set and Target Variable\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "# Function to Compute Adjusted R-Squared\n",
    "def adjusted_r2(r2, n, p):\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "# Function for Best Subset Selection\n",
    "def best_subset_selection(X, y):\n",
    "    best_score = -np.inf\n",
    "    best_model = None\n",
    "    best_features = None\n",
    "    n = len(y)\n",
    "\n",
    "    # Iterate through all possible feature subsets\n",
    "    for k in range(1, len(X.columns) + 1):  # k: Number of features in subset\n",
    "        for subset in itertools.combinations(X.columns, k):\n",
    "            X_subset = X[list(subset)]\n",
    "\n",
    "            # Train Model\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_subset, y)\n",
    "\n",
    "            # Compute Adjusted R2\n",
    "            r2 = r2_score(y, model.predict(X_subset))\n",
    "            adj_r2 = adjusted_r2(r2, n, k)\n",
    "\n",
    "            # Compute Cross-Validation Score (for generalization)\n",
    "            cv_score = np.mean(cross_val_score(model, X_subset, y, cv=5, scoring='r2'))\n",
    "\n",
    "            # Select Model with Best Adjusted R2 or CV Score\n",
    "            if adj_r2 > best_score:\n",
    "                best_score = adj_r2\n",
    "                best_model = model\n",
    "                best_features = subset\n",
    "\n",
    "    return best_features, best_model, best_score\n",
    "\n",
    "# Run Best Subset Selection\n",
    "best_features, best_model, best_score = best_subset_selection(X, y)\n",
    "\n",
    "# Output Results\n",
    "print(f\"Best Features: {best_features}\")\n",
    "print(f\"Best Adjusted R2 Score: {best_score:.4f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9774dab",
   "metadata": {},
   "source": [
    "### 2. Stepwise Selection (Forward or Backward Selection)\n",
    "A Smarter Approach: Optimized versions of Subset Selection. Stepwise selection is an improvement over Best Subset Selection. \n",
    "- It adds or removes predictors one step at a time based on statistical significance or performance metrics.\n",
    "\n",
    "🔸 Types of Stepwise Selection:\n",
    "- Forward Stepwise Selection ✅ (Starts with nothing and adds predictors one by one)\n",
    "- Backward Stepwise Selection ❌ (Starts with all predictors and removes them one by one)\n",
    "\n",
    "##### **Forward Stepwise Selection (FSS)**\n",
    "🔹 What is it?\n",
    "- Forward Selection starts with no predictors and adds the most significant one at each step until adding more predictors does not improve the model.\n",
    "\n",
    "🔹 Steps of Forward Stepwise Selection: \n",
    "- 1️⃣ Start with an empty model (only the intercept).\n",
    "- 2️⃣ For each predictor not yet in the model, fit a model including that predictor.\n",
    "- 3️⃣ Choose the predictor that improves model performance the most (e.g., lowest AIC, highest adjusted $𝑅^2$, or lowest p-value).\n",
    "- 4️⃣ Repeat steps 2-3 until no predictor significantly improves performance.\n",
    "- 5️⃣ Final model contains the best subset of features.\n",
    "\n",
    "Pros: \n",
    "- ✅ Computationally more efficient than Best Subset Selection.\n",
    "- ✅ Avoids overfitting by stopping when no further improvements are found.\n",
    "\n",
    "Cons: \n",
    "- ❌ A variable added early cannot be removed later, even if it becomes irrelevant.\n",
    "- ❌ Might miss optimal feature combinations since it makes decisions one step at a time.\n",
    "\n",
    "🔹 Python Example: Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28233c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load Dataset\n",
    "data = load_diabetes()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "# Define X and y\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "# Forward Selection Function\n",
    "def forward_selection(X, y):\n",
    "    selected_features = []\n",
    "    remaining_features = list(X.columns)\n",
    "    best_score = float('-inf')\n",
    "\n",
    "    while remaining_features:\n",
    "        scores = []\n",
    "        for feature in remaining_features:\n",
    "            model = sm.OLS(y, sm.add_constant(X[selected_features + [feature]])).fit()\n",
    "            scores.append((model.rsquared_adj, feature))\n",
    "\n",
    "        scores.sort(reverse=True)  # Sort by best adjusted R²\n",
    "        if scores[0][0] > best_score:\n",
    "            best_score = scores[0][0]\n",
    "            selected_features.append(scores[0][1])\n",
    "            remaining_features.remove(scores[0][1])\n",
    "        else:\n",
    "            break  # Stop if adding a new feature does not improve the model\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "selected_features_fss = forward_selection(X, y)\n",
    "print(f\"Selected Features (Forward Stepwise Selection): {selected_features_fss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ee829",
   "metadata": {},
   "source": [
    "##### **Backward Stepwise Selection (BSS)**\n",
    "🔹 What is it?\n",
    "- Backward Selection starts with all predictors and removes the least significant one at each step until removing more predictors worsens model performance.\n",
    "\n",
    "🔹 Steps of Backward Stepwise Selection: \n",
    "- 1️⃣ Start with a full model (all predictors included).\n",
    "- 2️⃣ For each predictor in the model, fit a new model excluding that predictor.\n",
    "- 3️⃣ Choose the predictor whose removal improves model performance the most (e.g., highest adjusted $𝑅^2$, lowest AIC, highest p-value above a threshold).\n",
    "- 4️⃣ Repeat steps 2-3 until no predictor can be removed without worsening model performance.\n",
    "- 5️⃣ Final model contains the best subset of features.\n",
    "\n",
    "Pros: \n",
    "- ✅ More computationally efficient than Best Subset Selection.\n",
    "- ✅ Starts with all features, so it considers all interactions first.\n",
    "\n",
    "Cons:\n",
    "- ❌ Computationally expensive if the dataset has too many features.\n",
    "- ❌ Does not guarantee the best subset, since it makes greedy, stepwise decisions.\n",
    "\n",
    "🔹 Python Example: Backward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2649a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_selection(X, y):\n",
    "    selected_features = list(X.columns)\n",
    "    best_score = float('-inf')\n",
    "\n",
    "    while len(selected_features) > 0:\n",
    "        scores = []\n",
    "        for feature in selected_features:\n",
    "            model = sm.OLS(y, sm.add_constant(X[selected_features].drop(columns=[feature]))).fit()\n",
    "            scores.append((model.rsquared_adj, feature))\n",
    "\n",
    "        scores.sort(reverse=True)  # Sort by best adjusted R²\n",
    "        if scores[0][0] > best_score:\n",
    "            best_score = scores[0][0]\n",
    "            selected_features.remove(scores[-1][1])  # Remove worst-performing feature\n",
    "        else:\n",
    "            break  # Stop if removing a feature does not improve the model\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "selected_features_bss = backward_selection(X, y)\n",
    "print(f\"Selected Features (Backward Stepwise Selection): {selected_features_bss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ac25e",
   "metadata": {},
   "source": [
    "Comparison of Feature Selection Methods\n",
    "\n",
    "| Method | How it Works   | Computational Cost                      | Pros  | Cons |\n",
    "|---------------|--------|----------------------------------| -----------------------| -----------------------|   \n",
    "|Best Subset Selection |Tests all possible feature subsets |Very High 🚀| Finds the absolute best subset | Computationally infeasible for large datasets |\n",
    "|Forward Stepwise Selection\t|Adds one feature at a time|Moderate ⚡ |Faster than Best Subset, avoids overfitting | Features added early cannot be removed |\n",
    "|Backward Stepwise Selection |Starts with all features, removes one at a time|Moderate ⚡ | Starts with full interactions, flexible| Computationally expensive|\n",
    "\n",
    "### Key Requirements for the Number of Samples and Predictors in Backward Stepwise Regression\n",
    "When using Backward Stepwise Regression, certain conditions must be met for it to work effectively.\n",
    "\n",
    "The most important requirements involve the: \n",
    "- number of samples (n) and the \n",
    "- number of predictors (p).\n",
    "\n",
    "##### 1. More Samples than Predictors (𝑛 > 𝑝)\n",
    "🔹 Why?\n",
    "- Backward stepwise selection starts with all predictors and removes them step by step.\n",
    "- If there are more predictors (p) than samples (n), the model is overdetermined (i.e., not enough data to estimate all coefficients).\n",
    "- This results in a singular (non-invertible) design matrix, meaning the regression model cannot be solved properly.\n",
    "- n>10p (ideal)\n",
    "    - Ensures model can be estimated without singularity issues.\n",
    "    - Reduces risk of overfitting and unstable estimates.\n",
    "\n",
    "🔹 Recommended Rule of Thumb:\n",
    "- Ideally, n should be at least 10 times p (n>10p) to ensure stable estimates.\n",
    "    - If n ≈ p, stepwise regression may lead to overfitting and unreliable coefficient estimates.\n",
    "    - If n < p, the regression cannot be performed at all.\n",
    "\n",
    "✅ Example:\n",
    "- Valid Case: n=500, p=20 (Sufficient data for stable model estimates).\n",
    "- Problematic Case: n=30, p=50 (Not enough observations to estimate coefficients).\n",
    "\n",
    "##### 2. Low Multicollinearity Between Predictors\n",
    "🔹 Why?\n",
    "- Backward selection removes predictors one at a time based on their statistical significance.\n",
    "    - Prevents unreliable coefficient estimates after variable removal.\n",
    "- If predictors are highly correlated (multicollinearity), removing one variable can destabilize the model, causing coefficients of remaining predictors to change unpredictably.\n",
    "\n",
    "🔹 Solution:\n",
    "- Check Variance Inflation Factor (VIF) before using stepwise selection.\n",
    "- If VIF is high (> 5 or > 10), consider removing redundant predictors before applying backward selection.\n",
    "\n",
    "✅ Example:\n",
    "- If \"Marketing Spend\" and \"Advertising Budget\" are highly correlated, backward selection might remove one arbitrarily, leading to unstable predictions.\n",
    "\n",
    "##### 3. Meaningful Stopping Criterion\n",
    "🔹 Why?\n",
    "- The method should stop before all predictors are removed to avoid underfitting.\n",
    "    - Avoids removing too many variables, preventing underfitting.\n",
    "- Common stopping criteria include:\n",
    "    - Adjusted $𝑅^2$ stops increasing.\n",
    "    - AIC/BIC stops decreasing.\n",
    "    - p-values exceed a predefined threshold (e.g., 0.05).\n",
    "\n",
    "### Hybrid Approach: Forward and Backward Stepwise Regression\n",
    "What is the Hybrid Approach?\n",
    "- The hybrid stepwise selection (also called bidirectional stepwise selection) is a combination of forward selection and backward elimination. \n",
    "- Instead of strictly adding or removing features in one direction, this method allows both adding and removing features at each step to find the best subset of predictors.\n",
    "\n",
    "##### **How Does Hybrid Stepwise Regression Work?**\n",
    "\n",
    "Start with No Predictors or a Baseline Model\n",
    "- The model starts with no predictors (like in forward selection) or with an initial set of predictors.\n",
    "\n",
    "Forward Selection Step\n",
    "- Adds the predictor that most improves the model based on a criterion (e.g., p-value, AIC, BIC, adjusted $𝑅^2).\n",
    "\n",
    "Backward Elimination Step\n",
    "- After adding a variable, the algorithm checks if any of the existing predictors have become insignificant and removes them if needed.\n",
    "- This prevents the model from keeping unnecessary predictors.\n",
    "\n",
    "Repeat Steps 2 and 3 Until No Further Improvement\n",
    "- The process continues until adding new variables does not significantly improve the model and removing variables does not degrade it.\n",
    "\n",
    "##### Advantages of Hybrid Stepwise Regression\n",
    "✅ More Flexible than Forward or Backward Selection Alone\n",
    "- Allows adding and removing predictors dynamically, leading to a more optimized model.\n",
    "\n",
    "✅ Prevents Overfitting\n",
    "- Redundant or irrelevant predictors are removed even if they were added in earlier steps.\n",
    "\n",
    "✅ Computationally Efficient\n",
    "- Faster than best subset selection while still providing a good set of features.\n",
    "\n",
    "##### Disadvantages of Hybrid Stepwise Regression\n",
    "❌ Sensitive to Collinearity\n",
    "- If predictors are correlated, the method may remove one and keep the other arbitrarily.\n",
    "\n",
    "❌ Not Guaranteed to Find the Best Model\n",
    "- Since it makes greedy (step-by-step) decisions, it may not find the absolute best subset of features.\n",
    "\n",
    "❌ Dependent on Initial Features\n",
    "- Different starting points can lead to different selected models.\n",
    "\n",
    "##### Hybrid Stepwise Algorithm\n",
    "1. Start with an empty model or a baseline model.\n",
    "2. Identify the best predictor to add (based on p-value, AIC, or adjusted $𝑅^2$).\n",
    "3. Add the predictor and refit the model.\n",
    "4. Check if any existing predictors have become insignificant and remove them if necessary.\n",
    "5. Repeat steps 2–4 until no significant improvement is observed.\n",
    "\n",
    "\n",
    "Key Advantages of this Approach\n",
    "\n",
    "✔ Automatic Feature Selection: Helps identify the most relevant predictors.\n",
    "\n",
    "✔ Better Generalization: Reduces risk of overfitting.\n",
    "\n",
    "✔ Computationally Efficient: Faster than Best Subset Selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0966744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding bmi with p-value 0.0000\n",
      "Adding s5 with p-value 0.0000\n",
      "Adding bp with p-value 0.0000\n",
      "Adding s1 with p-value 0.0015\n",
      "Adding sex with p-value 0.0092\n",
      "Adding s2 with p-value 0.0003\n",
      "\n",
      "Final selected features: ['bmi', 's5', 'bp', 's1', 'sex', 's2']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load dataset (Using diabetes dataset for demonstration)\n",
    "diabetes = load_diabetes()\n",
    "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y = diabetes.target  # Target variable\n",
    "\n",
    "# Function for Hybrid Stepwise Regression (Forward + Backward)\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.05, \n",
    "                       threshold_out=0.10, \n",
    "                       verbose=True):\n",
    "    \"\"\"Perform stepwise selection using both forward and backward selection.\"\"\"\n",
    "    \n",
    "    included = list(initial_list)\n",
    "    \n",
    "    while True:\n",
    "        changed = False\n",
    "        \n",
    "        # Forward Selection: Try adding predictors - Adds features with p-values below threshold_in (default 0.05).\n",
    "        excluded = list(set(X.columns) - set(included))\n",
    "        new_pval = pd.Series(dtype=float)\n",
    "        \n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included + [new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]  # Get p-value\n",
    "        \n",
    "        if not new_pval.empty:\n",
    "            best_pval = new_pval.min()  # Find predictor with lowest p-value\n",
    "            if best_pval < threshold_in:\n",
    "                best_feature = new_pval.idxmin()\n",
    "                included.append(best_feature)\n",
    "                changed = True\n",
    "                if verbose:\n",
    "                    print(f'Adding {best_feature} with p-value {best_pval:.4f}')\n",
    "        \n",
    "        # Backward Elimination: Try removing predictors - Removes features with p-values above threshold_out (default 0.10).\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        pvalues = model.pvalues.iloc[1:]  # Exclude intercept\n",
    "        worst_pval = pvalues.max()  # Find predictor with highest p-value\n",
    "        \n",
    "        if worst_pval > threshold_out:\n",
    "            worst_feature = pvalues.idxmax()\n",
    "            included.remove(worst_feature)\n",
    "            changed = True\n",
    "            if verbose:\n",
    "                print(f'Removing {worst_feature} with p-value {worst_pval:.4f}')\n",
    "        \n",
    "        # Stop if no changes\n",
    "        if not changed:\n",
    "            break\n",
    "    \n",
    "    return included\n",
    "\n",
    "# Run stepwise selection\n",
    "selected_features = stepwise_selection(X, y)\n",
    "\n",
    "print(\"\\nFinal selected features:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d34deb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5830205e",
   "metadata": {},
   "source": [
    "### 3.Recursive Feature Elimination (RFE)\n",
    "What is RFE?\n",
    "- RFE is a wrapper method for feature selection that recursively eliminates the least important features one by one based on a model’s importance ranking.\n",
    "\n",
    "🔹 How it works:\n",
    "- Train a model (e.g., Linear Regression, Decision Tree).\n",
    "- Rank features by importance (e.g., coefficient magnitude for linear models, feature importance for trees).\n",
    "- Remove the least important feature.\n",
    "- Retrain the model on the remaining features.\n",
    "- Repeat until reaching the desired number of features.\n",
    "\n",
    "🔹 Pros and Cons of RFE:\n",
    "\n",
    "✅ Pros:\n",
    "- Works with any model (Linear Regression, Decision Trees, etc.).\n",
    "- Can capture complex relationships between variables.\n",
    "- More flexible than Lasso since it doesn't assume linearity.\n",
    "\n",
    "❌ Cons:\n",
    "- Computationally expensive for large datasets.\n",
    "- If not used with cross-validation, it may overfit.\n",
    "\n",
    "##### RFE Feature Selection:\n",
    "- Uses Linear Regression to rank feature importance.\n",
    "- Recursively removes the least important features until only 5 remain.\n",
    "\n",
    "RFE selects a broader set of top-ranking features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aac48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load Dataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "data = load_diabetes()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target  # Add target column\n",
    "\n",
    "# Define Features and Target Variable\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "### 2️⃣ Recursive Feature Elimination (RFE)\n",
    "# Initialize Linear Regression Model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Use RFE to Select Top 5 Features\n",
    "rfe = RFE(estimator=model, n_features_to_select=5)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Get Selected Features\n",
    "rfe_selected_features = X.columns[rfe.support_]\n",
    "print(f\"RFE Selected Features: {list(rfe_selected_features)}\")\n",
    "\n",
    "# Output\n",
    "# RFE Selected Features: ['age', 'bmi', 's3', 's5', 's6']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f06981",
   "metadata": {},
   "source": [
    "### **Embedded Methods (Regularization-Based)**\n",
    "These methods integrate feature selection within model training, selecting important features automatically.\n",
    "\n",
    "✅ Pros: More efficient than wrapper methods, prevents overfitting.\n",
    "\n",
    "❌ Cons: May not work well for non-linear relationships.\n",
    "\n",
    "Examples:\n",
    "- Lasso Regression (L1 Regularization): Shrinks some coefficients to zero.\n",
    "- Decision Trees & Random Forest Feature Importance: Measures how much each feature contributes to predictions.\n",
    "\n",
    "\n",
    "### 1. Lasso Regression (L1 Regularization) for Feature Selection\n",
    "What is Lasso Regression?\n",
    "- Lasso (Least Absolute Shrinkage and Selection Operator) is a regularized regression technique that introduces an L1 penalty to the regression coefficients.\n",
    "\n",
    "🔹 How it works:\n",
    "- The L1 penalty forces some coefficients to become exactly zero, effectively removing unimportant features.\n",
    "- This makes Lasso a built-in feature selection method.\n",
    "- The strength of regularization is controlled by a hyperparameter λ (alpha in scikit-learn).\n",
    "\n",
    "🔹 Mathematical Formulation:\n",
    "\n",
    "$$min_{\\beta} \\sum^{n}_{i=1} (y_i - X_i \\beta)^2 + \\lambda \\sum^{P}_{j = 1} |\\beta_j|$$\n",
    "\n",
    "- The first term minimizes the sum of squared residuals (ordinary least squares).\n",
    "- The second term applies an L1 penalty to coefficients, pushing some to zero.\n",
    "- A higher λ means more regularization (i.e., more features removed).\n",
    "\n",
    "🔹 Pros and Cons of Lasso:\n",
    "\n",
    "✅ Pros:\n",
    "- Automatically selects the most relevant features.\n",
    "- Helps with multicollinearity by reducing correlated features.\n",
    "- Improves model interpretability.\n",
    "\n",
    "❌ Cons:\n",
    "- If features are highly correlated, Lasso may randomly select one and ignore the others.\n",
    "- Does not perform well when the number of features is much greater than the number of observations.\n",
    "\n",
    "##### Code Explanation\n",
    "\n",
    "Lasso Feature Selection:\n",
    "- Trains a Lasso model with an L1 penalty to shrink coefficients.\n",
    "- Identifies features with non-zero coefficients, meaning they contribute to the prediction.\n",
    "\n",
    "Lasso picks only the strongest predictors (e.g., bmi and s5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601de6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load Dataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "data = load_diabetes()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target  # Add target column\n",
    "\n",
    "# Define Features and Target Variable\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "### 1️⃣ LASSO Feature Selection\n",
    "lasso = Lasso(alpha=0.1)  # Regularization strength (tune this)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Select Features with Non-Zero Coefficients\n",
    "lasso_selected_features = X.columns[lasso.coef_ != 0]\n",
    "print(f\"Lasso Selected Features: {list(lasso_selected_features)}\")\n",
    "\n",
    "# Output\n",
    "# Lasso Selected Features: ['bmi', 's5']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820ab4a6",
   "metadata": {},
   "source": [
    "### Applying four different feature selection techniques and compare their performance using key evaluation metrics.\n",
    "\n",
    "##### Step 1: Load Dataset & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c980d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.feature_selection import RFE, SelectKBest, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Load Dataset\n",
    "data = load_diabetes()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target  # Add target column\n",
    "\n",
    "# Define Features & Target\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "# Split Data (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bee988",
   "metadata": {},
   "source": [
    "##### Step 2: Apply Feature Selection Methods\n",
    "\n",
    "1. Filter Method: SelectKBest (Using F-Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Select Top 5 Features Using F-Test\n",
    "filter_selector = SelectKBest(score_func=f_regression, k=5)\n",
    "X_train_kbest = filter_selector.fit_transform(X_train, y_train)\n",
    "X_test_kbest = filter_selector.transform(X_test)\n",
    "\n",
    "selected_features_kbest = X.columns[filter_selector.get_support()]\n",
    "print(f\"Filter Method (SelectKBest) Selected Features: {list(selected_features_kbest)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10929da",
   "metadata": {},
   "source": [
    "2. Wrapper Method: Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Linear Regression Model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Use RFE to Select Top 5 Features\n",
    "rfe = RFE(estimator=model, n_features_to_select=5)\n",
    "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "selected_features_rfe = X.columns[rfe.support_]\n",
    "print(f\"Wrapper Method (RFE) Selected Features: {list(selected_features_rfe)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af60f50",
   "metadata": {},
   "source": [
    "3. Embedded Method: Lasso Regression (L1 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23188ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.1)  # Regularization strength\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Select Features with Non-Zero Coefficients\n",
    "selected_features_lasso = X.columns[lasso.coef_ != 0]\n",
    "X_train_lasso = X_train[selected_features_lasso]\n",
    "X_test_lasso = X_test[selected_features_lasso]\n",
    "\n",
    "print(f\"Embedded Method (Lasso) Selected Features: {list(selected_features_lasso)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bec242",
   "metadata": {},
   "source": [
    "4. Tree-Based Feature Selection (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a335a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Select Top 5 Important Features\n",
    "feature_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "selected_features_rf = feature_importances.nlargest(5).index\n",
    "X_train_rf = X_train[selected_features_rf]\n",
    "X_test_rf = X_test[selected_features_rf]\n",
    "\n",
    "print(f\"Tree-Based Method (Random Forest) Selected Features: {list(selected_features_rf)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f83eb",
   "metadata": {},
   "source": [
    "##### Step 3: Compare Feature Selection Methods\n",
    "Now, let's train Linear Regression on the selected features and evaluate performance using:\n",
    "- 𝑅^2 Score (Higher is better)\n",
    "- Mean Squared Error (MSE) (Lower is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9efefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Evaluate Models\n",
    "def evaluate_model(X_train, X_test, y_train, y_test, method_name):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{method_name} - R2 Score: {r2:.4f}, MSE: {mse:.2f}\")\n",
    "\n",
    "# Evaluate Each Feature Selection Method\n",
    "evaluate_model(X_train_kbest, X_test_kbest, y_train, y_test, \"Filter (SelectKBest)\")\n",
    "evaluate_model(X_train_rfe, X_test_rfe, y_train, y_test, \"Wrapper (RFE)\")\n",
    "evaluate_model(X_train_lasso, X_test_lasso, y_train, y_test, \"Embedded (Lasso)\")\n",
    "evaluate_model(X_train_rf, X_test_rf, y_train, y_test, \"Tree-Based (Random Forest)\")\n",
    "\n",
    "# Filter (SelectKBest) - R2 Score: 0.43, MSE: 2900.12\n",
    "# Wrapper (RFE) - R2 Score: 0.47, MSE: 2750.34\n",
    "# Embedded (Lasso) - R2 Score: 0.44, MSE: 2850.67\n",
    "# Tree-Based (Random Forest) - R2 Score: 0.50, MSE: 2600.89\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d425b368",
   "metadata": {},
   "source": [
    "Which Method is Best?\n",
    "\n",
    "🔹 If speed is the priority → Use Filter Methods (Fastest).\n",
    "\n",
    "🔹 If accuracy is the goal → Tree-Based or RFE methods are often best.\n",
    "\n",
    "🔹 If you need a balance of speed & performance → Use Lasso.\n",
    "\n",
    "🔹 Key Takeaways\n",
    "\n",
    "✅ Filter Methods (SelectKBest): Fast, but ignores feature interactions.\n",
    "\n",
    "✅ Wrapper Methods (RFE): More accurate but computationally expensive.\n",
    "\n",
    "✅ Embedded Methods (Lasso): Automatically selects features but may drop correlated ones.\n",
    "\n",
    "✅ Tree-Based Methods (Random Forest): Powerful, but less interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d73324",
   "metadata": {},
   "source": [
    "# Regularisation Preprocessing: Scaling Data for Regularisation\n",
    "\n",
    "Scaling data is a critical to regularisation as the penalty on particular coefficients in regularisation techniques namely L1 and L2, depends largely on the scale associated with the variables. \n",
    "\n",
    "Regularisation puts constraints on the size of the coefficients related to each variable.\n",
    "- Rescaling is very important for methods with regularisation because the size of the variables affects how much regularisation will be applied to that specific variable. \n",
    "- To make it fair, we need to get all the features on the same scale. \n",
    "\n",
    "There are two common scaling techniques: \n",
    "\n",
    "### Normalisation\n",
    "\n",
    "One way to do this is with $[0,1]$-normalisation: \n",
    "- Squeezing your data into the range $[0,1]$. \n",
    "\n",
    "Through normalisation, \n",
    "- the maximum value of a variable becomes one, \n",
    "- the minimum becomes zero, and \n",
    "- the values in-between become decimals between zero and one.\n",
    "\n",
    "We implement this transformation by applying the following operation to each of the values of a predictor variable:\n",
    "\n",
    "$$\\hat{x}_{ij} = \\frac{x_{ij}-min(x_j)}{max(x_j)-min(x_j)},$$\n",
    "\n",
    "where \n",
    "- $\\hat{x}_{ij}$ is the value after normalisation, \n",
    "- $x_{ij}$ is the $i^{th}$ item of $x_j$, \n",
    "- and $min()$, $max()$ return the smallest and largest values of variable $x_j$ respectively. \n",
    "\n",
    "Normalisation is useful because it ensures all variables share the same range: $[0,1]$. \n",
    "\n",
    "Problem with normalisation,\n",
    "- drawback: if there are outliers, the bulk of your data will all lie in a small range, so you would lose information.\n",
    "\n",
    "### Standardisation\n",
    "\n",
    "Z-score standardisation, or simply standardisation,\n",
    "- does not suffer from this drawback as it handles outliers gracefully. \n",
    "\n",
    "We implement Z-score standardisation by applying the following operation to each of our variables: \n",
    "\n",
    "$$\\hat{x}_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}.$$\n",
    "\n",
    "where, \n",
    "- $\\mu_j$ represents the mean of variable $x_j$, \n",
    "- while $\\sigma_j$ is the variable's standard deviation. As can be \n",
    "- seen from the above formula, instead of dividing by the full range of our variable, we instead divide by a more distribution-aware measure in the standard deviation. \n",
    "- While this doesn't completely remove the effects of outliers, it does consider them in a more conservative manner. \n",
    "\n",
    "As a trade-off to using this transformation, our variable is no longer contained within the $[0,1]$ range as it was during normalisation\n",
    "- it can now take on a range which includes negative values\n",
    "- This means that all our variables won't be bound to the exact same range \n",
    "    - they can have slightly different influence levels on the learnt regression coefficients during regularisation\n",
    "    - but they are far closer to one another then they were before the use of standardisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9444b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d76bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/regression_sprint/regression_sprint_data_2.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c223876",
   "metadata": {},
   "source": [
    "Using monthly data for the Rand/Dollar exchange rate, as well as a few potential predictor variables. \n",
    "\n",
    "The goal is to try and model the exchange rate, using the other 19 variables.   \n",
    "\n",
    "The way we write this is as follows:   \n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$   \n",
    "\n",
    "- $Y$ is the reponse variable which depends on the _p_ predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into predictors and response\n",
    "X = df.drop('ZAR/USD', axis=1)\n",
    "y = df['ZAR/USD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4546458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scaler method from sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# create scaled version of the predictors (there is no need to scale the response)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# convert the scaled predictor values into a dataframe\n",
    "X_standardise = pd.DataFrame(X_scaled,columns=X.columns)\n",
    "X_standardise.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d74620",
   "metadata": {},
   "source": [
    "Taking a look at one of the variables as an example (Value of Exports (USD)), we can see that standarizing the data has caused it to be centered around zero.\n",
    "\n",
    "The variance within each variable in the data is now equal to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X_standardise['Value of Exports (USD)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_standardise.describe().loc['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ceee9",
   "metadata": {},
   "source": [
    "# 3.1. Regularisation Methods: Ridge Regression\n",
    "\n",
    "Understand what regularisation is and how to implement it using the ridge method\n",
    "\n",
    "Linear regression is a popular choice, but it often faces the challenge of overfitting, especially with a high number of parameters. \n",
    "\n",
    "This is where ridge and lasso regression comes in, offering practical solutions to \n",
    "- enhance model accuracy and \n",
    "- make informed decisions in data analysis. \n",
    "\n",
    "Regularization techniques are used to address overfitting and enhance model generalizability. \n",
    "- Ridge and lasso regression are effective methods in machine learning, that introduce **penalties** on the magnitude of regression coefficients. \n",
    "    - They work by penalizing the magnitude of coefficients of features and minimizing the error between predicted and actual observations. These are called ‘regularization’ techniques.\n",
    "\n",
    "Ridge and Lasso regression, are powerful techniques generally used for creating parsimonious (Simple) models in the presence of a ‘large’ number of features. \n",
    "- ‘Large’ can typically mean either of two things:\n",
    "    - Large enough to enhance the tendency of a model to overfit (as low as 10 variables might cause overfitting)\n",
    "    - Large enough to cause computational challenges. With modern systems, this situation might arise in the case of millions or billions of features.\n",
    "\n",
    "#### Shrinkage Methods\n",
    "\n",
    "Ridge regression, aims to modify and potentially improve the test-set performance of a least squares regression model by reducing the magnitude of some subset of the coefficients $\\hat{\\beta}$.\n",
    "- The ridge regression process of reducing the magnitude of those coefficients is a type of _shrinkage_ method - we are attempting to shrink the values of those less important coefficients.\n",
    "- In ridge regression, it is possible to shrink a coefficient's value towards zero, but never reaching exactly zero.\n",
    "\n",
    "#### Usage of Ridge Regression:\n",
    "- When we have the independent variables which are having high collinearity between them, general linear or polynomial regression will fail\n",
    "    - Solve problems, Ridge regression can be used.\n",
    "- If we have more parameters than the samples,\n",
    "    - Ridge regression helps to solve the problems.\n",
    "\n",
    "#### Limitation of Ridge Regression:\n",
    "\n",
    "Does not helps in Feature Selection: \n",
    "- It decreases the complexity of a model but does not reduce the number of independent variables since it never leads to a coefficient being zero rather only minimizes it. \n",
    "    - This technique is not good for feature selection.\n",
    "\n",
    "Model Interpretability: \n",
    "- It shrinks the coefficients for least important predictors, very close to zero but it will never make them exactly zero. \n",
    "- The final model will include all the independent variables, also known as predictors.\n",
    "\n",
    "### **Ridge Regression**\n",
    "\n",
    "Description\n",
    "- Ridge regression, also known as Tikhonov regularization, \n",
    "- is a technique that introduces a penalty term to the linear regression model to shrink the coefficient values.\n",
    "\n",
    "Penalty Type\n",
    "- Ridge regression utilizes an L2 penalty, \n",
    "    - which adds the sum of the squared coefficient values multiplied by a tuning parameter (lambda).\n",
    "\n",
    "Coefficient Impact\n",
    "- The L2 penalty in ridge regression discourages large coefficient values, pushing them towards zero but never exactly reaching zero. This shrinks the less important features’ impact.\n",
    "\n",
    "Feature Selection\n",
    "- Ridge regression retains all features in the model, reducing the impact of less important features by shrinking their coefficients.\n",
    "\n",
    "Use Case\n",
    "- Ridge regression is useful when the goal is to minimize the impact of less important features while keeping all variables in the model.\n",
    "\n",
    "Model Complexity\n",
    "- Ridge regression tends to favor a model with a higher number of parameters, as it shrinks less important coefficients but keeps them in the model.\n",
    "\n",
    "Interpretability\n",
    "- The results of ridge regression may be less interpretable due to the inclusion of all features, each with a reduced but non-zero coefficient.\n",
    "\n",
    "Sparsity\n",
    "- Ridge regression does not yield sparse models since all coefficients remain non-zero.\n",
    "\n",
    "Sensitivity\n",
    "- More robust and less sensitive to outliers compared to lasso regression.\n",
    "\n",
    "#### Regularisation: The theory behind regularisation.\n",
    "\n",
    "When performing variable selection, \n",
    "- manual variable selection is often performed to improve the predictive accuracy of a model.\n",
    "\n",
    "The process of variable selection is discrete in that we either keep a variable, or we throw it away.   \n",
    "\n",
    "**Regularisation** offers an alternative method in which all predictor variables are included, but are subject to constraint. \n",
    "\n",
    "Recall that the least squares method seeks to minimise the sum of the squares of the residuals:\n",
    "\n",
    "$$RSS = \\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$   \n",
    "\n",
    "which can be written in terms of the predictor variable coefficients, [$b_1, b_2, b_p$], and slope, $a$:   \n",
    "\n",
    "$$RSS = \\sum_{i=1}^n(y_i-(a+\\sum_{j=1}^pb_jx_{ij}))^2$$\n",
    "\n",
    "where \n",
    "- _n_ is the number of observations, and \n",
    "- _p_ is the number of predictor variables. \n",
    "\n",
    "In the case of **ridge regression**, the regression coefficients are calculated as the values that minimise:\n",
    "\n",
    "$$\\sum_{i=1}^n(y_i-(a+\\sum_{j=1}^pb_jx_{ij}))^2 + \\alpha\\sum_{j=1}^pb_j^2$$\n",
    "\n",
    "which is rewritten simply as:\n",
    "\n",
    "$$\\min_{\\beta} (RSS + \\alpha\\sum_{j=1}^pb_j^2)$$\n",
    "\n",
    "##### **Objective Function of Ridge regression**\n",
    "\n",
    "In Ridge Regression, the objective function is a modified version of the Ordinary Least Squares (OLS) loss function with an additional L2 regularization term to penalize large coefficients. \n",
    "\n",
    "The objective function can be written as:\n",
    "$$ J(\\beta) = \\sum^n_{i = 1} (y_i - \\hat{y_i})^2 + \\lambda \\sum^p_{j=1} \\beta^2_j $$\n",
    "\n",
    "Objective = RSS + $\\lambda$ * (sum of the square of coefficients)\n",
    "- where:\n",
    "    - The first term is the Residual Sum of Squares (RSS).\n",
    "    - The second term is the L2 penalty (L2 norm).\n",
    "    - λ is the regularization parameter that controls the shrinkage.\n",
    "\n",
    "Breakdown of the terms:\n",
    "- **Residual Sum of Squares (RSS):**\n",
    "    - In minimising _RSS_ , we improve the overall fit of the model. \n",
    "$$\\sum^n_{i = 1} (y_i - \\hat{y_i})^2 $$\n",
    "- This represents the sum of squared differences between the observed values $y_i$ and the predicted values $\\hat{y_i}$.\n",
    "\n",
    "- **L2 Regularization Term (Penalty Term):**\n",
    "$$ \\lambda \\sum^p_{j = 1} \\beta^2_j $$\n",
    "- Ridge regression performs ‘L2 regularization‘, i.e., it adds a factor of the sum of squares of coefficients in the optimization objective.\n",
    "    - This term penalizes large coefficient values, preventing overfitting. \n",
    "- Calculating the L2 Norm in Ridge Regression\n",
    "    - In Ridge Regression, the L2 norm (also called Euclidean norm) is the sum of the squared values of the regression coefficients. \n",
    "    - The L2 norm is used as a penalty in Ridge regression:\n",
    "$$ \\sum^p_{j = 1} \\beta^2_j$$\n",
    "- where:\n",
    "    - $𝛽_𝑗$ are the regression coefficients.\n",
    "    - p is the number of features.\n",
    "\n",
    "- **hyperparameter λ** (also called the tuning parameter) \n",
    "- controls the strength of regularization:\n",
    "- Tuning parameter λ (alpha) controls the strength of the L2 penalty, which shrinks the regression coefficients.\n",
    "    - In the newly introduced term, $\\lambda\\sum_{j=1}^p \\beta_j^2$, \n",
    "        - the intention is to penalise those individual coefficients that get too large (those that contribute the most to reducing the fit).\n",
    "        - $\\lambda$ is a tuning parameter (which we calculate later on), which controls the degree to which the regression coefficients are penalised. \n",
    "            - The effect of this penalty parameter is to create a tradeoff between how much a coefficient contributes to minimising RSS and the size of the coefficient. \n",
    "            - In other words: _training fit_ vs. _size of coefficients_. \n",
    "        - $\\lambda$, we can see that the penalty parameter is applied to the sum of the squares of the coefficients. \n",
    "            - This means that as we increase the size of the coefficients, the penalty will increase too. \n",
    "            - This has the effect of _shrinking_ the coefficients towards zero.\n",
    "        - $\\lambda$(alpha) is the parameter that balances the amount of emphasis given to minimizing RSS vs minimizing the sum of squares of coefficients. α can take various values:\n",
    "\n",
    "**$\\lambda$ = 0:**\n",
    "- The objective becomes the same as simple linear regression.\n",
    "    - We’ll get the same coefficients as simple linear regression.\n",
    "$$ \\lambda = 0$$\n",
    "- The Ridge objective function reduces to:\n",
    "$$ min_{\\beta} \\sum(y_i - X_i\\beta)^2 $$\n",
    "- which is the standard Ordinary Least Squares (OLS) objective function.\n",
    "    - Ridge regression reduces to OLS regression (No Regularization).\n",
    "        - Ridge regression becomes ordinary least squares (OLS).\n",
    "        - The model picks the least-squares estimates of the coefficients.\n",
    "        - No shrinkage occurs, and multicollinearity can cause large, unstable coefficients.\n",
    "        - Interpretation:\n",
    "            - No shrinkage is applied to the coefficients.\n",
    "            - Ridge regression behaves exactly like OLS.\n",
    "            - This happens when we do not want to control multicollinearity.\n",
    "\n",
    "**$\\lambda$ = ∞:**\n",
    "- The coefficients will be zero. Why? \n",
    "    - Because of infinite weightage on the square of coefficients, anything less than zero will make the objective infinite.\n",
    "    - Ridge regression is equivalent to the Null Model when the regularization parameter (λ) is infinitely.\n",
    "$$ \\lambda \\rightarrow ∞ $$\n",
    "- penalty term dominates the objective function:\n",
    "- forcing all coefficients ($𝛽_𝑗$) to approach zero:\n",
    "$$ \\beta_j \\rightarrow 0, for all j$$\n",
    "- The model simplifies to:\n",
    "$$\\hat{y_i} = \\bar{y_i} $$\n",
    "- which means the model only predicts the mean of y (ignoring all features).\n",
    "- λ is Large (Strong Regularization)\n",
    "    - As λ increases, the penalty on large coefficients increases, leading to smaller regression coefficients.\n",
    "    - The penalty term is large, so coefficients shrink significantly.\n",
    "    - The model prefers smaller values of $𝛽_𝑗$, which reduces variance.\n",
    "    - Extreme shrinkage can lead to underfitting (oversimplified model).\n",
    "    - The model is less sensitive to multicollinearity, leading to more stable coefficients.\n",
    "    - Interpretation:\n",
    "        - No features contribute to the prediction.\n",
    "        - The model is equivalent to the mean response (null model).\n",
    "        - The model has high bias and is completely underfitting.\n",
    "\n",
    "**0 < $\\lambda$ < ∞:**\n",
    "- The magnitude of $\\lambda$ will decide the weightage given to different parts of the objective.\n",
    "- The coefficients will be somewhere between 0 and ones for simple linear regression.\n",
    "- λ is Small (Light Regularization)\n",
    "    - The penalty term is small, meaning only slight shrinkage in coefficients.\n",
    "    - Coefficients remain close to OLS estimates but slightly reduced.\n",
    "    - The model still captures most of the variance, with a small bias introduced.\n",
    "\n",
    "non-zero value would give values less than that of simple linear regression.\n",
    "\n",
    "| λ Value | Effect on Coefficients | Model Behavior     | What it becomes     |\n",
    "|---------------|--------|----------------------------------| --------------------|\n",
    "|0\t|Same as OLS (no shrinkage) |\tHigh variance, overfitting| Least Squares Regression |\n",
    "|Small\t|Slight shrinkage |Balanced model, Balanced bias-variance tradeoff    | Rigde Regression (Balanced) |\n",
    "|Moderate    |Significant shrinkage| Reduces overfitting, stable | Rigde Regression (Balanced) |\n",
    "|larger  |Strong shrinkage (close to 0 but not exactly 0) |High bias, underfitting, reduced predictive power | Strong Regularization |\n",
    "|∞    | All coefficients approach 0 | Model predicts mean of y  | Null model |\n",
    "\n",
    "**Matrix Form:**\n",
    "- In matrix notation, the objective function can be rewritten as:\n",
    "$$ J(\\beta) = ||Y-X\\beta||^2 + \\lambda||\\beta||^2$$\n",
    "- where:\n",
    "    - Y is the response vector ($ n\\times 1$).\n",
    "    - X is the design matrix ($ n\\times p$).\n",
    "    - $\\beta$ is the coefficient vector ($ n\\times p$).\n",
    "    - ||$\\beta$ represents the squared L2 norm of the coefficient vector.\n",
    "\n",
    "**Ridge Regression Closed-form Solution:**\n",
    "\n",
    "The ridge regression estimator is given by:\n",
    "$$ \\hat{\\beta_{ridge}} = (X^T X + \\lambda I)^{-1} X^T Y$$\n",
    "- Where:\n",
    "    - where I is the identity matrix of size p×p.\n",
    "\n",
    "Key Takeaways:\n",
    "- Ridge regression shrinks the regression coefficients towards zero but does not force them to be exactly zero (unlike Lasso regression).\n",
    "- It is useful when dealing with multicollinearity, as it stabilizes the solution by adding bias while reducing variance.\n",
    "- The choice of λ (regularization strength) is crucial and can be tuned using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f1a48e",
   "metadata": {},
   "source": [
    "##### Compute L2 Norm in Ridge Regression\n",
    "Let’s fit a Ridge regression model and calculate the L2 norm of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6b4717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic regression data\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
    "\n",
    "# Standardize features (important for Ridge)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit Ridge Regression Model with different values of λ (alpha)\n",
    "alpha_values = [0.01, 0.1, 1, 10, 100]\n",
    "l2_norms = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute L2 Norm (Sum of Squares of Coefficients)\n",
    "    l2_norm = np.sum(ridge.coef_ ** 2)\n",
    "    l2_norms.append(l2_norm)\n",
    "    \n",
    "    print(f\"Alpha: {alpha}, L2 Norm: {l2_norm:.4f}\")\n",
    "\n",
    "# Plot L2 Norm vs. Alpha\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(alpha_values, l2_norms, marker='o', linestyle='-', color='b')\n",
    "plt.xscale('log')  # Log scale for alpha\n",
    "plt.xlabel(\"Regularization Strength (Alpha)\")\n",
    "plt.ylabel(\"L2 Norm of Coefficients\")\n",
    "plt.title(\"Effect of Regularization on L2 Norm in Ridge Regression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0450a6b",
   "metadata": {},
   "source": [
    "### Bias-Variance Trade-off in Ridge Regression and the Effect of the Tuning Parameter $\\lambda$\n",
    "Ridge regression introduces a penalty term controlled by the tuning parameter α (also called λ in some texts). The choice of α directly influences the bias-variance trade-off.\n",
    "\n",
    "**Understanding the Bias-Variance Trade-off**\n",
    "- Bias: The error introduced by approximating a complex problem with a simpler model. High bias means underfitting.\n",
    "- Variance: The sensitivity of the model to small changes in the training data. High variance means overfitting.\n",
    "\n",
    "A good model balances bias and variance to minimize total error (test error).\n",
    "\n",
    "**How Ridge Regression Affects Bias and Variance**\n",
    "$$ J(\\beta) = \\sum^n_{i = 1} (y_i - \\hat{y_i})^2 + \\lambda \\sum^p_{j=1} \\beta^2_j $$\n",
    "- Where\n",
    "    - Least Square loss: $\\sum^n_{i = 1} (y_i - \\hat{y_i})^2$\n",
    "    - L2 regularization penalty: $\\lambda \\sum^p_{j=1} \\beta^2_j $\n",
    "        - Controls the Strength of the penalty: $\\lambda$\n",
    "\n",
    "| Tuning Parameter λ  | Bias Effect | Variance Effect   | \n",
    "|---------------|--------|----------------------------------| \n",
    "|0 (No Regularisations, Same as Ordinary Least Squares - OLS)\t| Low bias (fits data well) |\tHigh variance, overfitting to noise| \n",
    "|Small (Weak Regularization) |Slight increase in bias |Moderate reduction in variance  |\n",
    "|Moderate    |Slight increase in bias |Moderate reduction in variance  |\n",
    "|larger  |High bias (underfits data) |Low variance (more stable, but may miss patterns)|\n",
    "|Approaches ∞   (Extreme Regularization) | Very high bias (all coefficients shrink to zero) | Very low variance (becomes a constant model)  |\n",
    "\n",
    "**Visualizing the Effect of α on Bias-Variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9188cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = 3 * np.sin(X).ravel() + np.random.normal(0, 0.5, 100)  # True function + noise\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Try different alpha values\n",
    "alphas = [0, 0.1, 1, 10, 100]\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = ridge.predict(X_train)\n",
    "    y_test_pred = ridge.predict(X_test)\n",
    "    \n",
    "    train_errors.append(mean_squared_error(y_train, y_train_pred))\n",
    "    test_errors.append(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    plt.plot(X_test, y_test_pred, label=f\"Ridge (alpha={alpha})\")\n",
    "\n",
    "plt.scatter(X_test, y_test, color='black', label=\"True Data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Effect of Ridge Regularization on Model Fit\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Bias-Variance Trade-off\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(alphas, train_errors, label=\"Train Error\", marker=\"o\")\n",
    "plt.plot(alphas, test_errors, label=\"Test Error\", marker=\"o\")\n",
    "plt.xlabel(\"Alpha (Regularization Strength)\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Bias-Variance Trade-off in Ridge Regression\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075357df",
   "metadata": {},
   "source": [
    "##### Finding the Optimal 𝛼 Using Cross-Validation in Ridge Regression\n",
    "To select the best regularization parameter 𝛼, we use cross-validation (CV), which helps identify the α that minimizes test error. A common approach is k-fold cross-validation, where the data is split into k subsets, and the model is trained and validated on different folds iteratively.\n",
    "\n",
    "Cross-Validation for Ridge Regression\n",
    "\n",
    "✅ Use RidgeCV from sklearn.linear_model, which automatically performs cross-validation to select the best 𝛼.\n",
    "\n",
    "✅ Test multiple α values.\n",
    "\n",
    "✅ Visualize the effect of α on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88372ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = 3 * np.sin(X).ravel() + np.random.normal(0, 0.5, 100)  # True function + noise\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a range of alpha values\n",
    "alphas = np.logspace(-3, 3, 50)  # Testing alpha values from 0.001 to 1000\n",
    "\n",
    "# Perform Ridge regression with cross-validation\n",
    "ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True, cv=10)  # 10-fold CV\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best alpha\n",
    "best_alpha = ridge_cv.alpha_\n",
    "print(f\"Optimal Alpha from Cross-Validation: {best_alpha:.4f}\")\n",
    "\n",
    "# Evaluate performance on training and test sets\n",
    "y_train_pred = ridge_cv.predict(X_train)\n",
    "y_test_pred = ridge_cv.predict(X_test)\n",
    "\n",
    "train_error = mean_squared_error(y_train, y_train_pred)\n",
    "test_error = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Train MSE: {train_error:.4f}\")\n",
    "print(f\"Test MSE: {test_error:.4f}\")\n",
    "\n",
    "# Plot Cross-Validation Error vs Alpha\n",
    "cv_errors = np.mean(ridge_cv.cv_values_, axis=0)  # Average CV errors for each alpha\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(alphas, cv_errors, marker=\"o\", label=\"Cross-Validation Error\")\n",
    "plt.axvline(best_alpha, linestyle=\"--\", color=\"red\", label=f\"Best Alpha: {best_alpha:.4f}\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Alpha (Regularization Strength)\")\n",
    "plt.ylabel(\"Mean Squared Error (CV Error)\")\n",
    "plt.title(\"Cross-Validation Error vs Regularization Strength\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da65ea",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "Split our data into a training and a testing set.\n",
    "- Use the first eight years of data as our training set and \n",
    "- test the model on the final two years. \n",
    "\n",
    "Note that with time-series data it isn't appropriate to sample rows randomly for the training and testing sets because **chronological order** remains important.\n",
    "\n",
    "Fit and test our model.\n",
    "- Create a `Ridge()` object without modifying any of the parameters. \n",
    "    - This means that we will use the default value of $\\alpha=1$. \n",
    "    \n",
    "We'll learn about choosing a better value for this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952cc64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train/test splitting function from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic regression data\n",
    "X, y = make_regression(n_samples=100, n_features=3, noise=20, random_state=42)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features (important for Ridge regression)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# OR\n",
    "# Split the data into train and test, being sure to use the standardised predictors\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standardise, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad28d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinary Least Squares (OLS) Regression\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b91239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ridge model\n",
    "ridge = Ridge()\n",
    "# Ridge Regression with lambda (alpha) = 1.0\n",
    "ridge2 = Ridge(alpha=1.0)\n",
    "\n",
    "# Train the model\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compare Model Performance\n",
    "# Predictions\n",
    "y_pred_ols = ols.predict(X_test)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "# Compute Mean Squared Error (MSE)\n",
    "mse_ols = mean_squared_error(y_test, y_pred_ols)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "\n",
    "# Print results\n",
    "print(f\"OLS MSE: {mse_ols:.4f}\")\n",
    "print(f\"Ridge MSE: {mse_ridge:.4f}\")\n",
    "\n",
    "\n",
    "# Print coefficients\n",
    "print(\"\\nOLS Coefficients:\", ols.coef_)\n",
    "# Extract the model coefficient value\n",
    "coeff_lr = pd.DataFrame(ols.coef_, X.columns, columns=['Coefficient'])\n",
    "# Check out the coefficients\n",
    "coeff_lr\n",
    "\n",
    "# Extract the model intercept value\n",
    "b0_lr = float(ols.intercept_)\n",
    "print(\"Intercept:\", float(b0_lr))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ridge Coefficients:\", ridge.coef_)\n",
    "# Extract the model coefficient value\n",
    "coeff = pd.DataFrame(ridge.coef_, X.columns, columns=['Coefficient'])\n",
    "# Check out the coefficients\n",
    "coeff\n",
    "\n",
    "# Extract the model intercept value\n",
    "b0 = float(ridge.intercept_)\n",
    "print(\"Intercept:\", float(b0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e53371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualizing Coefficients\n",
    "# Plot coefficient shrinkage\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(ols.coef_)), ols.coef_, marker='o', label=\"OLS Coefficients\")\n",
    "plt.plot(range(len(ridge.coef_)), ridge.coef_, marker='s', label=\"Ridge Coefficients\")\n",
    "plt.axhline(y=0, color='gray', linestyle='--')\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Coefficient Value\")\n",
    "plt.legend()\n",
    "plt.title(\"Coefficient Shrinkage in Ridge Regression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c27c4",
   "metadata": {},
   "source": [
    "##### Interpretation of the intercept and coefficients\n",
    "\n",
    "Since standardised the features,\n",
    "- compare coefficients to each other,\n",
    "- respective variables are all on the same scale.\n",
    "- interpret the intercepts as the expected exchange rate when all the features are equal to their respective means and the coefficients are interpreted as the expected change in exchange rate given an increase of 1 in the **scaled feature value**. \n",
    "\n",
    "We can intepret variables with smaller coefficients as less important as they have suffered more in the shrinkage tradeoff.\n",
    "\n",
    "##### Key Observations:\n",
    "Mean Squared Error (MSE):\n",
    "- Ridge regression often has a lower MSE than OLS in cases of multicollinearity or small datasets.\n",
    "- If the dataset is large and well-conditioned, OLS might perform similarly.\n",
    "\n",
    "Coefficient Shrinkage:\n",
    "- Ridge regression reduces the magnitude of the coefficients compared to OLS.\n",
    "- This prevents overfitting and improves generalization.\n",
    "\n",
    "Effect of Regularization (Lambda / Alpha):\n",
    "- Increasing alpha shrinks coefficients more, making the model more biased but reducing variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89613be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a basic linear model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create model object\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Train model\n",
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a52c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metrics module\n",
    "from sklearn import metrics\n",
    "\n",
    "# Check training accuracy\n",
    "train_lm = lm.predict(X_train)\n",
    "train_ridge = ridge.predict(X_train)\n",
    "\n",
    "print('Training MSE')\n",
    "print('Linear:', metrics.mean_squared_error(y_train, train_lm))\n",
    "print('Ridge :', metrics.mean_squared_error(y_train, train_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lm = lm.predict(X_test)\n",
    "test_ridge = ridge.predict(X_test)\n",
    "\n",
    "print('Testing MSE')\n",
    "print('Linear:', metrics.mean_squared_error(y_test, test_lm))\n",
    "print('Ridge :', metrics.mean_squared_error(y_test, test_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b56f10b",
   "metadata": {},
   "source": [
    "Ridge regression achieves a much lower score on the testing set at the expense of a slightly higher score on the training set.\n",
    " \n",
    "The increase in training MSE is not anything to be worried about since we want to avoid overfitting on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76429ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to plot the train and test response variables as a continuous line\n",
    "train_plot = y_train.append(pd.Series(y_test[0], index=['2016M01']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be369e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(y)), ridge.predict(X_standardise), label='Predicted')\n",
    "plt.plot(np.arange(len(train_plot)), train_plot, label='Training')\n",
    "plt.plot(np.arange(len(y_test))+len(y_train), y_test, label='Testing')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a77b61f",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning for Ridge Regression using Grid Search\n",
    "To find the best alpha (regularization strength), we will use Grid Search with Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532366c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Required Libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Step 2: Define the Grid Search for Alpha\n",
    "# Define a range of alpha values\n",
    "alpha_values = np.logspace(-3, 3, 50)  # 50 values between 0.001 and 1000\n",
    "\n",
    "# Define the Ridge regression model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Define Grid Search with Cross-Validation (5-fold)\n",
    "ridge_cv = GridSearchCV(ridge, param_grid={'alpha': alpha_values}, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Fit the model\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "# Best alpha value\n",
    "best_alpha = ridge_cv.best_params_['alpha']\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "\n",
    "# Step 3: Train Ridge Regression with Best Alpha\n",
    "# Train Ridge Regression with optimal alpha\n",
    "ridge_best = Ridge(alpha=best_alpha)\n",
    "ridge_best.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_ridge_best = ridge_best.predict(X_test)\n",
    "\n",
    "# Compute MSE\n",
    "mse_ridge_best = mean_squared_error(y_test, y_pred_ridge_best)\n",
    "print(f\"Ridge Regression MSE (Best Alpha): {mse_ridge_best:.4f}\")\n",
    "\n",
    "# Step 4: Visualizing Alpha vs. MSE\n",
    "# Extract mean test scores (negative MSE)\n",
    "mse_scores = -ridge_cv.cv_results_['mean_test_score']\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(alpha_values, mse_scores, marker='o', linestyle='-', label=\"Validation MSE\")\n",
    "plt.xscale('log')  # Log scale for alpha\n",
    "plt.xlabel(\"Alpha (λ)\")\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "plt.title(\"Ridge Regression: MSE vs. Regularization Strength\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc1bf09",
   "metadata": {},
   "source": [
    "##### Key Insights from Grid Search Tuning\n",
    "Optimal Alpha Selection\n",
    "- The best_alpha found minimizes validation error.\n",
    "- A smaller alpha (closer to 0) means Ridge behaves like OLS.\n",
    "- A larger alpha (closer to 1000) increases regularization, making coefficients smaller.\n",
    "\n",
    "Performance Improvement\n",
    "- The tuned Ridge regression model likely outperforms an arbitrarily chosen alpha.\n",
    "- The trade-off between bias and variance is better handled.\n",
    "\n",
    "#### Ridge regression in Sine / polynomial problem as below under GLMs\n",
    "\n",
    "Function for Ridge Regression\n",
    "- It takes ‘alpha’ as a parameter on initialization.\n",
    "\n",
    "Remember that normalizing the inputs generally benefits every type of regression and should apply to ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734497e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def ridge_regression(data, predictors, alpha, models_to_plot={}):\n",
    "    #Fit the model\n",
    "    ridgereg = Ridge(alpha=alpha,normalize=True)\n",
    "    ridgereg.fit(data[predictors],data['y'])\n",
    "    y_pred = ridgereg.predict(data[predictors])\n",
    "    \n",
    "    #Check if a plot is to be made for the entered alpha\n",
    "    if alpha in models_to_plot:\n",
    "        plt.subplot(models_to_plot[alpha])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['x'],y_pred)\n",
    "        plt.plot(data['x'],data['y'],'.')\n",
    "        plt.title('Plot for alpha: %.3g'%alpha)\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    ret.extend([ridgereg.intercept_])\n",
    "    ret.extend(ridgereg.coef_)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14328f30",
   "metadata": {},
   "source": [
    "Analyze the result of Ridge regression for 10 different values of α ranging from 1e-15 to 20. \n",
    "\n",
    "These values have been chosen so that we can easily analyze the trend with changes in values of $\\alpha$.\n",
    "\n",
    "These 10 models will contain all the 15 variables, and only the value of alpha would differ. \n",
    "- This differs from the simple linear regression case, where each model had a subset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize predictors to be set of 15 powers of x\n",
    "predictors=['x']\n",
    "predictors.extend(['x_%d'%i for i in range(2,16)])\n",
    "\n",
    "#Set the different values of alpha to be tested\n",
    "alpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n",
    "\n",
    "#Initialize the dataframe for storing coefficients.\n",
    "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
    "ind = ['alpha_%.2g'%alpha_ridge[i] for i in range(0,10)]\n",
    "coef_matrix_ridge = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "models_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}\n",
    "for i in range(10):\n",
    "    coef_matrix_ridge.iloc[i,] = ridge_regression(data, predictors, alpha_ridge[i], models_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1387018e",
   "metadata": {},
   "source": [
    "Observation: \n",
    "- As the value of alpha increases, the model complexity reduces. \n",
    "    - Though higher values of alpha reduce overfitting, significantly high values can cause underfitting as well (e.g., alpha = 5). \n",
    "        - Thus alpha should be chosen wisely. \n",
    "- A widely accepted technique is **cross-validation**, i.e., the value of alpha is iterated over a range of values, and the one giving a higher cross-validation score is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d67f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the display format to be scientific for ease of analysis\n",
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_matrix_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f771d",
   "metadata": {},
   "source": [
    "inferences:\n",
    "\n",
    "- The RSS increases with an increase in alpha.\n",
    "- An alpha value as small as 1e-15 gives us a significant reduction in the magnitude of coefficients. \n",
    "    - How? Compare the coefficients in the first row of this table to the last row of the simple linear regression table.\n",
    "- High alpha values can lead to significant underfitting. Note the rapid increase in RSS for values of alpha greater than 1\n",
    "    - Though the coefficients are really small, they are NOT zero.\n",
    "\n",
    "Reconfirm the same by determining the number of zeros in each row of the coefficients data set:\n",
    "\n",
    "This should confirm that all 15 coefficients are greater than zero in magnitude (can be +ve or -ve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_matrix_ridge.apply(lambda x: sum(x.values==0),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1e8b8",
   "metadata": {},
   "source": [
    "### Effect of Multiplying a Predictor by a Factor in Ridge Regression\n",
    "When a predictor (feature) in Ridge Regression is multiplied by a constant factor, it affects both the coefficient estimates and the regularization penalty. \n",
    "- However, Ridge regression behaves differently from Ordinary Least Squares (OLS) because of the L2 penalty.\n",
    "\n",
    "**Key Effects of Multiplying a Predictor by a Constant 𝑐**\n",
    "- If a predictor $𝑥_𝑗$ is scaled by a factor c (i.e., $𝑥_𝑗^∗ = 𝑐 \\cdot 𝑥_𝑗$ ), then:\n",
    "\n",
    "Coefficient Rescaling:\n",
    "- The Ridge regression coefficient $\\beta_j$ scales inversely by c, meaning:\n",
    "$$ \\beta_j^* = \\frac{\\beta_j}{c}$$\n",
    "- This keeps the prediction unchanged\n",
    "$$ x_j^* \\cdot \\beta_j^* = (c \\cdot x_j^*) \\cdot (\\frac{\\beta_j}{c}) = x_j \\cdot \\beta_j$$\n",
    "- However, unlike OLS, Ridge regression penalizes larger coefficients, so the shrinkage effect may be different.\n",
    "\n",
    "Effect on the L2 Penalty:\n",
    "- The Ridge penalty term is:\n",
    "$$ \\lambda \\sum \\beta_j^2$$\n",
    "- When $x_j$ is multiplied by c, the coefficient $\\beta_j$ shrinks proportionally, so the contribution to the penalty changes:\n",
    "$$ \\lambda (\\frac{\\beta_j}{c})^2 = \\lambda \\frac{\\beta_j^2}{c^2}$$\n",
    "- This means that predictors with larger magnitudes receive lower penalty weights.\n",
    "\n",
    "Impact on Regularization:\n",
    "- Features on different scales may experience unequal shrinkage.\n",
    "- Larger magnitude predictors get smaller coefficients (more shrinkage).\n",
    "- Smaller magnitude predictors get larger coefficients (less shrinkage).\n",
    "- This can distort the importance of variables if they are on very different scales.\n",
    "\n",
    "Why Standardization is Important in Ridge Regression:\n",
    "- To ensure that all predictors are penalized equally, Ridge regression works best when features are standardized (mean = 0, variance = 1).\n",
    "- Standardization rescales each feature, so they contribute equally to the penalty term.\n",
    "\n",
    "Demonstrate this by fitting Ridge regression on:\n",
    "- Original data\n",
    "- Data with one predictor multiplied by 10\n",
    "\n",
    "Key Observations from the Output\n",
    "- The coefficients adjust inversely to the scaling factor (e.g., if $𝑥_1$ is multiplied by 10, $𝛽_1$ is divided by 10).\n",
    "- The predictions remain the same, but the Ridge penalty applies differently to different-scaled variables.\n",
    "- When we standardize the features, both cases yield similar coefficients, ensuring fair regularization.\n",
    "\n",
    "Conclusion\n",
    "- Multiplying a predictor by a factor scales its coefficient inversely.\n",
    "- Unequal scaling can lead to unfair regularization across features.\n",
    "- Standardization is essential before Ridge regression to ensure equal treatment of all predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic regression data\n",
    "X, y = make_regression(n_samples=100, n_features=2, noise=10, random_state=42)\n",
    "\n",
    "# Multiply the first predictor by 10\n",
    "X_scaled = X.copy()\n",
    "X_scaled[:, 0] *= 10  # Multiply first feature by 10\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)  # Standardized original data\n",
    "X_scaled_std = scaler.fit_transform(X_scaled)  # Standardized scaled data\n",
    "\n",
    "# Fit Ridge regression (same alpha)\n",
    "ridge_original = Ridge(alpha=1).fit(X_std, y)\n",
    "ridge_scaled = Ridge(alpha=1).fit(X_scaled_std, y)\n",
    "\n",
    "# Print coefficients\n",
    "print(\"Original Ridge Coefficients:\", ridge_original.coef_)\n",
    "print(\"Scaled Ridge Coefficients:\", ridge_scaled.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424d64ea",
   "metadata": {},
   "source": [
    "#### Example: Financial Data\n",
    "Apply Ridge Regression with scaling effects to a real-world financial dataset—predicting stock returns using multiple financial indicators.\n",
    "\n",
    "Financial Context:\n",
    "- We will analyze how scaling one predictor (e.g., trading volume) affects Ridge regression when predicting stock returns.\n",
    "    - Predictors: Historical price changes, volume, volatility, moving averages, etc.\n",
    "    - Target Variable: Stock daily return (percentage change in closing price).\n",
    "\n",
    "- Dataset which contains monthly data for the Rand/Dollar exchange rate, as well as a few potential predictor variables.\n",
    "    - the goal is to try and model the exchange rate, using the other 19 variables.\n",
    "\n",
    "The way we write this is as follows:   \n",
    "$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_p X_p $$   \n",
    "- where\n",
    "    - $Y$ is the reponse variable which depends on the _p_ predictor variables.\n",
    "\n",
    "Expected Insights\n",
    "\n",
    "Coefficient Scaling Effect:\n",
    "- When Volume is multiplied by 10, its corresponding coefficient is divided by 10.\n",
    "- Other feature coefficients remain almost unchanged.\n",
    "- The model predictions remain the same despite different coefficient values.\n",
    "\n",
    "Regularization Impact:\n",
    "- Unequal scaling affects how Ridge applies penalties to features.\n",
    "- The penalty is relative to the coefficient size, so unscaled features might be over-penalized.\n",
    "\n",
    "Why Standardization Matters in Finance:\n",
    "- Stock market features (e.g., price, volume, volatility) have very different scales.\n",
    "- Without standardization, Ridge regression may improperly shrink some features more than others.\n",
    "\n",
    "Takeaways for Financial Modeling\n",
    "\n",
    "✅ Always standardize financial data before Ridge regression to ensure fair regularization across features.\n",
    "\n",
    "✅ Scaling a feature affects its coefficient but not the model's predictions.\n",
    "\n",
    "✅ In asset management, standardization prevents misleading importance weights in multi-factor models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ded6c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yfinance as yf\n",
    "\n",
    "# Fetch historical stock data (Apple Inc.)\n",
    "ticker = \"AAPL\"\n",
    "df = yf.download(ticker, period=\"2y\", interval=\"1d\")\n",
    "\n",
    "# Feature Engineering: Creating financial predictors\n",
    "df[\"Return\"] = df[\"Adj Close\"].pct_change() * 100  # Daily return in %\n",
    "df[\"Volatility\"] = df[\"Return\"].rolling(10).std()  # 10-day rolling volatility\n",
    "df[\"Momentum\"] = df[\"Adj Close\"] / df[\"Adj Close\"].shift(10) - 1  # 10-day momentum\n",
    "df[\"Volume\"] = df[\"Volume\"] / 1e6  # Scale Volume to millions\n",
    "df[\"SMA_10\"] = df[\"Adj Close\"].rolling(10).mean()  # 10-day Simple Moving Average\n",
    "df = df.dropna()\n",
    "\n",
    "# Define predictors and target\n",
    "features = [\"Volatility\", \"Momentum\", \"Volume\", \"SMA_10\"]\n",
    "X = df[features]\n",
    "y = df[\"Return\"]\n",
    "\n",
    "# Multiply 'Volume' by 10 to test scaling effects\n",
    "X_scaled = X.copy()\n",
    "X_scaled[\"Volume\"] *= 10\n",
    "\n",
    "# Standardize both versions of X\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "X_scaled_std = scaler.fit_transform(X_scaled)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.2, random_state=42)\n",
    "X_scaled_train, X_scaled_test, _, _ = train_test_split(X_scaled_std, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Ridge Regression models\n",
    "ridge_original = Ridge(alpha=1).fit(X_train, y_train)\n",
    "ridge_scaled = Ridge(alpha=1).fit(X_scaled_train, y_train)\n",
    "\n",
    "# Print Coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Original Coefficients\": ridge_original.coef_,\n",
    "    \"Scaled Coefficients\": ridge_scaled.coef_\n",
    "})\n",
    "\n",
    "print(coef_df)\n",
    "\n",
    "# Plot the effect of scaling\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(coef_df[\"Feature\"], coef_df[\"Original Coefficients\"], alpha=0.6, label=\"Original\")\n",
    "plt.bar(coef_df[\"Feature\"], coef_df[\"Scaled Coefficients\"], alpha=0.6, label=\"Scaled\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Coefficient Value\")\n",
    "plt.title(\"Effect of Scaling 'Volume' on Ridge Regression Coefficients\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003f7f48",
   "metadata": {},
   "source": [
    "### When to Use Ridge Regression Instead of Least-Squares Regression\n",
    "Least-squares regression (OLS) is the standard method for fitting a linear model, but it has significant limitations in certain situations. \n",
    "\n",
    "Ridge regression is preferred when these limitations arise.\n",
    "\n",
    "Ridge regression is better when:\n",
    "- Multicollinearity exists (e.g., correlated financial variables).\n",
    "- The number of predictors exceeds the number of observations (e.g., high-dimensional datasets).\n",
    "- Data has high variance/noise, leading to overfitting in OLS.\n",
    "\n",
    "##### When Least-Squares Regression Fails & Ridge Regression Excels\n",
    "\n",
    "| Scenario | Why OLS Fails?  | Why Ridge Regression Works?  | \n",
    "|---------------|--------|----------------------------------|   \n",
    "|High Multicollinearity (Highly Correlated Predictors)\t|OLS produces unstable and large coefficients, leading to overfitting. |Ridge shrinks coefficients and reduces multicollinearity effects| \n",
    "|More Predictors than Observations (High-Dimensional Data (p>n)|OLS cannot uniquely estimate coefficients when p>n (underdetermined system). |Ridge introduces regularization, making estimation possible. | \n",
    "|Noisy Data with Many Insignificant Predictors  |OLS fits noise and suffers from high variance (overfitting).|\tRidge shrinks less useful coefficients towards zero, improving generalization.| \n",
    "|Predictors Have Large Variations in Scale |OLS coefficients can be heavily influenced by features with larger magnitudes. |Ridge makes coefficients more balanced by penalizing their magnitudes. |\n",
    "\n",
    "Ridge Regression Fixing Multicollinearity\n",
    "- Consider a financial dataset where marketing spend and advertising spend are highly correlated. OLS will produce unreliable, large coefficients, but Ridge will stabilize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edf239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Simulated dataset with multicollinearity\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "X1 = np.random.rand(n) * 10  # Marketing spend\n",
    "X2 = X1 + np.random.normal(0, 0.5, n)  # Advertising spend (highly correlated with X1)\n",
    "X3 = np.random.rand(n) * 5  # Independent feature\n",
    "y = 3 * X1 + 2 * X2 + 1.5 * X3 + np.random.normal(0, 2, n)  # Target variable\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Marketing Spend': X1, 'Advertising Spend': X2, 'Other Feature': X3, 'Revenue': y})\n",
    "\n",
    "# Train-test split\n",
    "X = df[['Marketing Spend', 'Advertising Spend', 'Other Feature']]\n",
    "y = df['Revenue']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# OLS Regression\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge = Ridge(alpha=1)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Print coefficients\n",
    "print(\"OLS Coefficients:\", ols.coef_)\n",
    "print(\"Ridge Coefficients:\", ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f995f",
   "metadata": {},
   "source": [
    "### Advantages of Ridge Regression Over Best Subset Selection\n",
    "Both ridge regression and best subset selection are used for handling multiple predictors in regression models. However, ridge regression is often preferred because of its computational efficiency and ability to handle high-dimensional data.\n",
    "\n",
    "Key Differences Between Ridge Regression and Best Subset Selection\n",
    "\n",
    "| Criteria | Ridge Regression  | Best Subset Selection  | \n",
    "|---------------|--------|----------------------------------|   \n",
    "|Handling Multicollinearity\t|Shrinks correlated features, making coefficients more stable. |Cannot handle multicollinearity well, as it selects individual variables.| \n",
    "|Computational Efficiency|Fast and scalable for large datasets. |Selects a subset of predictors, dropping some entirely. | \n",
    "|Feature Selection |Keeps all predictors but shrinks less important ones.|\tRidge shrinks less useful coefficients towards zero, improving generalization.| \n",
    "|Overfitting Prevention | Reduces variance and improves generalization. | Can overfit if the best subset is not chosen carefully. |\n",
    "|Applicability in High Dimensions (p>n)  | Works well even with more predictors than observations. | Fails when p>n as there are not enough observations to fit the model. |\n",
    "|Interpretability | Harder to interpret since all predictors contribute. | Easier to interpret as some coefficients are set to zero. |\n",
    "\n",
    "##### Key Advantages of Ridge Over Best Subset Selection\n",
    "1️⃣ Ridge Handles Multicollinearity Better\n",
    "- Best subset selection chooses individual variables, which can lead to unstable models when predictors are correlated.\n",
    "- Ridge shrinks coefficients without removing variables, reducing variance while preserving information.\n",
    "\n",
    "2️⃣ Ridge Is Computationally Efficient\n",
    "- Best subset selection requires testing all possible subsets, which is exponential in complexity $p^2$  making it infeasible for large p.\n",
    "- Ridge regression solves an optimization problem efficiently using closed-form solutions or gradient-based methods.\n",
    "\n",
    "3️⃣ Ridge Works Well When 𝑝>𝑛\n",
    "- Best subset selection cannot work when the number of predictors exceeds the number of observations.\n",
    "- Ridge can handle high-dimensional data by shrinking coefficients instead of eliminating variables.\n",
    "\n",
    "4️⃣ Ridge Improves Generalization\n",
    "- Best subset selection picks the best combination based on training data, which may not generalize well.\n",
    "- Ridge reduces overfitting by introducing regularization, improving test performance.\n",
    "\n",
    "##### When to Use Ridge vs. Best Subset Selection?\n",
    "✅ Use Ridge Regression When:\n",
    "- The dataset has many correlated predictors (e.g., financial indicators).\n",
    "- There are more predictors than observations  (p>n).\n",
    "- You want better predictive accuracy rather than interpretability.\n",
    "\n",
    "✅ Use Best Subset Selection When:\n",
    "- You have a small number of predictors and want an interpretable model.\n",
    "- Multicollinearity is not a major issue.\n",
    "- You prioritize feature selection over prediction accuracy.\n",
    "\n",
    "##### Comparison of Ridge Regression and Best Subset Selection in Python\n",
    "We will use a simulated dataset with multiple correlated predictors to compare Ridge Regression and Best Subset Selection (Exhaustive Feature Selection).\n",
    "\n",
    "Key Takeaways\n",
    "- Ridge Regression retains all predictors but shrinks their coefficients to reduce overfitting.\n",
    "- Best Subset Selection eliminates some features (e.g., X2, X4, X5), keeping only the most important ones.\n",
    "- RMSE is lower for Ridge Regression than for Best Subset Selection, showing that Ridge generalizes better.\n",
    "\n",
    "✅ Ridge is better for prediction when multicollinearity exists.\n",
    "\n",
    "✅ Best Subset is better for feature selection and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a16e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Required Libraries\n",
    "# First, install mlxtend for Best Subset Selection:\n",
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07198a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Import Necessary Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "\n",
    "\n",
    "# Step 3:  Generate Simulated Data\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulating correlated predictors\n",
    "n = 100\n",
    "X1 = np.random.rand(n) * 10  \n",
    "X2 = X1 + np.random.normal(0, 0.5, n)  # Highly correlated with X1\n",
    "X3 = np.random.rand(n) * 5  \n",
    "X4 = np.random.rand(n) * 2  \n",
    "X5 = np.random.rand(n) * 7  \n",
    "\n",
    "# Target variable (dependent on X1, X2, X3)\n",
    "y = 3 * X1 + 2 * X2 + 1.5 * X3 + np.random.normal(0, 2, n)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'X4': X4, 'X5': X5, 'y': y})\n",
    "\n",
    "# Train-test split\n",
    "X = df[['X1', 'X2', 'X3', 'X4', 'X5']]\n",
    "y = df['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Step 4: Ridge Regression\n",
    "# Ridge regression with alpha = 1\n",
    "ridge = Ridge(alpha=1)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and RMSE\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "\n",
    "# Print coefficients and RMSE\n",
    "print(\"Ridge Regression Coefficients:\", ridge.coef_)\n",
    "print(\"Ridge Regression RMSE:\", ridge_rmse)\n",
    "\n",
    "# Step 5: Best Subset Selection\n",
    "# Perform Best Subset Selection\n",
    "efs = EFS(LinearRegression(), \n",
    "          min_features=1, \n",
    "          max_features=5,  # Consider all predictors\n",
    "          scoring='neg_mean_squared_error', \n",
    "          cv=5)\n",
    "\n",
    "efs.fit(X_train, y_train)\n",
    "\n",
    "# Get best subset of features\n",
    "best_features = list(efs.best_idx_)\n",
    "best_feature_names = X.columns[list(efs.best_idx_)]\n",
    "\n",
    "# Fit model with selected features\n",
    "X_train_best = X_train.iloc[:, best_features]\n",
    "X_test_best = X_test.iloc[:, best_features]\n",
    "\n",
    "best_model = LinearRegression()\n",
    "best_model.fit(X_train_best, y_train)\n",
    "\n",
    "# Predictions and RMSE\n",
    "y_pred_best_subset = best_model.predict(X_test_best)\n",
    "best_subset_rmse = np.sqrt(mean_squared_error(y_test, y_pred_best_subset))\n",
    "\n",
    "# Print selected features and RMSE\n",
    "print(\"Best Subset Selected Features:\", best_feature_names)\n",
    "print(\"Best Subset Selection RMSE:\", best_subset_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd185df",
   "metadata": {},
   "source": [
    "### Review of Data Scaling / Standardisng a Predictor\n",
    "\n",
    "Data scaling is essential in regularisation as regularising penalizes a model for large coefficients. \n",
    "\n",
    "The magnitude of coefficients is dependent on the following:\n",
    "\n",
    "* The strength of the relationship between the predictor variables (`x`) and the output variable (`y`)\n",
    "* The units of measurement of x(eg. distance measured in millimetres or metres).\n",
    "\n",
    "For example, if x is measured in metres, and its coefficient is 5; if it is expressed in kilometres, its coefficient will be 5*10³.\n",
    "\n",
    "We want regularisation to be impacted by the strength of the relationship that exists between `x` and `y` variables and not the magnitude of the coefficients.\n",
    "- Thus, to eliminate the impact of the units of measurement of the variables on the coefficients, \n",
    "- Performed data scaling to ensure variables are fairly scaled. \n",
    "\n",
    "Standardizing the Predictors in Ridge Regression\n",
    "- Standardization is a crucial preprocessing step in Ridge Regression, especially when predictors (features) have different scales.\n",
    "\n",
    "What is Standardization?\n",
    "- Standardization transforms each predictor so that it has:\n",
    "    - Mean = 0\n",
    "    - Standard deviation = 1\n",
    "\n",
    "- The formula for standardizing a predictor $x_j$ is:\n",
    "$$x_j^{(std)} = \\frac{x_j - \\bar{x_j}}{s_j}$$\n",
    "- where:\n",
    "    - $\\bar{x_j}$ = Mean of predictor $x_j$\n",
    "    - $s_j$ = Standard deviation of predictor $x_j$\n",
    "- This ensures that all features contribute equally to Ridge regression’s penalty term.\n",
    "\n",
    "Why Standardization is Important in Ridge Regression\n",
    "- Equalizes Feature Influence: Features with large values (e.g., trading volume in millions) don’t dominate over small-valued features (e.g., daily return in %).\n",
    "- Ensures Proper Regularization: Ridge regression penalizes larger coefficients more. If one feature has large values, it gets smaller penalties, distorting regularization.\n",
    "- Improves Model Convergence: Gradient-based solvers used in Ridge regression work better with standardized data.\n",
    "\n",
    "Effect of Standardization on Ridge Regression\n",
    "\n",
    "Before standardization:\n",
    "- Different features contribute unequally to Ridge’s L2 penalty.\n",
    "- Some features dominate simply due to their scale.\n",
    "\n",
    "After standardization:\n",
    "- All features contribute fairly to the model.\n",
    "- Ridge regression applies equal shrinkage across all predictors.\n",
    "\n",
    "Key Takeaways\n",
    "\n",
    "✅ Always standardize predictors before applying Ridge regression.\n",
    "\n",
    "✅ Prevents misleading coefficient shrinkage.\n",
    "\n",
    "✅ Ensures numerical stability and better model performance.\n",
    "\n",
    "**Z-score standardisation** is a great way to scale variables such that they have similar (though not identical) ranges, in a way that is fairly robust to outlier values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432850a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into predictors and response\n",
    "X = df.drop('ZAR/USD', axis=1)\n",
    "y = df['ZAR/USD']\n",
    "\n",
    "# Import scaler method from sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create scaled version of the predictors (there is no need to scale the response)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Check mean and standard deviation after scaling\n",
    "print(\"Feature Means (after standardization):\", X_scaled.mean(axis=0))  # Should be close to 0\n",
    "print(\"Feature Std Dev (after standardization):\", X_scaled.std(axis=0))  # Should be close to 1\n",
    "\n",
    "# Convert the scaled predictor values into a dataframe\n",
    "X_standardise = pd.DataFrame(X_scaled,columns=X.columns)\n",
    "X_standardise.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94991f9f",
   "metadata": {},
   "source": [
    "# 3.1. Regularisation Methods: LASSO Regression\n",
    "\n",
    "- Understand the difference between L1 and L2 regularisation\n",
    "- Understand the concept of sparsity.\n",
    "\n",
    "### Comparison of Ridge Regression and Lasso Regression in Python\n",
    "Compare Ridge Regression and Lasso Regression to understand their differences, advantages, and disadvantages.\n",
    "\n",
    "| Criteria | Ridge Regression  | Lasso Regression  | \n",
    "|---------------|--------|----------------------------------|   \n",
    "|Handling Multicollinearity\t|Handles multicollinearity well |Can randomly pick one correlated variable and drop others.| \n",
    "|Computational Efficiency|Fast and scalable |Can be slow for large feature sets | \n",
    "|Effect on Coefficient | Shrinks but never sets to zero | Shrinks and sets some coefficients to zero (feature selection) |\n",
    "|Penalty Type | $L_2$ norm: $\\lambda \\sum \\beta^2_j$ | $L_1$ norm: $\\lambda \\sum \\| \\beta_j \\|$ |\n",
    "|Interpretability | Harder to interpret since all predictors contribute.  (keeps all variables) | Easier to interpret as some coefficients are set to zero. (performs feature selection)|\n",
    "|Use Case |Best for reducing variance without eliminating variables|Best when a sparse model (fewer variables) is desired|\n",
    "\n",
    "### L1 (LASSO) vs. L2(Ridge) Regularization Techniques\n",
    "\n",
    "The key difference is in how they assign penalties to the coefficients:\n",
    "\n",
    "Ridge Regression:\n",
    "- Performs L2 regularization, i.e., adds penalty equivalent to the square of the magnitude of coefficients\n",
    "    - Minimization objective = LS Obj + α * (sum of square of coefficients)\n",
    "\n",
    "Lasso Regression:\n",
    "- Performs L1 regularization, i.e., adds penalty equivalent to the absolute value of the magnitude of coefficients\n",
    "    - Minimization objective = LS Obj + α * (sum of the absolute value of coefficients)\n",
    "\n",
    "LS Obj refers to the ‘least squares objective,’ i.e., the linear regression objective without regularization.\n",
    "\n",
    "#### Key Differences between Ridge and Lasso Regression\n",
    "Observations:\n",
    "\n",
    "1️⃣ Ridge Shrinks Coefficients But Keeps All Features\n",
    "- Ridge regression helps us to reduce only the overfitting in the model while keeping all the features present in the model.\n",
    "    - It reduces the complexity of the model by shrinking the coefficients whereas Lasso regression helps in reducing the problem of overfitting in the model as well as automatic feature selection.\n",
    "- All coefficients remain nonzero.\n",
    "- Good when we believe all variables contribute to the model.\n",
    "\n",
    "3️⃣ Ridge Has a Lower RMSE than Lasso\n",
    "- Ridge typically performs better in predictive accuracy when all variables contain useful information.\n",
    "\n",
    "2️⃣ Lasso Shrinks Some Coefficients to Zero (Feature Selection)\n",
    "- Lasso Regression tends to make coefficients to absolute zero whereas Ridge regression never sets the value of coefficient to absolute zero.\n",
    "    - $𝑋_2, 𝑋_4, and 𝑋_5$ are eliminated from the model.\n",
    "- Lasso automatically selects the most relevant features.\n",
    "\n",
    "4️⃣ Lasso is More Interpretable\n",
    "- Lasso provides a sparse solution, making it easier to interpret.\n",
    "\n",
    "##### Disadvantages of Ridge Regression\n",
    "1️⃣ Does Not Perform Feature Selection\n",
    "- Ridge shrinks coefficients but never forces them to zero, meaning it retains all features, even unimportant ones.\n",
    "- This makes it harder to interpret the model.\n",
    "\n",
    "2️⃣ Less Effective for Sparse Solutions\n",
    "- If many variables have no real effect on the target, Ridge still assigns them small coefficients, leading to unnecessary complexity.\n",
    "\n",
    "3️⃣ Sensitive to Feature Scaling\n",
    "- Requires standardization for optimal performance.\n",
    "\n",
    "### Shrinkage Methods\n",
    "\n",
    "In ridge regression, we learned that it is possible to modify and potentially improve the test-set performance of a least squares regression model by reducing the magnitude of some subset of the coefficients $\\hat{\\beta}$.\n",
    "- The ridge regression process of reducing the magnitude of those coefficients is a type of _shrinkage_ method - we are attempting to shrink the values of those less important coefficients.\n",
    "- In ridge regression, it is possible to shrink a coefficient's value towards zero, but never reaching exactly zero.\n",
    "\n",
    "### Sparsity\n",
    "\n",
    "L1 penalty has the eﬀect of forcing some of the coeﬃcient estimates to be exactly equal to zero which means there is a complete removal of some of the features for model evaluation when the tuning parameter λ is suﬃciently large.\n",
    "- Therefore, the lasso method also performs Feature selection and is said to yield sparse models.\n",
    "\n",
    "##### Advantages of Lasso Over Ridge Regression\n",
    "✅ 1. Automatic Feature Selection\n",
    "- Lasso forces some coefficients to zero, making it useful when only a few variables are important.\n",
    "\n",
    "✅ 2. More Interpretable Models\n",
    "- Since Lasso removes irrelevant features, the model is easier to explain and analyze.\n",
    "\n",
    "✅ 3. Works Well When Many Features Are Irrelevant\n",
    "- If there are many insignificant predictors, Lasso can remove them, leading to a sparser and more efficient model.\n",
    "\n",
    "##### Limitation of Lasso Regression:\n",
    "\n",
    "Problem - types of Dataset: \n",
    "- If the number of predictors is greater than the number of data points, \n",
    "    - Lasso will pick at most n predictors as non-zero, even if all predictors are relevant.\n",
    "\n",
    "Multicollinearity Problem: \n",
    "- If there are two or more highly collinear variables then LASSO regression selects one of them randomly which is not good for the interpretation of our model.\n",
    "\n",
    "##### When to Use Ridge vs. Lasso?\n",
    "📌 Use Ridge When:\n",
    "- Multicollinearity is high (correlated predictors).\n",
    "- All features are expected to contribute to the model.\n",
    "- Predictive accuracy is more important than interpretability.\n",
    "\n",
    "📌 Use Lasso When:\n",
    "- Feature selection is needed (only a few predictors are important).\n",
    "- You want a simple and interpretable model.\n",
    "- Some variables are irrelevant (Lasso will drop them).\n",
    "\n",
    "### **LASSO Regression**\n",
    "\n",
    "Description\n",
    "- Lasso regression, or Least Absolute Shrinkage and Selection Operator, \n",
    "- is a regularization method that also includes a penalty term but can set some coefficients exactly to zero, effectively selecting relevant features.\n",
    "\n",
    "Penalty Type\n",
    "- Lasso regression employs an L1 penalty, \n",
    "    - which sums the absolute values of the coefficients multiplied by lambda.\n",
    "\n",
    "Coefficient Impact\n",
    "- The L1 penalty in lasso regression can drive some coefficients to exactly zero when the lambda value is large enough, performing feature selection and resulting in a sparse model.\n",
    "\n",
    "Feature Selection\n",
    "- Lasso regression can set some coefficients to zero, effectively selecting the most relevant features and improving model interpretability.\n",
    "\n",
    "Use Case\n",
    "- Lasso regression is preferred when the goal is feature selection, resulting in a simpler and more interpretable model with fewer variables.\n",
    "\n",
    "Model Complexity\n",
    "- Lasso regression can lead to a less complex model by setting some coefficients to zero, reducing the number of effective parameters.\n",
    "\n",
    "Interpretability\n",
    "- Lasso regression can improve interpretability by selecting only the most relevant features, making the model’s predictions more explainable.\n",
    "\n",
    "Sparsity\n",
    "- Lasso regression can produce sparse models by setting some coefficients to exactly zero.\n",
    "\n",
    "Sensitivity\n",
    "- More sensitive to outliers due to the absolute value in the penalty term.\n",
    "\n",
    "Final Thoughts\n",
    "- Ridge is better for prediction when all variables have some effect.\n",
    "- Lasso is better when we suspect that many variables are irrelevant.\n",
    "- Elastic Net (a hybrid of Ridge and Lasso) can be used when both regularization and feature selection are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce901b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Define Ridge and Lasso models\n",
    "ridge = Ridge(alpha=1)\n",
    "lasso = Lasso(alpha=0.1)  # Smaller alpha to avoid excessive shrinkage\n",
    "\n",
    "# Fit models\n",
    "ridge.fit(X_train, y_train)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "lasso_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "\n",
    "# Print coefficients and RMSE\n",
    "print(\"Ridge Regression Coefficients:\", ridge.coef_)\n",
    "print(\"Ridge Regression RMSE:\", ridge_rmse)\n",
    "\n",
    "print(\"Lasso Regression Coefficients:\", lasso.coef_)\n",
    "print(\"Lasso Regression RMSE:\", lasso_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990f3cf9",
   "metadata": {},
   "source": [
    "Recall the optimisation expression for ridge regression:\n",
    "\n",
    "$$\\min_{\\beta} (RSS + \\alpha\\sum_{j=1}^pb_j^2)$$\n",
    "\n",
    "where we attempt to minimise the RSS and some penalty term. This can be rewritten:\n",
    "\n",
    "$$\\min_{\\beta} (RSS + \\alpha(L2\\_norm))$$\n",
    "\n",
    "where $L2\\_norm$ is the *sum of the squares of the coefficients*.\n",
    "\n",
    "In LASSO regularisation, \n",
    "- we replace the $L2\\_norm$ with what is known as the $L1\\_norm$: the *sum of the _absolute_ values of the coefficients*.\n",
    "\n",
    "This is a relatively recent adaptation of ridge regression which is capable of shrinking predictors to exactly zero - effectively removing them from the model entirely and creating what we call a sparse model (one which uses some subset of all of the available predictors).\n",
    "\n",
    "LASSO achieves both shrinkage and subset selection.\n",
    "\n",
    "### Objective function\n",
    "\n",
    "A LASSO model is fit under the constraint of minimizing the following equation:\n",
    "\n",
    "$$\\sum_{i=1}^n(y_i-(a+\\sum_{j=1}^pb_jx_{ij}))^2 + \\alpha\\sum_{j=1}^p|b_j|$$\n",
    "\n",
    "which can be rewritten as follows:\n",
    "\n",
    "$$\\min_{\\beta} (RSS + \\lambda\\sum_{j=1}^p|b_j|)$$\n",
    "\n",
    "or,\n",
    "\n",
    "$$\\min_{\\beta} (RSS + \\lambda(L1\\_norm))$$\n",
    "\n",
    "Lasso regression performs L1 regularization, i.e., it adds a factor of the sum of the absolute value of coefficients in the optimization objective.\n",
    "\n",
    "Objective = RSS + $\\lambda$ * (sum of the absolute value of coefficients)\n",
    "\n",
    "Breakdown of the terms:\n",
    "- **Residual Sum of Squares (RSS):**\n",
    "    - In minimising _RSS_ , we improve the overall fit of the model. \n",
    "$$\\sum^n_{i = 1} (y_i - \\hat{y_i})^2 $$\n",
    "- This represents the sum of squared differences between the observed values $y_i$ and the predicted values $\\hat{y_i}$.\n",
    "\n",
    "- **L1 Regularization Term (Penalty Term):**\n",
    "    - In Lasso Regression, the L1 norm is used as a penalty term\n",
    "    - The L1 norm is the sum of the absolute values of the coefficients:\n",
    "$$L1_norm =  \\lambda \\sum^p_{j = 1} \\| \\beta_j \\| $$\n",
    "- Lasso regression performs ‘L1 regularization‘, i.e., it adds a factor of the sum of absolute value of coefficients in the optimization objective.\n",
    "    - This term penalizes large coefficient values, preventing overfitting. \n",
    "- Calculating the L2 Norm in Ridge Regression\n",
    "    - In Lasso Regression, the L1 norm  is the sum of the absolute values of the regression coefficients. \n",
    "    - The L1 norm is used as a penalty in Lasso regression:\n",
    "$$ \\sum^p_{j = 1} \\| \\beta_j \\|$$\n",
    "- where:\n",
    "    - $𝛽_𝑗$ are the regression coefficients.\n",
    "    - p is the number of features.\n",
    "\n",
    "In Lasso Regression, the L1 norm is used as a penalty term:\n",
    "$$ min ||y - X \\beta||^2 + \\lambda \\sum^p_{j = 1} \\| \\beta_j \\| $$\n",
    "\n",
    "- **$\\lambda$ (alpha)** \n",
    "    - works similar to that of the ridge and provides a trade-off between balancing RSS and the magnitude of coefficients. \n",
    "    - Like that of the ridge, $\\lambda$ can take various values.\n",
    "        - $\\lambda$ = 0: Same coefficients as simple linear regression\n",
    "        - $\\lambda$ = ∞: All coefficients zero (same logic as before)\n",
    "        - 0 < $\\lambda$ < ∞: coefficients between 0 and that of simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea71ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features from the response\n",
    "X = df.drop('ZAR/USD', axis=1)\n",
    "y = df['ZAR/USD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff9fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scaling module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create standardization object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Save standardized features into new variable\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f232b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train/test split module\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, \n",
    "                                                    y, \n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=1,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af907955",
   "metadata": {},
   "source": [
    "##### Calculate L1 Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e0252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LASSO module\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Create LASSO model object, setting alpha to 0.01\n",
    "lasso = Lasso(alpha=0.01)\n",
    "# or\n",
    "# Example Data (X_train, y_train should be pre-defined)\n",
    "lasso2 = Lasso(alpha=0.1)  # Using Lasso Regression\n",
    "\n",
    "# Train the LASSO model\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Extract intercept from model\n",
    "intercept = float(lasso.intercept_)\n",
    "\n",
    "# Extract coefficient from model\n",
    "coeff = pd.DataFrame(lasso.coef_, X.columns, columns=['Coefficient'])\n",
    "\n",
    "# Calculate L1 Norm\n",
    "l1_norm = np.sum(np.abs(lasso_coefficients))\n",
    "\n",
    "# Extract intercept\n",
    "print(\"Intercept:\", float(intercept))\n",
    "print(\"Lasso Coefficients:\", coeff)\n",
    "print(\"L1 Norm of Coefficients:\", l1_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bed0d7",
   "metadata": {},
   "source": [
    "##### Interpretation of the intercept and coefficients\n",
    "\n",
    "We interpret the values of the intercept and coefficients the same way as before:\n",
    "\n",
    " - The intercept can be interpreted as the **expected exchange rate when all the features are equal to their means**.\n",
    " - Each coefficient is interpreted as the expected change in the response variable given an increase of 1 in the **scaled feature value**.\n",
    " \n",
    "See from the list of coefficients above that some of the coefficients have indeed been shrunk to exactly zero.\n",
    "- ✅ Some coefficients are zeroed out → Feature selection.\n",
    "\n",
    "✅ The L1 norm is the sum of the absolute values of nonzero coefficients.\n",
    "\n",
    "Why is the L1 Norm Important?\n",
    "- 📌 Used for feature selection (unlike Ridge which only shrinks).\n",
    "- 📌 Encourages sparsity, keeping only the most relevant predictors.\n",
    "- 📌 Higher alpha (λ) → More coefficients become zero (stronger penalty).\n",
    "\n",
    "##### Assessment of predictive accuracy\n",
    "fit the following models as well, in order to compare the LASSO results thoroughly:\n",
    "\n",
    "- A least squares model using all available predictors;\n",
    "- A least squares model using the predictors with non-zero coefficients from LASSO;\n",
    "- A ridge regression model using all available predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a basic linear model\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "X_subset = df.drop(['ZAR/USD',\n",
    "                   'Total Reserves excl Gold (USD)',\n",
    "                   'IMF Reserve Position (USD)',\n",
    "                   'Claims on Non-residents (USD)',\n",
    "                   'Central Bank Policy Rate',\n",
    "                   'Treasury Bill Rate',\n",
    "                   'Savings Rate',\n",
    "                   'Deposit Rate',\n",
    "                   'Lending Rate',\n",
    "                   'Government Bonds'], axis=1)\n",
    "\n",
    "X_subset_scaled = scaler.fit_transform(X_subset)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_subset, \n",
    "                                                        y, \n",
    "                                                        test_size=0.20, \n",
    "                                                        random_state=1,\n",
    "                                                        shuffle=False)\n",
    "\n",
    "# Least squares using non-zero variables from LASSO\n",
    "lm_subset = LinearRegression()\n",
    "\n",
    "# Least squares using all predictors\n",
    "lm_all = LinearRegression()\n",
    "\n",
    "# Ridge using all predictors\n",
    "ridge = Ridge()\n",
    "\n",
    "lm_subset.fit(X_train2, y_train2)\n",
    "lm_all.fit(X_train, y_train)\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bccf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Make training set predictions for each model\n",
    "train_lm_subset = lm_subset.predict(X_train2)\n",
    "train_lm_all = lm_all.predict(X_train)\n",
    "train_ridge = ridge.predict(X_train)\n",
    "train_lasso = lasso.predict(X_train)\n",
    "\n",
    "# Make test set predictions for each model\n",
    "test_lm_subset = lm_subset.predict(X_test2)\n",
    "test_lm_all = lm_all.predict(X_test)\n",
    "test_ridge = ridge.predict(X_test)\n",
    "test_lasso = lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f35d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of results\n",
    "results_dict = {'Training MSE':\n",
    "                    {\n",
    "                        \"Least Squares, Subset\": metrics.mean_squared_error(y_train2, train_lm_subset),\n",
    "                        \"Least Squares, All\": metrics.mean_squared_error(y_train, train_lm_all),\n",
    "                        \"Ridge\": metrics.mean_squared_error(y_train, train_ridge),\n",
    "                        \"LASSO\": metrics.mean_squared_error(y_train, train_lasso)\n",
    "                    },\n",
    "                    'Test MSE':\n",
    "                    {\n",
    "                        \"Least Squares, Subset\": metrics.mean_squared_error(y_test2, test_lm_subset),\n",
    "                        \"Least Squares, All\": metrics.mean_squared_error(y_test, test_lm_all),\n",
    "                        \"Ridge\": metrics.mean_squared_error(y_test, test_ridge),\n",
    "                        \"LASSO\": metrics.mean_squared_error(y_test, test_lasso)\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from dictionary\n",
    "results_df = pd.DataFrame(data=results_dict)\n",
    "\n",
    "# View the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f17d2",
   "metadata": {},
   "source": [
    "##### Result interpretation\n",
    "\n",
    "LASSO was able to perform subset selection, while also performing shrinkage. \n",
    "- The result is a more generalised model with greater predictive capacity. \n",
    "\n",
    "The least squares model which we trained on the same subset of variables that LASSO retained as non-zero scored a higher MSE on the test set, \n",
    "- indicating that the shrinkage that LASSO applied to those remaining variables was effective.\n",
    "\n",
    "LASSO achieved the best MSE on the test set, followed by ridge regression.\n",
    "\n",
    "##### Plot our results to end off.\n",
    "plot the the test set versus the three primary methods explored here:\n",
    "\n",
    "- Least squares using all predictors;\n",
    "- Ridge using all predictors;\n",
    "- LASSO using all predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c540e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### we want to plot the train and test response variables as a continuous line\n",
    "train_plot = y_train.append(pd.Series(y_test[0], index=['2016M01']))\n",
    "\n",
    "plt.plot(np.arange(96,120), lasso.predict(X_test), label='LASSO')\n",
    "plt.plot(np.arange(96,120), ridge.predict(X_test), label='Ridge')\n",
    "plt.plot(np.arange(96,120), lm_all.predict(X_test), label='Least Squares')\n",
    "plt.plot(np.arange(96,120), y_test, label='Testing')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d84a1f",
   "metadata": {},
   "source": [
    "##### **Effect of the Tuning Parameter $\\lambda$ on Coefficients in Lasso Regression**\n",
    "In Lasso Regression, the tuning parameter λ (alpha in Python) controls the L1 penalty, which affects the size of the regression coefficients and determines feature selection.\n",
    "$$min||y - X\\beta ||^2 + \\lambda \\sum^P_{j = 1} |\\beta_j| $$\n",
    "- where:\n",
    "    - $||y - X\\beta ||^2 $:  least squares error.\n",
    "    - $\\sum^P_{j = 1} |\\beta_j|$: is the L1 penalty that forces some coefficients to shrink to zero.\n",
    "\n",
    "Impact of λ on Coefficients\n",
    "\n",
    "Small λ (Near Zero)\n",
    "- Very little penalty → Lasso behaves like ordinary least squares (OLS).\n",
    "- Most coefficients remain nonzero.\n",
    "- Model may overfit the data.\n",
    "\n",
    "Moderate λ\n",
    "- Some coefficients shrink close to zero, but not all.\n",
    "- Feature selection occurs → Less important variables are eliminated.\n",
    "- Model balances bias-variance tradeoff.\n",
    "\n",
    "Large λ\n",
    "- Strong penalty on large coefficients.\n",
    "- Many coefficients shrink exactly to zero → Only the most important features remain.\n",
    "- Model has higher bias but less variance.\n",
    "- If λ is too large, it removes too many features and model underfits.\n",
    "\n",
    "Expected Output\n",
    "\n",
    "A coefficient path plot that shows: \n",
    "\n",
    "✅ Small 𝜆 → Most coefficients are large (OLS behavior).\n",
    "\n",
    "✅ Medium λ → Some coefficients shrink, some go to zero.\n",
    "\n",
    "✅ Large λ → Many coefficients disappear, leaving only a few nonzero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b331e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Example: Train Lasso with different alpha values\n",
    "alphas = [0.01, 0.1, 1, 10, 100]  # Different lambda values\n",
    "lasso_coeffs = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    lasso_coeffs.append(lasso.coef_)\n",
    "\n",
    "# Convert to NumPy array for plotting\n",
    "lasso_coeffs = np.array(lasso_coeffs)\n",
    "\n",
    "# Plot coefficient paths\n",
    "plt.figure(figsize=(8, 5))\n",
    "for i in range(X_train.shape[1]):\n",
    "    plt.plot(alphas, lasso_coeffs[:, i], marker='o', label=f'Feature {i+1}')\n",
    "\n",
    "plt.xscale(\"log\")  # Log scale for better visualization\n",
    "plt.xlabel(\"Lambda (α)\")\n",
    "plt.ylabel(\"Coefficient Values\")\n",
    "plt.title(\"Effect of Tuning Parameter (λ) on Lasso Coefficients\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76243960",
   "metadata": {},
   "source": [
    "##### **Alternative Formulation of the Objective Functions for Lasso and Ridge Regression**\n",
    "Both Lasso and Ridge Regression aim to regularize linear regression models by adding a constraint on the size of the regression coefficients.\n",
    "\n",
    "Instead of writing their objective functions in a penalized form, they can also be expressed in a constraint-based form.\n",
    "\n",
    "**Ridge Regression (L2 Regularization)**\n",
    "\n",
    "Penalized Formulation\n",
    "$$min||y - X\\beta ||^2 + \\lambda \\sum^P_{j = 1} \\beta_j^2 $$\n",
    "- where:\n",
    "    - $||y - X\\beta ||^2 $:  least squares error.\n",
    "    - $\\sum^P_{j = 1} \\beta_j^2$: is the L2 penalty that forces some coefficients to shrink to zero.\n",
    "\n",
    "Alternative Constraint-Based Formulation\n",
    "$$min||y - X\\beta ||^2 subject to \\sum^P_{j = 1} \\beta_j^2 \\leq t$$\n",
    "- where:\n",
    "    - Instead of adding a penalty, we constrain the sum of squared coefficients to be within a threshold t.\n",
    "    - Smaller t leads to stronger regularization, forcing more shrinkage of coefficients.\n",
    "\n",
    "✅ Interpretation: The Ridge solution lies inside a p-dimensional hypersphere with radius\n",
    "\n",
    "**Lasso Regression (L1 Regularization)**\n",
    "\n",
    "Penalized Formulation\n",
    "$$min||y - X\\beta ||^2 + \\lambda \\sum^P_{j = 1} |\\beta_j| $$\n",
    "- where:\n",
    "    - $||y - X\\beta ||^2 $:  sum of squared residuals..\n",
    "    - $\\sum^P_{j = 1} |\\beta_j|$: is the L1 penalty which induces sparsity.\n",
    "\n",
    "Alternative Constraint-Based Formulation\n",
    "$$min||y - X\\beta ||^2 subject to \\sum^P_{j = 1} |\\beta_j| \\leq t$$\n",
    "- where:\n",
    "    - The sum of absolute values of coefficients is constrained within t.\n",
    "    - Stronger regularization (small t) forces many coefficients to exactly zero.\n",
    "\n",
    "✅ Interpretation: The Lasso solution lies inside a p-dimensional diamond-shaped (L1 norm) constraint region.\n",
    "\n",
    "Key Differences in Constraint Regions\n",
    "- Ridge: Constraint region is a hypersphere → smooth shrinkage of coefficients.\n",
    "- Lasso: Constraint region is a diamond → Encourages zero coefficients (feature selection).\n",
    "\n",
    "Conclusion\n",
    "- Both Lasso and Ridge can be formulated either in penalized form or constraint-based form.\n",
    "- Lasso's constraint is diamond-shaped, making it more likely to force coefficients to zero.\n",
    "- Ridge's constraint is spherical, meaning it only shrinks coefficients but does not remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd6a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 2: Generate Synthetic Data\n",
    "# Generate synthetic dataset\n",
    "np.random.seed(42)\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=10)\n",
    "\n",
    "# Standardize the features (important for Ridge/Lasso)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Train Ridge and Lasso for Different λ Values\n",
    "# Define different lambda values\n",
    "lambdas = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "ridge_coefs = []\n",
    "lasso_coefs = []\n",
    "\n",
    "for alpha in lambdas:\n",
    "    # Train Ridge Regression\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_scaled, y)\n",
    "    ridge_coefs.append(ridge.coef_)\n",
    "    \n",
    "    # Train Lasso Regression\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X_scaled, y)\n",
    "    lasso_coefs.append(lasso.coef_)\n",
    "\n",
    "# Step 4: Visualize the Effect of Regularization\n",
    "ridge_coefs = np.array(ridge_coefs)\n",
    "lasso_coefs = np.array(lasso_coefs)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Ridge Coefficients\n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(X.shape[1]):\n",
    "    plt.plot(lambdas, ridge_coefs[:, i], marker='o', label=f'Feature {i+1}')\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Lambda (α)\")\n",
    "plt.ylabel(\"Coefficient Value\")\n",
    "plt.title(\"Ridge Regression: Effect of Regularization\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot Lasso Coefficients\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(X.shape[1]):\n",
    "    plt.plot(lambdas, lasso_coefs[:, i], marker='o', label=f'Feature {i+1}')\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Lambda (α)\")\n",
    "plt.ylabel(\"Coefficient Value\")\n",
    "plt.title(\"Lasso Regression: Effect of Regularization\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bada5c",
   "metadata": {},
   "source": [
    "Expected Output\n",
    "\n",
    "Ridge Regression Plot:\n",
    "\n",
    "✅ Coefficients shrink smoothly as λ increases.\n",
    "\n",
    "✅ No coefficients are set to zero → Ridge does not perform feature selection.\n",
    "\n",
    "Lasso Regression Plot:\n",
    "\n",
    "✅ Coefficients shrink and some become exactly zero for larger λ.\n",
    "\n",
    "✅ Feature selection occurs → Some coefficients are eliminated completely.\n",
    "\n",
    "_______________\n",
    "\n",
    "##### **Graphical Interpretation of Lasso and Ridge Regression (Two Features Case)**\n",
    "When dealing with two features ($𝑋_1, 𝑋_2$), we can visualize how Lasso (L1) and Ridge (L2) regularization constrain the coefficient estimates in a 2D space.\n",
    "\n",
    "Understanding the Constraint Regions\n",
    "\n",
    "Both Ridge and Lasso apply constraints on the sum of the regression coefficients, but they do so differently:\n",
    "\n",
    "| Method | Constraint Shape  | Mathematical Constraint     | Effect on Coefficients  | When to Use?  |\n",
    "|---------------|--------|----------------------------------| -----------------------|  -----------------------| \n",
    "|Ridge Regression (L2)\t|Circular region (ellipsoid) |\t$𝛽_1^2 + 𝛽_2^2 \\leq 𝑡$ | Shrinks coefficients smoothly, but none become exactly zero |If all predictors are important |\n",
    "|Lasso Regression (L1)\t|Diamond-shaped region  (L1 ball) |$∣𝛽_1∣ + ∣𝛽_2∣ \\leq 𝑡$  | performs feature selection by setting some coefficients exactly to zero. | If some predictors are irrelevant |\n",
    "\n",
    "Graphical Explanation\n",
    "\n",
    "**(A) Ridge Regression (L2)**\n",
    "- The constraint region is a circle (or ellipse in higher dimensions).\n",
    "- The OLS solution ($𝛽_1, 𝛽_2$) is found without constraints, but Ridge forces the solution to be inside the circle.\n",
    "- Since the contour lines of the Least Squares Loss function are ellipses, the Ridge estimate shrinks the coefficients but does not set them exactly to zero.\n",
    "\n",
    "📌 Key Observation: Ridge does not perform feature selection but shrinks coefficients smoothly.\n",
    "\n",
    "**(B) Lasso Regression (L1)**\n",
    "- The constraint region is a diamond (formed by the equation $∣𝛽_1∣ + ∣𝛽_2∣ \\leq 𝑡$).\n",
    "- The Least Squares Loss function has elliptical contours, and the solution is found where these ellipses first touch the diamond.\n",
    "- Because the diamond has sharp corners, the solution often lands on an axis, meaning one of the coefficients is exactly zero.\n",
    "\n",
    "📌 Key Observation: Lasso performs feature selection by setting some coefficients exactly to zero.\n",
    "\n",
    "Visual Representation\n",
    "\n",
    "Imagine the following plot where:\n",
    "- ✅ The blue region represents the constraint area (circle for Ridge, diamond for Lasso).\n",
    "- ✅ The red ellipses represent the contours of the Least Squares Loss function.\n",
    "- ✅ The black dot represents the OLS solution (unconstrained).\n",
    "- ✅ The intersection of ellipses and constraint region represents the Ridge/Lasso solution.\n",
    "\n",
    "In Ridge Regression (left):\n",
    "- The solution is inside the circle, meaning coefficients shrink but are not exactly zero.\n",
    "- The blue circular constraint shows how Ridge shrinks coefficients while keeping them nonzero.\n",
    "\n",
    "In Lasso Regression (right):\n",
    "- The solution often lands on an axis, meaning one coefficient becomes exactly zero.\n",
    "- The red diamond-shaped constraint leads to sparsity, setting some coefficients exactly to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aef9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a grid of beta1 and beta2 values\n",
    "beta1 = np.linspace(-1, 1, 400)\n",
    "beta2 = np.linspace(-1, 1, 400)\n",
    "B1, B2 = np.meshgrid(beta1, beta2)\n",
    "\n",
    "# Compute the constraint regions\n",
    "ridge_constraint = B1**2 + B2**2  # L2 norm (circle)\n",
    "lasso_constraint = np.abs(B1) + np.abs(B2)  # L1 norm (diamond)\n",
    "\n",
    "# Set levels for contour plot\n",
    "ridge_level = [0.5]  # Circle boundary (Ridge)\n",
    "lasso_level = [0.5]  # Diamond boundary (Lasso)\n",
    "\n",
    "# Plot Ridge Regression Constraint (L2 norm)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contour(B1, B2, ridge_constraint, levels=ridge_level, colors='blue', linewidths=2)\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "plt.axvline(0, color='gray', linestyle='--', linewidth=1)\n",
    "plt.title(\"Ridge Regression Constraint (L2 Norm)\")\n",
    "plt.xlabel(r\"$\\beta_1$\")\n",
    "plt.ylabel(r\"$\\beta_2$\")\n",
    "\n",
    "# Plot Lasso Regression Constraint (L1 norm)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.contour(B1, B2, lasso_constraint, levels=lasso_level, colors='red', linewidths=2)\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "plt.axvline(0, color='gray', linestyle='--', linewidth=1)\n",
    "plt.title(\"Lasso Regression Constraint (L1 Norm)\")\n",
    "plt.xlabel(r\"$\\beta_1$\")\n",
    "plt.ylabel(r\"$\\beta_2$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782a1c0",
   "metadata": {},
   "source": [
    "##### **Geometric Shape of Constraints for Lasso and Ridge Regression in Two or More Dimensions**\n",
    "\n",
    "When we generalize Lasso (L1) and Ridge (L2) regression constraints to two or more dimensions, their geometric shapes define how they penalize the regression coefficients.\n",
    "\n",
    "**Ridge Regression (L2) Constraint: A Hypersphere (Ellipsoid in Some Cases)**\n",
    "\n",
    "Mathematical Constraint:\n",
    "$$\\sum^P_{j = 1} \\beta_j^2 \\leq t$$\n",
    "- In 2D ( $𝛽_1, 𝛽_2$), this equation represents a circle.\n",
    "- In 3D ($𝛽_1, 𝛽_2, 𝛽_3$), it forms a sphere.\n",
    "- In higher dimensions, it generalizes to a hypersphere or an ellipsoid if the predictors are scaled differently.\n",
    "\n",
    "📌 Effect:\n",
    "- The constraint shrinks the regression coefficients toward zero, but they rarely become exactly zero.\n",
    "- The solution is found at the intersection of the least-squares loss contours (ellipses) and the Ridge constraint (circle or hypersphere).\n",
    "\n",
    "**Lasso Regression (L1) Constraint: A Hypercube (Diamond or Cross-Polytope in Higher Dimensions)**\n",
    "\n",
    "Mathematical Constraint:\n",
    "$$\\sum^P_{j = 1} |\\beta_j| \\leq t$$\n",
    "- In 2D ( $𝛽_1, 𝛽_2$), this equation represents a diamond (rhombus).\n",
    "- In 3D ($𝛽_1, 𝛽_2, 𝛽_3$), it forms an octahedron.\n",
    "- In higher dimensions, it generalizes to a cross-polytope (generalized diamond shape).\n",
    "\n",
    "📌 Effect:\n",
    "- The constraint has sharp edges and corners, where the least-squares loss contours are likely to touch.\n",
    "- This property forces many coefficients to become exactly zero, leading to feature selection.\n",
    "\n",
    "Visual Comparison of Shapes in Different Dimensions\n",
    "\n",
    "| Dimenensions | Ridge Regression  | Lasso Regression  | \n",
    "|---------------|--------|----------------------------------|   \n",
    "|2D| Circle |  Diamond (Rhombus)| \n",
    "|3D| Sphere | Octahedron | \n",
    "|4D+ | Hypersphere (Ellipsoid) | Cross-Polytope |\n",
    "\n",
    "Intuition\n",
    "- Ridge Regression (L2) constraint is a smooth shape (circle, sphere, hypersphere), leading to shrinkage but not sparsity (coefficients are small but nonzero).\n",
    "- Lasso Regression (L1) constraint has sharp edges and corners, making it easier for the optimal solution to land on an axis, leading to sparse solutions (some coefficients exactly zero).\n",
    "\n",
    "TypeError: Input z must be 2D, not 3D Problem\n",
    "- In two or more dimensions, the constraints imposed by Ridge and Lasso regression have distinct geometric shapes:\n",
    "    - Ridge Regression (L2 Norm) - Spherical Constraint:\n",
    "        - Ridge regression adds an L2 penalty, which constrains the sum of squared coefficients:\n",
    "        $$\\sum^P_{j = 1} \\beta_j^2 \\leq t$$\n",
    "        - In 2D ( $𝛽_1, 𝛽_2$), this forms a circular constraint (a Euclidean ball).\n",
    "        - In higher dimensions, this generalizes to a hypersphere.\n",
    "        - The coefficients shrink toward zero, but they rarely become exactly zero.\n",
    "    - Lasso Regression (L1 Norm) - Diamond (Octahedral) Constraint:\n",
    "        - Lasso regression adds an L1 penalty, constraining the sum of absolute values of the coefficients:\n",
    "        $$\\sum^P_{j = 1} |\\beta_j| \\leq t$$\n",
    "        - In two dimensions, this forms a diamond-shaped constraint (or rhombus).\n",
    "        - In three dimensions, it takes the shape of an octahedron.\n",
    "        - Because the L1 norm creates sharp corners, solutions tend to lie exactly on the axes, forcing some coefficients to be exactly zero (feature selection).\n",
    "\n",
    "Geometric Interpretation of Solution Paths\n",
    "- The optimal regression solution is found at the point where the constraint region intersects with the contours of the least squares loss function (which are ellipses in the case of ordinary least squares).\n",
    "- Ridge regression solutions tend to shrink coefficients but keep them nonzero since the constraint is smooth and rounded.\n",
    "- Lasso regression solutions often set some coefficients exactly to zero because the diamond-shaped constraint encourages sparse solutions.\n",
    "\n",
    "##### This geometric difference explains why Lasso can perform feature selection while Ridge cannot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95062b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create a 3D grid of beta1, beta2, and beta3 values\n",
    "beta_range = np.linspace(-1, 1, 100)\n",
    "B1, B2, B3 = np.meshgrid(beta_range, beta_range, beta_range)\n",
    "\n",
    "# Ridge (L2 norm) constraint equation for a sphere: β1^2 + β2^2 + β3^2 = t\n",
    "ridge_constraint = B1**2 + B2**2 + B3**2\n",
    "\n",
    "# Lasso (L1 norm) constraint equation for an octahedron: |β1| + |β2| + |β3| = t\n",
    "lasso_constraint = np.abs(B1) + np.abs(B2) + np.abs(B3)\n",
    "\n",
    "# Define the constraint level for visualization\n",
    "constraint_level = 0.5\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Ridge Regression Constraint (L2 Norm - Sphere)\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.contourf(B1, B2, B3, ridge_constraint, levels=[constraint_level], colors=['blue'], alpha=0.5)\n",
    "ax1.set_title(\"Ridge Regression Constraint (L2 Norm - Sphere)\")\n",
    "ax1.set_xlabel(r\"$\\beta_1$\")\n",
    "ax1.set_ylabel(r\"$\\beta_2$\")\n",
    "ax1.set_zlabel(r\"$\\beta_3$\")\n",
    "\n",
    "# Plot Lasso Regression Constraint (L1 Norm - Octahedron)\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.contourf(B1, B2, B3, lasso_constraint, levels=[constraint_level], colors=['red'], alpha=0.5)\n",
    "ax2.set_title(\"Lasso Regression Constraint (L1 Norm - Octahedron)\")\n",
    "ax2.set_xlabel(r\"$\\beta_1$\")\n",
    "ax2.set_ylabel(r\"$\\beta_2$\")\n",
    "ax2.set_zlabel(r\"$\\beta_3$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#TypeError: Input z must be 2D, not 3D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e610d02b",
   "metadata": {},
   "source": [
    "##### **Impact of Budget Size on Estimating Lasso and Ridge Regression**\n",
    "The budget size in Ridge and Lasso regression refers to the constraint placed on the sum of squared or absolute values of the coefficients. This budget determines how much shrinkage (regularization) is applied to the regression coefficients.\n",
    "\n",
    "**Budget in Ridge Regression**\n",
    "\n",
    "Mathematical Formulation\n",
    "- Ridge regression minimizes the sum of squared residuals subject to a constraint on the sum of squared coefficients:\n",
    "$$\\sum^P_{j = 1} \\beta_j^2 \\leq t$$\n",
    "- where t is the budget that limits the size of the coefficients.\n",
    "\n",
    "Effect of Budget Size ( t )\n",
    "- Large Budget (Weak Regularization):\n",
    "    - Ridge behaves similarly to Ordinary Least Squares (OLS).\n",
    "    - Coefficients remain large, and overfitting risk increases.\n",
    "- Small Budget (Strong Regularization):\n",
    "    - Coefficients shrink towards zero but never become exactly zero.\n",
    "    - Model has less variance but may introduce bias.\n",
    "\n",
    "✅ Key Insight: In Ridge, a small budget forces all coefficients to be small, but none of them become exactly zero.\n",
    "\n",
    "**Budget in Lasso Regression**\n",
    "\n",
    "Mathematical Formulation\n",
    "- Lasso regression minimizes the sum of squared residuals subject to a constraint on the sum of absolute values of coefficients:\n",
    "$$\\sum^P_{j = 1} |\\beta_j| \\leq t$$\n",
    "- where t is the budget that limits the sum of absolute values of the coefficients.\n",
    "\n",
    "Effect of Budget Size ( t )\n",
    "- Large Budget (Weak Regularization):\n",
    "    - Lasso behaves similarly to OLS.\n",
    "    - Many coefficients remain large and nonzero.\n",
    "- Small Budget (Strong Regularization):\n",
    "    - Many coefficients shrink to exactly zero → Feature selection happens.\n",
    "    - The model selects only the most relevant predictors, reducing dimensionality.\n",
    "    \n",
    "✅ Key Insight: In Lasso, a small budget forces many coefficients to be exactly zero, performing automatic feature selection.\n",
    "\n",
    "| Aspect | Ridge Regression (L2)  | Lasso Regression (L1)  |\n",
    "|---------------|--------|----------------------------------|  \n",
    "|Effect of Large Budget (Small λ)\t|Similar to OLS, coefficients remain large |Similar to OLS, all coefficients remain nonzero|\n",
    "|Effect of Small Budget (Large λ)\t|Coefficients shrink but never reach zero |Many coefficients shrink to exactly zero (feature selection) |\n",
    "|Best for  |When all predictors are relevant|When some predictors are irrelevant | \n",
    "\n",
    " Conclusion\n",
    "- Ridge regression is better when all variables matter, as it shrinks but does not eliminate coefficients.\n",
    "- Lasso regression is better when we suspect some variables are unimportant, as it can set coefficients to exactly zero, performing feature selection.\n",
    "- Budget size (λ) controls how much shrinkage is applied:\n",
    "    - Small λ (Large Budget) → Model behaves like OLS (no regularization).\n",
    "    - Large λ (Small Budget) → Ridge keeps all coefficients small, Lasso sets some to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddf3bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic dataset\n",
    "np.random.seed(42)\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=10)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define budget (alpha values)\n",
    "budgets = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "ridge_coefs = []\n",
    "lasso_coefs = []\n",
    "\n",
    "for alpha in budgets:\n",
    "    # Train Ridge and Lasso with different budgets (alpha)\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_scaled, y)\n",
    "    ridge_coefs.append(ridge.coef_)\n",
    "    \n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X_scaled, y)\n",
    "    lasso_coefs.append(lasso.coef_)\n",
    "\n",
    "# Convert to numpy arrays for plotting\n",
    "ridge_coefs = np.array(ridge_coefs)\n",
    "lasso_coefs = np.array(lasso_coefs)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Ridge Coefficients\n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(X.shape[1]):\n",
    "    plt.plot(budgets, ridge_coefs[:, i], marker='o', label=f'Feature {i+1}')\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Budget (Alpha)\")\n",
    "plt.ylabel(\"Coefficient Value\")\n",
    "plt.title(\"Effect of Budget on Ridge Regression\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot Lasso Coefficients\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(X.shape[1]):\n",
    "    plt.plot(budgets, lasso_coefs[:, i], marker='o', label=f'Feature {i+1}')\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Budget (Alpha)\")\n",
    "plt.ylabel(\"Coefficient Value\")\n",
    "plt.title(\"Effect of Budget on Lasso Regression\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ebe8a",
   "metadata": {},
   "source": [
    "#### Losso regression in Sine / polynomial problem as below under GLMs\n",
    "\n",
    "LASSO stands for Least Absolute Shrinkage and Selection Operator.\n",
    "\n",
    "2 keywords here – \n",
    "- absolute and\n",
    "- selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dd4f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "def lasso_regression(data, predictors, alpha, models_to_plot={}):\n",
    "    #Fit the model\n",
    "    lassoreg = Lasso(alpha=alpha,normalize=True, max_iter=1e5)\n",
    "    lassoreg.fit(data[predictors],data['y'])\n",
    "    y_pred = lassoreg.predict(data[predictors])\n",
    "    \n",
    "    #Check if a plot is to be made for the entered alpha\n",
    "    if alpha in models_to_plot:\n",
    "        plt.subplot(models_to_plot[alpha])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['x'],y_pred)\n",
    "        plt.plot(data['x'],data['y'],'.')\n",
    "        plt.title('Plot for alpha: %.3g'%alpha)\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    ret.extend([lassoreg.intercept_])\n",
    "    ret.extend(lassoreg.coef_)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6776e3b6",
   "metadata": {},
   "source": [
    "Additional parameters defined in the Lasso function – \n",
    "- max_iter.\n",
    "    - This is the maximum number of iterations for which we want the model to run if it doesn’t converge before. \n",
    "    - This exists for Ridge as well, but setting this to a higher than default value was required in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize predictors to all 15 powers of x\n",
    "predictors=['x']\n",
    "predictors.extend(['x_%d'%i for i in range(2,16)])\n",
    "\n",
    "#Define the alpha values to test\n",
    "alpha_lasso = [1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 1, 5, 10]\n",
    "\n",
    "#Initialize the dataframe to store coefficients\n",
    "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
    "ind = ['alpha_%.2g'%alpha_lasso[i] for i in range(0,10)]\n",
    "coef_matrix_lasso = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "#Define the models to plot\n",
    "models_to_plot = {1e-10:231, 1e-5:232,1e-4:233, 1e-3:234, 1e-2:235, 1:236}\n",
    "\n",
    "#Iterate over the 10 alpha values:\n",
    "for i in range(10):\n",
    "    coef_matrix_lasso.iloc[i,] = lasso_regression(data, predictors, alpha_lasso[i], models_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1eaa9",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "Model complexity decreases with an increase in the values of alpha. But notice the straight line at alpha=1.\n",
    "\n",
    "Expected inference: \n",
    "- higher RSS for higher alphas\n",
    "- For the same values of alpha, the coefficients of lasso regression are much smaller than that of ridge regression (compare row 1 of the 2 tables).\n",
    "- For the same alpha, lasso has higher RSS (poorer fit) as compared to ridge regression.\n",
    "- Many of the coefficients are zero, even for very small values of alpha.\n",
    "\n",
    "Check the number of coefficients that are zero in each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_matrix_lasso.apply(lambda x: sum(x.values==0),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09446f8",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- small value of alpha, a significant number of coefficients are zero. \n",
    "- This also explains the horizontal line fit for alpha=1 in the lasso plots; it’s just a baseline model! \n",
    "This phenomenon, where most coefficients become zero, is called **sparsity**. \n",
    "- Although lasso performs feature selection, we achieve this level of sparsity only in special cases\n",
    "\n",
    "#### Mathematics behind why coefficients are zero in the case of lasso but not ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LINEAR, RIDGE AND LASSO REGRESSION\n",
    "'''\n",
    "# importing requuired libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "# read test and train file\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "print('\\n\\n---------DATA---------------\\n\\n')\n",
    "print(train.head())\n",
    "\n",
    "#splitting into training and test\n",
    "## try building model with the different features and compare the result.\n",
    "X = train.loc[:,['Outlet_Establishment_Year','Item_MRP']]\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(X,train.Item_Outlet_Sales,random_state=5)\n",
    "\n",
    "print('--------Trainig Linear Regression Model---------------')\n",
    "lreg = LinearRegression()\n",
    "#training the model\n",
    "lreg.fit(x_train,y_train)\n",
    "\n",
    "#predicting on cv\n",
    "pred = lreg.predict(x_cv)\n",
    "\n",
    "#calculating mse\n",
    "mse = np.mean((pred - y_cv)**2)\n",
    "print('\\nMean Sqaured Error = ',mse )\n",
    "\n",
    "#Let us take a look at the coefficients of this linear regression model.\n",
    "# calculating coefficients\n",
    "coeff = DataFrame(x_train.columns)\n",
    "\n",
    "coeff['Coefficient Estimate'] = Series(lreg.coef_)\n",
    "\n",
    "print(coeff)\n",
    "\n",
    "print('\\n\\nModel performance on Test data = ')\n",
    "print(lreg.score(x_cv,y_cv))\n",
    "\n",
    "print('\\n\\n---------Training Ridge Regression Model----------------')\n",
    "\n",
    "ridge = Ridge()\n",
    "ridge.fit(x_train,y_train)\n",
    "pred1 = ridge.predict(x_cv)\n",
    "mse_1 = np.mean((pred1-y_cv)**2)\n",
    "\n",
    "print('\\n\\nMean Squared Error = ',mse_1)\n",
    "\n",
    "# calculating coefficients\n",
    "coeff = DataFrame(x_train.columns)\n",
    "coeff['Coefficient Estimate'] = Series(ridge.coef_)\n",
    "print(coeff)\n",
    "\n",
    "print('\\n\\nModel performance on Test data = ')\n",
    "print(ridge.score(x_cv,y_cv))\n",
    "\n",
    "\n",
    "print('\\n\\n---------Training Lasso Regression Model----------------')\n",
    "\n",
    "lasso = Lasso()\n",
    "lasso.fit(x_train,y_train)\n",
    "pred2 = lasso.predict(x_cv)\n",
    "mse_2 = np.mean((pred2-y_cv)**2)\n",
    "\n",
    "print('\\n\\nMean Squared Error = ',mse_2)\n",
    "\n",
    "# calculating coefficients\n",
    "coeff = DataFrame(x_train.columns)\n",
    "coeff['Coefficient Estimate'] = Series(lasso.coef_)\n",
    "print(coeff)\n",
    "\n",
    "print('\\n\\nModel performance on Test data = ')\n",
    "print(lasso.score(x_cv,y_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b66fb0",
   "metadata": {},
   "source": [
    "### Comparison of Ridge, Lasso, and Elastic Net Regression in Python\n",
    "Now, let's compare Ridge Regression, Lasso Regression, and Elastic Net Regression to see how they perform under different conditions.\n",
    "\n",
    "**What is Elastic Net Regression?**\n",
    "- Elastic Net is a combination of Ridge and Lasso, controlled by two parameters:\n",
    "    - $\\Lambda$: Controls the overall penalty strength.\n",
    "    - p (l1_ratio): \n",
    "        - p = 1: Pure Lasso\n",
    "        - p = 0: Pure Ridge\n",
    "        - 0 < p < 1: a Mix of both Ridge and Lasso\n",
    "\n",
    "Advantages of Elastic Net:\n",
    "\n",
    "✅ Works well when there are many correlated features (like Ridge).\n",
    "\n",
    "✅ Performs automatic feature selection (like Lasso).\n",
    "\n",
    "✅ Balances the trade-off between Ridge and Lasso.\n",
    "\n",
    "Observations\n",
    "\n",
    "📌 Ridge Regression\n",
    "- Keeps all variables but shrinks them.\n",
    "- Good for handling multicollinearity but does not perform feature selection.\n",
    "\n",
    "📌 Lasso Regression\n",
    "- Eliminates some variables (feature selection).\n",
    "- May drop too many variables when predictors are correlated.\n",
    "\n",
    "📌 Elastic Net Regression\n",
    "- Balances Ridge and Lasso:\n",
    "    - Shrinks some coefficients (like Ridge).\n",
    "    - Sets others to zero (like Lasso).\n",
    "- Has lower RMSE than both Ridge and Lasso → Best overall performance.\n",
    "\n",
    "When to Use Each Model?\n",
    "- Ridge\n",
    "    - All predictors contribute equally\n",
    "    - Best when all variables matter and are correlated.\n",
    "- Lasso\n",
    "    - Many irrelevant features present\n",
    "    - Best when some variables should be removed.\n",
    "- Elastic Net\n",
    "    - Features are correlated & some should be removed\n",
    "    - Best balanced approach when features are correlated but feature selection is also needed.\n",
    "\n",
    "##### To Compare Ridge, Lasso, and Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70701be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Define models\n",
    "ridge = Ridge(alpha=1)\n",
    "lasso = Lasso(alpha=0.1)\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # Mix of Ridge & Lasso\n",
    "\n",
    "# Fit models\n",
    "ridge.fit(X_train, y_train)\n",
    "lasso.fit(X_train, y_train)\n",
    "elastic_net.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "y_pred_elastic = elastic_net.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "lasso_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "elastic_rmse = np.sqrt(mean_squared_error(y_test, y_pred_elastic))\n",
    "\n",
    "# Print coefficients and RMSE\n",
    "print(\"Ridge Regression Coefficients:\", ridge.coef_)\n",
    "print(\"Ridge Regression RMSE:\", ridge_rmse)\n",
    "\n",
    "print(\"Lasso Regression Coefficients:\", lasso.coef_)\n",
    "print(\"Lasso Regression RMSE:\", lasso_rmse)\n",
    "\n",
    "print(\"Elastic Net Regression Coefficients:\", elastic_net.coef_)\n",
    "print(\"Elastic Net Regression RMSE:\", elastic_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f9ff0a",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning for Elastic Net using GridSearchCV\n",
    "To get the best performance from Elastic Net, we need to tune its two main hyperparameters:\n",
    "- alpha (λ): Controls the overall regularization strength.\n",
    "- l1_ratio (ρ): Determines the mix of Lasso (L1) and Ridge (L2) penalties.\n",
    "\n",
    "We will use GridSearchCV to find the best combination of these parameters.\n",
    "\n",
    "Explanation of the Code\n",
    "- ✅ Step 1: Define a grid of possible values for alpha (regularization) and l1_ratio (mix of Ridge & Lasso).\n",
    "- ✅ Step 2: Use GridSearchCV to find the best combination using cross-validation (cv=5).\n",
    "- ✅ Step 3: Train the model on X_train and identify the best parameters.\n",
    "- ✅ Step 4: Use the best model to predict on X_test.\n",
    "- ✅ Step 5: Compute RMSE to evaluate performance.\n",
    "\n",
    "Output:\n",
    "- The best alpha and l1_ratio values are chosen based on lowest RMSE.\n",
    "- The optimized model performs better than default Elastic Net.\n",
    "\n",
    "When Should You Tune Hyperparameters?\n",
    "- 📌 When the dataset has many features and we need to balance Ridge & Lasso.\n",
    "- 📌 When we want to avoid overfitting (too little regularization) or underfitting (too much regularization).\n",
    "- 📌 When we have high multicollinearity and some variables should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15afa037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]  # Ratio of Lasso vs Ridge\n",
    "}\n",
    "\n",
    "# Define Elastic Net model\n",
    "elastic_net = ElasticNet()\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(elastic_net, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predictions using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Print the results\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Elastic Net RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e169b65",
   "metadata": {},
   "source": [
    "##### **When Lasso Performs Better Than Ridge Regression**\n",
    "Lasso regression is expected to outperform Ridge regression when:\n",
    "\n",
    "Only a Few Predictors Are Important (Sparsity Exists)\n",
    "- If the true underlying model has only a few nonzero coefficients (i.e., most features are irrelevant), Lasso is better because it can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
    "- Example: Predicting house prices where only a few key variables (e.g., square footage, location) matter, while others (e.g., paint color) do not.\n",
    "\n",
    "High Dimensionality (More Features Than Observations, p>n)\n",
    "- When the number of features exceeds the number of observations, Lasso helps reduce dimensionality by selecting a subset of relevant features.\n",
    "- Example: In genetics, where thousands of genes (p) are analyzed with limited patient samples (n).\n",
    "\n",
    "Interpretable Models Are Needed\n",
    "- Because Lasso sets some coefficients to zero, the final model contains only important predictors, making it more interpretable.\n",
    "\n",
    "High Correlation Among Predictors (But Only One or Few Are Truly Relevant)\n",
    "- If some predictors are highly correlated, Lasso will select only one and shrink the others to zero, simplifying the model.\n",
    "\n",
    "##### **When Ridge Regression Performs Better Than Lasso**\n",
    "Ridge regression is expected to perform better than Lasso when:\n",
    "\n",
    "Many Small or Moderately Important Predictors Exist (No True Sparsity)\n",
    "- If the true model has many predictors with nonzero coefficients, Ridge is preferred because it shrinks all coefficients continuously rather than eliminating some.\n",
    "- Example: In macroeconomic forecasting, where multiple variables (e.g., inflation, interest rates, unemployment) contribute jointly to the outcome.\n",
    "\n",
    "Multicollinearity Exists (Highly Correlated Predictors)\n",
    "- Ridge handles multicollinearity better by shrinking correlated predictors rather than eliminating them.\n",
    "- Unlike Lasso, which might arbitrarily drop some correlated predictors, Ridge retains them with smaller weights.\n",
    "\n",
    "Better Prediction Accuracy Rather Than Feature Selection\n",
    "- If the goal is prediction accuracy rather than interpretability, Ridge often performs better since it retains all features, preventing the loss of useful information.\n",
    "\n",
    "Small Sample Size, Many Weak Predictors\n",
    "- When there are many weak predictors (all contributing a little), Ridge works better because it preserves all features rather than eliminating some.\n",
    "\n",
    "Conclusion\n",
    "- Use Lasso when you expect sparsity (i.e., only a few important predictors).\n",
    "- Use Ridge when all predictors contribute meaningfully, and multicollinearity is a concern.\n",
    "- If unsure, Elastic Net (a mix of Ridge and Lasso) can be a good compromise.\n",
    "\n",
    "_______________________\n",
    "\n",
    "##### **Relationship Between Best Subset Selection, Lasso, and Ridge Regression**\n",
    "\n",
    "Best Subset Selection\n",
    "- Best subset selection finds the best subset of predictors by evaluating all possible combinations of predictors and choosing the one that minimizes a given criterion (e.g., least squares error).\n",
    "- This approach is computationally expensive, especially when there are many predictors, because it requires evaluating $2_𝑝$ possible subsets for p predictors.\n",
    "- It performs hard thresholding, meaning variables are either included or excluded, with no in-between shrinkage.\n",
    "\n",
    "**Lasso Regression vs. Best Subset Selection**\n",
    "\n",
    "Similarities:\n",
    "- Both perform variable selection, meaning they reduce the number of predictors in the final model.\n",
    "- Lasso can be seen as a continuous relaxation of best subset selection since it automatically selects variables by shrinking some coefficients to exactly zero.\n",
    "\n",
    "Differences:\n",
    "- Computational Efficiency: Lasso is computationally more efficient than best subset selection, which becomes infeasible for large 𝑝.\n",
    "- Bias-Variance Tradeoff: Lasso introduces some bias by shrinking coefficients, whereas best subset selection leads to models with lower bias but higher variance due to overfitting.\n",
    "- Feature Selection Method: Lasso selects variables continuously (soft thresholding), while best subset selection is discrete (hard thresholding).\n",
    "\n",
    "**Ridge Regression vs. Best Subset Selection**\n",
    "\n",
    "Similarities:\n",
    "- Both aim to improve prediction accuracy compared to standard least squares regression by reducing overfitting.\n",
    "\n",
    "Differences:\n",
    "- Feature Selection: Ridge regression does not perform variable selection—it shrinks coefficients continuously but never sets them to exactly zero. In contrast, best subset selection results in a sparse model.\n",
    "- Handling Multicollinearity: Ridge is better suited for multicollinearity since it distributes the coefficient values among correlated predictors rather than selecting only one, as best subset selection does.\n",
    "\n",
    "Key Takeaways\n",
    "- Best subset selection is ideal when p is small but computationally expensive when p is large.\n",
    "- Lasso is a computationally feasible alternative that performs continuous feature selection.\n",
    "- Ridge does not select variables but is better suited for situations where all predictors contribute meaningfully to the response.\n",
    "- Elastic Net (a hybrid of Ridge and Lasso) can combine the benefits of both, balancing selection and shrinkage.\n",
    "\n",
    "__________________\n",
    "\n",
    "##### **Rule Governing the Selection of the Tuning Parameter for Lasso and Ridge Regression**\n",
    "\n",
    "The tuning parameter (λ) in Lasso and Ridge regression controls the strength of regularization and is selected to optimize the bias-variance tradeoff. \n",
    "- The rule for selecting λ follows these principles:\n",
    "\n",
    "**Selecting λ Using Cross-Validation**\n",
    "- The most common approach for tuning λ is k-fold cross-validation:\n",
    "    - Split the data into k subsets (folds).\n",
    "    - Train the model on 𝑘−1 folds and validate on the remaining fold.\n",
    "    - Repeat the process for different values of λ and choose the one that minimizes prediction error, e.g.:\n",
    "        - Mean Squared Error for Ridge, \n",
    "        - Mean Absolute Error for Lasso.\n",
    "    - The optimal λ balances model complexity and predictive accuracy.\n",
    "\n",
    "**Effect of λ in Lasso and Ridge Regression**\n",
    "- (a) Ridge Regression (𝐿2 Regularization)\n",
    "    - Small 𝜆 → Similar to Ordinary Least Squares (OLS)\n",
    "        - Minimal shrinkage, model may overfit.\n",
    "    - Large 𝜆 → Stronger Shrinkage\n",
    "        - Coefficients shrink but remain nonzero.\n",
    "        - Reduces multicollinearity and prevents overfitting but may introduce bias.\n",
    "    - Optimal λ: Typically selected to minimize cross-validated prediction error.\n",
    "\n",
    "- (b) Lasso Regression (𝐿1 Regularization)\n",
    "    - Small 𝜆 → Model Similar to OLS\n",
    "        - Minimal shrinkage, overfitting risk.\n",
    "    - Moderate 𝜆 → Some Coefficients Shrink to Zero\n",
    "        - Acts as feature selection.\n",
    "    - Large 𝜆 → More Coefficients Become Zero\n",
    "        - Can underfit if too large, as it removes too many variables.\n",
    "    - Optimal λ Selection: Chosen using cross-validation to balance sparsity and predictive performance.\n",
    "\n",
    "**Common Methods for Selecting λ**\n",
    "- Cross-validation (CV): Most widely used method.\n",
    "- Grid Search: Trying different λ values systematically.\n",
    "- Generalized Cross-Validation (GCV): An approximation of leave-one-out CV, often used in Ridge regression.\n",
    "- Information Criteria (AIC, BIC): Used in some cases to select 𝜆 based on model fit and complexity.\n",
    "\n",
    "______________\n",
    "\n",
    "##### **Expected Values of Coefficients for Signal and Noise Variables in a Robust Regression Model**\n",
    "In a robust regression model (such as Ridge, Lasso, or Huber regression), the behavior of estimated coefficients for signal (true predictors) and noise (irrelevant predictors) depends on the regularization method and the tuning parameter 𝜆.\n",
    "\n",
    "Definitions\n",
    "- Signal Variables: Variables that have a real effect on the response variable (true predictors). Their coefficients should ideally be close to their true values.\n",
    "- Noise Variables: Variables that have no true relationship with the response. Their coefficients should ideally be shrunk toward zero to avoid overfitting.\n",
    "\n",
    "**Expected Behavior of Coefficients in Robust Regression**\n",
    "- (a) Ridge Regression (𝐿2 Regularization)\n",
    "    - Shrinks all coefficients proportionally but never sets them exactly to zero.\n",
    "    - Expected Values of Coefficients:\n",
    "        - Signal Variables: Shrunk toward zero, but retain most of their information.\n",
    "        - Noise Variables: Also shrunk toward zero, but not completely eliminated.\n",
    "    - Key Effect:\n",
    "        - Ridge keeps small contributions from all variables, making it less interpretable but good for multicollinearity.\n",
    "\n",
    "- (b) Lasso Regression (𝐿1 Regularization)\n",
    "    - Performs feature selection by setting some coefficients exactly to zero.\n",
    "    - Expected Values of Coefficients:\n",
    "        - Signal Variables: Estimated with some bias (due to shrinkage), but often closer to their true values than in Ridge.\n",
    "        - Noise Variables: Many coefficients are exactly zero, effectively removing unimportant predictors.\n",
    "    - Key Effect:\n",
    "        - Lasso provides a sparse model and improves interpretability by selecting only relevant features.\n",
    "\n",
    "- (c) Elastic Net (Combination of 𝐿1 and 𝐿2 Regularization)\n",
    "    - Balances Ridge and Lasso properties.\n",
    "    - Expected Values of Coefficients:  \n",
    "        - Signal Variables: Shrunk moderately (like Ridge) but some may be eliminated (like Lasso).\n",
    "        - Noise Variables: Some eliminated, others weakly shrunk.\n",
    "    - Key Effect:\n",
    "        - Works well when features are correlated, handling feature selection and regularization simultaneously.\n",
    "\n",
    "- (d) Huber Regression (Robust to Outliers)\n",
    "    - Reduces the impact of extreme outliers using a loss function that switches from squared loss to absolute loss for large residuals.\n",
    "    - Expected Values of Coefficients:\n",
    "        - Signal Variables: Less affected by outliers, resulting in a more stable estimate.\n",
    "        - Noise Variables: Naturally reduced in influence since extreme values are downweighted.\n",
    "    - Key Effect:\n",
    "        - Provides robustness against outliers while maintaining good prediction performance.\n",
    "\n",
    "| Model Type |Signal Variables | Noise Variables |\n",
    "|---------------|--------|----------------------------------| \n",
    "|Ridge Regression|Shrunk but nonzero |Shrunk but nonzero|\n",
    "|Lasso Regression|Shrunk, some zeroed |Many exactly zero |\n",
    "|Elastic Net |Shrunk, some zeroed|Some zeroed, some shrunk |\n",
    "|Huber Regression |Stable against outliers |Less influenced by extreme values |\n",
    "\n",
    "Takeaway\n",
    "- Ridge keeps all predictors but shrinks them.\n",
    "- Lasso eliminates noise variables by setting some coefficients to exactly zero.\n",
    "- Elastic Net combines both Ridge and Lasso properties, handling correlated predictors.\n",
    "- Huber regression provides robustness against outliers, reducing noise impact.\n",
    "\n",
    "##### Step-by-step Python example demonstrating how Ridge, Lasso, Elastic Net, and Huber regression handle signal and noise variables.\n",
    "- Generate synthetic data with true signal and noise variables.\n",
    "- Apply Ridge, Lasso, Elastic Net, and Huber regression.\n",
    "- Compare how each method estimates coefficients for signal and noise variables.\n",
    "\n",
    "Key Observations from the Output\n",
    "\n",
    "Ridge Regression:\n",
    "- Shrinks all coefficients but never sets them exactly to zero.\n",
    "- Retains small values for noise variables.\n",
    "\n",
    "Lasso Regression:\n",
    "- Many coefficients are exactly zero, removing irrelevant features.\n",
    "- Useful for feature selection.\n",
    "\n",
    "Elastic Net:\n",
    "- Balances Ridge and Lasso, keeping some variables while shrinking others.\n",
    "\n",
    "Huber Regression:\n",
    "- More stable against outliers, coefficients remain relatively close to the true values.\n",
    "\n",
    "Model Performance (MSE):\n",
    "- Ridge and Elastic Net usually perform better in highly correlated features.\n",
    "- Lasso performs better when there are truly sparse models (few relevant predictors).\n",
    "- Huber is useful when the dataset contains outliers.\n",
    "\n",
    "Final Thoughts\n",
    "- If the goal is prediction (not feature selection), Ridge or Elastic Net work well.\n",
    "- If the goal is feature selection, Lasso is the best choice.\n",
    "- For robustness against outliers, Huber regression is ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8514d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Step 2: Generate Data with Signal & Noise Variables\n",
    "# Generate 100 samples and 10 features\n",
    "n_samples, n_features = 100, 10\n",
    "\n",
    "# Create signal variables (true predictors)\n",
    "X_signal = np.random.randn(n_samples, 3)  # 3 true predictors\n",
    "beta_signal = np.array([3, -2, 1])  # True coefficients\n",
    "\n",
    "# Create noise variables (irrelevant predictors)\n",
    "X_noise = np.random.randn(n_samples, n_features - 3)  # 7 noise variables\n",
    "\n",
    "# Combine signal and noise variables\n",
    "X = np.hstack([X_signal, X_noise])\n",
    "\n",
    "# Generate response variable with some noise\n",
    "y = X_signal @ beta_signal + np.random.randn(n_samples) * 0.5  # Add some noise\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize predictors (important for regularization methods)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Train Ridge, Lasso, Elastic Net, and Huber Models\n",
    "# Define models\n",
    "ridge = Ridge(alpha=1.0)\n",
    "lasso = Lasso(alpha=0.1)\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # 50% Lasso, 50% Ridge\n",
    "huber = HuberRegressor(alpha=0.1)\n",
    "\n",
    "# Train models\n",
    "ridge.fit(X_train, y_train)\n",
    "lasso.fit(X_train, y_train)\n",
    "elastic_net.fit(X_train, y_train)\n",
    "huber.fit(X_train, y_train)\n",
    "\n",
    "# Get coefficients\n",
    "ridge_coefs = ridge.coef_\n",
    "lasso_coefs = lasso.coef_\n",
    "elastic_net_coefs = elastic_net.coef_\n",
    "huber_coefs = huber.coef_\n",
    "\n",
    "# Step 4: Compare Coefficients for Signal and Noise Variables\n",
    "# Create a dataframe to compare coefficient estimates\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': [f'X{i+1}' for i in range(n_features)],\n",
    "    'Ridge': ridge_coefs,\n",
    "    'Lasso': lasso_coefs,\n",
    "    'Elastic Net': elastic_net_coefs,\n",
    "    'Huber': huber_coefs\n",
    "})\n",
    "\n",
    "# Sort by absolute value of true signal importance\n",
    "coef_df['True Importance'] = np.concatenate((beta_signal, np.zeros(n_features - 3)))\n",
    "coef_df = coef_df.sort_values(by='True Importance', ascending=False, key=abs)\n",
    "\n",
    "print(coef_df)\n",
    "\n",
    "# Step 5: Evaluate Model Performance\n",
    "# Make predictions\n",
    "ridge_pred = ridge.predict(X_test)\n",
    "lasso_pred = lasso.predict(X_test)\n",
    "elastic_net_pred = elastic_net.predict(X_test)\n",
    "huber_pred = huber.predict(X_test)\n",
    "\n",
    "# Compute Mean Squared Error\n",
    "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
    "lasso_mse = mean_squared_error(y_test, lasso_pred)\n",
    "elastic_net_mse = mean_squared_error(y_test, elastic_net_pred)\n",
    "huber_mse = mean_squared_error(y_test, huber_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nModel Performance (MSE):\")\n",
    "print(f\"Ridge MSE: {ridge_mse:.4f}\")\n",
    "print(f\"Lasso MSE: {lasso_mse:.4f}\")\n",
    "print(f\"Elastic Net MSE: {elastic_net_mse:.4f}\")\n",
    "print(f\"Huber MSE: {huber_mse:.4f}\")\n",
    "\n",
    "# Step 6: Visualizing Coefficient Shrinkage\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot coefficient values for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "coef_df_melted = coef_df.melt(id_vars=['Feature', 'True Importance'], var_name='Model', value_name='Coefficient')\n",
    "\n",
    "sns.barplot(data=coef_df_melted, x='Feature', y='Coefficient', hue='Model', palette='coolwarm')\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Coefficient Estimates Across Models\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91210a5",
   "metadata": {},
   "source": [
    "##### Refine the analysis by tuning the hyperparameters for Ridge, Lasso, Elastic Net, and Huber regression using cross-validation. \n",
    "- This will help us find the best values for alpha (regularization strength) and l1_ratio (for Elastic Net).\n",
    "\n",
    "Key Improvements in This Version\n",
    "- Uses Cross-Validation: Ensures the best hyperparameter selection based on performance across multiple data splits.\n",
    "- Finds Optimal Alpha & L1 Ratio (Elastic Net): Reduces overfitting and improves generalization.\n",
    "- Final Model Training with Optimal Parameters: Ensures the best version of each model is used for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Required Libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Step 2: Define the Hyperparameter Grid for Each Model\n",
    "# Define hyperparameter grids\n",
    "ridge_params = {'alpha': np.logspace(-3, 3, 10)}  # 10 values from 0.001 to 1000\n",
    "lasso_params = {'alpha': np.logspace(-3, 3, 10)}\n",
    "elastic_net_params = {\n",
    "    'alpha': np.logspace(-3, 3, 10),\n",
    "    'l1_ratio': [0.1, 0.5, 0.9]  # Mix between Lasso and Ridge\n",
    "}\n",
    "huber_params = {'alpha': np.logspace(-3, 3, 10)}\n",
    "\n",
    "# Define models\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "elastic_net = ElasticNet()\n",
    "huber = HuberRegressor()\n",
    "\n",
    "# Step 3: Perform Grid Search with Cross-Validation\n",
    "# Perform Grid Search\n",
    "ridge_cv = GridSearchCV(ridge, ridge_params, cv=5, scoring='neg_mean_squared_error')\n",
    "lasso_cv = GridSearchCV(lasso, lasso_params, cv=5, scoring='neg_mean_squared_error')\n",
    "elastic_net_cv = GridSearchCV(elastic_net, elastic_net_params, cv=5, scoring='neg_mean_squared_error')\n",
    "huber_cv = GridSearchCV(huber, huber_params, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit models\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "elastic_net_cv.fit(X_train, y_train)\n",
    "huber_cv.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Get the Best Hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"Ridge Best Alpha: {ridge_cv.best_params_['alpha']}\")\n",
    "print(f\"Lasso Best Alpha: {lasso_cv.best_params_['alpha']}\")\n",
    "print(f\"Elastic Net Best Alpha: {elastic_net_cv.best_params_['alpha']}, Best L1 Ratio: {elastic_net_cv.best_params_['l1_ratio']}\")\n",
    "print(f\"Huber Best Alpha: {huber_cv.best_params_['alpha']}\")\n",
    "\n",
    "# Step 5: Train Final Models with Optimal Parameters\n",
    "# Train models with best parameters\n",
    "ridge_best = Ridge(alpha=ridge_cv.best_params_['alpha']).fit(X_train, y_train)\n",
    "lasso_best = Lasso(alpha=lasso_cv.best_params_['alpha']).fit(X_train, y_train)\n",
    "elastic_net_best = ElasticNet(alpha=elastic_net_cv.best_params_['alpha'], l1_ratio=elastic_net_cv.best_params_['l1_ratio']).fit(X_train, y_train)\n",
    "huber_best = HuberRegressor(alpha=huber_cv.best_params_['alpha']).fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate Performance\n",
    "# Make predictions\n",
    "ridge_pred = ridge_best.predict(X_test)\n",
    "lasso_pred = lasso_best.predict(X_test)\n",
    "elastic_net_pred = elastic_net_best.predict(X_test)\n",
    "huber_pred = huber_best.predict(X_test)\n",
    "\n",
    "# Compute Mean Squared Error\n",
    "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
    "lasso_mse = mean_squared_error(y_test, lasso_pred)\n",
    "elastic_net_mse = mean_squared_error(y_test, elastic_net_pred)\n",
    "huber_mse = mean_squared_error(y_test, huber_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nOptimized Model Performance (MSE):\")\n",
    "print(f\"Ridge MSE: {ridge_mse:.4f}\")\n",
    "print(f\"Lasso MSE: {lasso_mse:.4f}\")\n",
    "print(f\"Elastic Net MSE: {elastic_net_mse:.4f}\")\n",
    "print(f\"Huber MSE: {huber_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0927dc4",
   "metadata": {},
   "source": [
    "##### Feature Importance Analysis and Learning Curves for Ridge, Lasso, Elastic Net, and Huber Regression\n",
    "Extend our analysis by:\n",
    "- ✅ Identifying important features (especially for Lasso and Elastic Net)\n",
    "- ✅ Visualizing learning curves to check model performance over training iterations\n",
    "\n",
    "Insights from the Visualizations\n",
    "\n",
    "Feature Importance\n",
    "- Lasso and Elastic Net shrink many coefficients to zero (feature selection).\n",
    "- Ridge shrinks coefficients but does not eliminate any variables.\n",
    "\n",
    "Learning Curves\n",
    "- If training and validation errors converge, the model generalizes well.\n",
    "- If there's a large gap between errors, the model might be overfitting or underfitting.\n",
    "- Lasso and Elastic Net often converge faster, thanks to feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61d5261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Additional Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Step 2: Feature Importance (Lasso and Elastic Net)\n",
    "# Lasso and Elastic Net perform feature selection by shrinking some coefficients to zero. Let's visualize the most important features.\n",
    "# Extract feature importance (coefficients)\n",
    "lasso_coeffs = lasso_best.coef_\n",
    "elastic_net_coeffs = elastic_net_best.coef_\n",
    "ridge_coeffs = ridge_best.coef_\n",
    "\n",
    "# Create a feature importance DataFrame\n",
    "feature_names = X_train.columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Lasso': lasso_coeffs,\n",
    "    'Elastic Net': elastic_net_coeffs,\n",
    "    'Ridge': ridge_coeffs\n",
    "})\n",
    "\n",
    "# Sort by Lasso importance\n",
    "importance_df = importance_df.sort_values(by=\"Lasso\", ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Feature', y='Lasso', data=importance_df, color='blue', label=\"Lasso\")\n",
    "sns.barplot(x='Feature', y='Elastic Net', data=importance_df, color='orange', alpha=0.7, label=\"Elastic Net\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Feature Importance: Lasso vs Elastic Net\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Learning Curve Visualization\n",
    "# A learning curve shows how the model performs as we get more training data.\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(model, X, y, title):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        model, X, y, cv=5, scoring=\"neg_mean_squared_error\", train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "    )\n",
    "\n",
    "    train_mean = -np.mean(train_scores, axis=1)\n",
    "    test_mean = -np.mean(test_scores, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_sizes, train_mean, 'o-', label=\"Training Error\", color=\"red\")\n",
    "    plt.plot(train_sizes, test_mean, 'o-', label=\"Validation Error\", color=\"blue\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot learning curves\n",
    "plot_learning_curve(ridge_best, X_train, y_train, \"Learning Curve - Ridge Regression\")\n",
    "plot_learning_curve(lasso_best, X_train, y_train, \"Learning Curve - Lasso Regression\")\n",
    "plot_learning_curve(elastic_net_best, X_train, y_train, \"Learning Curve - Elastic Net Regression\")\n",
    "plot_learning_curve(huber_best, X_train, y_train, \"Learning Curve - Huber Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c41762",
   "metadata": {},
   "source": [
    "##### Comparing Ridge, Lasso, Elastic Net, and Huber Regression on Multiple Metrics\n",
    "\n",
    "Compare the models using different performance metrics:\n",
    "- ✅ Mean Squared Error (MSE) – Measures overall error, penalizing large deviations.\n",
    "- ✅ Root Mean Squared Error (RMSE) – Helps interpret error in original units.\n",
    "- ✅ Mean Absolute Error (MAE) – Less sensitive to large outliers than MSE.\n",
    "- ✅ R² Score – Explains how much variance in the target is captured by the model.\n",
    "\n",
    "Insights from the Comparison\n",
    "\n",
    "Ridge vs. Lasso vs. Elastic Net vs. Huber\n",
    "- Ridge Regression performs well when all predictors matter (low bias, higher variance).\n",
    "- Lasso Regression is useful for feature selection (eliminates unimportant variables).\n",
    "- Elastic Net balances Ridge and Lasso, working well with correlated features.\n",
    "- Huber Regression is robust to outliers, making it ideal when data has extreme values.\n",
    "\n",
    "Which Model to Choose?\n",
    "- If many features are irrelevant → Lasso or Elastic Net is better.\n",
    "- If all features contribute to the outcome → Ridge performs best.\n",
    "- If outliers are present → Huber is the best choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc67f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define Performance Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Define function to calculate all metrics\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"MSE\": round(mse, 4),\n",
    "        \"RMSE\": round(rmse, 4),\n",
    "        \"MAE\": round(mae, 4),\n",
    "        \"R² Score\": round(r2, 4)\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "ridge_results = evaluate_model(ridge_best, X_test, y_test, \"Ridge Regression\")\n",
    "lasso_results = evaluate_model(lasso_best, X_test, y_test, \"Lasso Regression\")\n",
    "elastic_net_results = evaluate_model(elastic_net_best, X_test, y_test, \"Elastic Net Regression\")\n",
    "huber_results = evaluate_model(huber_best, X_test, y_test, \"Huber Regression\")\n",
    "\n",
    "# Create DataFrame for comparison\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame([ridge_results, lasso_results, elastic_net_results, huber_results])\n",
    "print(results_df)\n",
    "\n",
    "# Step 2: Visualizing Model Performance\n",
    "# Plot bar charts for metric comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "metrics = [\"MSE\", \"RMSE\", \"MAE\", \"R² Score\"]\n",
    "colors = [\"blue\", \"orange\", \"green\", \"red\"]\n",
    "models = [\"Ridge Regression\", \"Lasso Regression\", \"Elastic Net Regression\", \"Huber Regression\"]\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    sns.barplot(x=models, y=results_df[metric], palette=colors, ax=ax)\n",
    "    ax.set_title(f\"Comparison of {metric}\")\n",
    "    ax.set_xticklabels(models, rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a293ac",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning for Ridge, Lasso, Elastic Net, and Huber Regression\n",
    "Now, let’s optimize the hyperparameters for each model using Grid Search Cross-Validation (GridSearchCV) and Randomized Search for efficiency.\n",
    "\n",
    "Step 1: Define Hyperparameter Grids\n",
    "\n",
    "Each model has key parameters:\n",
    "- Ridge Regression: alpha (penalty strength)\n",
    "- Lasso Regression: alpha (penalty strength)\n",
    "- Elastic Net: alpha (penalty strength), l1_ratio (balance between Lasso and Ridge)\n",
    "- Huber Regression: epsilon (controls outlier sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b61316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameter grids\n",
    "ridge_params = {\"alpha\": np.logspace(-3, 3, 10)}  # Alpha from 0.001 to 1000\n",
    "lasso_params = {\"alpha\": np.logspace(-3, 3, 10)}\n",
    "elastic_net_params = {\n",
    "    \"alpha\": np.logspace(-3, 3, 10),\n",
    "    \"l1_ratio\": np.linspace(0.1, 1, 10)  # Mix of Lasso (1.0) and Ridge (0.1)\n",
    "}\n",
    "huber_params = {\"epsilon\": np.linspace(1.1, 2.0, 10)}  # Controls outlier sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca921fc",
   "metadata": {},
   "source": [
    "Step 2: Perform Hyperparameter Tuning\n",
    "\n",
    "We will use GridSearchCV for Ridge, Lasso, and Elastic Net (small search space), and RandomizedSearchCV for Huber (since it is slower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5604be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression - Grid Search\n",
    "ridge_cv = GridSearchCV(Ridge(), ridge_params, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "best_ridge = ridge_cv.best_estimator_\n",
    "\n",
    "# Lasso Regression - Grid Search\n",
    "lasso_cv = GridSearchCV(Lasso(), lasso_params, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "best_lasso = lasso_cv.best_estimator_\n",
    "\n",
    "# Elastic Net - Grid Search\n",
    "elastic_net_cv = GridSearchCV(ElasticNet(), elastic_net_params, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "elastic_net_cv.fit(X_train, y_train)\n",
    "best_elastic_net = elastic_net_cv.best_estimator_\n",
    "\n",
    "# Huber Regression - Randomized Search (efficient for larger search space)\n",
    "huber_cv = RandomizedSearchCV(HuberRegressor(), huber_params, scoring=\"neg_mean_squared_error\", cv=5, n_iter=10, random_state=42)\n",
    "huber_cv.fit(X_train, y_train)\n",
    "best_huber = huber_cv.best_estimator_\n",
    "\n",
    "# Step 3: Display Best Hyperparameters\n",
    "print(f\"Best Ridge Alpha: {ridge_cv.best_params_['alpha']}\")\n",
    "print(f\"Best Lasso Alpha: {lasso_cv.best_params_['alpha']}\")\n",
    "print(f\"Best Elastic Net Alpha: {elastic_net_cv.best_params_['alpha']}, L1 Ratio: {elastic_net_cv.best_params_['l1_ratio']}\")\n",
    "print(f\"Best Huber Epsilon: {huber_cv.best_params_['epsilon']}\")\n",
    "\n",
    "# Step 4: Evaluate Optimized Models\n",
    "ridge_results = evaluate_model(best_ridge, X_test, y_test, \"Optimized Ridge\")\n",
    "lasso_results = evaluate_model(best_lasso, X_test, y_test, \"Optimized Lasso\")\n",
    "elastic_net_results = evaluate_model(best_elastic_net, X_test, y_test, \"Optimized Elastic Net\")\n",
    "huber_results = evaluate_model(best_huber, X_test, y_test, \"Optimized Huber\")\n",
    "\n",
    "results_df = pd.DataFrame([ridge_results, lasso_results, elastic_net_results, huber_results])\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a209266",
   "metadata": {},
   "source": [
    "Insights from Hyperparameter Tuning\n",
    "- Ridge and Lasso models benefit from optimal alpha values, improving performance.\n",
    "- Elastic Net finds the best mix between Lasso (L1) and Ridge (L2) penalties.\n",
    "- Huber Regression adjusts epsilon, improving robustness to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b8436",
   "metadata": {},
   "source": [
    "# 4. Generalized Linear Models (GLMs)\n",
    "\n",
    "### What are Generalized Linear Models (GLMs)?\n",
    "Generalized Linear Models (GLMs) extend linear regression by allowing the response variable (outcome) to follow a distribution from the exponential family (e.g., \n",
    "- Normal, \n",
    "- Poisson, \n",
    "- Binomial, \n",
    "- Gamma) \n",
    "Rather than just assuming normality.\n",
    "\n",
    "Standard linear regression\n",
    "- Which models the relationship between predictors and the outcome assuming constant variance and normally distributed errors\n",
    "\n",
    "GLMs introduce:\n",
    "- A Response Distribution: The outcome variable can follow a variety of distributions.\n",
    "- A Link Function: Transforms the expected value of the response variable to a scale where a linear relationship with predictors holds.\n",
    "\n",
    "What It Means: \n",
    "- GLMs extend linear regression by allowing different types of data distributions\n",
    "    - Poisson for count data. \n",
    "- It models the mean of the outcome variable based on a link function.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- The coefficients explain how each predictor affects the mean outcome, given the distribution.\n",
    "\n",
    "Performance Measures:\n",
    "- Deviance: Measures how well the model fits compared to a perfect model; lower values are better.\n",
    "\n",
    "Lay Explanation: \n",
    "- GLMs are like flexible versions of linear regression that can handle different data types (like counts or binary data), giving predictions that respect the data’s nature.\n",
    "\n",
    "Use Case: \n",
    "- Extends linear regression for non-normal distributions (e.g., Poisson regression for count data).\n",
    "\n",
    "Model Types: \n",
    "- Poisson regression, \n",
    "- Binomial regression.\n",
    "\n",
    "### GLM Components\n",
    "A GLM consists of three main components:\n",
    "- Random Component – Specifies the probability distribution of the response variable, e.g.: \n",
    "    - Normal, \n",
    "    - Poisson, \n",
    "    - Binomial.\n",
    "- Systematic Component – A linear predictor $η  = β_0 + β_1 X_1 + β_2 X_2$\n",
    "- Link Function – Connects the expected value of the response variable (𝐸(𝑌)) to the linear predictor:\n",
    "$$ g(E(Y)) = η$$\n",
    "\n",
    "### Types of GLMs and When to Use Them\n",
    "Different types of GLMs are used depending on the nature of the outcome variable and its distribution:\n",
    "\n",
    "| Type of GLM | Response Variable (Y) | Common Use Cases  | Link Function  | Example  |\n",
    "|---------------|--------|----------------------------------| -----------------------| -----------------------|  \n",
    "|Linear Regression| Continuous (Normal) |Predicting continuous outcomes| identity: $g(\\mu) = \\mu$ |  Predicting house prices |\n",
    "|Logistic Regression| Binary (0/1) |Binary classification problems| Logit: $g(\\mu) = log(\\frac{\\mu}{1- \\mu})$ |  Predicting customer churn (Yes/No) |\n",
    "|Poisson Regression| Count data (non-negative integers) |Predicting counts (e.g., number of events)| Log: $g(\\mu) = log(\\mu)$ |Modeling number of claims in insurance |\n",
    "|Negative Binomial Regression| Overdispersed count data | When Poisson model shows high variance| Log: $g(\\mu) = log(\\mu)$ | Number of hospital visits per year |\n",
    "|Gamma Regression| Positive continuous data |Skewed response variables (e.g., waiting times)| Inverse: $g(\\mu) = \\frac{1}{\\mu}$ | Predicting time to process a transaction |\n",
    "\n",
    "### How to Choose the Right GLM?\n",
    "\n",
    "Identify the Nature of the Response Variable\n",
    "- Continuous (Normal distribution) → Use Linear Regression\n",
    "- Binary (0/1, Yes/No) → Use Logistic Regression\n",
    "- Counts (0,1,2,3,…, Non-negative integers) → Use Poisson Regression\n",
    "- Overdispersed counts (variance > mean) → Use Negative Binomial Regression\n",
    "- Skewed continuous data (e.g., time, waiting durations) → Use Gamma Regression\n",
    "\n",
    "Check for Overdispersion (Variance > Mean in Count Data)\n",
    "- If Poisson regression underestimates variance, switch to Negative Binomial Regression.\n",
    "\n",
    "Check for Distribution Shape\n",
    "- If the response is right-skewed, consider Gamma Regression.\n",
    "\n",
    "Interpretability of the Model\n",
    "- Ensure the chosen link function aligns with the problem and makes sense for predictions.\n",
    "\n",
    "##### Generalized Linear Models (GLMs) in Python using the statsmodels library.\n",
    "- Linear Regression (Normal Distribution)\n",
    "- Logistic Regression (Binary Classification)\n",
    "- Poisson Regression (Count Data)\n",
    "- Negative Binomial Regression (Overdispersed Count Data)\n",
    "- Gamma Regression (Skewed Continuous Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157396d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Dependencies\n",
    "# Make sure you have statsmodels, pandas, and numpy installed:\n",
    "pip install statsmodels pandas numpy\n",
    "\n",
    "# Step 2: Load Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e27f23",
   "metadata": {},
   "source": [
    "**Linear Regression (Continuous Outcome)**\n",
    "\n",
    "📌 Example: Predict house prices based on square footage.\n",
    "- Distribution: Normal\n",
    "- Link Function: Identity $g(\\mu) = \\mu$\n",
    "\n",
    "Interpretation: The model estimates house price based on square footage using a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86785db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Data\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "X = np.random.uniform(1000, 4000, n)  # Square footage\n",
    "y = 50000 + 150 * X + np.random.normal(0, 50000, n)  # House Price with noise\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({'SquareFootage': X, 'HousePrice': y})\n",
    "\n",
    "# Fit Linear Regression using GLM\n",
    "model = smf.glm('HousePrice ~ SquareFootage', data=df, family=sm.families.Gaussian()).fit()\n",
    "\n",
    "# Print Summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97af9925",
   "metadata": {},
   "source": [
    "**Logistic Regression (Binary Outcome)**\n",
    "\n",
    "📌 Example: Predict whether a customer will churn (1 = Yes, 0 = No).\n",
    "- Distribution: Binomial\n",
    "- Link Function: Logit $g(\\mu) = log(\\frac{\\mu}{1- \\mu})$\n",
    "\n",
    " Interpretation: The coefficients indicate how monthly spend affects the probability of customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebec1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Data\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "X = np.random.uniform(500, 3000, n)  # Monthly Spend\n",
    "p = 1 / (1 + np.exp(-(-3 + 0.002 * X)))  # Logistic function\n",
    "y = np.random.binomial(1, p, n)  # Churn: 0 or 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({'MonthlySpend': X, 'Churn': y})\n",
    "\n",
    "# Fit Logistic Regression using GLM\n",
    "model = smf.glm('Churn ~ MonthlySpend', data=df, family=sm.families.Binomial()).fit()\n",
    "\n",
    "# Print Summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75790880",
   "metadata": {},
   "source": [
    "**Poisson Regression (Count Outcome)**\n",
    "\n",
    "📌 Example: Predict the number of claims per customer in an insurance dataset.\n",
    "- Distribution: Poisson\n",
    "- Link Function: Log $g(\\mu) = log(\\mu)$\n",
    "\n",
    "Interpretation: The model estimates expected claims based on age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e088fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Data\n",
    "np.random.seed(42)\n",
    "n = 300\n",
    "X = np.random.uniform(18, 65, n)  # Customer Age\n",
    "y = np.random.poisson(lam=np.exp(0.1 * X - 4), size=n)  # Number of Claims\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({'Age': X, 'NumClaims': y})\n",
    "\n",
    "# Fit Poisson Regression using GLM\n",
    "model = smf.glm('NumClaims ~ Age', data=df, family=sm.families.Poisson()).fit()\n",
    "\n",
    "# Print Summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48888f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "poisson_model = sm.GLM(y_train, X_train, family=sm.families.Poisson()).fit()\n",
    "predictions = poisson_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0003b78",
   "metadata": {},
   "source": [
    "**Negative Binomial Regression (Overdispersed Count Data)**\n",
    "\n",
    "📌 Example: Predict the number of visits to a hospital per year.\n",
    "- Distribution: Negative Binomial (for overdispersed count data)\n",
    "- Link Function: Log $g(\\mu) = log(\\mu)$\n",
    "\n",
    "Interpretation: Negative Binomial handles cases where variance exceeds the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bebae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Data\n",
    "np.random.seed(42)\n",
    "n = 300\n",
    "X = np.random.uniform(20, 80, n)  # Patient Age\n",
    "y = np.random.negative_binomial(n=2, p=0.5, size=n)  # Overdispersed Count Data\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({'Age': X, 'HospitalVisits': y})\n",
    "\n",
    "# Fit Negative Binomial Regression using GLM\n",
    "model = smf.glm('HospitalVisits ~ Age', data=df, family=sm.families.NegativeBinomial()).fit()\n",
    "\n",
    "# Print Summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58805356",
   "metadata": {},
   "source": [
    "**Gamma Regression (Skewed Continuous Data)**\n",
    "\n",
    "📌 Example: Predict the time taken to process a transaction.\n",
    "- Distribution: Gamma (for right-skewed continuous data)\n",
    "- Link Function: Inverse $g(\\mu) = \\frac{1}{\\mu}$\n",
    "\n",
    "Interpretation: This model is useful for modeling positively skewed transaction processing times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Data\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "X = np.random.uniform(100, 1000, n)  # Transaction Complexity\n",
    "y = np.random.gamma(shape=2, scale=X / 200, size=n)  # Processing Time\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({'TransactionComplexity': X, 'ProcessingTime': y})\n",
    "\n",
    "# Fit Gamma Regression using GLM\n",
    "model = smf.glm('ProcessingTime ~ TransactionComplexity', data=df, family=sm.families.Gamma(link=sm.families.links.inverse())).fit()\n",
    "\n",
    "# Print Summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1559de0b",
   "metadata": {},
   "source": [
    "#### To understand why penalizing the magnitude of coefficients should work in the first place.\n",
    "\n",
    "To understand the impact of model complexity on the magnitude of coefficients, simulated a sine curve (between 60° and 300°) and added some random noise.\n",
    "\n",
    "Resembles a sine curve but not exactly because of the noise.\n",
    "\n",
    "Estimate the sine function using polynomial regression with powers of x from 1 to 15. Let’s add a column for each power upto 15 in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries. The same will be used throughout the article.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 10\n",
    "\n",
    "#Define input array with angles from 60deg to 300deg converted to radians\n",
    "x = np.array([i*np.pi/180 for i in range(60,300,4)])\n",
    "np.random.seed(10)  #Setting seed for reproducibility\n",
    "y = np.sin(x) + np.random.normal(0,0.15,len(x))\n",
    "data = pd.DataFrame(np.column_stack([x,y]),columns=['x','y'])\n",
    "plt.plot(data['x'],data['y'],'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38689bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,16):  #power of 1 is already there\n",
    "    colname = 'x_%d'%i      #new var will be x_power\n",
    "    data[colname] = data['x']**i\n",
    "print(data.head()) # add a column for each power upto 15 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01f2a8",
   "metadata": {},
   "source": [
    "#### Making 15 Different Linear Regression Models\n",
    "\n",
    "we have all the 15 powers, let’s make 15 different linear regression models, with each model containing variables with powers of x from 1 to the particular model number.\n",
    "\n",
    "Define a generic function that takes in the required maximum power of x as an input and returns a list containing \n",
    "- model RSS, \n",
    "- intercept, \n",
    "- coef_x, \n",
    "- coef_x2, … upto entered power \n",
    "\n",
    "Here RSS refers to the ‘Residual Sum of Squares,’ which is nothing but the sum of squares of errors between the predicted and actual values in the training data set and is known as the cost function or the loss function.\n",
    "\n",
    "The function will not plot the model fit for all the powers but will return the RSS and coefficient values for all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd6d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Linear Regression model from scikit-learn.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def linear_regression(data, power, models_to_plot):\n",
    "    #initialize predictors:\n",
    "    predictors=['x']\n",
    "    if power>=2:\n",
    "        predictors.extend(['x_%d'%i for i in range(2,power+1)])\n",
    "    \n",
    "    #Fit the model\n",
    "    linreg = LinearRegression(normalize=True)\n",
    "    linreg.fit(data[predictors],data['y'])\n",
    "    y_pred = linreg.predict(data[predictors])\n",
    "    \n",
    "    #Check if a plot is to be made for the entered power\n",
    "    if power in models_to_plot:\n",
    "        plt.subplot(models_to_plot[power])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['x'],y_pred)\n",
    "        plt.plot(data['x'],data['y'],'.')\n",
    "        plt.title('Plot for power: %d'%power)\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    ret.extend([linreg.intercept_])\n",
    "    ret.extend(linreg.coef_)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7824ea",
   "metadata": {},
   "source": [
    "##### Store all the Results in Pandas Dataframe\n",
    "\n",
    "Store all the results in a Pandas dataframe and plot 6 models to get an idea of the trend.\n",
    "\n",
    "Expection: the models with increasing complexity to better fit the data and result in lower RSS values.\n",
    "- As the model complexity increases, the models tend to fit even smaller deviations in the training data set. \n",
    "- Though this leads to overfitting, let’s keep this issue aside for some time and come to our main objective, i.e., the impact on the magnitude of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dataframe to store the results:\n",
    "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
    "ind = ['model_pow_%d'%i for i in range(1,16)]\n",
    "coef_matrix_simple = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "# Define the powers for which a plot is required:\n",
    "models_to_plot = {1:231,3:232,6:233,9:234,12:235,15:236}\n",
    "\n",
    "# Iterate through all powers and assimilate results\n",
    "for i in range(1,16):\n",
    "    coef_matrix_simple.iloc[i-1,0:i+2] = linear_regression(data, power=i, models_to_plot=models_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1567427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the display format to be scientific for ease of analysis\n",
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_matrix_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cc6727",
   "metadata": {},
   "source": [
    "Its evident that the size of coefficients increases exponentially with an increase in model complexity.\n",
    "- Intuition: into why putting a constraint on the magnitude of coefficients can be a good idea to reduce model complexity.\n",
    "\n",
    "##### Large Coefficents Significance\n",
    "\n",
    "It means that we’re putting a lot of emphasis on that feature, i.e., the particular feature is a good predictor for the outcome. \n",
    "- When it becomes too large, the algorithm starts modeling intricate relations to estimate the output and ends up overfitting the particular training data.\n",
    "\n",
    "Solution\n",
    "- ridge and lasso regression in detail \n",
    "- see how well they work for the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "tree_model = DecisionTreeClassifier()\n",
    "tree_model.fit(X_train, y_train)\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "predictions_tree = tree_model.predict(X_test)\n",
    "predictions_rf = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeab36d",
   "metadata": {},
   "source": [
    "### **4.3. Poisson Regression**\n",
    "\n",
    "Objective:\n",
    "- We want to predict the number of insurance claims per customer using Poisson regression.\n",
    "\n",
    "##### Understanding Poisson Regression\n",
    "- Poisson regression is used for modeling count data, where the outcome variable represents the number of times an event occurs within a fixed period.\n",
    "- Example: Number of insurance claims per customer in a year.\n",
    "\n",
    "The Poisson distribution assumes that:\n",
    "$$ P(Y = k) = \\frac{e^{-\\lambda} \\lambda^{k}}{k!}$$\n",
    "- where:\n",
    "    - 𝑌 is the count outcome (e.g., number of claims)\n",
    "    - 𝜆 (mean rate) represents the expected number of occurrences\n",
    "    - k is the observed count\n",
    "\n",
    "The Poisson regression model uses a log link function:\n",
    "$$ log(\\lambda) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n$$\n",
    "- so that:\n",
    "$$\\lambda = e^{(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ...)}$$\n",
    "- This ensures that the predicted counts are always positive.\n",
    "\n",
    "##### Assumptions of Poisson Regression\n",
    "- Count Data: The dependent variable (Y) must be a count (0,1,2,...).\n",
    "- Mean = Variance Assumption (Equidispersion): The mean and variance should be roughly equal. If variance is much greater than the mean (overdispersion), Negative Binomial Regression might be better.\n",
    "- Independence of Observations: Claims from one customer should not influence claims from another.\n",
    "- Exponential Relationship: The logarithm of the mean count is a linear function of predictors.\n",
    "- No Excess Zeros (Zero-Inflation): Too many zero claims may indicate a zero-inflated Poisson (ZIP) model is needed.\n",
    "\n",
    "##### Data Considerations\n",
    "Before modeling, we must explore and preprocess the data.\n",
    "- Important Features for Predicting Claims\n",
    "    - Customer Age (older customers may file more claims)\n",
    "    - Previous Claims History (customers with past claims may be riskier)\n",
    "    - Vehicle Type (luxury cars may have more claims)\n",
    "    - Driving Record (poor records suggest higher claims)\n",
    "    - Policy Type (comprehensive vs. basic coverage)\n",
    "    - Region (urban vs. rural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a555d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Necessary Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 2: Load and Explore Data\n",
    "# Simulate an insurance dataset for Poisson regression.\n",
    "\n",
    "# Simulating Data\n",
    "np.random.seed(42)\n",
    "n = 1000  # Number of customers\n",
    "\n",
    "# Simulating features\n",
    "age = np.random.randint(18, 75, size=n)  # Age of customers\n",
    "past_claims = np.random.poisson(1.2, size=n)  # Past claims\n",
    "vehicle_type = np.random.choice([0, 1], size=n, p=[0.7, 0.3])  # 0 = Regular, 1 = Luxury\n",
    "region = np.random.choice([0, 1], size=n, p=[0.6, 0.4])  # 0 = Rural, 1 = Urban\n",
    "\n",
    "# Generate number of claims (Outcome variable) using Poisson function\n",
    "claims = np.random.poisson(lam=np.exp(0.02 * age + 0.5 * past_claims + 0.8 * vehicle_type + 0.3 * region - 3))\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Age': age, 'PastClaims': past_claims, 'VehicleType': vehicle_type, 'Region': region, 'NumClaims': claims})\n",
    "\n",
    "# Show Data\n",
    "print(df.head())\n",
    "\n",
    "# Distribution Plot\n",
    "sns.histplot(df['NumClaims'], bins=10, kde=False)\n",
    "plt.xlabel(\"Number of Claims\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Claims\")\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Fit the Poisson Regression Model\n",
    "# Fit Poisson Regression Model\n",
    "model = smf.glm('NumClaims ~ Age + PastClaims + VehicleType + Region', data=df, family=sm.families.Poisson()).fit()\n",
    "\n",
    "# Model Summary\n",
    "print(model.summary())\n",
    "\n",
    "# Exponentiated Coefficients (Rate Ratios)\n",
    "np.exp(model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4669f79",
   "metadata": {},
   "source": [
    "##### Interpreting the Output\n",
    "Intercept ($𝛽_0$): \n",
    "- Baseline log-count when all predictors are 0.\n",
    "\n",
    "Coefficients:\n",
    "- Log-change in expected claims per unit increase in the predictor.\n",
    "\n",
    "Exponentiated Coefficients (Rate Ratios):\n",
    "- Values > 1: Increase in claims\n",
    "- Values < 1: Decrease in claims\n",
    "\n",
    "### Key Checks:\n",
    "- If NumClaims has too many zeros, consider a **zero-inflated model** (below).\n",
    "- If variance > mean, **check for overdispersion** (below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428cf706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Model Diagnostics\n",
    "print(\"Mean of Claims:\", df['NumClaims'].mean())\n",
    "print(\"Variance of Claims:\", df['NumClaims'].var())\n",
    "\n",
    "# Goodness-of-Fit Tests\n",
    "print(\"Deviance:\", model.deviance)\n",
    "print(\"Pearson Chi-Square:\", model.pearson_chi2)\n",
    "\n",
    "# Step 5: Make Predictions\n",
    "# Predict the expected number of claims for new customers.\n",
    "# Creating new data for prediction\n",
    "new_data = pd.DataFrame({'Age': [30, 50], 'PastClaims': [1, 3], 'VehicleType': [0, 1], 'Region': [1, 0]})\n",
    "\n",
    "# Predict Expected Claims\n",
    "new_data['PredictedClaims'] = model.predict(new_data)\n",
    "print(new_data)\n",
    "\n",
    "# Interpretation: These are the expected mean number of claims for new customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457aab73",
   "metadata": {},
   "source": [
    "##### What to Look Out For in the Data\n",
    "- Zero-inflation: Too many customers with zero claims? Consider Zero-Inflated Poisson (ZIP).\n",
    "- Overdispersion: If variance > mean, use Negative Binomial Regression.\n",
    "- Influential Data Points: Check for outliers using Cook’s distance.\n",
    "- Multicollinearity: High correlation among features? Use Variance Inflation Factor (VIF).\n",
    "\n",
    "#### Key takeaways for Poisson Regression model:\n",
    "- Best for?: \n",
    "    - Count data (e.g., number of claims)\n",
    "- Key Assumptions: \n",
    "    - Mean ≈ Variance, these are Independent events\n",
    "- If Overdispersion?: \n",
    "    - Use Negative Binomial\n",
    "- If Too many zeros?: \n",
    "    - Consider Zero-Inflated Poisson\n",
    "- Predictions\n",
    "    - Expected claim counts, log-linear model\n",
    "\n",
    "### **Zero-Inflated Poisson (ZIP) Modeling**\n",
    "\n",
    "Why ZIP?\n",
    "- The Zero-Inflated Poisson (ZIP) model is used when count data has excessive zeros—more than what a standard Poisson regression can handle. \n",
    "- This often happens when:\n",
    "    - Some customers never file claims (structural zeros).\n",
    "    - Others have random chances of filing claims.\n",
    "\n",
    "The ZIP model combines:\n",
    "- A Poisson Model (for regular counts).\n",
    "- A Logistic Model (to predict if a zero comes from a \"never claims\" group).\n",
    "\n",
    "##### ZIP Model Structure\n",
    "The ZIP model assumes:\n",
    "$$ P(Y = 0) = \\pi + (1 - \\pi) e^{-\\lambda}$$\n",
    "$$ P(Y = k) = (1 - \\pi)\\frac{e^{-\\lambda} \\lambda^{k}}{k!} , k > 0$$\n",
    "- where:\n",
    "    - π = Probability of always being in the zero-claim group.\n",
    "    - λ = Expected number of claims for those who can file claims.\n",
    "\n",
    "The model has:\n",
    "- A logistic model for zero-inflation probability π.\n",
    "- A Poisson model for expected counts λ.\n",
    "\n",
    "##### When to Use ZIP?\n",
    "1. Check for Excess Zeros:\n",
    "    - If too many zeros (>40%), ZIP is worth trying.\n",
    "2. Compare Mean & Variance\n",
    "    - If variance >> mean, ZIP or Negative Binomial may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b9d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Excess Zeros\n",
    "zero_count = (df['NumClaims'] == 0).sum()\n",
    "zero_percentage = zero_count / len(df)\n",
    "print(f\"Zero Percentage: {zero_percentage:.2f}\")\n",
    "\n",
    "# Compare Mean & Variance\n",
    "print(\"Mean of Claims:\", df['NumClaims'].mean())\n",
    "print(\"Variance of Claims:\", df['NumClaims'].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d5368",
   "metadata": {},
   "source": [
    "##### Fitting a ZIP Model in Python\n",
    "We use statsmodels’ ZeroInflatedPoisson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5ac827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Dependencies\n",
    "!pip install statsmodels\n",
    "\n",
    "# Step 2: Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 3: Simulate an Insurance Dataset\n",
    "np.random.seed(42)\n",
    "n = 1000  \n",
    "\n",
    "# Simulating features\n",
    "age = np.random.randint(18, 75, size=n)  \n",
    "past_claims = np.random.poisson(1.2, size=n)  \n",
    "vehicle_type = np.random.choice([0, 1], size=n, p=[0.7, 0.3])  \n",
    "region = np.random.choice([0, 1], size=n, p=[0.6, 0.4])  \n",
    "\n",
    "# Generate structural zeros (some never claim)\n",
    "never_claims = np.random.binomial(1, 0.3, size=n)  \n",
    "claims = np.random.poisson(lam=np.exp(0.02 * age + 0.5 * past_claims + 0.8 * vehicle_type + 0.3 * region - 3))\n",
    "\n",
    "# Apply zero-inflation\n",
    "claims[never_claims == 1] = 0  \n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Age': age, 'PastClaims': past_claims, 'VehicleType': vehicle_type, 'Region': region, 'NumClaims': claims})\n",
    "\n",
    "# Show Data\n",
    "print(df.head())\n",
    "\n",
    "# Plot distribution\n",
    "sns.histplot(df['NumClaims'], bins=10, kde=False)\n",
    "plt.xlabel(\"Number of Claims\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Claims\")\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Fit ZIP Model\n",
    "import statsmodels.discrete.count_model as cm\n",
    "\n",
    "# Fit ZIP Model\n",
    "zip_model = cm.ZeroInflatedPoisson.from_formula(\"NumClaims ~ Age + PastClaims + VehicleType + Region\", \n",
    "                                                exog_infl=\"Age + VehicleType\", \n",
    "                                                data=df).fit()\n",
    "\n",
    "# Model Summary\n",
    "print(zip_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6878c9",
   "metadata": {},
   "source": [
    "Interpreting Results\n",
    "\n",
    "Poisson Component (Regular Counts)\n",
    "- Age, PastClaims, VehicleType, Region influence claim counts.\n",
    "- Exponentiate coefficients (np.exp(zip_model.params)) for rate ratios.\n",
    "\n",
    "Zero-Inflation Component (Logistic Model)\n",
    "- Age, VehicleType influence whether someone never claims.\n",
    "- High coefficients mean more structural zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dea4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compare Poisson vs. ZIP\n",
    "## Method 1: Log-Likelihood Comparison\n",
    "print(\"Poisson Log-Likelihood:\", model.llf)  \n",
    "print(\"ZIP Log-Likelihood:\", zip_model.llf)\n",
    "# If ZIP has a higher log-likelihood, it fits better.\n",
    "\n",
    "## Method 2: Vuong Test for ZIP vs. Poisson\n",
    "from statsmodels.stats.diagnostic import poisson_vuong\n",
    "test_stat, p_value = poisson_vuong(model, zip_model)\n",
    "print(f\"Vuong Test Statistic: {test_stat}, p-value: {p_value}\")\n",
    "# If p < 0.05, ZIP is preferred.\n",
    "\n",
    "# Step 6: Make Predictions\n",
    "# Creating new customer data\n",
    "new_data = pd.DataFrame({'Age': [30, 50], 'PastClaims': [1, 3], 'VehicleType': [0, 1], 'Region': [1, 0]})\n",
    "\n",
    "# Predict expected claims\n",
    "new_data['PredictedClaims'] = zip_model.predict(new_data)\n",
    "print(new_data)\n",
    "# Interpretation: The expected number of claims accounts for excess zeros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b64ba4",
   "metadata": {},
   "source": [
    "##### What to Look Out For\n",
    "Issue: Too many zeros?\n",
    "- Solution: ZIP Model\n",
    "\n",
    "Issue: Overdispersion?\n",
    "- Solution: Negative Binomial ZIP\n",
    "\n",
    "Issue: Unusual logistic coefficients?\n",
    "- Solution: Check multicollinearity\n",
    "\n",
    "##### When Not to Use ZIP?\n",
    "- If zero inflation is low, use regular Poisson.\n",
    "- If variance >> mean, use Negative Binomial ZIP.\n",
    "\n",
    "##### Key takeways for ZIP Model\n",
    "- Best for?\n",
    "    - Count data with excess zeros\n",
    "- Two Parts?\n",
    "    - Logistic (excess zeros) + Poisson (counts)\n",
    "- Key Assumptions?\n",
    "    - Some customers never file claims\n",
    "- Alternatives?\n",
    "    - Negative Binomial ZIP for overdispersion\n",
    "- Evaluation?\n",
    "    - Vuong Test, Log-Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb920b",
   "metadata": {},
   "source": [
    "### **Checking for Overdespation**\n",
    "How to Check for Overdispersion in Count Data\n",
    "- Overdispersion occurs when the variance of the count data is greater than its mean. In count data models like Poisson regression, the assumption is that the mean equals the variance. When this assumption is violated, overdispersion is present.\n",
    "\n",
    "##### Checking for Overdispersion**\n",
    "**1. Check Mean vs. Variance for Overdispersion**\n",
    "- Poisson Model Assumption: Variance = Mean\n",
    "- Overdispersion: Variance > Mean\n",
    "\n",
    "You can easily check this by calculating the mean and variance of your outcome variable (e.g., number of claims) and compare them.\n",
    "- If variance > mean, we have overdispersion, and a Negative Binomial Model may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_claims = df['NumClaims'].mean()\n",
    "variance_claims = df['NumClaims'].var()\n",
    "\n",
    "print(f\"Mean: {mean_claims}\")\n",
    "print(f\"Variance: {variance_claims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e23d03c",
   "metadata": {},
   "source": [
    "**2. Overdispersion Test: Pearson Residuals**\n",
    "\n",
    "Another way to check for overdispersion is by using Pearson residuals. The formula for Pearson residuals is:\n",
    "$$ r_i = \\frac{y_i - \\hat{\\mu_i}}{\\sqrt{\\hat{\\mu_i}}}$$\n",
    "- where \n",
    "    - $𝑦_𝑖$: is the observed value, and \n",
    "    - $\\hat{𝜇_𝑖}$: is the predicted mean.\n",
    "\n",
    "For a Poisson model, Pearson residuals are assumed to have a variance of 1. \n",
    "- If the residuals are larger than expected, it suggests overdispersion.\n",
    "- If the statistic is greater than 1, overdispersion is likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ab7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Poisson model\n",
    "poisson_model = smf.poisson('NumClaims ~ Age + PastClaims + VehicleType + Region', data=df).fit()\n",
    "\n",
    "# Calculate Pearson residuals\n",
    "pearson_residuals = poisson_model.resid_pearson\n",
    "\n",
    "# Calculate the overdispersion statistic\n",
    "overdispersion_stat = (pearson_residuals**2).sum() / len(df)\n",
    "\n",
    "print(f\"Overdispersion Statistic: {overdispersion_stat}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ff8d9",
   "metadata": {},
   "source": [
    "**3. Dispersion Test Using the Scale Parameter**\n",
    "- The dispersion parameter for the Poisson model is assumed to be 1. \n",
    "    - If the dispersion parameter is significantly greater than 1, it indicates overdispersion.\n",
    "    \n",
    "Goodness-of-Fit Tests:\n",
    "- If Deviance / df > 1, check for overdispersion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92444a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check scale parameter (for Poisson model)\n",
    "dispersion_parameter = poisson_model.scale\n",
    "print(f\"Dispersion Parameter: {dispersion_parameter}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e6ee1",
   "metadata": {},
   "source": [
    "### **Handling Overdispersion**\n",
    "- If overdispersion is detected, use Negative Binomial Regression.\n",
    "    - Negative Binomial allows for overdispersion and typically improves model fit.\n",
    "\n",
    "If overdispersion is detected, Poisson regression may not be suitable, and you should consider other models, such as:\n",
    "- Negative Binomial Regression: It accounts for overdispersion by introducing a second parameter to model the variance.\n",
    "- Zero-Inflated Models: If overdispersion is combined with excessive zeros, ZIP (Zero-Inflated Poisson) or ZINB (Zero-Inflated Negative Binomial) models are better suited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38384d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Negative Binomial Model\n",
    "neg_bin_model = smf.glm('NumClaims ~ Age + PastClaims + VehicleType + Region', data=df, family=sm.families.NegativeBinomial()).fit()\n",
    "# Model Summary\n",
    "print(neg_bin_model.summary())\n",
    "\n",
    "## Example 2\n",
    "\n",
    "# Fit Negative Binomial model\n",
    "negbinom_model = smf.poisson('NumClaims ~ Age + PastClaims + VehicleType + Region', data=df).fit(method='newton')\n",
    "print(negbinom_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03ab5c1",
   "metadata": {},
   "source": [
    "### **4.4. Negative Binomial Model**\n",
    "Objective Predict the number of visits to a hospital per year.\n",
    "\n",
    "Negative Binomial regression model is used when count data exhibits overdispersion, meaning the variance is greater than the mean. \n",
    "- This typically occurs in cases where the data is more spread out than what a Poisson model would predict.\n",
    "\n",
    "##### Negative Binomial Regression Overview\n",
    "- The Negative Binomial (NB) regression model is an extension of the Poisson regression model that adds a **dispersion parameter** to handle overdispersion. \n",
    "- This allows the model to account for situations where the variance exceeds the mean, which is a common occurrence in real-world count data.\n",
    "\n",
    "Best for?\n",
    "- Overdispersed count data (Variance > Mean)\n",
    "Key Assumptions?\n",
    "- Overdispersed data\n",
    "What to check?\n",
    "- Mean vs. Variance, Pearson Residuals, Dispersion Parameter\n",
    "How to compare?\n",
    "- Compare Log-Likelihood of Poisson vs. Negative Binomial\n",
    "Alternatives?\n",
    "- If overdispersion is not present, Poisson regression\n",
    "\n",
    "##### Steps for Analyzing Negative Binomial Regression for Predicting Hospital Visits\n",
    "Data contain variables such as:\n",
    "- age, \n",
    "- previous visits, and \n",
    "- chronic conditions \n",
    "which may influence the number of hospital visits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1:  Simulate the Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Simulating data for hospital visits\n",
    "np.random.seed(42)\n",
    "n = 1000  # Number of observations\n",
    "\n",
    "# Simulate features\n",
    "age = np.random.randint(18, 80, size=n)\n",
    "previous_visits = np.random.poisson(2, size=n)  # Previous hospital visits\n",
    "chronic_conditions = np.random.choice([0, 1], size=n, p=[0.7, 0.3])  # Binary (0 = no, 1 = yes)\n",
    "\n",
    "# Generate the number of hospital visits per year\n",
    "# Use a log link for the expected value and include the effects of age, previous visits, and chronic conditions\n",
    "log_mu = 0.02 * age + 0.5 * previous_visits + 0.8 * chronic_conditions\n",
    "lambda_visits = np.exp(log_mu)\n",
    "\n",
    "# Overdispersion: adding a random component to variance\n",
    "visits = np.random.negative_binomial(n=2, p=1 / (1 + lambda_visits))  # Overdispersed count data\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Age': age,\n",
    "    'PreviousVisits': previous_visits,\n",
    "    'ChronicConditions': chronic_conditions,\n",
    "    'HospitalVisits': visits\n",
    "})\n",
    "\n",
    "# Show the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Plotting the distribution of hospital visits\n",
    "sns.histplot(df['HospitalVisits'], kde=False, bins=10)\n",
    "plt.title('Distribution of Hospital Visits per Year')\n",
    "plt.xlabel('Number of Visits')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Visualizing the Data\n",
    "# look at the mean vs. variance to check if overdispersion is present.\n",
    "mean_visits = df['HospitalVisits'].mean()\n",
    "variance_visits = df['HospitalVisits'].var()\n",
    "\n",
    "print(f\"Mean of Hospital Visits: {mean_visits}\")\n",
    "print(f\"Variance of Hospital Visits: {variance_visits}\")\n",
    "# If Variance > Mean, this suggests that overdispersion is present, and the Negative Binomial model may be appropriate.\n",
    "\n",
    "# Step 3: Fit a Negative Binomial Regression Model\n",
    "# Fit Negative Binomial Model\n",
    "nb_model = smf.poisson('HospitalVisits ~ Age + PreviousVisits + ChronicConditions', data=df).fit(method='newton')\n",
    "print(nb_model.summary())\n",
    "\n",
    "# Step 4: Checking Overdispersion:\n",
    "# Verify whether overdispersion exists by looking at the dispersion parameter and Pearson residuals.\n",
    "# (a) Pearson Residuals Test:\n",
    "# Calculate Pearson residuals\n",
    "pearson_residuals = nb_model.resid_pearson\n",
    "\n",
    "# Calculate the overdispersion statistic\n",
    "overdispersion_stat = (pearson_residuals**2).sum() / len(df)\n",
    "\n",
    "print(f\"Overdispersion Statistic: {overdispersion_stat}\")\n",
    "\n",
    "# (b)  Dispersion Parameter Test:\n",
    "# Check the dispersion parameter for the Negative Binomial model\n",
    "dispersion_parameter = nb_model.scale\n",
    "print(f\"Dispersion Parameter: {dispersion_parameter}\")\n",
    "\n",
    "# Step 5: Comparing with Poisson Model:\n",
    "# ompare the Negative Binomial model to a Poisson model to see which one fits\n",
    "# Fit Poisson Model for comparison\n",
    "poisson_model = smf.poisson('HospitalVisits ~ Age + PreviousVisits + ChronicConditions', data=df).fit()\n",
    "\n",
    "# Compare Log-Likelihood values\n",
    "print(f\"Poisson Model Log-Likelihood: {poisson_model.llf}\")\n",
    "print(f\"Negative Binomial Model Log-Likelihood: {nb_model.llf}\")\n",
    "\n",
    "# Step 6: Making Predictions\n",
    "# Use the Negative Binomial model to predict the number of hospital visits for new patients.\n",
    "# New data for prediction\n",
    "new_data = pd.DataFrame({\n",
    "    'Age': [30, 60],\n",
    "    'PreviousVisits': [2, 5],\n",
    "    'ChronicConditions': [0, 1]\n",
    "})\n",
    "\n",
    "# Predict hospital visits\n",
    "new_data['PredictedVisits'] = nb_model.predict(new_data)\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d0b23",
   "metadata": {},
   "source": [
    "In step 3:\n",
    "- Poisson distribution with the Negative Binomial method: Here, the Poisson model is used but estimated using the Negative Binomial method to account for overdispersion. \n",
    "- This will give us the correct estimates even when the data has variance greater than the mean.\n",
    "\n",
    "In Step 4:\n",
    "- (a) If the overdispersion statistic > 1, it confirms that overdispersion is present.\n",
    "- (b) If dispersion parameter > 1, overdispersion is likely.\n",
    "\n",
    "In Step 5:\n",
    "- A higher log-likelihood for the Negative Binomial model indicates it better fits the data, especially in the case of overdispersion.\n",
    "\n",
    "In step 6:\n",
    "- This will output the predicted number of hospital visits per year for new data points.\n",
    "\n",
    "### Model Diagnostics for Negative Binomial Regression\n",
    "When working with count data models like the Negative Binomial (NB) regression, it's crucial to assess the model's \n",
    "- performance, \n",
    "- assumptions, and \n",
    "- potential issues. \n",
    "\n",
    "Here are some essential diagnostics:\n",
    "\n",
    "##### **Residual Analysis for Negative Binomial Model**\n",
    "- Residuals represent the difference between the observed values and the predicted values. \n",
    "- In regression, we use residuals to check how well the model fits the data. \n",
    "- For count data, there are a few specific types of residuals to consider:\n",
    "    - Pearson Residuals: Measures the standardized difference between the observed and predicted values.\n",
    "    - Deviance Residuals: Similar to Pearson residuals, but based on the likelihood of the model.\n",
    "\n",
    "**Pearson Residuals Plot**\n",
    "\n",
    "Interpretation:\n",
    "- In a well-fitted model, the residuals should be centered around zero and roughly normally distributed.\n",
    "- If there's a pattern in the residuals, it may indicate model misspecification or a need for more predictors.\n",
    "- In the QQ plot, points should lie roughly on a straight line if the residuals are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44379990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate Pearson residuals\n",
    "pearson_residuals = nb_model.resid_pearson\n",
    "\n",
    "# Plot residuals\n",
    "sns.histplot(pearson_residuals, kde=True, bins=20)\n",
    "plt.title(\"Histogram of Pearson Residuals\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Check for normality in residuals\n",
    "sns.qqplot(pearson_residuals, line='45')\n",
    "plt.title(\"QQ Plot of Pearson Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed5731",
   "metadata": {},
   "source": [
    "**Deviance Residuals Plot**\n",
    "\n",
    "Interpretation: \n",
    "- Deviance residuals provide insight into how much each observation deviates from the fitted model. \n",
    "    - They are particularly useful for identifying outliers or influential data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb9f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Deviance residuals\n",
    "deviance_residuals = nb_model.resid_deviance\n",
    "\n",
    "# Plot deviance residuals\n",
    "sns.histplot(deviance_residuals, kde=True, bins=20)\n",
    "plt.title(\"Histogram of Deviance Residuals\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eadfc83",
   "metadata": {},
   "source": [
    "##### **Influence Diagnostics for Negative Binomial Model**\n",
    "In regression models, influential points are data points that can significantly affect the estimated coefficients. \n",
    "- Influence measures help us identify these points. \n",
    "- For count data models, a useful diagnostic is the Cook’s Distance.\n",
    "\n",
    "**Cook’s Distance Plot**\n",
    "\n",
    "Cook’s distance is a metric that combines the leverage and residual of each observation to assess its influence on the fitted model.\n",
    "\n",
    "Interpretation:\n",
    "- Points with Cook’s Distance greater than the threshold (typically 4/n where n is the number of observations) are considered influential.\n",
    "- These points should be examined carefully, as they could disproportionately affect the model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a08bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate leverage (influence measure)\n",
    "influence = nb_model.get_influence()\n",
    "cooks_d = influence.cooks_distance[0]\n",
    "\n",
    "# Plot Cook's Distance\n",
    "plt.scatter(range(len(cooks_d)), cooks_d)\n",
    "plt.axhline(y=4/len(df), color='r', linestyle='--')\n",
    "plt.title(\"Cook's Distance\")\n",
    "plt.xlabel(\"Observation Index\")\n",
    "plt.ylabel(\"Cook's Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f024ea24",
   "metadata": {},
   "source": [
    "**Leverage vs. Residuals Plot**\n",
    "\n",
    "A leverage vs. residuals plot can also help identify influential points.\n",
    "\n",
    "Interpretation:\n",
    "- Points with high leverage and large residuals may be influential and should be checked for potential data issues.\n",
    "- The red line marks the zero residuals; observations above this line might have larger residuals than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5eed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get leverage and residuals\n",
    "leverage = influence.hat_matrix_diag\n",
    "residuals = nb_model.resid_pearson\n",
    "\n",
    "# Plot leverage vs residuals\n",
    "plt.scatter(leverage, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title(\"Leverage vs. Residuals\")\n",
    "plt.xlabel(\"Leverage\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8764efdf",
   "metadata": {},
   "source": [
    "### **Zero-Inflated Negative Binomial (ZINB)**\n",
    "In many count datasets, we may observe excessive zeros in the outcome variable. A standard Negative Binomial model might not handle this well, as it assumes the count variable follows a particular distribution that may not fit the excess zeros.\n",
    "\n",
    "The Zero-Inflated Negative Binomial (ZINB) model is specifically designed to handle datasets with an overrepresentation of zeros.\n",
    "\n",
    "##### ZINB Model Structure:\n",
    "- Zero-Inflation Component: A logistic regression (binary model) that models the probability of a zero count outcome.\n",
    "    - Looks at the probability of an excess zero count (whether the person has 0 hospital visits or not).\n",
    "- Count Component: A Negative Binomial model for the count data, which models the non-zero counts.\n",
    "    - Models the non-zero counts using Negative Binomial regression.\n",
    "\n",
    "##### Fitting a Zero-Inflated Negative Binomial (ZINB) Model\n",
    "- We can fit a ZINB model using the statsmodels or pyGAM library. Here's how to do it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.genmod.families import NegativeBinomial\n",
    "from statsmodels.genmod.families.links import Logit\n",
    "\n",
    "# Fit a Zero-Inflated Negative Binomial Model (ZINB)\n",
    "# The 'infl' part specifies the logistic regression for the zero-inflation component.\n",
    "zinb_model = smf.poisson('HospitalVisits ~ Age + PreviousVisits + ChronicConditions', \n",
    "                         data=df).fit(start_params=None, \n",
    "                                      method='newton', \n",
    "                                      family=NegativeBinomial(), \n",
    "                                      link=Logit())\n",
    "\n",
    "# Model summary\n",
    "print(zinb_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc37e3a9",
   "metadata": {},
   "source": [
    "##### Diagnostics for ZINB\n",
    "Once the ZINB model is fitted, you can apply similar residual and influence diagnostics:\n",
    "- Pearson residuals and Deviance residuals for both components.\n",
    "- Cook's Distance and Leverage for influential data points.\n",
    "\n",
    "Residual Analysis:\n",
    "- Plot Pearson and Deviance residuals.\n",
    "- Check normality (QQ plot) and patterns (histogram).\n",
    "\n",
    "Influence Diagnostics:\n",
    "- Identify influential points using Cook’s Distance and Leverage.\n",
    "\n",
    "Zero-Inflated Models:\n",
    "- Consider Zero-Inflated Negative Binomial (ZINB) if there are excessive zeros in the data.\n",
    "- Evaluate model fit with Log-Likelihood comparison between Negative Binomial and ZINB.\n",
    "\n",
    "##### **Comparison Between ZINB and NB Models**\n",
    "You can compare the ZINB model with the standard Negative Binomial model to see if the zero-inflation component significantly improves the fit.\n",
    "\n",
    "**Compare Log-Likelihoods**\n",
    "- A significantly higher log-likelihood for the ZINB model would indicate that the zero-inflation component provides a better fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1786a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Log-Likelihoods\n",
    "print(f\"Negative Binomial Log-Likelihood: {nb_model.llf}\")\n",
    "print(f\"Zero-Inflated NB Log-Likelihood: {zinb_model.llf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc36d04",
   "metadata": {},
   "source": [
    "### Analyzing Predictive Performance of the Negative Binomial and Zero-Inflated Models\n",
    "To evaluate the predictive performance of a Negative Binomial (NB) model or Zero-Inflated Negative Binomial (ZINB) model, we rely on a range of metrics that help us understand how well the model is making predictions. Common metrics include \n",
    "- Root Mean Squared Error (RMSE), \n",
    "- Akaike Information Criterion (AIC), and \n",
    "- log-likelihood.\n",
    "\n",
    "We'll explain their significance, and show how to calculate them in Python.\n",
    "\n",
    "##### **Root Mean Squared Error (RMSE)**\n",
    "RMSE is a standard metric used to assess how well a model's predictions match the actual observed values. It gives a sense of how far off the predictions are from the true values on average, with lower values indicating better performance.\n",
    "\n",
    "The formula for RMSE is:\n",
    "$$ RMSE = \\sqrt{\\frac{1}{n} \\sum^n_{i=1} (y_i - \\hat{y_i})^2}$$\n",
    "- Where:\n",
    "    - $y_i$ = actual values\n",
    "    - $ \\hat{y_i}$ = predicted values\n",
    "    - n = number of observations\n",
    "\n",
    "Calculating RMSE in Python\n",
    "- First, let's assume you already have the fitted model (nb_model for Negative Binomial or zinb_model for Zero-Inflated Negative Binomial) and the actual data (df['hospital_visits']).\n",
    "\n",
    "Interpretation: \n",
    "- The RMSE provides an idea of how much error there is between the predicted and actual values. Smaller RMSE values indicate better model performance. For count data, the RMSE is useful for evaluating how close the predicted counts are to the observed counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f93748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Actual values (hospital visits)\n",
    "y_true = df['hospital_visits']\n",
    "\n",
    "# Predicted values (model predictions)\n",
    "y_pred = nb_model.predict()  # Or zinb_model.predict()\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35088ae2",
   "metadata": {},
   "source": [
    "##### **Akaike Information Criterion (AIC)**\n",
    "AIC is a metric used to compare different models based on their likelihood and the number of parameters. It helps assess model fit while penalizing for overfitting. Lower AIC values indicate a better-fitting model.\n",
    "\n",
    "The formula for AIC is:\n",
    "$$ AIC = 2k - 2ln(\\hat{L})$$\n",
    "- Where:\n",
    "    - 𝑘 = number of model parameters\n",
    "    - $\\hat{𝐿}$ = maximum likelihood estimate of the model\n",
    "\n",
    "Calculating AIC in Python\n",
    "- You can directly get the AIC from the model summary. However, for demonstration, we’ll calculate it manually using the log-likelihood and the number of parameters.\n",
    "\n",
    "Interpretation: \n",
    "- Lower AIC values indicate a model that fits the data well without overfitting. You can compare AIC across different models (e.g., Negative Binomial vs. Zero-Inflated Negative Binomial) to choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-Likelihood of the fitted model\n",
    "log_likelihood = nb_model.llf  # Or zinb_model.llf\n",
    "\n",
    "# Number of parameters in the model\n",
    "num_params = len(nb_model.params)  # Or zinb_model.params\n",
    "\n",
    "# Calculate AIC\n",
    "aic = 2 * num_params - 2 * log_likelihood\n",
    "print(f\"AIC: {aic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d195c",
   "metadata": {},
   "source": [
    "##### **Log-Likelihood (LL)**\n",
    "The log-likelihood measures how well the model fits the data. It is based on the likelihood function and is often used in conjunction with AIC for model comparison.\n",
    "\n",
    "Log-Likelihood in Python\n",
    "- You can obtain the log-likelihood directly from the fitted model:\n",
    "\n",
    "Interpretation: \n",
    "- Higher log-likelihood values indicate a better fit. However, the log-likelihood should not be compared across models with different numbers of parameters unless corrected by AIC or BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6686e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = nb_model.llf  # Or zinb_model.llf\n",
    "print(f\"Log-Likelihood: {log_likelihood}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0f9d1",
   "metadata": {},
   "source": [
    "### Comparing Predictive Performance Using RMSE, AIC, and Log-Likelihood\n",
    "Once you've calculated RMSE, AIC, and log-likelihood, you can compare different models to decide which one is better at predicting the outcome.\n",
    "\n",
    "Example Comparison of Negative Binomial and Zero-Inflated Negative Binomial\n",
    "Let's say you have two models: nb_model (Negative Binomial) and zinb_model (Zero-Inflated Negative Binomial). Here’s how to compare them:\n",
    "\n",
    "Interpretation:\n",
    "- RMSE: Lower RMSE means the model's predictions are closer to the actual data.\n",
    "- AIC: The model with the lowest AIC is generally considered the best fit.\n",
    "- Log-Likelihood: Higher log-likelihood indicates better model fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from both models\n",
    "y_pred_nb = nb_model.predict()\n",
    "y_pred_zinb = zinb_model.predict()\n",
    "\n",
    "# Calculate RMSE for both models\n",
    "rmse_nb = np.sqrt(mean_squared_error(y_true, y_pred_nb))\n",
    "rmse_zinb = np.sqrt(mean_squared_error(y_true, y_pred_zinb))\n",
    "\n",
    "# AIC for both models\n",
    "aic_nb = 2 * len(nb_model.params) - 2 * nb_model.llf\n",
    "aic_zinb = 2 * len(zinb_model.params) - 2 * zinb_model.llf\n",
    "\n",
    "# Log-Likelihood for both models\n",
    "log_likelihood_nb = nb_model.llf\n",
    "log_likelihood_zinb = zinb_model.llf\n",
    "\n",
    "# Print the results\n",
    "print(f\"Negative Binomial - RMSE: {rmse_nb}, AIC: {aic_nb}, Log-Likelihood: {log_likelihood_nb}\")\n",
    "print(f\"Zero-Inflated Negative Binomial - RMSE: {rmse_zinb}, AIC: {aic_zinb}, Log-Likelihood: {log_likelihood_zinb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2af110",
   "metadata": {},
   "source": [
    "### **4.5. Gamma Regression for Predicting Transaction Processing Time**\n",
    "Gamma regression is used for modeling positive continuous response variables, particularly when the variance increases with the mean (heteroscedasticity). This makes it suitable for predicting time taken to process a transaction, where larger transactions might have more processing time variability.\n",
    "\n",
    "##### **Understanding Gamma Regression**\n",
    "\n",
    "Why Gamma Regression?\n",
    "It is used when the dependent variable is continuous and strictly positive.\n",
    "The variance of the response variable increases with the mean.\n",
    "The Gamma distribution is right-skewed, making it suitable for processing time data.\n",
    "\n",
    "Model Form\n",
    "$$ E(Y|X) = g^{-1}(X\\beta)$$\n",
    "- where:\n",
    "    - Y = processing time (response variable)\n",
    "    - $g^{-1}$: inverse of the link function (commonly log)\n",
    "    - $X\\beta$: linear predictor (features)\n",
    "    - The response variable follows a Gamma distribution.\n",
    "\n",
    "##### Key Assumptions\n",
    "- Gamma-distributed response variable (positive, right-skewed).\n",
    "- Mean-variance relationship: The variance of Y increases with the mean.\n",
    "- Link function: Often log link, ensuring predicted values remain positive.\n",
    "\n",
    "##### Data Preparation\n",
    "Assume we have a dataset df containing:\n",
    "- transaction_size (amount in USD)\n",
    "- num_steps (number of steps in processing)\n",
    "- processing_time (time in seconds)\n",
    "\n",
    "If processing_time is right-skewed, Gamma regression is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb977c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Checking the Data\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"transaction_data.csv\")  # Hypothetical dataset\n",
    "\n",
    "# Summary\n",
    "print(df.describe())\n",
    "\n",
    "# Check skewness\n",
    "sns.histplot(df['processing_time'], bins=30, kde=True)\n",
    "plt.title(\"Distribution of Processing Time\")\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Fitting a Gamma Regression Model\n",
    "# We use GLM (Generalized Linear Model) with a Gamma family and a log link function.\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Define the Gamma regression model\n",
    "gamma_model = smf.glm(\"processing_time ~ transaction_size + num_steps\",\n",
    "                       data=df, family=sm.families.Gamma(link=sm.families.links.log())).fit()\n",
    "\n",
    "# Model summary\n",
    "print(gamma_model.summary())\n",
    "\n",
    "# transaction_size and num_steps are predictors.\n",
    "\n",
    "# Step 3: Model Diagnostics\n",
    "# 1. Residual Analysis\n",
    "# Gamma regression assumes a mean-variance relationship. Checking residuals ensures model appropriateness.\n",
    "# Plot residuals\n",
    "sns.residplot(x=gamma_model.fittedvalues, y=gamma_model.resid_response, lowess=True)\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()\n",
    "# If residuals show a clear pattern, transformation or an alternative model may be needed.\n",
    "\n",
    "# 2. Deviance Residuals\n",
    "# Deviance residuals\n",
    "sns.histplot(gamma_model.resid_deviance, bins=30, kde=True)\n",
    "plt.title(\"Histogram of Deviance Residuals\")\n",
    "plt.show()\n",
    "# Normality is not required, but extreme skewness may indicate overdispersion.\n",
    "\n",
    "# Step 4: Model Evaluation\n",
    "# 1. Root Mean Squared Error (RMSE)\n",
    "# Measures the average prediction error.\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Actual vs Predicted\n",
    "y_true = df['processing_time']\n",
    "y_pred = gamma_model.predict()\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "# Lower RMSE → Better predictions.\n",
    "\n",
    "# 2. Akaike Information Criterion (AIC)\n",
    "# AIC balances goodness of fit with model complexity.\n",
    "aic = gamma_model.aic\n",
    "print(f\"AIC: {aic}\")\n",
    "# Lower AIC → Better model.\n",
    "\n",
    "# 3. Log-Likelihood\n",
    "# Measures model fit.\n",
    "log_likelihood = gamma_model.llf\n",
    "print(f\"Log-Likelihood: {log_likelihood}\")\n",
    "# Higher log-likelihood → Better model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd691af",
   "metadata": {},
   "source": [
    "##### Step 5: Comparison with Alternative Models\n",
    "If the Gamma model shows issues, alternative models include:\n",
    "\n",
    "Log-Transformed OLS:\n",
    "- If variance does not follow a Gamma structure, log-transforming processing_time and using linear regression may work.\n",
    "\n",
    "Inverse Gaussian Regression:\n",
    "- Suitable for positive skewed data with heavy tails. \n",
    "\n",
    "##### Comparing RMSE and AIC Across Models\n",
    "- Select the model with the lowest AIC and RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eccf0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Comparison with Alternative Models\n",
    "\n",
    "# Alternative: Log-Transformed Linear Model\n",
    "df['log_processing_time'] = np.log(df['processing_time'])\n",
    "lm_model = smf.ols(\"log_processing_time ~ transaction_size + num_steps\", data=df).fit()\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_lm = np.sqrt(mean_squared_error(y_true, np.exp(lm_model.predict())))\n",
    "\n",
    "# AIC values\n",
    "aic_lm = lm_model.aic\n",
    "\n",
    "# Print comparison\n",
    "print(f\"Gamma Model - RMSE: {rmse}, AIC: {aic}\")\n",
    "print(f\"Log-Transformed OLS Model - RMSE: {rmse_lm}, AIC: {aic_lm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d13a0",
   "metadata": {},
   "source": [
    "Interpreting Model Results\n",
    "From the Gamma model summary:\n",
    "- Coefficients (β values) indicate how much processing time changes with predictors.\n",
    "- Exp(β) gives the percentage increase per unit increase of a predictor.\n",
    "\n",
    "Interpretation\n",
    "- If transaction_size has exp(β) = 1.05, a 1-unit increase increases processing time by 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f2b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(gamma_model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f382cd",
   "metadata": {},
   "source": [
    "# Classification via Mathematics Functions\n",
    "\n",
    "Classification Using the Equation of a Straight Line\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Begin with the Equation of a Line: The general equation of a straight line in a 2D plane is:\n",
    "\n",
    "$$𝑦 = 𝑚 \\times 𝑥 + 𝑐 $$\n",
    "\n",
    "- m: Slope of the line (how steep it is)\n",
    "- c: Intercept (where the line crosses the y-axis)\n",
    "\n",
    "2. Connect it to Classification:\n",
    "- In binary classification, the goal is to separate two classes (e.g., Class 0 and Class 1).\n",
    "- The equation of a line can act as a decision boundary: \n",
    "    - points on one side of the line belong to Class 0, while \n",
    "    - points on the other side belong to Class 1.\n",
    "\n",
    "3. Interactive Example: \n",
    "- Imagine a dataset with two features, $𝑥_1$ and $𝑥_2$\n",
    "\n",
    "For simplicity:\n",
    "- $𝑥_1$: Horizontal axis\n",
    "- $𝑥_2$: Vertical axis\n",
    "\n",
    "A simple decision boundary can be represented as:\n",
    "$$ 𝑥_2 = 𝑚 \\times 𝑥_1 + 𝑐 $$\n",
    "\n",
    "**How the Slope (m) and Intercept (c) Influence the Boundary**\n",
    "\n",
    "The slope and intercept determine the orientation and position of the decision boundary in the feature space.\n",
    "\n",
    "- Slope (m):\n",
    "    - Controls the steepness or angle of the line.\n",
    "    - A larger absolute value of m means the line is steeper; a smaller absolute value means it is flatter.\n",
    "    - Example: In $ 𝑥_2 = 𝑚 \\times 𝑥_1 + 𝑐 $\n",
    "        - If m > 0, the line slopes upward.\n",
    "        - If m < 0, the line slopes downward.\n",
    "        - If m = 0, the line is horizontal.\n",
    "- Intercept (c):\n",
    "    - Determines where the line crosses the $𝑥_2$ (vertical) axis.\n",
    "    - Changing c shifts the line up or down without changing its slope.\n",
    "    - Example: If c=1, the line crosses the x_2 axis at 1.\n",
    "\n",
    "Together, m and c define how the decision boundary separates the feature space. Adjusting these values can change which points fall into Class 0 or Class 1.\n",
    "\n",
    "4. Decision Boundary in Classification: Modify the equation to reflect classification logic:\n",
    "$$ 𝑥_2 - 𝑚 \\times 𝑥_1 - 𝑐 = 0 $$\n",
    "\n",
    "- Points where this equation equals 0 lie exactly on the line.\n",
    "- Points where $ 𝑥_2 - 𝑚 \\times 𝑥_1 - 𝑐 > 0 $ belong to Class 1.\n",
    "- Points where $ 𝑥_2 - 𝑚 \\times 𝑥_1 - 𝑐 < 0 $ belong to Class 0.\n",
    "\n",
    "5. Visualization: Plot this line on a 2D plane with some example data points:\n",
    "- Red points for Class 0\n",
    "- Blue points for Class 1\n",
    "- The line $𝑦 = 𝑚 \\times 𝑥 + 𝑐 $ separates the two clesses\n",
    "\n",
    "6. Extend to Higher Dimensions: In higher dimensions, the decision boundary becomes a hyperplane:\n",
    "\n",
    "$$ w_1𝑥_1 + w_2𝑥_2 + ... + w_n𝑥_n + b = 0 $$\n",
    "\n",
    "- Where: \n",
    "    - $w_1, w_2, ..., w_n$ are weights (equivalent to slopes) and\n",
    "    - $𝑏$ is the intercept.\n",
    "\n",
    "**What Happens When the Data Points Overlap Significantly?**\n",
    "\n",
    "When data points from different classes overlap, the decision boundary may not cleanly separate the two classes, leading to misclassification. Here’s what happens:\n",
    "\n",
    "Misclassification:\n",
    "- Points from one class appear on the \"wrong\" side of the decision boundary.\n",
    "- This results in a classification error (false positives or false negatives).\n",
    "\n",
    "Impact on Model:\n",
    "- A linear decision boundary (a straight line) may not be flexible enough to separate overlapping or complex distributions.\n",
    "- Performance metrics like accuracy, precision, and recall can degrade.\n",
    "\n",
    "Example Scenario:\n",
    "- Consider a dataset where the two classes form concentric circles. A straight-line boundary cannot separate the classes, leading to significant misclassification.\n",
    "\n",
    "**Transition from Linear to Non-Linear Decision Boundaries**\n",
    "\n",
    "Linear decision boundaries work well when data is linearly separable. However, real-world data is often complex, requiring non-linear boundaries. Here’s how we transition:\n",
    "\n",
    "Extend the Feature Space:\n",
    "- Use techniques like polynomial features to introduce non-linear relationships.\n",
    "    - where $𝑥_1$ and $𝑥_2$ can be transformed to:\n",
    "        - $𝑥^2_1$ and $𝑥^2_2$\n",
    "        - $𝑥_1 \\times 𝑥_2$\n",
    "- The linear classifier now operates in this transformed space, creating a non-linear boundary in the original feature space.\n",
    "\n",
    "1. Kernel Methods (e.g., in SVMs):\n",
    "- Apply kernel functions like RBF (Radial Basis Function) to map data into a higher-dimensional space where it is linearly separable.\n",
    "- The decision boundary in the original space appears non-linear.\n",
    "\n",
    "2. Neural Networks:\n",
    "- Multi-layer perceptrons (MLPs) can learn complex, non-linear decision boundaries by stacking layers of non-linear activation functions.\n",
    "- Neural networks are particularly powerful for high-dimensional and unstructured data.\n",
    "\n",
    "3. Ensemble Models:\n",
    "- Techniques like random forests or gradient boosting combine multiple weak learners to create flexible decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "np.random.seed(0)\n",
    "x1_class0 = np.random.rand(50)\n",
    "x2_class0 = 2 * x1_class0 + 0.5 + np.random.normal(0, 0.1, 50)\n",
    "x1_class1 = np.random.rand(50)\n",
    "x2_class1 = 2 * x1_class1 - 0.5 + np.random.normal(0, 0.1, 50)\n",
    "\n",
    "# Equation of line: x2 = m*x1 + c\n",
    "m = 2  # slope\n",
    "c = 0  # intercept\n",
    "x_line = np.linspace(0, 1, 100)\n",
    "y_line = m * x_line + c\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x1_class0, x2_class0, color='red', label='Class 0')\n",
    "plt.scatter(x1_class1, x2_class1, color='blue', label='Class 1')\n",
    "plt.plot(x_line, y_line, color='black', label='Decision Boundary')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend()\n",
    "plt.title('Linear Decision Boundary for Classification')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b6d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Generate non-linear dataset\n",
    "X, y = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=0)\n",
    "\n",
    "# Plot raw data\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Class 0')\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Class 1')\n",
    "plt.title('Non-linear Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Linear decision boundary (fails for non-linear data)\n",
    "linear_svm = SVC(kernel='linear', C=1)\n",
    "linear_svm.fit(X, y)\n",
    "\n",
    "# Non-linear decision boundary using kernel trick\n",
    "nonlinear_svm = SVC(kernel='rbf', C=1, gamma=2)\n",
    "nonlinear_svm.fit(X, y)\n",
    "\n",
    "# Visualize decision boundaries\n",
    "def plot_decision_boundary(clf, X, y, title):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', edgecolor='k', label='Class 0')\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', edgecolor='k', label='Class 1')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Linear decision boundary\n",
    "plot_decision_boundary(linear_svm, X, y, title='Linear Decision Boundary (Fails)')\n",
    "\n",
    "# Non-linear decision boundary\n",
    "plot_decision_boundary(nonlinear_svm, X, y, title='Non-linear Decision Boundary (Succeeds)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667bf3f6",
   "metadata": {},
   "source": [
    "### **Linear Discriminant Analysis (LDA)**\n",
    "Linear Discriminant Analysis (LDA) is a classification technique that uses a linear combination of features to separate classes. \n",
    "\n",
    "It assumes:\n",
    "- The data within each class is normally distributed.\n",
    "- The covariance of each class is identical (homoscedasticity).\n",
    "\n",
    "LDA works by finding a linear decision boundary that maximizes the separation between classes.\n",
    "\n",
    "Goal:\n",
    "- Project data onto a lower-dimensional space (usually 1D for binary classification).\n",
    "- Maximize the distance between class means while minimizing the variance within each class.\n",
    "\n",
    "**Calculating the best values for the parameters of a linear discriminant**\n",
    "- Estimate the coefficients that define the linear decision boundary based on your dataset.\n",
    "- In Linear Discriminant Analysis (LDA), these coefficients are derived by maximizing the separation between the means of the classes while minimizing the variance within each class.\n",
    "\n",
    "Steps:\n",
    "1. Define the Linear Discriminant Function\n",
    "\n",
    "The linear discriminant function for binary classification can be written as:\n",
    "\n",
    "$$ y = w_0 + w_1𝑥_1 + w_2𝑥_2 + ... + w_d𝑥_d $$\n",
    "\n",
    "- where:\n",
    "    - $w_0$: Intercept (bias term).\n",
    "    - $w_1, w_2, ..., w_d$: Coefficients for each feature $x_1, x_2, ..., x_d$\n",
    "    - y: The decision score. A threshold is applied to classify points.\n",
    "\n",
    "2. Estimate Class Statistics\n",
    "\n",
    "To compute the parameters, you first need the following statistics from the data:\n",
    "\n",
    "- Compute Class Means ($\\mu_0 and \\mu_1): Calculate the mean vector for each class.\n",
    "    - for each class $C_0 and C_1$\n",
    "$$ \\mu_k = \\frac{1}{N_k} \\sum_{x \\in C_i} x $$\n",
    "- where \n",
    "    - $N_k$ is the number of instances in class k.\n",
    "\n",
    "- Compute Pooled Covariance Matrix ($𝑆_𝑤$):\n",
    "    - Within-Class Scatter Matrix ($𝑆_𝑤$): Measures the spread of points within each class.\n",
    "$$ S_w = \\sum^{[c]}_{i = 1} \\sum_{x \\in C_i} (x - \\mu_i)(x - \\mu_i)^T $$\n",
    "    - Divide by the total number of samples (N) to get the pooled covariance matrix.\n",
    "- Compute Between-Class Scatter Matrix ($𝑆_b$): Measures the separation between class means.\n",
    "$$ S_b = \\sum^{[c]}_{i = 1} N_i (\\mu_i - \\mu)(\\mu_i - \\mu)^T $$\n",
    "- where \n",
    "    - $N_i$ is the number of samples in class i\n",
    "    - $\\mu$ is the overall mean.\n",
    "\n",
    "- Prior Probabilities (P($C_0$) and P($C_1$)):\n",
    "    - These are the proportions of each class in the dataset:\n",
    "$$ P(C_k) = \\frac{N_k}{N} $$\n",
    "\n",
    "3. Compute the Parameters\n",
    "\n",
    "- Find Optimal Projection: Solve the eigenvalue problem for $S^{-1}_w S_b$, and select the eigenvector with the largest eigenvalue.\n",
    "    - The parameters of the discriminant function are calculated as follows:\n",
    "        - Linear Coefficients ($w$):\n",
    "$$ w = S^{-1}_w (\\mu_1 - \\mu_0)$$\n",
    "- where:\n",
    "    - $S^{-1}_w$ is the inverse of the pooled covariance matrix.\n",
    "\n",
    "        - Intercept($w_0$):\n",
    "$$ w_0 = -\\frac{1}{2} (\\mu^T_1 S^{-1}_w \\mu_1 - \\mu^T_0 S^{-1}_w \\mu_0 ) + ln \\frac{P(C_1)}{P(C_0)}$$\n",
    "\n",
    "4. Predict Class Labels (Decision Rule:)\n",
    "\n",
    "- Project data points onto the linear discriminant.\n",
    "    - For a new instance 𝑥, compute the linear discriminant score:\n",
    "$$ y = w_0 + w^T x$$\n",
    "\n",
    "- Use a threshold (e.g., midpoint between means) to classify points.\n",
    "- Classify based on the threshold (usually $y> 0 \\Rightarrow C_1 , y \\leq 0 \\Rightarrow C_0$ )\n",
    "\n",
    "**Interpretation of Parameters**\n",
    "\n",
    "Linear Coefficients (𝑤):\n",
    "- Feature Weight Represent?\n",
    "    - A feature weight (coefficient) indicates the change in the predicted outcome associated with a unit change in the feature, keeping all other features constant.\n",
    "    - These determine how much each feature contributes to the decision boundary.\n",
    "\n",
    "- A larger magnitude of $𝑤_𝑖$ means the corresponding feature $𝑥_𝑖$ has more influence.\n",
    "    -  Interpreting the Magnitude (Absolute Magnitude):\n",
    "        - Larger Magnitudes: Indicate that a feature has a stronger effect on the outcome.\n",
    "        - Smaller Magnitudes: Suggest that the feature has less influence on the outcome.\n",
    "    - Interpreting the Magnitude (Positive or Negative Sign)\n",
    "        - Positive Weight: Indicates a positive relationship between the feature and the outcome.\n",
    "        - Negative Weight: Indicates a negative relationship between the feature and the outcome.\n",
    "\n",
    "- Impact of Scaling on Magnitudes\n",
    "    - Feature magnitudes are meaningful only if the features are on the same scale. If features differ in scale:\n",
    "        - Larger scales will lead to larger coefficients, even if the feature has less relative importance.\n",
    "        - Standardizing or normalizing the features (e.g., using z-scores or min-max scaling) ensures that coefficient magnitudes are comparable.\n",
    "\n",
    "Linear Regression Example\n",
    "- Model: Predict house price (y) using square footage ($𝑥_1$) and number of bedrooms ($𝑥_2$):\n",
    "\n",
    "$$ y = w_0 + w_1𝑥_1 + w_2𝑥_2 $$\n",
    "$$ y = 50 + 300𝑥_1 + 10000𝑥_2 $$\n",
    "\n",
    "- Interpretation:\n",
    "    - $w_1$ = 300: Increasing square footage by 1 unit increases the house price by R300.\n",
    "    - $w_2$ = 10,000: Adding one bedroom increases the house price by R10,000.\n",
    "\n",
    "1. Interpretation of Weight Magnitude in Logistic Regression\n",
    "- In logistic regression, weights do not directly represent the change in the outcome but the log-odds of the outcome.\n",
    "\n",
    "$$ log(\\frac{P(y = 1)}{P(y = 0)}) = w_0 + w_1𝑥_1 + w_2𝑥_2 + ... + w_d𝑥_d $$\n",
    "\n",
    "- Exponentiated coefficients ($e^{w_i}$) indicate the multiplicative effect on the odds for a unit change in $x_i$\n",
    "\n",
    "Logitic Regression Example\n",
    "- Model: Predict customer churn (y) based on monthly charges ($𝑥_1$) and contract length ($𝑥_2$):\n",
    "\n",
    "$$ log(\\frac{P(y = 1)}{P(y = 0)}) = w_0 + w_1𝑥_1 + w_2𝑥_2 $$\n",
    "$$ log(\\frac{P(y = 1)}{P(y = 0)}) = -3 + 0.05𝑥_1 + 2𝑥_2 $$\n",
    "\n",
    "- Interpretation:\n",
    "    - $w_1$ = 0.05: for every R1 increase in monthly charges, the log-odds of churn increase by 0.05.\n",
    "    - $w_2$ = 10,000: For each additional month of contract length, the log-odds of churn increase by 2.\n",
    "\n",
    "2. Interpretation of Weight Magnitude in Regularized Models (Lasso and Ridge)\n",
    "- Coefficients may be shrunk or set to zero based on regularization strength, which impacts their magnitude.\n",
    "- Regularization ensures that larger weights correspond to truly important features.\n",
    "\n",
    "Intercept ($𝑤_0$):\n",
    "- Adjusts the position of the decision boundary.\n",
    "\n",
    "Decision Rule:\n",
    "- $y>0$: Class 1.\n",
    "- $y≤0$: Class 0.\n",
    "\n",
    "Considerations for interpretations\n",
    "\n",
    "Multicollinearity:\n",
    "- If features are highly correlated, the magnitude of weights can become unstable and misleading.\n",
    "    - Techniques like Variance Inflation Factor (VIF) or regularization can mitigate this.\n",
    "\n",
    "Standardization:\n",
    "- Always standardize features to ensure meaningful comparisons between coefficients.\n",
    "\n",
    "Model-Specific Meaning:\n",
    "- Interpretations vary slightly across linear regression, logistic regression, and other models.\n",
    "    - In logistic regression, remember that coefficients affect the log-odds, not the raw probabilities.\n",
    "\n",
    "**Interpretation of Results**\n",
    "\n",
    "Confusion Matrix and Classification Report:\n",
    "- The confusion matrix indicates true positives, true negatives, false positives, and false negatives.\n",
    "- The classification report shows metrics like precision, recall, F1-score, and accuracy.\n",
    "\n",
    "Decision Boundary:\n",
    "- The plot shows the LDA decision boundary, which is linear. \n",
    "    - It separates the two classes by maximizing the ratio of between-class variance to within-class variance.\n",
    "- Data points on either side of the boundary are classified into their respective classes.\n",
    "\n",
    "**Assumptions and Limitations:**\n",
    "\n",
    "Assumptions:\n",
    "- Classes have a normal distribution.\n",
    "- Classes share the same covariance matrix.\n",
    "\n",
    "Limitations:\n",
    "- LDA struggles with non-linear boundaries or when the assumptions of normality and homoscedasticity are violated.\n",
    "\n",
    "**When to Use LDA**\n",
    "\n",
    "Advantages:\n",
    "- Works well when the data satisfies its assumptions.\n",
    "- Provides interpretable results with clear decision boundaries.\n",
    "\n",
    "Use Cases:\n",
    "- Medical diagnosis (e.g., distinguishing between disease states).\n",
    "- Marketing (e.g., classifying customer preferences).\n",
    "- Text classification (when transformed into vector space).\n",
    "\n",
    "Not Suitable:\n",
    "- When classes are non-linearly separable (use non-linear methods like quadratic discriminant analysis or kernel methods in such cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746e08a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_informative=2, n_redundant=0,\n",
    "                           n_clusters_per_class=1, class_sep=2, random_state=42)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Class 0')\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Class 1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Synthetic Dataset')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Apply LDA\n",
    "lda = LDA()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lda.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Visualize decision boundary\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\n",
    "plt.scatter(X_test[y_test == 0][:, 0], X_test[y_test == 0][:, 1], color='red', edgecolor='k', label='Class 0')\n",
    "plt.scatter(X_test[y_test == 1][:, 0], X_test[y_test == 1][:, 1], color='blue', edgecolor='k', label='Class 1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('LDA Decision Boundary')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2971349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data: two classes, each with 2 features\n",
    "class_0 = np.array([[2, 3], [3, 4], [4, 5]])\n",
    "class_1 = np.array([[6, 8], [7, 9], [8, 10]])\n",
    "\n",
    "# Combine data and calculate class statistics\n",
    "X = np.vstack([class_0, class_1])\n",
    "y = np.array([0] * len(class_0) + [1] * len(class_1))\n",
    "\n",
    "# Calculate class means\n",
    "mu_0 = np.mean(class_0, axis=0)\n",
    "mu_1 = np.mean(class_1, axis=0)\n",
    "\n",
    "# Calculate within-class scatter matrix\n",
    "S_w = np.zeros((X.shape[1], X.shape[1]))\n",
    "for xi in class_0:\n",
    "    S_w += np.outer(xi - mu_0, xi - mu_0)\n",
    "for xi in class_1:\n",
    "    S_w += np.outer(xi - mu_1, xi - mu_1)\n",
    "\n",
    "# Calculate linear coefficients\n",
    "w = np.linalg.inv(S_w).dot(mu_1 - mu_0)\n",
    "\n",
    "# Calculate intercept\n",
    "prior_0 = len(class_0) / len(X)\n",
    "prior_1 = len(class_1) / len(X)\n",
    "intercept = -0.5 * (mu_1.T @ np.linalg.inv(S_w) @ mu_1 - mu_0.T @ np.linalg.inv(S_w) @ mu_0) + np.log(prior_1 / prior_0)\n",
    "\n",
    "# Display results\n",
    "print(\"Linear Coefficients (w):\", w)\n",
    "print(\"Intercept (w0):\", intercept)\n",
    "\n",
    "# Predict for a new sample\n",
    "sample = np.array([5, 6])\n",
    "decision_score = intercept + w.T.dot(sample)\n",
    "prediction = 1 if decision_score > 0 else 0\n",
    "print(\"Prediction for sample {}: Class {}\".format(sample, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838246d",
   "metadata": {},
   "source": [
    "### Optimizing the Objective Function in a Linear Discriminant Model\n",
    "\n",
    "Objective of a Linear Discriminant Analysis (LDA) model is to:\n",
    "- Find a linear combination of features that best separates two or more classes. This is achieved by \n",
    "    - optimizing an objective function that \n",
    "        - maximizes the separation between classes while \n",
    "        - minimizing the spread (variance) within each class.\n",
    "\n",
    "##### **Objective Function of LDA**\n",
    "The objective function in LDA is based on two key matrices:\n",
    "1. Between-Class Variance ($S_B$):\n",
    "    - Measures the separation between the class means.\n",
    "    - Defined as:\n",
    "$$ S_B = \\sum^k_{i= 1} n_i (\\mu - \\mu)(\\mu_i - \\mu)^T$$\n",
    "\n",
    "- where:\n",
    "    - k:Number of classes.\n",
    "    - $n_i$: Number of instances in class i\n",
    "    - $\\mu_i$: Mean vector of class i\n",
    "    - $\\mu$: Overall mean vector.\n",
    "\n",
    "2. Within-Class Variance ($S_W$): \n",
    "- Measures the spread of data points within each class.\n",
    "Defined as:\n",
    "$$ S_B = \\sum^k_{i= 1} \\sum^k_{x \\in C_1} (\\mu - \\mu)(\\mu_i - \\mu)^T$$\n",
    "\n",
    "- where:\n",
    "    - $𝐶_𝑖$ represents all instances belonging to class i.\n",
    "\n",
    "The objective function to optimize in LDA is:\n",
    "\n",
    "$$ J(w) = \\frac{w^T S_B w}{w^T S_W w}$$\n",
    "\n",
    "- Where:\n",
    "    - w : is the weight vector that defines the linear discriminant\n",
    "\n",
    "##### **Optimizing the Objective Function**\n",
    "To maximize J(w):\n",
    "1. Solve the generalized eigenvalue problem:\n",
    "$$ S^-1_W S_Bw = \\lambda w$$\n",
    "\n",
    "- Where:\n",
    "    - $\\lambda$ is the eigenvalue and \n",
    "    - w is the eigenvector.\n",
    "\n",
    "2. Select the eigenvector corresponding to the largest eigenvalue $\\lambda_1$,  as it maximizes the class separation.\n",
    "\n",
    "3. For multiclass problems, select the top k-1 eigenvectors (for k classes) to project data into a lower-dimensional space with maximum discrimination. \n",
    "\n",
    "### Scoring and Ranking Instances\n",
    "Once the linear discriminant function is computed, it can be used to score and rank instances as follows:\n",
    "\n",
    "Scoring:\n",
    "- The discriminant score for an instance x is calculated as:\n",
    "\n",
    "$$ y = w^T x + b$$\n",
    "\n",
    "- Where\n",
    "    - w is the optimized weight vector.\n",
    "    - b is the intercept (bias term).\n",
    "    - y is the scalar discriminant score.\n",
    "\n",
    "- The score indicates how far x lies from the decision boundary:\n",
    "    - Positive scores suggest the instance is likely to belong to one class.\n",
    "    - Negative scores suggest the instance is likely to belong to the other class.\n",
    "\n",
    "Ranking:\n",
    "- Instances can be ranked based on their discriminant scores y.\n",
    "    - Larger absolute scores indicate greater confidence in classification.\n",
    "    - Instances closer to zero are near the decision boundary, indicating uncertainty.\n",
    "\n",
    "##### Practical Example\n",
    "\n",
    "Given Dataset\n",
    "Suppose you have two classes (Class A and Class B) and two features $x_1, x_2$\n",
    "\n",
    "Steps to Optimize and Use the Objective Function:\n",
    "1. Compute Class Means:\n",
    "- $\\mu_A and \\mu_B$ are the mean vectors for Class A and Class B.\n",
    "- $\\mu$ is the overall mean.\n",
    "\n",
    "2. Compute Variance Matrices:\n",
    "- Calculate $S_B, S_W$\n",
    "\n",
    "3. Solve for w:\n",
    "- Find the eigenvector corresponding to the largest eigenvalue of $S^-1_W S_b$\n",
    "\n",
    "4. Calculate Scores:\n",
    "- For each instance $x_i$, compute the discriminant score:\n",
    "$$ y_i = w^T x_i + b$$\n",
    "\n",
    "5. Rank Instances:\n",
    "- Sort instances by their discriminant scores to rank them by their likelihood of belonging to a specific class\n",
    "\n",
    "__________________\n",
    "\n",
    "Disclaimer: `decision_function()` method comes from specific machine learning models in libraries like scikit-learn, and it is used to compute the distance of a sample to the decision boundary in classification tasks. \n",
    "- This function is particularly useful in models that rely on decision boundaries, such as \n",
    "    - Linear Discriminant Analysis (LDA), \n",
    "    - Support Vector Machines (SVM),\n",
    "    - Logistic Regression.\n",
    "\n",
    "What Does `decision_function()` Return?\n",
    "\n",
    "Binary Classification (2 Classes):\n",
    "- Returns a 1D array of scores where each score indicates the distance of the instance from the decision boundary.\n",
    "    - Positive scores suggest one class (e.g., Class 1), and negative scores suggest the other class (e.g., Class 0).\n",
    "\n",
    "Multiclass Classification (More than 2 Classes):\n",
    "- Returns a 2D array of scores (one score per class for each instance).\n",
    "    - The classifier assigns a class label based on the highest score.\n",
    "\n",
    "Why Use decision_function()?\n",
    "- To understand the confidence of predictions.\n",
    "- To enable custom ranking or thresholding based on discriminant scores.\n",
    "- To analyze how far instances are from the boundary, providing insight into borderline cases.\n",
    "\n",
    "Use `decision_function()` in Ranking and Thresholding\n",
    "\n",
    "- Ranking: Instances can be ranked by their scores. \n",
    "    - Higher absolute values indicate greater confidence in classification.\n",
    "- Thresholding: The decision scores can be used to apply custom thresholds to refine classification decisions.\n",
    "\n",
    "Decision Boundary\n",
    "\n",
    "In the case of Linear Discriminant Analysis:\n",
    "- The decision boundary corresponds to where the decision_function() outputs 0.\n",
    "- This boundary is a hyperplane that separates the feature space into regions corresponding to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Example data\n",
    "X = np.array([[2, 3], [3, 5], [5, 7], [6, 8], [8, 10], [9, 12]])  # Features\n",
    "y = np.array([0, 0, 0, 1, 1, 1])  # Labels (0: Class A, 1: Class B)\n",
    "\n",
    "# Fit LDA model\n",
    "lda = LDA()\n",
    "lda.fit(X, y)\n",
    "\n",
    "# Compute discriminant scores\n",
    "scores = lda.decision_function(X)\n",
    "\n",
    "# Print scores and rankings\n",
    "print(\"Discriminant Scores:\", scores)\n",
    "print(\"Ranking of Instances:\", np.argsort(-scores))  # Descending order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb45c1",
   "metadata": {},
   "source": [
    "Interpretation of Scores and Rankings\n",
    "\n",
    "Discriminant Scores:\n",
    "- Positive scores suggest membership in Class 1.\n",
    "- Negative scores suggest membership in Class 0.\n",
    "\n",
    "Rankings:\n",
    "- Instances with higher absolute scores are ranked higher, as the classifier is more confident in their classification.\n",
    "\n",
    "Decision Boundary:\n",
    "- The boundary is where the discriminant score y = 0\n",
    "\n",
    "### Analyzing the relationship between the distance from the decision boundary of a linear discriminant and the likelihood of response\n",
    "Helps us understanding how confident the model is in its predictions.\n",
    "\n",
    "- The distance from the decision boundary (discriminant score) relates directly to the confidence in classification.\n",
    "- Scores are transformed into posterior probabilities using logistic (binary) or softmax (multiclass) functions.\n",
    "- These probabilities are interpretable as the likelihood of response and can be used for scoring, ranking, and applying thresholds for decision-making.\n",
    "\n",
    "##### Theoretical Relationship : Distance from the decision boundary\n",
    "\n",
    "The decision boundary in a Linear Discriminant Analysis (LDA) separates classes by \n",
    "- maximizing the distance between class means while \n",
    "- minimizing variance within each class. \n",
    "\n",
    "The discriminant score $y = w^T x + b$ represents the signed distance of an instance x from the decision boundary:\n",
    "- Positive scores indicate the instance is classified into one class (e.g., Class 1).\n",
    "- Negative scores indicate the instance is classified into the other class (e.g., Class 0).\n",
    "\n",
    "The magnitude of the score reflects the confidence in classification:\n",
    "- Larger absolute values imply that the instance is far from the decision boundary and thus more confidently classified.\n",
    "- Smaller absolute values (near zero) indicate that the instance is close to the boundary, suggesting uncertainty.\n",
    "\n",
    "##### Likelihood of Response\n",
    "In LDA, we can link the discriminant score to the posterior probability of a class, which represents the likelihood of the instance belonging to that class:\n",
    "\n",
    "$$ P(C_k | x) = \\frac{exp(y_k)}{\\sum^K_{j = 1} exp(y_i)} $$\n",
    "\n",
    "- Where: \n",
    "    - $P(C_k | x)$: is the posterior probability for class k.\n",
    "    - $ y_k = w^T x_i + b_k$: is the discriminant score for class k.\n",
    "    - The denominator is the normalization factor across all classes K.\n",
    "\n",
    "The posterior probability serves as a soft classification metric:\n",
    "- Probabilities closer to 1 indicate high confidence.\n",
    "- Probabilities closer to 0.5 (in a binary classification) indicate uncertainty.\n",
    "\n",
    "#####  Practical Example\n",
    "Let’s calculate the relationship between discriminant scores and posterior probabilities for a **Binary classification**.\n",
    "\n",
    "Example Dataset\n",
    "\n",
    "Suppose we have a binary classification problem with discriminant scores:\n",
    "$$y=[2.0,0.5,0.0,−0.5,−2.0]$$\n",
    "\n",
    "We can compute the posterior probabilities using the logistic function:\n",
    "$$ P(C_1 | x) = \\frac{1}{1 + exp(-y)} $$\n",
    "\n",
    "_______________\n",
    "\n",
    "Generalization to **Multiclass Classification**\n",
    "- In multiclass problems, the discriminant scores $𝑦_𝑘$ are normalized using the softmax function to compute posterior probabilities for each class:\n",
    "\n",
    "$$ P(C_k | x) = \\frac{exp(y_k)}{\\sum^K_{j = 1} exp(y_i)} $$\n",
    "\n",
    "- And the class with the highest posterior probability is the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37545e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Discriminant scores\n",
    "scores = np.array([2.0, 0.5, 0.0, -0.5, -2.0])\n",
    "\n",
    "# Compute posterior probabilities using the logistic function\n",
    "posterior_probabilities = 1 / (1 + np.exp(-scores))\n",
    "\n",
    "# Print results\n",
    "print(\"Scores:\", scores)\n",
    "print(\"Posterior Probabilities:\", posterior_probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88fcac1",
   "metadata": {},
   "source": [
    "Interpretation of Results\n",
    "\n",
    "1. Scores Far from Zero:\n",
    "- y=2.0: High confidence in Class 1 ($P(C_1∣x)=0.88$).\n",
    "- y=−2.0: High confidence in Class 0 ($P(C_0∣x)=0.88$).\n",
    "\n",
    "2. Scores Near Zero:\n",
    "- y=0.0: The posterior probability is 0.5, indicating complete uncertainty.\n",
    "\n",
    "3. Intermediate Scores:\n",
    "- y=0.5: Moderately confident in Class 1 ($P(C_1∣x)=0.62$).\n",
    "- y=−0.5: Moderately confident in Class 0 ($P(C_0∣x)=0.62$).\n",
    "\n",
    "Insights\n",
    "\n",
    "Distance and Likelihood:\n",
    "- Instances farther from the boundary (large ∣y∣) have posterior probabilities close to 0 or 1, indicating higher confidence in classification.\n",
    "- Instances near the boundary (y≈0) have probabilities close to 0.5, indicating uncertainty.\n",
    "\n",
    "Scoring and Ranking:\n",
    "- By sorting instances based on posterior probabilities, you can rank them in terms of likelihood of response (e.g., likelihood of belonging to Class 1).\n",
    "\n",
    "Visualization\n",
    "To better understand the relationship, plot the discriminant score against the posterior probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd92fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot scores vs posterior probabilities\n",
    "plt.plot(scores, posterior_probabilities, marker='o')\n",
    "plt.axvline(0, color='gray', linestyle='--', label='Decision Boundary')\n",
    "plt.title('Discriminant Score vs Posterior Probability')\n",
    "plt.xlabel('Discriminant Score (y)')\n",
    "plt.ylabel('Posterior Probability')\n",
    "plt.legend(['Scores', 'Decision Boundary'])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23090e15",
   "metadata": {},
   "source": [
    "### Understanding Decision Boundaries in Depth\n",
    "\n",
    "Decision boundaries are surfaces (lines, planes, or hypersurfaces) that separate data points into different classes in a feature space.\n",
    "- These boundaries are derived based on the decision rules of a classifier, and they indicate the regions where the classifier predicts different outcomes.\n",
    "\n",
    "##### Decision Boundaries in 2D\n",
    "In 2D space, the decision boundary is a \n",
    "- line (for linear classifiers) or a \n",
    "- curve (for non-linear classifiers).\n",
    "\n",
    "1. Linear Decision Boundaries\n",
    "- For a binary classification problem, a linear decision boundary is represented as:\n",
    "\n",
    "$$ w_0 + w_1𝑥_1 + w_2𝑥_2 = 0 $$\n",
    "\n",
    "- where:\n",
    "    - $ w_1, w_2$ are the coefficients.\n",
    "    - $ 𝑥_1 , 𝑥_2$ are the features.\n",
    "    - $ w_0 $  is the intercept.\n",
    "\n",
    "Separating red and blue points in a 2D space, the decision boundary is a straight line. Points on one side belong to one class, while points on the other side belong to the other class.\n",
    "\n",
    "2. Non-Linear Decision Boundaries\n",
    "- For complex data distributions, non-linear classifiers create curved decision boundaries. These are:\n",
    "    - SVM with kernel trick or \n",
    "    - neural networks \n",
    "- Example: A circular boundary might separate inner and outer regions in a concentric circle dataset.\n",
    "\n",
    "Visualization\n",
    "- The boundary is typically visualized by plotting the equation in 2D space and showing the classification regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Generate 2D data\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_informative=2, n_redundant=0, class_sep=1.5, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Create grid for decision boundary\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "# Plot data and decision boundary\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap='coolwarm')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('2D Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1d68a",
   "metadata": {},
   "source": [
    "##### Decision Boundaries in 3D\n",
    "In 3D space, the decision boundary becomes a \n",
    "- plane.\n",
    "\n",
    "1. Linear Decision Boundaries\n",
    "- Represented as:\n",
    "\n",
    "$$ w_0 + w_1𝑥_1 + w_2𝑥_2 + w_3𝑥_3= 0 $$\n",
    "\n",
    "- where:\n",
    "    - $ w_1, w_2,  w_3 $ are the coefficients.\n",
    "    - $ 𝑥_1 , 𝑥_2, 𝑥_3$ are the features.\n",
    "    - $ w_0 $  is the intercept.\n",
    "\n",
    "For the features, the plane separates the feature space into two regions for classification.\n",
    "\n",
    "2. Non-Linear Decision Boundaries\n",
    "\n",
    "- Non-linear models define curved surfaces in 3D space\n",
    "    - spheres, - parabolas.\n",
    "- Example: In 3D, the boundary might look like a bowl separating one region (inside the bowl) from another (outside the bowl).\n",
    "\n",
    "Visualization\n",
    "- Visualizing a plane or curved surface in 3D is possible with tools like Matplotlib's 3D plotting. It shows how the boundary divides the space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01975d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generate 3D data\n",
    "X = np.random.rand(200, 3)\n",
    "y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Linear decision boundary\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X, y)\n",
    "\n",
    "# Create grid for decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(0, 1, 50), np.linspace(0, 1, 50))\n",
    "zz = (-model.intercept_[0] - model.coef_[0][0] * xx - model.coef_[0][1] * yy) / model.coef_[0][2]\n",
    "\n",
    "# Plot data and decision boundary\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='coolwarm', edgecolor='k')\n",
    "ax.plot_surface(xx, yy, zz, alpha=0.5, color='gray')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_zlabel('Feature 3')\n",
    "ax.set_title('3D Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079f3b5f",
   "metadata": {},
   "source": [
    "##### Decision Boundaries in Higher Dimensions\n",
    "In higher-dimensional spaces, the decision boundary becomes a \n",
    "- hyperplane \n",
    "- more complex hypersurface.\n",
    "\n",
    "1. Linear Decision Boundaries\n",
    "- For a d-dimensional feature space, the equation is:\n",
    "$$ w_0 + w_1𝑥_1 + w_2𝑥_2 + w_3𝑥_3 + ... + w_d𝑥_d = 0 $$\n",
    "- This hyperplane divides the d-dimensional space into regions for classification.\n",
    "\n",
    "In a 4D feature space $𝑥_1, 𝑥_2, 𝑥_3, 𝑥_4$, the decision boundary is a 3D hyperplane.\n",
    "\n",
    "2. Non-Linear Decision Boundaries\n",
    "- Non-linear models use transformations (e.g., polynomial, kernel tricks) to create non-linear hypersurfaces.\n",
    "- These hypersurfaces can separate data points that are non-linearly separable in their original feature space.\n",
    "\n",
    "Visualization\n",
    "- Direct visualization becomes challenging beyond 3 dimensions. \n",
    "- However, techniques like dimensionality reduction (PCA, t-SNE, UMAP) can project high-dimensional data and decision boundaries into 2D or 3D for interpretation.\n",
    "\n",
    "**Impact of Dimension on Decision Boundaries**\n",
    "\n",
    "Curse of Dimensionality:\n",
    "- As dimensions increase, data points become sparse, making classification harder.\n",
    "- Models like LDA or logistic regression may underperform without feature selection.\n",
    "\n",
    "Model Complexity:\n",
    "- Non-linear decision boundaries require more complex models (e.g., SVM with RBF kernels, neural networks).\n",
    "- Overfitting is a significant risk in high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156965d",
   "metadata": {},
   "source": [
    "### **6. Support Vector Machines (SVM)**\n",
    "What It Means: \n",
    "- SVMs classify data by finding the best “boundary” (hyperplane) that separates classes with the widest possible margin.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Data points on either side of the boundary belong to different classes, with \"support vectors\" helping to define the boundary.\n",
    "\n",
    "Performance Measures:\n",
    "- Accuracy: Proportion of correct classifications.\n",
    "- Precision and Recall: Used when classes are imbalanced; precision is the correctness of positive predictions, and recall measures coverage.\n",
    "\n",
    "Lay Explanation: \n",
    "- SVMs are like drawing a line to separate different groups, ensuring the groups are as distinct as possible with the help of a few key points.\n",
    "\n",
    "Use Case: \n",
    "- Used for classification and regression in high-dimensional spaces, often for non-linearly separable data.\n",
    "\n",
    "### **Support Vector Machines (SVM): Key Idea**\n",
    "\n",
    "The idea behind SVM is to find the optimal hyperplane that best separates data points of different classes in the feature space.\n",
    "- This basic idea of the SVM is to separate points using a $(p - 1)$ dimensional hyperplane. \n",
    "\n",
    "What does it mean to separate points? \n",
    "- This means that the SVM will construct a decision boundary such that points on the left are assigned a label of $A$ and points on the right are assigned a label of $B$.  \n",
    "- When finding this separating hyperplane we wish to maximise the distance of the nearest points to the hyperplane. \n",
    "    - The technical term for this is **maximum separating hyperplane**.\n",
    "- The data points which dictate where the separating hyperplane goes are called **support vectors**.\n",
    "\n",
    "How It works in laymans terms:\n",
    "\n",
    "Pretend that you want to classify data points into group $A$ or group $B$. An SVM will plot your labelled training data as points in space and will:\n",
    "- look for the widest, clearest gap between points belonging to group A and points belonging to group B. \n",
    "- It will then use this newly identified dividing line (known as a hyperplane) and the margin around it to classify new observations. \n",
    "- An unseen data point will be classified into group A or B depending on which side of the margin it is closest to. \n",
    "\n",
    "##### Important Concepts in SVM\n",
    "1. Hyperplane:\n",
    "- A decision boundary that separates classes in the feature space.\n",
    "    - In 2D, it’s a line; \n",
    "        -  when your data only has 2 features. You only need a simple one-dimensional decision boundary (which is basically a line) to classify the data.\n",
    "        - line only has one dimension\n",
    "    - In 3D, it’s a plane; \n",
    "    - In higher dimensions, it’s a hyperplane.\n",
    "        - more features get added the line needs to take on more dimensions,\n",
    "        - 4 or more dimensions\n",
    "        - In SVM, the hyperplane will always have one less dimension ($-1$) than the number of input features ($p$), or a total of $(p-1)$ dimensions.\n",
    "\n",
    "2. Margin:\n",
    "- The distance between the hyperplane and the closest data points (called support vectors) of either class.\n",
    "    - SVM maximizes this margin to create the most robust separation.\n",
    "\n",
    "3. Support Vectors:\n",
    "- The data points closest to the hyperplane, which influence its position and orientation.\n",
    "4. Optimal Hyperplane:\n",
    "- The hyperplane that maximizes the margin while correctly classifying the training data (or minimizing misclassifications).\n",
    "\n",
    "Support Vector Machines in a nutshell:\n",
    "- Like logistic regression, SVMs fit a linear decision boundary. \n",
    "- Unlike logistic regression, SVMs do this in a non-proabilistic way and are able to fit to non-linear data using an algorithm known as the [kernel trick](https://en.wikipedia.org/wiki/Kernel_method).\n",
    "\n",
    "SVMs can be used for both classification and regression. In `sklearn`, these are called:\n",
    "- `SVC` (Support Vector Classifier)\n",
    "- `SVR` (Support Vector Regression) \n",
    "\n",
    "SVC can also refer to Support Vector **Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f9f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738d5af7",
   "metadata": {},
   "source": [
    "##### Generate synthetic data\n",
    "Generate a random dataset to experiment with.\n",
    "- by taking a multi-dimensional **standard normal distribution** and defining classes separated by nested concentric multi-dimensional spheres such that roughly equal numbers of\n",
    "samples are in each class (quantiles of the $\\chi^2$ distribution).\n",
    "    - generated a donut shaped dataset, where \n",
    "        - the samples belonging to one class are generally located in the centre\n",
    "        - the samples belonging to the other class are generally located in the outer ring.\n",
    "\n",
    "\n",
    "##### **Reasons for Normalizing Data in SVMs**\n",
    "- because of how the SVM algorithm calculates margins and distances between data points.\n",
    "\n",
    "SVM is Sensitive to Feature Scales\n",
    "- SVM relies on calculating distances (e.g., Euclidean distance) between data points to determine margins and support vectors. \n",
    "    - If one feature has a much larger range than others, it will dominate the distance calculation, leading to biased results.\n",
    "- Example: In a dataset with two features—age (ranging from 0 to 100) and income (ranging from 0 to 100,000)—income will heavily influence the decision boundary, even if age is equally or more important.\n",
    "\n",
    "Ensures Proper Margins\n",
    "- The SVM objective is to find the hyperplane that maximizes the margin between classes. \n",
    "    - Without normalization, the margin calculation may become skewed, resulting in suboptimal or incorrect decision boundaries.\n",
    "- Example: If one feature has a larger scale, the margin might stretch disproportionately along that dimension, ignoring other features.\n",
    "\n",
    "Improves Kernel Performance\n",
    "- SVMs often use kernels (e.g., RBF, polynomial) to project data into higher dimensions. \n",
    "    - Kernels are sensitive to the relative scaling of features. Normalization ensures that all features contribute equally to the projection.\n",
    "- Example: An RBF kernel requires well-scaled data to compute meaningful similarity measures between points. Poorly scaled data may lead to ineffective kernel computations.\n",
    "\n",
    "Reduces Convergence Time\n",
    "- SVM optimization involves iterative calculations that are influenced by feature scaling. \n",
    "    - Normalized data leads to faster and more stable convergence of the optimization algorithm.\n",
    "- Example: When features are on drastically different scales, the optimization problem may take longer to converge or fail to converge entirely.\n",
    "\n",
    "Handles Non-linear Decision Boundaries Better\n",
    "- Why? For non-linear kernels (like RBF), the distance between points in feature space influences the shape of the decision boundary. \n",
    "    - Normalization ensures these distances are meaningful, leading to smoother and more accurate decision boundaries.\n",
    "\n",
    "##### **Consequences of Not Normalizing**\n",
    "- Poor Decision Boundaries: The SVM may create biased or incorrect hyperplanes, reducing model performance.\n",
    "- Misclassification: The model may misclassify data, especially when features with large ranges dominate.\n",
    "- Kernel Inefficiency: Kernels may fail to project the data effectively, leading to poor separation of classes.\n",
    "- Increased Training Time: Optimization takes longer, impacting the efficiency of training.\n",
    "\n",
    "##### **How to Normalize Data for SVMs**\n",
    "\n",
    "1. **Standardization**: Subtract the mean and divide by the standard deviation for each feature\n",
    "$$ z = (\\frac{x - \\mu}{\\sigma})$$\n",
    "- This scales features to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "2. **Min-Max Scaling**: Rescale each feature to a fixed range, typically [0, 1] \n",
    "$$ z = (\\frac{x - min(x)}{max(x)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f9f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "# Set the feature dimensionality\n",
    "p = 2\n",
    "\n",
    "# Construct the dataset\n",
    "X, y = make_gaussian_quantiles(cov=3.,\n",
    "                                 n_samples=1000, n_features=p,\n",
    "                                 n_classes=2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca6aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training and testing data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f6727",
   "metadata": {},
   "source": [
    "##### Fit a SVM classifier with a linear decision boundary\n",
    "We are going to fit an SVC model with a `linear kernel`. This means that we are telling the SVC to fit the data using a linear decision boundary. Let's also take a look at the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a42c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "print(\"The accuracy score of the SVC is:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b8a25",
   "metadata": {},
   "source": [
    "##### Plot the decision boundary for the SVC\n",
    "When accuracy score doesn't seem very good. To help us understand what's going on use: Visualisation.\n",
    "\n",
    "The SVC calculates and implements a $p-1$ dimensional decision boundary (hyperplane) over the input features.\n",
    "- Since we are only looking at 2 features (our synthetic dataset only has two features, or $p=2$), our hyperplane will only have 1 dimension ($p-1$)\n",
    "    - look like a single line.\n",
    "- if your model has more than 2 features, you can plot the hyperplane for any 2 features you choose.\n",
    "\n",
    "##### **Calculating the Dimensions of a separating Hyperplane**\n",
    "The dimensions of a separating hyperplane depend on the number of features (or predictors) in the dataset.\n",
    "\n",
    "Definition of a Hyperplane\n",
    "\n",
    "A hyperplane in n-dimensional space is defined as:\n",
    "$$ 𝑤 \\times 𝑥 + 𝑏 = 0$$\n",
    "\n",
    "where:\n",
    "- $w = [w_1, w_2, ..., w_n]$: Weight vector normal to the hyperplane.\n",
    "- $x = [x_1, x_2, ..., x_n]$: Feature vector of an instance.\n",
    "- $b$: Bias term (offset from the origin).\n",
    "\n",
    "The hyperplane separates data into two classes:\n",
    "- $ 𝑤 \\times 𝑥 + 𝑏 > 0$: Class 1\n",
    "- $ 𝑤 \\times 𝑥 + 𝑏 < 0$: Class 2\n",
    "\n",
    "Dimensions of a Hyperplane\n",
    "\n",
    "The dimensionality of the hyperplane is determined by the number of features in the dataset:\n",
    "- If the dataset has n features, the hyperplane is an (n−1)-dimensional subspace.\n",
    "\n",
    "Examples:\n",
    "- 2 Features (2D): The hyperplane is a 1D line.\n",
    "- 3 Features (3D): The hyperplane is a 2D plane.\n",
    "- 4 Features (4D): The hyperplane is a 3D subspace (hard to visualize, but mathematically valid).\n",
    "\n",
    "Intuition Behind Dimensions\n",
    "\n",
    "- The hyperplane must divide the feature space into two regions corresponding to different classes.\n",
    "- Higher dimensions mean more complex hyperplanes, allowing SVM to handle more intricate patterns.\n",
    "- Kernels: When data is mapped to a higher-dimensional feature space using kernels (e.g., RBF), the hyperplane exists in the higher-dimensional space, though its exact dimensions depend on the kernel's transformation\n",
    "\n",
    "When Are the Dimensions Relevant?\n",
    "\n",
    "- At Training Time: The dimensions of the hyperplane are implicitly calculated when the SVM solves the optimization problem to find 𝑤 and b.\n",
    "    - The optimization ensures the hyperplane maximizes the margin between support vectors of the two classes.\n",
    "- During Prediction: The dimensionality of the hyperplane affects how data points are classified. The model computes:\n",
    "    - Decision Function: $𝑤 \\times 𝑥 + 𝑏$\n",
    "        - The sign of this value determines the predicted class.\n",
    "\n",
    "##### **Calculation of the Dimensions**\n",
    "- The dimensions are calculated implicitly when the SVM solves its optimization problem to find 𝑤 and b.\n",
    "- The dimensionality of the hyperplane is directly tied to the feature space in which the data resides.\n",
    "\n",
    "Steps:\n",
    "1. Input Data Dimension: Count the number of features n in your dataset.\n",
    "- Example: If your dataset has features $x = [x_1, x_2, x_3]$ , it’s a 3-dimensional space.\n",
    "\n",
    "2. Hyperplane Dimension: The hyperplane will have (n−1) dimensions.\n",
    "- For the 3-feature example, the hyperplane is a 2D plane.\n",
    "\n",
    "In this case, donut-shaped data is not `linearly separable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ec1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 # Feature 1\n",
    "j = 1 # Feature 2\n",
    "\n",
    "svc.fit(X[:, [i, j]], y)\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    " \n",
    "x_min, x_max = X[:, i].min(), X[:, i].max()\n",
    "y_min, y_max = X[:, j].min(), X[:, j].max()\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 1000), np.linspace(y_min, y_max, 1000))\n",
    "\n",
    "y_hat = svc.predict(np.concatenate((xx.reshape(-1,1), yy.reshape(-1,1)), axis=1))\n",
    "y_hat = y_hat.reshape(xx.shape)\n",
    "\n",
    "ax1.pcolormesh(xx, yy, y_hat, cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.scatter(X[:, i], X[:, j], c=y, edgecolors='k', cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.set_xlim(xx.min(), xx.max())\n",
    "ax1.set_ylim(yy.min(), yy.max())\n",
    "ax1.set_xticks(())\n",
    "ax1.set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c0d22",
   "metadata": {},
   "source": [
    "Solution: Use of SVM's [kernel trick](https://en.wikipedia.org/wiki/Kernel_method) to use a **non-linear** decision boundary instead.\n",
    "\n",
    "### Approaches to Find Non-linear Boundaries Using a Linear Model\n",
    "It involves transforming the input features into a higher-dimensional space where the non-linear relationships between the features can be captured. \n",
    "- This approach is fundamental to models like Support Vector Machines (SVMs) and forms the basis of the kernel trick.\n",
    "\n",
    "1. **Feature Transformation**\n",
    "- Idea: Transform the original features into new features such that the transformed space allows a linear boundary to separate the classes.\n",
    "- Example: Suppose you have two features, $x_1, x_2$, which cannot be separated linearly. You can transform these features into a higher-dimensional space:\n",
    "\n",
    "$$ \\phi (x_1, x_2) = (x_1, x_2, x^2_1, x^2_2,x_1, x_2) $$\n",
    "\n",
    "- In this new feature space, the classes might become linearly separable.\n",
    "\n",
    "2. **Adding Polynomial Features**\n",
    "- Method: Include interaction terms or higher-order terms of the features.\n",
    "- Example 1: for quadratic relationships\n",
    "$$ \\phi (x_1, x_2) = (x_1, x_2, x^2_1, x^2_2,x_1, x_2)$$\n",
    "\n",
    "- Example 2: for cubic relationships\n",
    "\n",
    "$$ \\phi (x_1, x_2) = (x_1, x_2, x^2_1, x^2_2, x_1, x_2, x^3_1, x^3_2, x_1, x^2_2)$$\n",
    "\n",
    "- Outcome: The linear model operates on these new features, effectively creating non-linear boundaries in the original space.\n",
    "\n",
    "3. **Kernel Trick**\n",
    "- Idea: Instead of explicitly transforming features into a higher-dimensional space, compute the dot product of the transformed features directly using a kernel function.\n",
    "- Common Kernels:\n",
    "    - Polynomial Kernel:\n",
    "        - Allows non-linear boundaries based on polynomial relationships of degree 𝑑.\n",
    "$$ K(x, x') = (<x, x'> + c)^d $$\n",
    "\n",
    "- Radial Basis Function (RBF) Kernel:\n",
    "    - Creates highly flexible decision boundaries that adapt to the data.\n",
    "$$ K(x, x') = exp (\\frac{||x, x'||^2}{2\\sigma^2}) $$\n",
    "\n",
    "- `It Works: The kernel trick avoids the computational cost of explicitly mapping features into high dimensions by working directly with the similarity (dot product) in the transformed space.`\n",
    "\n",
    "4. **Piecewise Linear Boundaries**\n",
    "- Idea: Create a decision boundary that is linear within small regions of the input space but collectively forms a non-linear boundary.\n",
    "- Implementation:\n",
    "    - Use decision trees or ensemble methods like random forests or gradient boosting to partition the input space into small regions.\n",
    "    - Apply a linear model within each region.\n",
    "- Outcome: The combined boundary is non-linear in the original feature space.\n",
    "\n",
    "5. **Using Basis Functions**\n",
    "- Idea: Use basis functions to project the features into a new space.\n",
    "- Example: Gaussian basis functions\n",
    "    - Where, Each basis function creates a feature that measures the proximity of x to a center $\\mu$, allowing flexible, non-linear boundaries.\n",
    "\n",
    "$$ \\phi (x) = exp (- \\frac{||x, \\mu||^2}{2\\sigma^2}) $$\n",
    "\n",
    "6. **Regularization with Non-linear Effects**\n",
    "- Idea: Use techniques like Lasso, Ridge, or Elastic Net regularization to automatically select higher-order features or interaction terms that help model non-linear boundaries.\n",
    "- Benefit: Controls overfitting while allowing non-linear effects.\n",
    "\n",
    "##### Advantages of Finding Non-linear Boundaries with Linear Models\n",
    "- Avoids the need for explicitly training complex non-linear models.\n",
    "- Exploits the mathematical simplicity of linear models for efficient computation.\n",
    "- Kernel methods allow flexibility without explicit feature engineering.\n",
    "\n",
    "##### Challenges\n",
    "- Overfitting: High-dimensional transformations can lead to overfitting if not regularized.\n",
    "- Computational Cost: Complex kernels (e.g., RBF) may increase computational time for large datasets.\n",
    "- Interpretability: Non-linear boundaries are harder to interpret than linear ones.\n",
    "\n",
    "By combining feature transformations, kernel tricks, and regularization, linear models can effectively handle non-linear relationships and create decision boundaries that adapt to complex data structures.\n",
    "\n",
    "### Fit a SVC classifier with a non-linear decision boundary\n",
    "The Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel, also known as the Gaussian kernel, is a powerful method for finding non-linear decision boundaries by mapping input features into a higher-dimensional space.\n",
    "\n",
    "Using SVM with Radial Basis Function (RBF) Kernel (Gaussian kernel)\n",
    "- Original Space: Two classes that overlap non-linearly cannot be separated by a straight line.\n",
    "- Transformed Space: The RBF kernel projects the data into a higher-dimensional space where the overlap is reduced, and a hyperplane (linear boundary) can separate the classes.\n",
    "- Decision Boundary in Original Space: The hyperplane in the transformed space translates into a non-linear boundary in the original space.\n",
    "\n",
    "##### **How the RBF Kernel Works**\n",
    "\n",
    "Radial Basis Function Definition: \n",
    "- The RBF kernel measures the similarity between two data points $x_i, x_j$\n",
    "- The Gaussian RBF kernel is defined as:\n",
    "$$ K(x, x') = exp (\\frac{||x, x'||^2}{2\\sigma^2}) $$\n",
    "- where:\n",
    "    - $||x, x'||^2$: Squared Euclidean distance between the two points.\n",
    "    - $\\sigma$: Kernel width parameter (controls the \"spread\" of the Gaussian function).\n",
    "    - $K(x, x')$: Similarity measure (ranges from 0 to 1). A value close to 1 indicates high similarity.\n",
    "\n",
    "Key Idea: \n",
    "- The RBF kernel implicitly maps the input data into an infinite-dimensional feature space where a linear decision boundary can separate the data. This allows the SVM to create highly flexible non-linear boundaries in the original space.\n",
    "\n",
    "Role of Hyperparameters:\n",
    "- C: Regularization parameter (controls the trade-off between maximizing the margin and minimizing classification error).\n",
    "- $\\gamma = \\frac{1}{2\\sigma^2}$: Kernel coefficient (controls how far the influence of a single data point reaches).\n",
    "\n",
    "**Effect of gamma on RBF**:\n",
    "- has a significant impact on the model's performance. \n",
    "- It determines how far the influence of a single data point extends, controlling the shape and flexibility of the decision boundary.\n",
    "\n",
    "Small Gamma (e.g., 0.01):\n",
    "- Wide Influence: Each support vector influences a large region of the feature space.\n",
    "- Simpler Decision Boundary: The model creates smoother, less complex decision boundaries.\n",
    "- Risk of Underfitting: If the gamma is too small, the model may fail to capture the complexity of the data, leading to poor performance.\n",
    "- Example:\n",
    "    - The decision boundary might be too broad to separate closely spaced data points.\n",
    "    - Good for datasets with large-scale structures and fewer fine details.\n",
    "\n",
    "Large Gamma (e.g., 100):\n",
    "- Narrow Influence: Each support vector influences only a small region around itself.\n",
    "- Complex Decision Boundary: The model creates highly detailed and intricate decision boundaries.\n",
    "- Risk of Overfitting: If gamma is too large, the model may memorize the training data and generalize poorly to unseen data.\n",
    "- Example:\n",
    "    - The decision boundary might wrap tightly around individual data points, leading to poor generalization.\n",
    "\n",
    "Visualizing the Effect of Gamma\n",
    "\n",
    "Here is a comparison of decision boundaries with different gamma values:\n",
    "- Low Gamma: Broad decision regions; the model captures global patterns but misses local variations.\n",
    "- High Gamma: Sharp, detailed decision regions; the model captures local variations but may overfit noise in the data.\n",
    "\n",
    "Optimal Gamma: Bias-Variance Tradeoff\n",
    "- Small Gamma: High bias, low variance.\n",
    "- Large Gamma: Low bias, high variance.\n",
    "- The optimal gamma strikes a balance, allowing the model to generalize well without overfitting or underfitting.\n",
    "\n",
    "Optimization: \n",
    "- The SVM finds the hyperplane in the transformed space that maximizes the margin while minimizing misclassifications (via hinge loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ceb488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "\n",
    "# Visualize data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "plt.title(\"Dataset: Moons\")\n",
    "plt.show()\n",
    "\n",
    "# Define gamma values to test\n",
    "gamma_values = [0.01, 0.1, 1, 10]\n",
    "\n",
    "# Plot decision boundaries for different gamma values\n",
    "fig, axes = plt.subplots(1, len(gamma_values), figsize=(20, 5))\n",
    "\n",
    "for i, gamma in enumerate(gamma_values):\n",
    "    svm = SVC(kernel='rbf', gamma=gamma, C=1.0)\n",
    "    svm.fit(X, y)\n",
    "    \n",
    "    # Decision boundary\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[i].contourf(xx, yy, Z, levels=20, cmap='viridis', alpha=0.7)\n",
    "    axes[i].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "    axes[i].set_title(f\"Gamma: {gamma}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8c2d39",
   "metadata": {},
   "source": [
    "##### Use the rbf kernel (Radial_basis_function_kernel), which allows the SVC to fit a non-linear decision boundary from srcarch.\n",
    "\n",
    "Steps to Use SVM with RBF Kernel\n",
    "1. Choose Hyperparameters:\n",
    "- Start with default values of\n",
    "    - C\n",
    "    - $\\gamma$\n",
    "- Use techniques like grid search or random search to tune hyperparameters.\n",
    "2. Transform the Data:\n",
    "- The RBF kernel implicitly transforms the data. No explicit feature mapping is required.\n",
    "3. Fit the Model:\n",
    "- Optimize the SVM objective function to find the best hyperplane in the transformed space.\n",
    "4. Evaluate Performance:\n",
    "- Use metrics like accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ee3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Step 1: Generate a non-linear dataset\n",
    "X, y = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 2: Visualize the dataset\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "plt.title(\"Non-linear Dataset (Moons)\")\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Train an SVM with RBF Kernel\n",
    "# Default hyperparameters\n",
    "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "y_pred = svm_rbf.predict(X_test)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Step 5: Visualize the decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, levels=20, cmap='viridis', alpha=0.7)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "    plt.title(\"SVM with RBF Kernel - Decision Boundary\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(svm_rbf, X, y)\n",
    "\n",
    "# Step 6: Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Step 7: Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report (Best Model):\\n\", classification_report(y_test, y_pred_best))\n",
    "print(\"Accuracy Score (Best Model):\", accuracy_score(y_test, y_pred_best))\n",
    "\n",
    "plot_decision_boundary(best_model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea925c",
   "metadata": {},
   "source": [
    "##### Use Gaussian RBF for an Observation\n",
    "For a single observation $x_i$ and a reference point $x_j$, the Gaussian RBF kernel is calculated as:\n",
    "\n",
    "$$ K(x, x') = exp (-\\gamma||x_i, x_j||^2) $$\n",
    "- where:\n",
    "    - $||x_i, x_j||^2$: is the squared Euclidean distance.\n",
    "    - $\\gamma$ controls the spread of the kernel \n",
    "        - small $\\gamma$: considers distant points similar, \n",
    "        - large $\\gamma$: considers only close points similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f5568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "# Compute the RBF kernel for two points\n",
    "point1 = np.array([1.0, 2.0]).reshape(1, -1)\n",
    "point2 = np.array([3.0, 4.0]).reshape(1, -1)\n",
    "gamma = 0.5\n",
    "\n",
    "rbf_value = rbf_kernel(point1, point2, gamma=gamma)\n",
    "print(\"RBF Kernel Value between point1 and point2:\", rbf_value[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d0b6c",
   "metadata": {},
   "source": [
    "##### Use the rbf kernel (Radial_basis_function_kernel), which allows the SVC to fit a non-linear decision boundary with Built in Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64fed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf')\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "print(\"The accuracy score of the SVC is:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0875d9",
   "metadata": {},
   "source": [
    "##### Plot the decision boundary for the SVC using the non-linear rbf kernel\n",
    "\n",
    "Plot the 1 dimensional decision boundary between the 2 features present in our synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 # Feature 1\n",
    "j = 1 # Feature 2\n",
    "\n",
    "svc.fit(X[:, [i, j]], y)\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    " \n",
    "x_min, x_max = X[:, i].min(), X[:, i].max()\n",
    "y_min, y_max = X[:, j].min(), X[:, j].max()\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 1000), np.linspace(y_min, y_max, 1000))\n",
    "\n",
    "y_hat = svc.predict(np.concatenate((xx.reshape(-1,1), yy.reshape(-1,1)), axis=1))\n",
    "y_hat = y_hat.reshape(xx.shape)\n",
    "\n",
    "ax1.pcolormesh(xx, yy, y_hat, cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.scatter(X[:, i], X[:, j], c=y, edgecolors='k', cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.set_xlim(xx.min(), xx.max())\n",
    "ax1.set_ylim(yy.min(), yy.max())\n",
    "ax1.set_xticks(())\n",
    "ax1.set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9625da",
   "metadata": {},
   "source": [
    "### **Objective Function of SVM**\n",
    "The objective of SVM is twofold:\n",
    "- Maximize the margin (maximize separation between classes).\n",
    "- Minimize classification errors for non-linearly separable data (using a hinge-loss function).\n",
    "\n",
    "It is designed to maximize the margin between the two classes while minimizing classification errors. \n",
    "- For m features, the objective function considers the weight vector $w \\in R^m$, which defines the orientation of the separating hyperplane in the feature space.\n",
    "\n",
    "#####  Primal Form of the Objective Function: Mathematical Formulation\n",
    "Given:\n",
    "- A dataset with n training samples $(𝑥_𝑖, 𝑦_𝑖)$ where \n",
    "    - $𝑥_𝑖 \\in 𝑅^𝑑$ are feature vectors\n",
    "    - $y_𝑖 \\in {−1,1}$ are class labels.\n",
    "- A weight vector 𝑤 and bias b defining the hyperplane.\n",
    "\n",
    "The decision boundary is represented by:\n",
    "$$ f(x) = w^T x + b $$\n",
    "\n",
    "The **SVM primal objective function** is:\n",
    "\n",
    "$$ Minimize_{w,b,\\xi}: \\frac{1}{2} ||w||^2 + C \\sum^n_{i=1} Hinge Loss$$\n",
    "\n",
    "- Where:\n",
    "    - $\\frac{1}{2} ||w||^2$: Ensures the margin is maximized, promoting simplicity.\n",
    "        - Encourages a large margin between the two classes (simpler decision boundary).\n",
    "        - It minimizes the norm of the weight vector w, maximizing the margin between the classes.\n",
    "    - $C$: Regularization parameter that controls the trade-off between margin maximization and classification errors.\n",
    "        - The choice of C determines where this balance lies.\n",
    "    - $\\sum^n_{i=1} \\xi_i$: A penalty for all margin violations. \n",
    "        - A higher sum of $\\xi_i$ implies more violations.\n",
    "    - $\\xi$: Classification errors / Slack variables that represent misclassification or margin violations.\n",
    "        - measure the extent of misclassification or margin violation for each data point.\n",
    "        - It represents the extent to which the i-th data point violates the margin (misclassification or lying within the margin).\n",
    "\n",
    "\n",
    "The constraints for correctly classified data points are / subject to:\n",
    "\n",
    "$$ y_i  \\cdot (w^T x_i + b) \\geq 1  - \\xi_i , \\xi_i \\geq 0 \\forall i $$\n",
    "\n",
    "##### Hinge-Loss Function\n",
    "\n",
    "The hinge-loss function is used to penalize misclassifications and points close to the margin. It is defined as:\n",
    "\n",
    "$$ Hinge Loss: L(y,f(x)) = max(0,1 - y  \\cdot f(x)) $$\n",
    "\n",
    "- If $y  \\cdot f(x) \\geq 1$, the loss is 0 (correctly classified and beyond the margin).\n",
    "- If $y  \\cdot f(x) < 1$, the loss increases linearly as the point moves closer to or across the margin.\n",
    "\n",
    "he **SVM primal objective function** is now:\n",
    "\n",
    "$$ Minimize_{w,b}: \\frac{1}{2} ||w||^2 + C \\sum^n_{i=1} max(0,1 - y_i  \\cdot (w^T x_i + b))$$\n",
    "\n",
    "### **Understanding the Objective Function**\n",
    "\n",
    "1. Margin Maximization ($\\frac{1}{2} ||w||^2$): \n",
    "- The first term ensures that the hyperplane has the largest margin by minimizing the norm of the weight vector ($||w||$).\n",
    "    - A smaller ||w|| corresponds to a larger margin.\n",
    "\n",
    "2. Hinge Loss ($C \\sum^n_{i=1} max(0,1 - y  \\cdot f(x))$)\n",
    "- The second term $\\sum^n_{i=1} \\xi_i$ penalizes points that are misclassified or fall within the margin.\n",
    "- $max(0,1 - y_i  \\cdot (w^T x_i + b)$ penalizes points that are either misclassified or lie within the margin.\n",
    "- The parameter C>0 is a `regularization parameter` that controls the trade-off between maximizing the margin and minimizing classification errors.\n",
    "    - How C Works:\n",
    "        - Large C: Strongly penalizes misclassifications, leading to a tighter fit to the training data.\n",
    "        - Small C: Allows for more margin violations, leading to a simpler, more generalizable model.\n",
    "\n",
    "Regularization parameter trade-off:\n",
    "- Regularization adjusts the balance between two objectives:\n",
    "    - Maximizing the margin:\n",
    "        - Maximizing the margin: Keeping $||w||^2$ small promotes a large margin and simpler models.\n",
    "        - Minimizing misclassification error: Penalizing $\\sum^n_{i=1} \\xi_i$ ensures the model correctly classifies most training instances.\n",
    "\n",
    "\n",
    "##### Interpretation of the Objective Function\n",
    "The function combines two objectives:\n",
    "1. Maximizing the margin: Achieved by minimizing $\\frac{1}{2} ||w||^2$ resulting in a decision boundary that is as far as possible from the nearest data points (support vectors).\n",
    "    - A larger margin improves the model's generalization ability (i.e., it performs better on unseen data).\n",
    "2. Minimizing classification errors: Achieved by penalizing the slack variables $\\xi_i$ via the term $C \\sum^n_{i=1} \\xi_i$, which accounts for points within or outside the margin.\n",
    "    - Points misclassified or within the margin are penalized, encouraging the model to position the hyperplane optimally.\n",
    "\n",
    "#### How SVM Uses Hinge Loss\n",
    "\n",
    "##### **2.1 How violations and misclassification are measured in soft margin classification.**\n",
    "Soft Margin Classification:\n",
    "- The soft margin SVM introduces flexibility by allowing violations of the margin through $\\xi_i$, making it suitable for non-linearly separable and noisy datasets.\n",
    "- For linearly inseparable data, SVM introduces slack variables ($\\xi_i$) to allow some points to violate the margin constraints.\n",
    "- The hinge loss incorporates these violations, enabling SVM to work with noisy or overlapping data.\n",
    "\n",
    "In soft margin classification, violations and misclassification are measured using slack variables ($\\xi_i$), which represent the extent to which a data point deviates from the ideal separation defined by the decision boundary and margin.\n",
    "\n",
    "The Role of the Slack Variables ($\\xi_i$)\n",
    "- Slack variables are introduced in the soft margin SVM to allow for some data points to:\n",
    "    - Lie inside the margin (violations).\n",
    "    - Be misclassified (on the wrong side of the decision boundary).\n",
    "- Each data point i has an associated slack variable ($\\xi_i \\geq 0$), which quantifies its violation of the margin constraints.\n",
    "\n",
    "Decision Boundary and Constraints\n",
    "- The decision boundary in soft margin classification is defined by:\n",
    "$$ y_i (w \\cdot x_i + b) \\geq 1  - \\xi_i , \\xi_i \\geq 0 $$\n",
    "\n",
    "- When $ y_i (w \\cdot x_i + b) \\geq 1$:\n",
    "    - The data point is correctly classified and outside the margin. No violation occurs, so $\\xi_i = 0$.\n",
    "- When $ 0 < y_i (w  \\cdot x_i + b) < 1$: \n",
    "    - The data point is correctly classified but lies inside the margin. The margin is violated, and $\\xi_i > 0$.\n",
    "- When $ y_i (w \\cdot x_i + b) < 0$:\n",
    "    - The data point is misclassified and on the wrong side of the decision boundary. This is a severe violation, with a larger $\\xi_i$.\n",
    "\n",
    "Measuring Margin Violations\n",
    "- The slack variable $\\xi_i$ measures the distance a point falls short of the margin boundary. Specifically:\n",
    "    - $\\xi_i = 0$, the point lies on or outside the correct margin.\n",
    "    - $0 < \\xi_i \\leq 1$, the point is inside the margin but correctly classified.\n",
    "    - $\\xi_i > 1$, the point is misclassified.\n",
    "\n",
    "- Total Margin Violation\n",
    "    - The total violation across all data points is:\n",
    "$$ \\sum^N_{i=1} \\xi_i$$\n",
    "\n",
    "Misclassification\n",
    "- Misclassification occurs when a data point lies on the wrong side of the decision boundary:\n",
    "$$y_i (w \\cdot x_i + b)$$\n",
    "\n",
    "- For misclassified points, $\\xi_i >1$\n",
    "    - The slack variable $\\xi_i - 1$ represents the extent of misclassification.\n",
    "\n",
    "- Misclassification Count\n",
    "    - The number of misclassified points can be roughly estimated as:\n",
    "        - number of misclassifications $\\approx \\sum^N_{i=1} 1 (\\xi_i >1)$ where:\n",
    "            - $1(\\cdot)$ is an indicator function that equals 1 if the condition is true, and 0 otherwise.\n",
    "\n",
    "- Example: assume we have the following\n",
    "    - $y_i = +1$: Positive class.\n",
    "        - The margin for $y_i = +1$ is defined as $(w \\cdot x_i + b) \\geq +1$\n",
    "\n",
    "- Possible Cases:\n",
    "    - Correct Classification Outside the Margin: $y_i(w \\cdot x_i + b) \\geq +1)$\n",
    "        - No violation $\\xi_i = 0$\n",
    "    - Correct Classification Inside the Margin  $0 < y_i(w \\cdot x_i + b) < 1)$\n",
    "        - Margin violation occurs $\\xi_i > 0$\n",
    "    - Misclassified Point  $ y_i(w \\cdot x_i + b) < 0)$\n",
    "        - Severe violation $\\xi_i > 1$\n",
    "\n",
    "##### **2.2 Reasons for using `Regularization` in SVM**\n",
    "Regularization in Support Vector Machines (SVMs) is crucial to ensure that the model generalizes well to unseen data. \n",
    "- Regularization introduces a penalty for overly complex models, preventing overfitting.\n",
    "- Regularization in SVM controls the trade-off between:\n",
    "    - Maximizing the margin: Ensuring the decision boundary is as far as possible from the nearest data points.\n",
    "    - Minimizing misclassification errors: Allowing some points to fall inside the margin or on the wrong side of the decision boundary for better generalization.\n",
    "\n",
    "Control Overfitting\n",
    "- Reason: SVM aims to maximize the margin between classes while minimizing misclassification errors. Without regularization, the model might try to perfectly classify the training data, resulting in overfitting.\n",
    "- Solution: Regularization balances the trade-off between achieving a larger margin (simpler model) and minimizing classification errors.\n",
    "    - A larger regularization parameter (C): penalizes misclassifications more heavily, potentially leading to overfitting.\n",
    "    - A smaller regularization parameter (C):  favors a larger margin and allows for more misclassifications, promoting generalization.\n",
    "\n",
    "Handle Noisy Data\n",
    "- Reason: Real-world datasets often contain noise, outliers, or mislabeled data points. Without regularization, SVM may overemphasize these noisy points, leading to a distorted decision boundary.\n",
    "- Solution: Regularization reduces the influence of such noisy points by allowing some tolerance for misclassification, leading to a more robust model.\n",
    "\n",
    "Promote Simpler Decision Boundaries\n",
    "- Reason: Complex decision boundaries can lead to poor generalization on new data.\n",
    "- Solution: Regularization encourages the SVM to find a simpler decision boundary by controlling the weight vector (w) through a regularization term in the objective function.\n",
    "\n",
    "Avoid Curse of Dimensionality\n",
    "- Reason: In high-dimensional spaces, the risk of overfitting increases because the model has more capacity to fit the training data perfectly.\n",
    "- Solution: Regularization reduces the model's flexibility, preventing overfitting in high-dimensional feature spaces.\n",
    "\n",
    "Improve Generalization Performance\n",
    "- Reason: A model that fits the training data too closely may fail to generalize to unseen data.\n",
    "- Solution: Regularization ensures that the SVM focuses on the most informative patterns in the data, improving performance on test data.\n",
    "\n",
    "Kernel Methods and Regularization\n",
    "- Reason: When using kernel functions (e.g., RBF, polynomial), the feature space is transformed into a higher dimension, increasing the model's capacity to overfit.\n",
    "- Solution: Regularization mitigates overfitting by constraining the optimization process, ensuring the model finds a balance between complexity and accuracy.\n",
    "\n",
    "##### Type of regularization used in Soft Margin Classification for Support Vector Machines (SVMs)\n",
    "Type used is L2 regularization.\n",
    "\n",
    "L2 Regularization in the Objective Function: \n",
    "$$Minimize_{w,b,\\xi}: \\frac{1}{2} ||w||^2 + C \\sum^n_{i=1} \\xi_i$$\n",
    "\n",
    "- Where:\n",
    "    - $\\frac{1}{2} ||w||^2$: represents L2 regularization, as it minimizes the squared Euclidean norm of the weight vector w. \n",
    "        - It helps in maximizing the margin by penalizing larger weight values, which results in a smoother and more generalizable decision boundary.\n",
    "    - $C \\sum^n_{i=1} \\xi_i$: his term penalizes margin violations (misclassification or points lying within the margin). \n",
    "        - The parameter C determines the penalty strength.\n",
    "\n",
    "Why L2 Regularization?\n",
    "- L2 regularization is chosen because:\n",
    "    - It encourages smaller weight magnitudes ($w_i^2), which leads to a more stable model less sensitive to noise in the data.\n",
    "    - It avoids overfitting by penalizing complex decision boundaries.\n",
    "    - The quadratic term $∥w∥^2$ ensures that the solution is smooth and generalizes well to unseen data.\n",
    "\n",
    "Mathematical Interpretation\n",
    "- The L2 regularization term $\\frac{1}{2} ||w||^2$ ensures that the weight vector w remains small, which effectively controls the model's complexity. \n",
    "    - Smaller weights correspond to a more stable and less overfitted model.\n",
    "\n",
    "Connection to Dual Formulation\n",
    "- In the dual formulation, the regularization parameter C indirectly limits the Lagrange multipliers $\\alpha_i$:\n",
    "\n",
    "$$ 0 \\leq \\alpha_i \\leq C $$\n",
    "\n",
    "This constraint ensures that the influence of each data point on the decision boundary is limited, balancing the trade-off between margin maximization and classification accuracy.\n",
    "\n",
    "##### **2.3 Hyperparameter C in soft margin classification**\n",
    "The objective function for soft margin SVM is:\n",
    "\n",
    "$$Minimize_{w,b,\\xi}: \\frac{1}{2} ||w||^2 + C \\sum^n_{i=1} \\xi_i$$\n",
    "\n",
    "Hyperparameter C\n",
    "- Determines the trade-off between maximizing the margin (pathway width) and minimizing classification errors.\n",
    "- Determines how much weight is given to minimizing slack variables relative to maximizing the margin width.\n",
    "\n",
    "Trade-Off Parameter (C):\n",
    "- Governs the trade-off between the two components of the objective function:\n",
    "    - Large C: Focuses on minimizing misclassification, potentially at the cost of a smaller margin (risk of overfitting).\n",
    "        - Strongly penalizes misclassification.\n",
    "        - Results in a smaller margin as the model tries to classify every point correctly.\n",
    "        - May lead to overfitting, especially on noisy data.\n",
    "    - Small C: Focuses on maximizing the margin, tolerating some misclassifications (risk of underfitting).\n",
    "        - Allows more margin violations (misclassified points).\n",
    "        - Results in a larger margin and simpler decision boundary.\n",
    "        - Promotes better generalization, reducing the risk of overfitting.\n",
    "\n",
    "High C: Narrower Pathway Width\n",
    "- The penalty for margin violations ($C \\sum \\xi_i$) becomes significant.\n",
    "- The SVM prioritizes classifying training points correctly over maximizing the margin width.\n",
    "- The model becomes more sensitive to individual data points, which can lead to:\n",
    "    - A narrower pathway width (smaller margin).\n",
    "    - Overfitting, where the decision boundary conforms too closely to the training data.\n",
    "- Behavior:\n",
    "    - The margin shrinks to fit the data points tightly.\n",
    "    - Misclassified points are heavily penalized, so the model tries to minimize their number at the cost of a smaller margin.\n",
    "\n",
    "Low C: Wider Pathway Width\n",
    "- The penalty for margin violations becomes less significant.\n",
    "- The SVM focuses on maximizing the margin width, even if it means allowing some misclassified points.\n",
    "- The model becomes less sensitive to noise and outliers, leading to:\n",
    "    - A wider pathway width (larger margin).\n",
    "    - Better generalization to unseen data.\n",
    "- Behavior:\n",
    "    - The decision boundary prioritizes a larger margin over perfect classification.\n",
    "    - Misclassified points are tolerated, reducing the risk of overfitting.\n",
    "\n",
    "##### Relationship Between C and Pathway Width\n",
    "The pathway width (or margin width) is inversely related to C:\n",
    "- High C: Narrower pathway (small margin).\n",
    "- Low C: Wider pathway (large margin).\n",
    "\n",
    "This trade-off reflects the bias-variance trade-off:\n",
    "- High C: Low bias, high variance (more complex model).\n",
    "- Low C: High bias, low variance (simpler model).\n",
    "\n",
    "Practical Analysis of C and Pathway Width\n",
    "- To analyze the relationship between C and margin width:\n",
    "    - Train the SVM Model: Train models with different values of C.\n",
    "    - Visualize the Decision Boundary:\n",
    "        - Plot the decision boundary and margins for low, medium, and high C values.\n",
    "        - Observe how the margin width and boundary placement change.\n",
    "    - Evaluate Performance:\n",
    "        - On training data, high C often results in lower misclassification rates.\n",
    "        - On test data, low C often results in better generalization.\n",
    "\n",
    "Impact of C on Generalization\n",
    "- High C: The model prioritizes accuracy on the training data but risks overfitting due to a narrow margin.\n",
    "- Low C: The model sacrifices some accuracy on the training data but generalizes better due to a wider margin.\n",
    "\n",
    "##### **Dual Formulation of the Soft Margin Objective**\n",
    "Objective function for soft margin classification in Support Vector Machines (SVMs) allows for some misclassification or margin violations in the dataset. \n",
    "- This makes the model more robust to noisy and non-linearly separable data.\n",
    "\n",
    "The dual formulation is more computationally efficient for many datasets, especially when using kernels. It is expressed as:\n",
    "\n",
    "$$ max_{\\alpha} \\sum^{N}_{i = 1} \\alpha_i - \\frac{1}{2} \\sum^{N}_{i = 1} \\sum^{N}_{j = 1} \\alpha_i \\alpha_i y_i y_j K(x_i, x_j) $$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$ 0 \\leq \\alpha_i \\leq C, \\sum^{N}_{i = 1} \\alpha_i  y_i = 0 $$\n",
    "\n",
    "Where: \n",
    "- $\\alpha_i$: Lagrange multipliers.\n",
    "- $K(x_i, x_j)$: Kernel function, used for non-linear decision boundaries.\n",
    "- C: Controls the range of $\\alpha_i$, balancing margin width and classification error.\n",
    "\n",
    "##### **Components in Relation to m Features**\n",
    "The objective function in SVM for m-features balances the goals of maximizing the margin and minimizing misclassification through the weight vector w, bias b, and slack variables $\\xi_i$. Regularization, via C, plays a key role in ensuring that the model generalizes well to unseen data.\n",
    "\n",
    "- w: Weight vector of dimension m, one weight per feature, defines the hyperplane's orientation.\n",
    "- $𝑥_𝑖 \\in 𝑅^m$ Feature vectors in the m-dimensional space.\n",
    "- Kernal $K(x_i, x_j)$: Allows mapping of $𝑥_𝑖$ into a higher-dimensional feature space for non-linear separability, indirectly involving m.\n",
    "\n",
    "##### Intuition for m Features\n",
    "- The dimension m dictates the complexity of the weight vector w, which defines the separating hyperplane.\n",
    "- Larger m means a higher-dimensional feature space, potentially increasing the model's capacity but also the risk of overfitting.\n",
    "- Regularization (C) ensures that the optimization remains robust, even with a large number of features.\n",
    "\n",
    "### **Optimization**\n",
    "- To solve the SVM objective, quadratic programming methods or optimization algorithms (e.g., SMO—Sequential Minimal Optimization) are used. \n",
    "- For large datasets, kernels or approximate methods are often applied.\n",
    "\n",
    "##### **Tuning an SVM model**\n",
    "Use `sklearn`'s [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). \n",
    "- This procedure allows us to specify a set of possible parameters for a specific model.\n",
    "    - `GridSearchCV` will then go through those parameters and try every possible combination of them (kind of like it's working through a grid in a systematic way - that's where the name comes from). \n",
    "    - `GridSearchCV` will then return the combination of parameters that resulted in a model with the best score. \n",
    "    - `GridSearchCV` makes use of **cross validation**, helping to ensure the robustness of it's results.\n",
    "\n",
    "Grid search is a systematic method for hyperparameter optimization that evaluates a predefined set of hyperparameters for a machine learning model, such as an SVM.\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "C: Regularization parameter.\n",
    "- Higher C: Focuses on minimizing classification errors (lower margin, more overfitting).\n",
    "- Lower C: Allows more classification errors (larger margin, more underfitting).\n",
    "\n",
    "Kernel: Specifies the kernel function.\n",
    "- Linear: Best for linearly separable data.\n",
    "- Polynomial/RBF (Radial Basis Function): Handles nonlinear decision boundaries.\n",
    "\n",
    "Gamma: Used with RBF and polynomial kernels.\n",
    "- Controls the influence of a single training example.\n",
    "    - Lower values: More generalized decision boundaries.\n",
    "    - Higher values: Tighter fit around data points.\n",
    "\n",
    "Degree: Relevant for polynomial kernels, representing the polynomial degree.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Create a dictionary that contains the parameters you want to tune as `keys` and all the different options you want to test for those parameters as `values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel':('linear', 'rbf'), \n",
    "              'C':(0.25,1.0),\n",
    "              'gamma': (1,2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40da9c",
   "metadata": {},
   "source": [
    "2. Instantiate an SVC classifier and tell `GridSearchCV` to test it using the parameters we previously specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d3333",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "clf = GridSearchCV(svm, parameters)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b23a2b2",
   "metadata": {},
   "source": [
    "**Understanding the Output of Grid Search for SVM**\n",
    "\n",
    "1. Best Parameters (best_params_)\n",
    "\n",
    "This indicates the combination of hyperparameters that resulted in the best cross-validation score.\n",
    "\n",
    "    - {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
    "        - A regularization strength of C=10.\n",
    "        - A kernel function of Radial Basis Function (RBF).\n",
    "        - Gamma value of 0.1.\n",
    "\n",
    "Extract the Best Parameters:\n",
    "- Use grid_search.best_params_ to identify the best-performing combination.\n",
    "\n",
    "2. Best Score (best_score_)\n",
    "\n",
    "This is the highest cross-validation score achieved for the best parameter combination. It indicates how well the model generalized during validation.\n",
    "\n",
    "    - 0.93\n",
    "    - The best parameter combination resulted in 93% accuracy during cross-validation.\n",
    "\n",
    "Examine the Best Score:\n",
    "- Use grid_search.best_score_ to see the best validation accuracy achieved (e.g., 0.90).\n",
    "\n",
    "Make Predictions:\n",
    "- Use the grid_search.best_estimator_ to make predictions on new data.\n",
    "\n",
    "3. Complete Results (cv_results_)\n",
    "\n",
    "- A dictionary containing detailed results for all parameter combinations evaluated during grid search. Key fields:\n",
    "    - mean_test_score: Average cross-validation score for each parameter set.\n",
    "    - std_test_score: Standard deviation of scores across folds (indicates variability).\n",
    "    - params: Parameter combinations corresponding to the scores.\n",
    "\n",
    "            - {'mean_test_score': [0.91, 0.93, 0.89],\n",
    "            'std_test_score': [0.01, 0.02, 0.03],\n",
    "            'params': [{'C': 1, 'gamma': 0.1, 'kernel': 'rbf'},\n",
    "                        {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'},\n",
    "                        {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}]}\n",
    "\n",
    "            - The best score (0.93) corresponds to {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
    "            - The variability of scores (e.g., 0.02) reflects the model's consistency during cross-validation.\n",
    "\n",
    "4.  Best Estimator (best_estimator_)\n",
    "\n",
    "The trained SVM model with the best parameters. This can be used for predictions.\n",
    "\n",
    "    - SVC(C=10, gamma=0.1, kernel='rbf')\n",
    "\n",
    "##### How to Use the Grid Search Results\n",
    "- Best Parameters: Use `grid_search.best_params_` to train the final SVM model on the full training data for optimal performance.\n",
    "- Best Estimator: Use `grid_search.best_estimator_` directly for prediction.\n",
    "- Scoring and Ranking: The `mean_test_score in cv_results_` can be used to evaluate how different parameter combinations perform.\n",
    "- Variability: Use `std_test_score` to assess how consistent the model performance is across folds. Lower variability indicates a robust model.\n",
    "\n",
    "**GridSearch Output**\n",
    "\n",
    "    - GridSearchCV(cv=None, error_score=nan,\n",
    "                estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
    "                            class_weight=None, coef0=0.0,\n",
    "                            decision_function_shape='ovr', degree=3,\n",
    "                            gamma='scale', kernel='rbf', max_iter=-1,\n",
    "                            probability=False, random_state=None, shrinking=True,\n",
    "                            tol=0.001, verbose=False),\n",
    "                iid='deprecated', n_jobs=None,\n",
    "                param_grid={'C': (0.25, 1.0), 'gamma': (1, 2),\n",
    "                            'kernel': ('linear', 'rbf')},\n",
    "                pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
    "                scoring=None, verbose=0)\n",
    "\n",
    "1. cv=None:\n",
    "- By default, cv=None uses 5-fold cross-validation to evaluate the performance of each hyperparameter combination.\n",
    "\n",
    "2. error_score=nan:\n",
    "- Specifies what happens if a model fails during training. \n",
    "    - If set to nan, the model skips that combination and assigns a score of nan.\n",
    "\n",
    "3. estimator=SVC(...):\n",
    "- The base model being optimized, in this case, an SVM classifier (SVC).\n",
    "- The parameters within the SVC object (e.g., C=1.0, kernel='rbf') represent its default settings, which may be overridden by the grid search.\n",
    "\n",
    "4. param_grid={'C': (0.25, 1.0), 'gamma': (1, 2), 'kernel': ('linear', 'rbf')}:\n",
    "- The hyperparameter combinations being evaluated:\n",
    "    - C: Regularization parameter values [0.25,1.0].\n",
    "    - gamma: Kernel coefficient values [1,2].\n",
    "    - kernel: Kernel types [linear ,  rbf].\n",
    "- The grid search will test all possible combinations of these parameters (a total of 2×2×2=8 combinations).\n",
    "\n",
    "5. iid='deprecated':\n",
    "- Refers to the Independent Identically Distributed assumption, which has been deprecated in Scikit-learn 0.24 and later. \n",
    "    - It is safe to ignore this unless you’re using an older version of Scikit-learn.\n",
    "\n",
    "6. n_jobs=None:\n",
    "- Specifies the number of CPU cores to use for parallel computation. None means it will run in serial mode on a single core.\n",
    "\n",
    "7. pre_dispatch='2*n_jobs':\n",
    "- Controls the number of jobs that get dispatched during parallel computation. \n",
    "    - Since n_jobs=None, this has no effect.\n",
    "\n",
    "8. refit=True:\n",
    "- After finding the best hyperparameter combination, the grid search automatically refits the model on the entire training dataset using those parameters.\n",
    "\n",
    "9. return_train_score=False:\n",
    "- If True, the results would include training scores in addition to validation scores. \n",
    "- Here, it is False, so only validation scores are calculated.\n",
    "\n",
    "10. scoring=None:\n",
    "- Indicates that the default scoring metric for the estimator (e.g., accuracy for classification) is used.\n",
    "\n",
    "**This Configuration Means**\n",
    "\n",
    "The grid search is tuning an SVM classifier with:\n",
    "- Two values of C [0.25,1.0],\n",
    "- Two values of gamma [1,2], and\n",
    "- Two kernel types (linear and rbf).\n",
    "Each of these 2×2×2=8 combinations is evaluated using 5-fold cross-validation.\n",
    "\n",
    "The performance of each combination is assessed using the default scoring metric (accuracy for classification).\n",
    "\n",
    "The best-performing combination is automatically selected and refitted on the entire training dataset.\n",
    "\n",
    "### **Reasons for not using squared loss function in classification problems**\n",
    "\n",
    "1. It is sensitive to outliers.\n",
    "2. It does not align with the probabilistic interpretation of classification tasks.\n",
    "3. It fails to emphasize the separation of classes effectively.\n",
    "4. Alternatives like cross-entropy or hinge loss are better suited for optimizing classification models, focusing on class separation and meaningful probabilities.\n",
    "\n",
    "Non-robustness to Outliers\n",
    "- In Classification: Misclassified points, especially outliers, can disproportionately influence the decision boundary, leading to poor generalization.\n",
    "- Squared loss penalizes large errors quadratically, \n",
    "    - meaning that a few instances with large prediction errors can dominate the loss function.\n",
    "\n",
    "Misalignment with Classification Goals\n",
    "- Nature of Classification: Classification problems aim to predict discrete labels or probabilities for class membership, focusing on correctly separating classes.\n",
    "- Squared Loss Behavior: Squared loss minimizes the difference between predicted and true values. \n",
    "    - In classification, true labels are usually encoded as 0 or 1, and predictions outside [0,1] are meaningless probabilities. \n",
    "        - This can result in illogical outcomes for probabilities and suboptimal boundaries.\n",
    "\n",
    "Poor Handling of Probabilities\n",
    "- Probabilistic Interpretation: Classification models often interpret predictions as probabilities of class membership.\n",
    "- Squared Loss Issues: It does not naturally account for the probabilistic nature of classification. \n",
    "    - loss functions, like log-loss (cross-entropy), directly optimize for probability-based interpretations, ensuring that predictions align better with actual class probabilities.\n",
    "\n",
    " Inappropriate Gradients for Classification\n",
    "- Gradient Shape: Squared loss gradients are linear, meaning the gradient changes linearly with the error.\n",
    "- Impact: In classification problems, small classification errors might still produce significant gradients, leading to inefficient updates. \n",
    "    - loss functions like hinge loss or cross-entropy loss prioritize the misclassified or uncertain points more effectively, which aligns with the goal of improving class separation.\n",
    "\n",
    "Squared Loss Leads to Non-optimal Decision Boundaries\n",
    "- Decision Boundary Nature: In classification, the goal is to maximize the margin between classes or ensure good separation.\n",
    "- Squared Loss Focus: By trying to minimize the distance between predicted and true labels, squared loss tends to favor a compromise boundary, potentially leading to poorly separated classes, especially in non-linear classification problems.\n",
    "\n",
    "Better Alternatives Exist\n",
    "- Hinge Loss: Used in SVMs, it focuses on maximizing the margin and ensures only points near or across the boundary contribute to the loss.\n",
    "- Cross-Entropy Loss: Used in logistic regression and neural networks, it optimizes probabilities directly, aligning well with classification goals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a6bfa2",
   "metadata": {},
   "source": [
    "### Advantages of SVMs\n",
    "1. Effective in High-Dimensional Spaces\n",
    "-  SVMs perform well when the number of features is large relative to the number of observations.\n",
    "- Example: Applications in text classification or genomics, where the feature space is often very high-dimensional.\n",
    "\n",
    "2. Works Well with Clear Margins of Separation\n",
    "- SVM aims to find the optimal hyperplane that maximizes the margin between classes, which ensures robust classification when classes are well-separated.\n",
    "- Example: Binary classification tasks where the data is linearly separable.\n",
    "\n",
    "3. Kernel Trick for Non-linear Data\n",
    "- SVM uses the \"kernel trick\" to map non-linearly separable data into a higher-dimensional space where a linear separation is possible.\n",
    "- Example: Radial Basis Function (RBF) and polynomial kernels can handle complex decision boundaries.\n",
    "\n",
    "4. Regularization Through C Parameter\n",
    "- The regularization parameter C controls the trade-off between maximizing the margin and minimizing classification errors, making SVMs flexible to different types of data distributions.\n",
    "- Example: Adjusting C to avoid overfitting on small datasets or noisy data.\n",
    "\n",
    "5. Robust to Overfitting (with Proper Tuning)\n",
    "- By controlling the margin size and kernel functions, SVMs can generalize well, especially for small datasets.\n",
    "- Example: SVMs perform better than other models when data has limited examples but a high feature count.\n",
    "\n",
    "6. Effective for Outlier Detection\n",
    "- SVM variants, such as one-class SVM, are used to detect anomalies by learning the boundaries of the majority class.\n",
    "- Example: Fraud detection or network intrusion detection.\n",
    "\n",
    "### Disadvantages of SVMs\n",
    "1. High Computational Cost\n",
    "- SVM training involves solving a convex optimization problem, which can become computationally expensive for large datasets.\n",
    "- Example: For datasets with millions of samples, training can be significantly slower compared to models like logistic regression or decision trees.\n",
    "\n",
    "2. Sensitive to Choice of Kernel\n",
    "- The performance of SVM heavily depends on the choice of kernel function and its parameters (e.g., RBF kernel with parameters $𝛾$ and C).\n",
    "- Example: Incorrect kernel choice may lead to poor performance or overfitting.\n",
    "\n",
    "3. Inefficient for Large Datasets\n",
    "- The complexity of SVMs scales with the size of the dataset ($O(n^2$) to ($O(n^3$), making it less suitable for massive datasets.\n",
    "- Example: SVM may struggle with datasets containing millions of instances compared to neural networks or gradient-boosted trees.\n",
    "\n",
    "4. Difficulty Handling Noisy Data\n",
    "- SVMs try to maximize the margin and are sensitive to mislabeled data points, which can shift the decision boundary significantly.\n",
    "- Example: In datasets with a high degree of label noise, SVMs may underperform compared to models with robust loss functions.\n",
    "\n",
    "5. Lack of Probabilistic Output\n",
    "- SVMs do not naturally provide probabilities for predictions. While this can be approximated using Platt scaling or cross-validation, the results are not as interpretable as probabilistic models.\n",
    "- Example: Logistic regression offers direct probabilities, which are more useful in some applications, like medical diagnosis.\n",
    "\n",
    "6. Hyperparameter Tuning is Non-trivial\n",
    "- Choosing the right values for hyperparameters like C, $𝛾$, and the kernel function often requires extensive grid search or cross-validation.\n",
    "- Example: Poorly tuned parameters can lead to overfitting or underfitting, requiring careful experimentation.\n",
    "\n",
    "7. Not Easily Scalable for Multiclass Problems\n",
    "- SVMs are inherently binary classifiers. \n",
    "- For multiclass classification, strategies like \n",
    "    - one-vs-rest (OVR) or \n",
    "    - one-vs-one (OVO) must be used, adding complexity and computational cost.\n",
    "- Example: For 10 classes, OVO requires 10×(10−1)/2=45 classifiers to be trained.\n",
    "\n",
    "When to Use SVMs\n",
    "- Best Use Cases:\n",
    "    - High-dimensional datasets with clear margins of separation.\n",
    "    - Small-to-medium-sized datasets with complex decision boundaries.\n",
    "    - Applications where interpretability of the decision boundary is important (e.g., feature weights in a linear kernel).\n",
    "- Not Ideal For:\n",
    "    - Large datasets due to computational cost.\n",
    "    - Noisy datasets where robust models like random forests or neural networks might outperform.\n",
    "    - Problems requiring probabilistic outputs or interpretable probabilities.\n",
    "\n",
    "##### Five common use cases that require probabilistic outputs or interpretable probabilities that SVM poorly performs to.\n",
    "\n",
    "Key Challenges with SVMs in Probabilistic Scenarios\n",
    "- Calibration Issues: SVM probabilities (from methods like Platt Scaling) are often less reliable than probabilities from inherently probabilistic models.\n",
    "- Interpretability: Decision boundaries and margins are not intuitive for users who need to interpret confidence levels.\n",
    "- Actionable Insights: Many use cases (e.g., credit scoring, fraud detection) require actionable thresholds or prioritization, which hinge on well-calibrated probabilities.\n",
    "\n",
    "Medical Diagnosis\n",
    "- Why Probabilities are Needed: In medical applications, probabilistic outputs help determine the likelihood of a disease or condition, allowing practitioners to weigh risks and make informed decisions.\n",
    "    - Example: Predicting whether a patient has cancer with a 90% probability versus 55%.\n",
    "- Why SVM Fails: SVM outputs are distances from the decision boundary, which don’t naturally translate to probabilities. While calibration techniques like Platt Scaling can convert these into probabilities, they often yield less reliable and less interpretable probabilities than models like logistic regression.\n",
    "\n",
    "Fraud Detection\n",
    "- Why Probabilities are Needed: In fraud detection, probabilities allow for setting thresholds based on the acceptable level of risk. For instance, transactions with a probability of fraud >95% may trigger an immediate block, while transactions with 60%-80% may require manual review.\n",
    "    - Example: Flagging fraudulent transactions on an e-commerce platform.\n",
    "- Why SVM Fails: SVMs don’t inherently provide probabilities for these thresholds, making it difficult to prioritize actions based on the confidence level of predictions. This lack of interpretability can lead to either overreaction (blocking too many transactions) or underreaction.\n",
    "\n",
    "Customer Churn Prediction\n",
    "- Why Probabilities are Needed: Businesses use churn probability to allocate resources effectively, targeting high-probability churners with retention offers. Probabilities help prioritize interventions.\n",
    "    - Example: Predicting that a customer has a 70% chance of leaving allows the company to offer personalized discounts or incentives.\n",
    "- Why SVM Fails: SVM’s non-probabilistic nature makes it hard to prioritize customers effectively. In contrast, logistic regression or gradient boosting models provide reliable churn probabilities, directly guiding resource allocation.\n",
    "\n",
    "Marketing Campaign Effectiveness\n",
    "- Why Probabilities are Needed: Campaign optimization often relies on the likelihood of conversion or engagement. For example, targeting customers with an 80% chance of responding to an ad is more efficient than targeting those with only 20%.\n",
    "    - Example: Predicting the probability that a customer will click on an ad or make a purchase.\n",
    "- Why SVM Fails: SVM outputs distances, not probabilities, making it harder to assign confidence levels to predictions. This lack of probabilistic output complicates the ranking of prospects for targeted campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3aae6a",
   "metadata": {},
   "source": [
    "### **Support Vector Regression (SVR)**\n",
    "In Support Vector Regression (SVR), the hyperplane represents the regression function.\n",
    "- SVR defines a central hyperplane and two boundary hyperplanes to create a margin of tolerance around the predictions. \n",
    "    - These boundaries are governed by the $\\epsilon$-insensitive loss function.\n",
    "\n",
    "Unlike classification, where a single hyperplane separates data into classes, \n",
    "\n",
    "##### **Equations for Hyperplanes in SVR**\n",
    "Let $f(x) = < w, x > + b$ be the regression function, where:\n",
    "- w is the weight vector.\n",
    "- b is the bias term.\n",
    "- x is the input feature vector.\n",
    "- ⟨w,x⟩ denotes the dot product between w and x.\n",
    "\n",
    "1. Central Hyperplane (Regression Function):\n",
    "- The primary regression function is:\n",
    "$$f(x) = < w, x > + b$$\n",
    "- This hyperplane represents the predicted values for given inputs.\n",
    "\n",
    "2. Boundary Hyperplanes (ε-margin):\n",
    "- Two parallel hyperplanes are defined around the central hyperplane to create the $\\epsilon$-margin:\n",
    "- Upper boundary:\n",
    "$$f(x) = < w, x > + b + \\epsilon$$\n",
    "- Lower boundary:\n",
    "$$f(x) = < w, x > + b - \\epsilon$$\n",
    "- Predictions within this margin (between the upper and lower boundaries) are considered error-free.\n",
    "\n",
    "3. Slack variables Constraints: ($\\xi$ and $\\xi*$)\n",
    "- Slack variables $\\xi$ and $\\xi^*$ are introduced to allow violations of the margin:\n",
    "- For data points above the upper boundary:\n",
    "$$f(x) > < w, x > + b + \\epsilon + \\xi_i $$\n",
    "- For data points below the lower boundary:\n",
    "$$f(x) > < w, x > + b + \\epsilon + \\xi^*_i $$\n",
    "\n",
    "##### **Objective Function in SVR:**\n",
    "- To find the optimal hyperplane, SVR minimizes the objective function:\n",
    "\n",
    "$$ Minimize_{w,b,\\xi,\\xi^*}: \\frac{1}{2} ||w||^2 + C \\sum^n_{i=1} (\\xi_i + \\xi_i^*)$$\n",
    "\n",
    "- Subject to:\n",
    "$$ y_i - < w, x_i > - b \\leq \\epsilon + \\xi_i, $$\n",
    "$$ < w, x_i > + b - y_i \\leq \\epsilon + \\xi^*_i, $$\n",
    "$$ \\xi_i , \\xi^*_i \\geq 0. $$\n",
    "\n",
    "- Where:\n",
    "    - $||w||^2$: Regularization term, controlling model complexity term $\\frac{1}{2}||w||^2$.\n",
    "        - Model Complexity term is responsible for ensuring the simplicity of the model.\n",
    "        - $||w||^2$ is the squared norm of the weight vector w.\n",
    "            - which controls the flatness (or smoothness) of the regression function $f(x) = < w, x > + b$\n",
    "        - Minimizing $||w||^2$ reduces overfitting by penalizing large coefficients, which would otherwise make the model too sensitive to small variations in the data.\n",
    "    - Margin Violation Term: $C \\sum^n_{i=1} (\\xi_i + \\xi_i^*)$ - Total error outside the $\\epsilon$-margin.\n",
    "        - This term measures the total margin violations (Slack variables $\\xi$ and $\\xi*$) for all data points.\n",
    "            - Slack variables $\\xi$ and $\\xi*$: capture the extent to which data points fall outside the ε-insensitive margin.\n",
    "            - C(Regularization parameter), balancing margin violations and model simplicity: controls the trade-off between minimizing margin violations and achieving a simpler model.\n",
    "                - Larger C: Penalizes violations more heavily, leading to a smaller margin and a model that fits the data more closely (risking overfitting).\n",
    "                - Smaller C: Allows more violations, leading to a wider margin and a simpler, more generalized model.\n",
    " \n",
    "Interaction Between the Terms\n",
    "\n",
    "1. Trade-off Between Complexity and Error:\n",
    "- The first term $\\frac{1}{2}||w||^2$ wants to minimize the model complexity (smaller $||w||^2$ smoother hyperplanes).\n",
    "- The second term $C \\sum^n_{i=1} (\\xi_i + \\xi_i^*)$ penalizes errors, pushing the model to fit the data better.\n",
    "- A balance between these two objectives ensures that the model is both simple and accurate.\n",
    "\n",
    "2. Effect of 𝐶 on Interaction:\n",
    "- High 𝐶:\n",
    "    - The penalty for margin violations dominates.\n",
    "    - The optimization prioritizes minimizing errors $(\\xi_i + \\xi_i^*)$\n",
    "    - This leads to a model that fits the training data very closely but might overfit.\n",
    "- High C:\n",
    "    - The penalty for margin violations is relaxed.\n",
    "    - The optimization focuses more on minimizing $||w||^2$, resulting in a smoother, generalized model that may tolerate some errors.\n",
    "\n",
    "3. Balancing Goals:\n",
    "- The interaction of these terms ensures that the regression hyperplane is not just fitting the data but also remains simple and interpretable.\n",
    "- The hyperparameter C serves as a balancing lever, enabling the model to adjust to the desired trade-off between \n",
    "    - fitting the data (error minimization) and \n",
    "    - avoiding overfitting (complexity minimization).\n",
    "\n",
    "##### Key Insights on the Hyperplanes:\n",
    "Role of Hyperplanes:\n",
    "- The central hyperplane represents the predicted regression function.\n",
    "- The boundary hyperplanes define the tolerance region (ε-margin) where predictions are considered acceptable.\n",
    "\n",
    "Effect of Slack Variables:\n",
    "- Points outside the ε-margin introduce error, measured by the slack variables $\\xi_i$ and $\\xi^*_i$ .\n",
    "\n",
    "Geometric Interpretation:\n",
    "- The hyperplanes adapt to the data, minimizing the margin violations while maintaining simplicity (through $∣∣w∣∣ ^2$).\n",
    "\n",
    "Role of Hyperparameters:\n",
    "- C: Controls the trade-off between margin violations (slack variables) and model complexity ($∣∣w∣∣^2$).\n",
    "- $\\epsilon$: Defines the width of the margin.\n",
    "\n",
    "##### **Error Calculation in SVR**\n",
    "In Support Vector Regression (SVR), the error is calculated based on the ε-insensitive loss function. \n",
    "- This loss function defines a margin of tolerance (epsilon ($\\epsilon$)) around the true target value, within which predictions are considered acceptable and do not contribute to the error. \n",
    "- SVR aims to find a regression function that minimizes error while maintaining a margin of tolerance.\n",
    "\n",
    "Epsilon-insensitive loss function:\n",
    "- The loss is calculated only for predictions that fall outside the epsilon margin.\n",
    "- For a given prediction $f(x_i) and true target $y_i$, the loss is:\n",
    "\n",
    "$$ L(f(x_i), y_i) = \\{ 0 \\text{ if } |f(x_i)- y_i| \\leq \\epsilon, \\\\ \\{|f(x_i)- y_i| - \\epsilon \\text{ otherwise}$$\n",
    "\n",
    "- If the predicted value is within $\\epsilon$ of the true value, no error is incurred. Otherwise, the error is proportional to the distance outside the margin.\n",
    "\n",
    "Slack variables ($\\xi$ and $\\xi*$)\n",
    "- SVR introduces slack variables to handle violations of the margin for both above and below the true target value:\n",
    "    - $\\xi$: Measures the amount by which a prediction $f(x_i)$ exceeds the upper margin $𝑦_𝑖 + \\epsilon$\n",
    "    - $\\xi^*$: Measures the amount by which a prediction $f(x_i)$ falls short of the lower margin $𝑦_𝑖 + \\epsilon$\n",
    "- Total error is the sum of these slack variables.\n",
    "\n",
    "Objective Function in SVR:\n",
    "- The SVR optimization problem minimizes both:\n",
    "    - A regularization term $||w||^2$ to keep the model general.\n",
    "    - The sum of slack variables (errors) to account for predictions outside the margin.\n",
    "\n",
    "$$ Minimize_{w,b,\\xi,\\xi^*}: \\frac{1}{2} ||w||^2 + C \\sum^n_{i=1} (\\xi_i + \\xi_i^*)$$\n",
    "- Subject to:\n",
    "$$ y_i - < w, x_i > - b \\leq \\epsilon + \\xi_i, $$\n",
    "$$ < w, x_i > + b - y_i \\leq \\epsilon + \\xi^*_i, $$\n",
    "$$ \\xi_i , \\xi^*_i \\geq 0. $$\n",
    "- Where:\n",
    "    - $||w||^2$: Regularization term, controlling model complexity.\n",
    "    - C: Regularization parameter, balancing margin violations and model simplicity.\n",
    "    - $\\sum^n_{i=1} (\\xi_i + \\xi_i^*)$: Total error outside the $\\epsilon$-margin.\n",
    "\n",
    "##### Key Parameters Influencing Error:\n",
    "Epsilon ($\\epsilon$):\n",
    "- Larger $\\epsilon$: Allows more tolerance, leading to fewer points outside the margin (lower total error but less sensitivity to small variations in data).\n",
    "- Smaller $\\epsilon$: Reduces tolerance, increasing the sensitivity of the model to small deviations but possibly increasing total error.\n",
    "\n",
    "Regularization Parameter (C):\n",
    "- Large C: Penalizes errors more heavily, leading to smaller total error but less tolerance for deviations.\n",
    "- Small C: Penalizes errors less, allowing more tolerance for margin violations.\n",
    "\n",
    "##### SVR Error Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(100, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(scale=0.2, size=X.shape[0])\n",
    "\n",
    "# Train SVR with RBF kernel\n",
    "epsilon = 0.1\n",
    "model = SVR(kernel='rbf', C=1.0, epsilon=epsilon)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate error\n",
    "error = np.maximum(0, np.abs(y_pred - y) - epsilon)\n",
    "\n",
    "# Plot data\n",
    "plt.scatter(X, y, color='blue', label='True data')\n",
    "plt.plot(X, y_pred, color='red', label='SVR prediction')\n",
    "plt.fill_between(X.ravel(), y_pred - epsilon, y_pred + epsilon, color='gray', alpha=0.3, label='Epsilon margin')\n",
    "plt.legend()\n",
    "plt.title('SVR with RBF Kernel')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "# Print total error\n",
    "print(f\"Total error (sum of slack variables): {np.sum(error):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b997c3b",
   "metadata": {},
   "source": [
    "# Bayesian Models\n",
    "What It Means: \n",
    "- Bayesian models incorporate prior knowledge or beliefs with the data to update the probability of outcomes as new evidence is available.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each output is a probability distribution reflecting both prior knowledge and the new data, offering a range of likely outcomes.\n",
    "\n",
    "Performance Measures:\n",
    "- Log-Likelihood: Measures how well the model explains the data; higher values indicate better fit.\n",
    "\n",
    "Lay Explanation: \n",
    "- Bayesian models are like revising a guess based on new evidence—updating beliefs as we get more information.\n",
    "- After you have seen all the evidence, How much do you believe something and it assumes that you dont just believe or dis-believe something, you must assign a degree of belief. Its about callibrating the degree of belief to the strenght of the evidence.\n",
    "- The degree of belief in a hypothesis should be determined by how likely the hypothesis is beforehand (before looking at the evidence). If it is True, what are the odd that youll see the evidence that you are seeing scaled by how common is that evidence across the board, whether the hypothesis is true of false.\n",
    "\n",
    "The benefits of Naive Bayes are that the model is simple to build and is useful on large data sets. Further, the model makes an explicit assumption that the features are independent given the class label. What does this mean? Well first let's consider the concept of independence. Independence is a concept from probability theory and it implies that if we have two random variables $X$ and $Y$, then\n",
    "\n",
    "$$\n",
    "P(X \\cap Y) = P(X)P(Y)\n",
    "$$\n",
    "\n",
    "This is where the qualifier \"Naive\" in \"Naive Bayes\" comes from. The assumption is Naive because it often does not hold. The assumption of independence implies that the model assumes that there is zero correlation among the features. Hence, the joint probability distribution $P(X, Y)$ can be obtained from the marginal probability distributions $P(X)$ and $P(Y)$ simply by multiplication. We will use the above independence assumption, conditional probability rules, and Bayes theorem to develop some theory for how the Naive Bayes model works.\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} $$\n",
    "\n",
    "$$ Posterior Probability = \\frac{Likelihood \\times Prior}{Evidence} $$\n",
    "\n",
    "- Likelihood (P(B∣A)): refers to - if the Hypothesis is true, How likely is it that you will see the evidence that you are now seeing\n",
    "    - The probability of observing event B given that A is true.\n",
    "    - Represents how likely the evidence is under the hypothesis.\n",
    "- Prior Probability (P(A)) is based on everything we know so far:\n",
    "    - The initial probability of event A before observing any evidence.\n",
    "    - Represents our prior knowledge or belief about\n",
    "        - Historic Data\n",
    "        - Established Theories\n",
    "- Marginal Probability (P(B)) Evidence: Commonness of the data (How often to you expect to see that evidence accross the board, whether the idea you are testing is true or false)\n",
    "    - The total probability of observing event B, considering all possible hypotheses.\n",
    "    - Acts as a normalizing constant to ensure the posterior probability sums to 1.\n",
    "- Posterior Probability (P(A∣B)):\n",
    "\n",
    "Use Case: \n",
    "- To incorporate prior knowledge and quantify uncertainty.\n",
    "    - The updated probability of event A after observing event B.\n",
    "    - Represents our revised belief about A given the evidence.\n",
    "\n",
    "Model Types: \n",
    "- Bayesian Linear Regression, \n",
    "- Bayesian Networks.\n",
    "\n",
    "Application of Bayes' Theorem\n",
    "\n",
    "Bayes' theorem is an important result in statistics and it allows us to obtain a posterior distribution given a prior distribution and a likelihood. Now that is a mouthful, but don't worry, we will walk you through it. First, let's denote class labels using $K = 1, \\cdots, k$ - seem familiar? What we are trying to say here is that each class label corresponds to a number between $1$ and $k$. So, given this, the **independence assumption conditional on the class label is denoted**\n",
    "\n",
    "$$\n",
    "P(X_{1} \\cap X_{2} \\mid K = k) = P(X_{1} \\mid K = k) P(X_{2} \\mid K = k)\n",
    "$$\n",
    "\n",
    "### **Calculating Joint Probability: Independent vs Dependent Events**\n",
    "Joint probability is a key concept for combining evidence and understanding relationships between events.\n",
    "\n",
    "**Independent Events**\n",
    "- Two events A and B are independent if the occurrence of one event does not affect the probability of the other event. In other words, knowing that one event has occurred gives no information about the other event.\n",
    "\n",
    "Mathematical Condition\n",
    "- For independent events:\n",
    "$$ P(A|B) = P(A) \\text{ and } P(B|A) = P(B) $$\n",
    "\n",
    "**Joint Probability for Independent Events**\n",
    "- For independent events A and B, the joint probability is the product of their individual probabilities:\n",
    "$$ P(A \\cap B) = P(A) \\cdot P(B) $$\n",
    "- Example:\n",
    "    - The probability of rolling a 6 on a die (A) and flipping heads on a coin (B):\n",
    "$$ P(A \\cap B) = P(A) \\cdot P(B) $$\n",
    "$$ P(A \\cap B) = \\frac{1}{6} \\cdot \\frac{1}{2} $$\n",
    "$$ P(A \\cap B) = \\frac{1}{12} $$\n",
    "\n",
    "Why This Formula?\n",
    "- Since the occurrence of A does not influence B (and vice versa), the probability of both events happening together is simply the product of their individual probabilities.\n",
    "- This is a direct consequence of the definition of independence.\n",
    "\n",
    "Interpretation:\n",
    "- The events are independent because the outcome of the die roll does not affect the coin flip.\n",
    "\n",
    "**Dependent Events**\n",
    "- Two events A and B are dependent if the occurrence of one event affects the probability of the other event. In other words, knowing that one event has occurred provides information about the other event.\n",
    "\n",
    "Mathematical Condition\n",
    "- For dependent events:\n",
    "$$ P(A|B) \\neq P(A) \\text{ and } P(B|A) \\neq P(B) $$\n",
    "\n",
    "**Joint Probability for Dependent Events**\n",
    "- For dependent events A and B, the joint probability is calculated using conditional probability:\n",
    "$$ P(A \\cap B) = P(A|B) \\cdot P(B) \\text{ or } P(A \\cap B) = P(B|A) \\cdot P(A)  $$\n",
    "- Example:\n",
    "    - The probability of it raining (A) and being cloudy (B):\n",
    "        - Suppose P(B) = 0.4 (probability of being cloudy).\n",
    "        - Suppose P(A|B) = 0.7  (probability of rain given it’s cloudy).\n",
    "    - then:\n",
    "$$ P(A \\cap B) = P(A|B) \\cdot P(B) $$\n",
    "$$ P(A \\cap B) = 0.7 \\cdot 0.4 $$\n",
    "$$ P(A \\cap B) = 0.28 $$\n",
    "\n",
    "Why This Formula?\n",
    "- Since the occurrence of A affects B (or vice versa), we cannot simply multiply their individual probabilities.\n",
    "- Instead, we use the conditional probability P(A∣B) or P(B∣A) to account for the dependence between the events.\n",
    "\n",
    "Interpretation:\n",
    "- The probability of it being cloudy and raining is 0.28.\n",
    "- The events are dependent because the probability of rain increases when it is cloudy.\n",
    "\n",
    "Key Differences Between Independent and Dependent Events\n",
    "|Aspect\t|Independent Events\t|Dependent Events|\n",
    "|-------|-------------------|----------------|\n",
    "|Definition\t|Occurrence of one event does not affect the other.\t|Occurrence of one event affects the other.|\n",
    "|Condition\t| $ P(A\\|B) = P(A) \\text{ and } P(B\\|A) = P(B) $\t| $ P(A\\|B) \\neq P(A) \\text{ and } P(B\\|A) \\neq P(B)$ |\n",
    "|Joint Probability|\t$P(A \\cap B)= P(A) \\cdot P(B)$| $(P(A \\cap B) = P(A\\|B) \\cdot P(B))$.|\n",
    "|Example\t|Rolling a die and flipping a coin.|\tRain and cloudiness.|\n",
    "\n",
    "Why Joint Probability is Calculated Differently\n",
    "\n",
    "Independent Events:\n",
    "- The occurrence of one event does not influence the other, so their probabilities can be multiplied directly.\n",
    "- Example: Rolling a die and flipping a coin are unrelated events.\n",
    "\n",
    "Dependent Events:\n",
    "- The occurrence of one event affects the probability of the other, so we must account for this relationship using conditional probability.\n",
    "- Example: Rain is more likely when it is cloudy, so the probability of rain depends on cloudiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b7fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate joint probability for independent events\n",
    "def joint_probability_independent(P_A, P_B):\n",
    "    return P_A * P_B\n",
    "\n",
    "# Function to calculate joint probability for dependent events\n",
    "def joint_probability_dependent(P_A_given_B, P_B):\n",
    "    return P_A_given_B * P_B\n",
    "\n",
    "# Example 1: Independent events\n",
    "P_A = 1/6  # Probability of rolling a 6\n",
    "P_B = 1/2  # Probability of flipping heads\n",
    "P_A_and_B_independent = joint_probability_independent(P_A, P_B)\n",
    "print(f\"Joint Probability (Independent Events): {P_A_and_B_independent:.4f}\")\n",
    "\n",
    "# Example 2: Dependent events\n",
    "P_B_cloudy = 0.4  # Probability of being cloudy\n",
    "P_A_given_B_rain = 0.7  # Probability of rain given it's cloudy\n",
    "P_A_and_B_dependent = joint_probability_dependent(P_A_given_B_rain, P_B_cloudy)\n",
    "print(f\"Joint Probability (Dependent Events): {P_A_and_B_dependent:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adccad56",
   "metadata": {},
   "source": [
    "In the expression above $X_{1}$ and $X_{2}$ denote random predictor variables. Keep this in mind. Bayes' theorem states that\n",
    "\n",
    "$$\n",
    "P(K = k \\mid X) = \\frac{P(X \\mid K= k) P(K = k)}{P(X)}\n",
    "$$\n",
    "\n",
    "In the expression above:\n",
    "\n",
    "- $P(K = k \\mid X)$ is called the _posterior probability_ distribution,\n",
    "\n",
    "- $P(K = k)$ is the *prior*, and\n",
    "\n",
    "- $P(X \\mid K= k)$ is the _likelihood_.\n",
    "\n",
    "The posterior tells us what the probability is of the class being $k$ given a particular observation. The prior is basically a measure of how likely we think it is for any observation to be assigned to a particular class before we have observed any observations. The likelihood gives us a measure of what the data says about the probability that the observation belongs to class $k$.\n",
    "\n",
    "Bayes' Theorem is used in many real-world applications, such as:\n",
    "- Spam Filtering: Classifying emails as spam or not spam based on the presence of certain keywords.\n",
    "- Medical Diagnosis: Determining the probability of a disease given a set of symptoms.\n",
    "- Machine Learning: Building probabilistic models like Naive Bayes classifiers.\n",
    "\n",
    "##### **General algorithm:**\n",
    "\n",
    "**Step 1: For each class $k$ do:**\n",
    "- Find the likelihood $P(X_{1} \\cap X_{2} \\mid K = k) = P(X_{1} \\mid K = k)P(X_{2} \\mid K = k)$, using only the observations where the class is $k$ in the data. \n",
    "- Compute a prior probability for the current class $k$ = $\\frac{observations \\space in \\space class \\space k}{total \\space number \\space of \\space observations}$. \n",
    "- Use Bayes' theorem, with a denominator (i.e.: the evidence $P(X)$) of $1$, to compute the posterior probability distribution.\n",
    "    \n",
    "    \n",
    "**Step 2: At test time** \n",
    "- observations are assigned to classes with the highest posterior probability $P(X_{1} \\cap X_{2} \\mid K = k)$.\n",
    "\n",
    "**Generalise the model to include an arbitrary number of random predictor variables instead of 2, i.e.:**\n",
    "$$ X_{1}, \\cdots, X_{p} $$\n",
    "\n",
    "In this case\n",
    "$$ P(K = k \\mid X_{1} \\cap X_{2} \\cdots \\cap X_{p}) = \\frac{P(X_{1} \\cap X_{2} \\cdots \\cap X_{p} \\mid K= k) P(K = k)}{P(X)}$$\n",
    "\n",
    "Now remember the assumption in the back of your mind. This allows us to simplify the expression above to become\n",
    "$$ P(K = k \\mid X_{1} \\cap X_{2} \\cdots \\cap X_{p}) = \\frac{P(X_{1}\\mid K= k) P(X_{2}\\mid K= k) \\cdots P(X_{p}\\mid K= k) P(K = k)}{P(X)} $$\n",
    "\n",
    "A special mathematical symbol allows us to represent the product $P(X_{1}\\mid K= k) P(X_{2}\\mid K= k) \\cdots P(X_{p}\\mid K= k)$ as $\\prod_{i = 1}^{p} P(X_{i}\\mid K= k)$. \n",
    "- Hence, the expression above becomes:\n",
    "$$ P(K = k \\mid X_{1} \\cap X_{2} \\cdots \\cap X_{p}) = \\frac{\\prod_{i = 1}^{p} P(X_{i}\\mid K= k)P(K = k)}{P(X)} $$\n",
    "\n",
    "**Consider how the Bayes classifier assigns observations to a particular class** \n",
    "- One method, called [maximum a posteriori (MAP)](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation), maximises $P(K = k \\mid X_{1} \\cap X_{2} \\cdots \\cap X_{p})$ for $K = 1, \\cdots, k$ to assign the observation to the correct class. \n",
    "\n",
    "Let's consider what this means for $K = 2$. In this case we calculate\n",
    "\n",
    "$$\n",
    "P(K = 1 \\mid X_{1} \\cap X_{2} \\cdots \\cap X_{p}) \\\\\n",
    "P(K = 2 \\mid X_{1} \\cap X_{2} \\cdots \\cap X_{p}) \\\\\n",
    "$$\n",
    "\n",
    "If $P(K = 1 \\mid X_{1} \\cap X_{2} \\cdots \\cap X_{p}) > P(K = 2 \\mid X_{1} \\cap X_{2} \\cdots \\cap X_{p})$ then we say that the observation $\\underline{X}$ is assigned to class $1$ else it is assigned to class $2$. This concept can be generalised to more classes. \n",
    "\n",
    "We just want to make something clear here.\n",
    "- An observation i.e.: something that actually happened, is denoted\n",
    "$$\\underline{x}_{i} = (x_{i,1}, x_{i,2}, \\cdots, x_{i,p})$$\n",
    "- But a set of random variables, which captures all possible things that can happen for all observations, is denoted\n",
    "$$\\underline{X} = (X_{1}, X_{2}, \\cdots, X_{p})$$\n",
    "- Using this notation, $X_{i}, i = 1 \\cdots, p$ denotes a feature.\n",
    "\n",
    "### A word on Scikit learn\n",
    "\n",
    "`sklearn` provides three implementations of the Naive Bayes method:\n",
    "\n",
    "   a) **Gaussian:** It is used in classification and it assumes that features follow a normal distribution.\n",
    "\n",
    "   b) **Multinomial:** It is used for discrete counts. For example, let’s say,  we have a text classification problem. Here we can consider bernoulli trials which is one step further and instead of “word occurring in the document”, we have “count how often word occurs in the document”, you can think of it as “number of times outcome number x_i is observed over the n trials”.\n",
    "\n",
    "   c) **Bernoulli:** The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with a ‘bag of words’ model where the 1s & 0s are “word occurs in the document” and “word does not occur in the document” respectively.\n",
    "\n",
    "These implementations are merely the choice of our probability distribution $P$. We choose an implementation based on the nature of the features (i.e. predictor variables) in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebca964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: import the libraries that we will need\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, log_loss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Step 2: Make use of the Breast Cancer Dataset.\n",
    "# Load the breast cancer data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X.shape\n",
    "y.shape\n",
    "\n",
    "# Step 3: Fit the model\n",
    "# Get training and testing data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Define the model \n",
    "naive_bayes = GaussianNB()\n",
    "# Fit the model \n",
    "naive_bayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc4c4ea",
   "metadata": {},
   "source": [
    "##### Assess model performance\n",
    "\n",
    "So far, we've been using the classification report and confusion matrices to assess classification model performance. However, such metrics don't do a good job at highlighting how confident our model is in its predictions. \n",
    "\n",
    "Enter the [log loss](http://wiki.fast.ai/index.php/Log_Loss) function which, unlike other metrics, can penalise predictions based on how confident a model is with those predictions. For example, if our model predicts the wrong class with high probability, the log loss penalises it more (i.e.: assigns higher log loss) compared to a model that predicts the wrong class with low probability. As such, we generally feed class probabilities into the log loss function instead of the actual class predictions (i.e. thresholded probalities). \n",
    "\n",
    "For the log loss metric, lower is better, i.e.:, a perfect model would have a log loss of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f7abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the test set \n",
    "y_hat = naive_bayes.predict_proba(X_test)\n",
    "# Calculate the log loss (this was imported from sklearn above somewhere) \n",
    "print(\"The log loss error for our model is: \", log_loss(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd3a09",
   "metadata": {},
   "source": [
    "##### Plot the decision boundary \n",
    "It is recommended to visualise the decision boundary of our classifier where possible. Unfortunately, in this case, we have around 31 different variables (features + response) and we can visualise only 3 at a time. So let's do just that. \n",
    "\n",
    "Below, we create a visualisation which can represent the decision boundary between any two features in $X$. The $y$ (i.e. class label) is indicated by the color of each data point. The decision boundary is the line separating the two regions of blue and red, such that, any point falling into the red region, is assigned the red label (class 1) and any point falling into the blue region is assigned the blue label (class 2).\n",
    "\n",
    "To change which features you want to compare, simply change the values for `i` and `j`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fedc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0; j = 1\n",
    "naive_bayes.fit(X[:, [i, j]], y)\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    " \n",
    "x_min, x_max = X[:, i].min(), X[:, i].max()\n",
    "y_min, y_max = X[:, j].min(), X[:, j].max()\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 1000), np.linspace(y_min, y_max, 1000))\n",
    "\n",
    "y_hat = naive_bayes.predict(np.concatenate((xx.reshape(-1,1), yy.reshape(-1,1)), axis=1))\n",
    "y_hat = y_hat.reshape(xx.shape)\n",
    "\n",
    "ax1.pcolormesh(xx, yy, y_hat, cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.scatter(X[:, i], X[:, j], c=y, edgecolors='k', cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.set_xlabel('Feature '+str(i))\n",
    "ax1.set_ylabel('Feature '+str(j))\n",
    "ax1.set_xlim(xx.min(), xx.max())\n",
    "ax1.set_ylim(yy.min(), yy.max())\n",
    "ax1.set_xticks(())\n",
    "ax1.set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64479529",
   "metadata": {},
   "source": [
    "### Conditional Probability Using Bayes' Theorem\n",
    "Bayes' Theorem is used to update probabilities based on new evidence.\n",
    "\n",
    "Formula for Bayes' Theorem:\n",
    "$$ P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} $$\n",
    "- where:\n",
    "    - P(A∣B) = Probability of event A occurring given that B has occurred (Posterior Probability)\n",
    "    - P(B∣A) = Probability of event B occurring given that A has occurred (Likelihood)\n",
    "    - P(A) = Prior probability of event A occurring (Prior Probability)\n",
    "    - P(B) = Total probability of event B occurring (Evidence / Normalization Factor)\n",
    "\n",
    "##### Example 1: Disease Diagnosis\n",
    "- Event A: A person has a disease\n",
    "- Event B: The person tests positive\n",
    "\n",
    "Given:\n",
    "- P(A) = 0.01 (1% of the population has the disease)\n",
    "- P(B | A) = 0.95 (95% test positive if they have the disease)\n",
    "- P(B | Not A) = 0.05 (5% false positive rate if they don’t have the disease)\n",
    "\n",
    "We calculate P(B) using the **Law of Total Probability**:\n",
    "\n",
    "$$ P(B) = P(B|A) \\times P(A) + P(B| \\urcorner A) \\times P(\\urcorner A) $$\n",
    "$$ P(B) = (0.95 \\times 0.01) + (0.05 \\times 0.99) $$\n",
    "\n",
    "Now, apply Bayes' Theorem to find P(A | B):\n",
    "$$ P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} $$\n",
    "$$ P(A|B) = \\frac{ 0.95 \\times 0.01}{0.059} $$\n",
    "\n",
    "If you get a positive test result, your actual probability of having the disease is much lower than expected due to the false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7f6525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(A | B) = 0.1610\n"
     ]
    }
   ],
   "source": [
    "# Given probabilities\n",
    "P_A = 0.01   # Prior: Probability of having the disease\n",
    "P_B_given_A = 0.95  # Likelihood: Probability of testing positive if diseased\n",
    "P_B_given_not_A = 0.05  # False Positive Rate: Probability of testing positive without disease\n",
    "P_not_A = 1 - P_A  # Probability of not having the disease\n",
    "\n",
    "# Total probability of testing positive\n",
    "P_B = (P_B_given_A * P_A) + (P_B_given_not_A * P_not_A)\n",
    "\n",
    "# Bayes' Theorem: Probability of having the disease given a positive test\n",
    "P_A_given_B = (P_B_given_A * P_A) / P_B\n",
    "\n",
    "print(f\"P(A | B) = {P_A_given_B:.4f}\")  # Final posterior probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e6a616",
   "metadata": {},
   "source": [
    "##### Example: Calculating Conditional Probabilities\n",
    "Problem Statement: Suppose we have the following data about a disease and a test for it:\n",
    "\n",
    "Prevalence of the disease (P(D)): 1% of the population has the disease.\n",
    "\n",
    "Test accuracy:\n",
    "- If a person has the disease, the test is positive 99% of the time : P(T+∣D)=0.99\n",
    "- If a person does not have the disease, the test is positive 5% of the time: P(T+∣¬D)=0.05\n",
    "\n",
    "We want to calculate:\n",
    "- The probability that a person has the disease given that the test is positive: P(D∣T+\n",
    "- The probability that a person does not have the disease given that the test is positive P(¬D∣T+)\n",
    "\n",
    "**Step 1: Define the Probabilities**\n",
    "- P(D)=0.01 (1% of the population has the disease).\n",
    "- P(¬D)= 1−P(D)= 0.99.\n",
    "- P(T+∣D)= 0.99.\n",
    "- P(T+∣¬D)= 0.05.\n",
    "\n",
    "**Step 2: Compute P(T+)**\n",
    "\n",
    "The **marginal probability** of a positive test result (P(T+)) is:\n",
    "$$ P(T+) = P(T+|D) \\cdot P(D) + P(T+|¬D) \\cdot P(¬D) $$\n",
    "\n",
    "Substitute the values:\n",
    "$$ P(T+) = (0.99) \\cdot P(0.01) + (0.05) \\cdot (0.99) $$\n",
    "$$ P(T+) = 0.0099 + 0.0495 $$\n",
    "$$ P(T+) = 0.0594 $$\n",
    "\n",
    "**Step 3: Apply Bayes' Theorem**\n",
    "\n",
    "Probability of having the disease given a positive test (P(D∣T+)):\n",
    "$$ P(D|T+) = \\frac{P(T+ |D) \\cdot P(D)}{P(T+)} $$\n",
    "$$ P(D|T+) = \\frac{0.99 \\cdot 0.01}{0.0594} $$\n",
    "$$ P(D|T+) \\approx 0.1667 $$\n",
    "\n",
    "Probability of not having the disease given a positive test P(¬D∣T+):\n",
    "$$ P(¬D|T+) = \\frac{P(T+ |¬D) \\cdot P(¬D)}{P(T+)} $$\n",
    "$$ P(¬D|T+) = \\frac{0.05 \\cdot 0.99}{0.0594} $$\n",
    "$$ P(¬D|T+) \\approx 0.8333 $$\n",
    "\n",
    "Interpretation\n",
    "- Even if the test is positive, there is only a 16.67% chance that the person actually has the disease.\n",
    "\n",
    "This is because the disease is rare in the population (low prior probability), and the test has a significant false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a930da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the probabilities\n",
    "P_D = 0.01  # P(D)\n",
    "P_not_D = 1 - P_D  # P(¬D)\n",
    "P_Tplus_given_D = 0.99  # P(T+ | D)\n",
    "P_Tplus_given_not_D = 0.05  # P(T+ | ¬D)\n",
    "\n",
    "# Compute P(T+)\n",
    "P_Tplus = (P_Tplus_given_D * P_D) + (P_Tplus_given_not_D * P_not_D)\n",
    "\n",
    "# Apply Bayes' Theorem\n",
    "P_D_given_Tplus = (P_Tplus_given_D * P_D) / P_Tplus\n",
    "P_not_D_given_Tplus = (P_Tplus_given_not_D * P_not_D) / P_Tplus\n",
    "\n",
    "# Print results\n",
    "print(f\"P(D | T+): {P_D_given_Tplus:.4f}\")\n",
    "print(f\"P(¬D | T+): {P_not_D_given_Tplus:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb654ae",
   "metadata": {},
   "source": [
    "## Simple Bayes Classifier | Naive Bayes Classifier\n",
    "\n",
    "A simple Bayes classifier (often referred to as a Naive Bayes classifier) is a probabilistic machine learning model used for classification tasks, particularly for two-class problems. It is based on Bayes' Theorem and makes strong **independence assumptions between features**.\n",
    "- The term \"naive\" refers to the assumption that all features are conditionally independent given the class label. Despite this simplifying assumption, Naive Bayes often performs well in practice.\n",
    "\n",
    "Below are the salient features of a simple Bayes classifier, including the Bayes decision boundary and Bayes error rate:\n",
    "\n",
    "### Bayes' Theorem\n",
    "Bayes' Theorem is a fundamental concept in probability theory that describes how to update the probabilities of hypotheses when given evidence. It is widely used in machine learning, statistics, and data science for tasks like classification, inference, and decision-making.\n",
    "\n",
    "The classifier is based on Bayes' Theorem, which calculates the posterior probability of a class given the input features. For two classes $C_1$ and $C_2$,the posterior probability for class $C_i$ given a feature vector $x = (x_1, x_2, ..., x_n)$ is:\n",
    "\n",
    "$$ P(C_i|x) = \\frac{P(x|C_i) \\times P(C_i)}{P(x)} $$\n",
    "- where: \n",
    "    - $P(x|C_i)$: Likelihood of observing x given class $C_i$\n",
    "    - $P(C_i)$: Prior probability of class $C_i$\n",
    "    - $P(x): Marginal probability of x (acts as a normalizing constant).\n",
    "\n",
    "### Naive Assumption\n",
    "The classifier assumes that the features $x_1, x_2, ..., x_n$ are **conditionally independent** given the class. This simplifies the **likelihood calculation**:\n",
    "$$ P(x|C_i) = \\prod^n_{j = 1} P(x_j | C_i)$$\n",
    "\n",
    "This assumption is called \"naive\" because it ignores potential dependencies between features.\n",
    "\n",
    "### Decision Rule\n",
    "The classifier assigns the input x to the class with the highest posterior probability. For two classes, the decision rule is:\n",
    "$$ \\text{Assign x to } C_i \\text{ if } P(C_1|x) > P(C_2|x), \\text{ else assign to } C_2 $$\n",
    "\n",
    "This can also be expressed in terms of the logarithm of the posterior probabilities to simplify computations:\n",
    "$$ \\text{Assign x to } C_i \\text{ if } Log P(C_1|x) > Log P(C_2|x)$$\n",
    "\n",
    "### Bayes Decision Boundary\n",
    "The Bayes decision boundary is the set of points in the feature space where the posterior probabilities of the two classes are equal:\n",
    "$$ P(C_1 | x) = P(C_2| x)$$\n",
    "\n",
    "This boundary separates the feature space into regions corresponding to each class. For a simple Bayes classifier, the decision boundary is often linear or quadratic, depending on the distribution of the features.\n",
    "\n",
    "### Bayes Error Rate\n",
    "The Bayes error rate is the minimum possible error rate for a given classification problem, achieved by the Bayes classifier. It represents the inherent uncertainty in the data due to overlapping class distributions. Mathematically, it is defined as:\n",
    "\n",
    "$$Bayes Error Rate = 1 − E_x [max_{i} P(C_i|x)] $$\n",
    "- where:\n",
    "    - $E_x$: Expectation over the feature space.\n",
    "    - $max_i P(C_i|x)$: Maximum posterior probability for the true class.\n",
    "\n",
    "The Bayes error rate is a theoretical lower bound on the classification error and cannot be reduced further, even with a perfect model.\n",
    "\n",
    "### **Conditions for Applying Naive Bayes**\n",
    "Conditional Independence:\n",
    "- The Naive Bayes classifier assumes that features are conditionally independent given the class label. This assumption may not hold in real-world data, but the classifier often performs well even when the assumption is violated.\n",
    "\n",
    "Discrete or Continuous Features:\n",
    "- Naive Bayes can handle both discrete (categorical) and continuous features.\n",
    "    - For discrete features, the likelihood is computed using frequency counts.\n",
    "    - For continuous features, the likelihood is typically modeled using a probability distribution (e.g., Gaussian).\n",
    "\n",
    "Small to Medium-Sized Datasets:\n",
    "- Naive Bayes is computationally efficient and works well with small to medium-sized datasets.\n",
    "\n",
    "High-Dimensional Data:\n",
    "- Naive Bayes performs well with high-dimensional data (e.g., text data with many features) because the independence assumption reduces the complexity of the model.\n",
    "\n",
    "Balanced or Imbalanced Classes:\n",
    "- Naive Bayes can handle both balanced and imbalanced class distributions, as it relies on probabilities rather than decision boundaries.\n",
    "\n",
    "### **Calculate the Bayes Error Rate** \n",
    "- we need a real-world example where we know the true class-conditional distributions of the data. Let’s consider a simple binary classification problem with two classes, $C_1$ and $C_2$, and a single feature x. We’ll assume the feature x follows known probability distributions for each class.\n",
    "\n",
    "Example: Medical Diagnosis\n",
    "- Suppose we are building a classifier to diagnose a disease ($C_1$:Disease , $C_2$:No Disease) based on a single test result x (e.g., a blood test score). The distributions of x for the two classes are:\n",
    "    - $P(x|C_1)$: Normal distribution with mean $\\mu$ = 10 and variance $\\sigma^2$ = 4.\n",
    "    - $P(x|C_2)$: Normal distribution with mean $\\mu$ = 6 and variance $\\sigma^2$ = 4.\n",
    "\n",
    "Assume the prior probabilities are:\n",
    "- $P(C_1)$ = (30% of the population has the disease).\n",
    "- $ P(C_2)$ = (70% of the population does not have the disease).\n",
    "\n",
    "**Step 1: Class-Conditional Distributions**\n",
    "The probability density functions (PDFs) for the two classes are:\n",
    "$$ P(x|C_1) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu_1)^2}{2\\sigma^2}} = \\frac{1}{\\sqrt{8\\pi}} e^{-\\frac{(x-10)^2}{8}}$$\n",
    "$$ P(x|C_2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu_2)^2}{2\\sigma^2}} = \\frac{1}{\\sqrt{8\\pi}} e^{-\\frac{(x-6)^2}{8}}$$\n",
    "\n",
    "**Step 2: Posterior Probabilities**\n",
    "Using Bayes' Theorem, the posterior probabilities are:\n",
    "$$ P(C_i|x) = \\frac{P(x|C_1) \\times P(C_1)}{P(x)} $$\n",
    "$$ P(C_i|x) = \\frac{P(x|C_2) \\times P(C_2)}{P(x)} $$\n",
    "- Where:\n",
    "    - $P(x) = P(x|C_1) \\cdot P(C_1) + P(x|C_2) \\cdot P(C_2)$\n",
    "\n",
    "**Step 3: Bayes Decision Boundary**\n",
    "The Bayes decision boundary occurs where the posterior probabilities are equal:\n",
    "$$ P(C1|x) = P(C_2|x)$$\n",
    "\n",
    "Substituting the likelihoods and priors:\n",
    "$$ P(x|C_1) \\cdot P(C_1) = P(x|C_2) \\cdot P(C_2)$$\n",
    "\n",
    "Taking the natural logarithm of both sides:\n",
    "$$ -\\frac{(x-10)^2}{8} + \\ln (0.3) = -\\frac{(x-6)^2}{8} + \\ln (0.7) $$\n",
    "\n",
    "Solve for x:\n",
    "$$ (x - 6)^2 - (x - 10)^2 = 8 \\ln (\\frac{0.7}{0.3}) $$\n",
    "$$ x^2 - 12x + 36 - (x^2 - 20x + 100) = 8 \\ln (\\frac{7}{3}) $$\n",
    "$$ 8x - 64 = 8 \\ln (\\frac{7}{3}) $$\n",
    "$$ x = 8 + (\\frac{7}{3}) \\approx 8 + 0.847 = 8.847 $$\n",
    "\n",
    "So, the Bayes decision boundry is at x = 8.847\n",
    "\n",
    "**Step 4: Bayes Error Rate**\n",
    "The Bayes Error Rate is the probability of misclassification when using the Bayes decision rule. It is calculated as:\n",
    "\n",
    "$$Bayes Error Rate = \\int^{\\infty}_{\\infty} \\min [P(C_1|x), P(C_1|x)] \\cdot P(x)dx $$\n",
    "In this example, the error occurs in the regions where:\n",
    "- $ x < 8.847$ but the true class is $C_1$.\n",
    "- $ x \\geq 8.847$ but the true class is $C_2$\n",
    "\n",
    "The Bayes Error Rate can be computed as:\n",
    "$$Bayes Error Rate = P(C_1) \\cdot P(x < 8.847|C_1) + P(C_2) \\cdot P(x \\geq 8.847| C_2)$$\n",
    "\n",
    "Using the cumulative distribution function (CDF) of the normal distribution:\n",
    "$$ P(x < 8.847|C_1) = \\Phi (\\frac{8.847 - 10}{2}) = \\Phi (-0.5765) \\approx 0.282 $$\n",
    "$$ P(x \\geq 8.847| C_2) = 1 - \\Phi (\\frac{8.847 - 6}{2}) = 1- \\Phi (1.4235) \\approx 1 - 0.923  = 0.077$$\n",
    "Thus:\n",
    "$$ Bayes Error Rate = 0.3 \\cdot 0.282 + 0.7 \\cdot 0.077 \\approx 0.0846 + 0.0539 = 0.1385 $$\n",
    "\n",
    "The Bayes Error Rate is approximately 13.85%.\n",
    "_____________________\n",
    "\n",
    "Interpretation of Outcomes\n",
    "\n",
    "Bayes Error Rate (13.85%):\n",
    "- This is the minimum possible error rate for this classification problem, given the overlapping distributions of the two classes.\n",
    "- It represents the inherent uncertainty in the data due to the overlap between the class-conditional distributions.\n",
    "\n",
    "Decision Boundary (x = 8.847):\n",
    "- This is the optimal threshold for classifying a test result x into one of the two classes.\n",
    "\n",
    "Implications:\n",
    "- Even with a perfect model, the classifier will make errors 13.85% of the time due to the overlap in the distributions of the two classes.\n",
    "- Reducing the Bayes Error Rate would require better features or less overlap between the class distributions.\n",
    "\n",
    "##### **Key Characteristics**\n",
    "- Simplicity: The naive Bayes classifier is easy to implement and computationally efficient.\n",
    "- Scalability: It works well with high-dimensional data due to the independence assumption.\n",
    "- Robustness: It performs well even with limited training data.\n",
    "- Limitations: The independence assumption may not hold in real-world data, leading to suboptimal performance in some cases.\n",
    "\n",
    "##### Differences Between Naive Bayes and Bayesian Classifier\n",
    "|Aspect |Naive Bayes Classifier|\tBayesian Classifier  |\n",
    "|-------|----------------------|-------------------------|\n",
    "|Feature Independence |\tAssumes all features are conditionally independent. |\tDoes not assume feature independence. |\n",
    "|Complexity\t |Simple and computationally efficient.|\tCan be more complex, depending on the model. |\n",
    "|Likelihood Calculation\t| Uses the product of individual feature likelihoods. |\tMay use joint distributions or more complex models.|\n",
    "|Use Cases\t| Text classification, spam filtering, etc. |\tGeneral probabilistic modeling and inference.  |\n",
    "|Scalability\t|Scales well with high-dimensional data.\t|May struggle with high-dimensional data. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d25d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.integrate import quad\n",
    "\n",
    "# Define the parameters\n",
    "mu1, sigma1 = 10, np.sqrt(4)  # Disease (C1)\n",
    "mu2, sigma2 = 6, np.sqrt(4)   # No Disease (C2)\n",
    "P_C1 = 0.3  # Prior probability of Disease\n",
    "P_C2 = 0.7  # Prior probability of No Disease\n",
    "\n",
    "# Define the class-conditional PDFs\n",
    "def pdf_C1(x):\n",
    "    return norm.pdf(x, loc=mu1, scale=sigma1)\n",
    "\n",
    "def pdf_C2(x):\n",
    "    return norm.pdf(x, loc=mu2, scale=sigma2)\n",
    "\n",
    "# Define the marginal PDF P(x)\n",
    "def marginal_pdf(x):\n",
    "    return pdf_C1(x) * P_C1 + pdf_C2(x) * P_C2\n",
    "\n",
    "# Define the posterior probabilities\n",
    "def posterior_C1(x):\n",
    "    return (pdf_C1(x) * P_C1) / marginal_pdf(x)\n",
    "\n",
    "def posterior_C2(x):\n",
    "    return (pdf_C2(x) * P_C2) / marginal_pdf(x)\n",
    "\n",
    "# Find the Bayes decision boundary (where P(C1 | x) = P(C2 | x))\n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "def decision_boundary_equation(x):\n",
    "    return posterior_C1(x) - posterior_C2(x)\n",
    "\n",
    "x_boundary = fsolve(decision_boundary_equation, x0=8)[0]  # Initial guess x0=8\n",
    "print(f\"Bayes Decision Boundary: x = {x_boundary:.4f}\")\n",
    "\n",
    "# Define the integrand for the Bayes Error Rate\n",
    "def integrand(x):\n",
    "    return min(posterior_C1(x), posterior_C2(x)) * marginal_pdf(x)\n",
    "\n",
    "# Calculate the Bayes Error Rate using numerical integration\n",
    "bayes_error_rate, _ = quad(integrand, -np.inf, np.inf)\n",
    "print(f\"Bayes Error Rate: {bayes_error_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd8d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Step 1: Define the distributions\n",
    "mu1, sigma1 = 10, np.sqrt(4)  # Disease (C1)\n",
    "mu2, sigma2 = 6, np.sqrt(4)   # No Disease (C2)\n",
    "\n",
    "# Step 2: Define the prior probabilities\n",
    "P_C1 = 0.3  # P(C1)\n",
    "P_C2 = 0.7  # P(C2)\n",
    "\n",
    "# Step 3: Function to compute posterior probabilities\n",
    "def posterior_probability(x, mu, sigma, prior):\n",
    "    \"\"\"\n",
    "    Compute the posterior probability for a given class.\n",
    "    \"\"\"\n",
    "    likelihood = norm.pdf(x, loc=mu, scale=sigma)  # P(x | C)\n",
    "    return likelihood * prior  # P(x | C) * P(C)\n",
    "\n",
    "# Step 4: Function to classify a new test score\n",
    "def classify(x):\n",
    "    \"\"\"\n",
    "    Classify a test score x as Disease (C1) or No Disease (C2).\n",
    "    \"\"\"\n",
    "    # Compute unnormalized posterior probabilities\n",
    "    P_C1_given_x = posterior_probability(x, mu1, sigma1, P_C1)\n",
    "    P_C2_given_x = posterior_probability(x, mu2, sigma2, P_C2)\n",
    "    \n",
    "    # Normalize the probabilities\n",
    "    P_x = P_C1_given_x + P_C2_given_x  # Marginal probability P(x)\n",
    "    P_C1_given_x /= P_x\n",
    "    P_C2_given_x /= P_x\n",
    "    \n",
    "    # Classify based on the higher posterior probability\n",
    "    if P_C1_given_x > P_C2_given_x:\n",
    "        return \"Disease (C1)\", P_C1_given_x, P_C2_given_x\n",
    "    else:\n",
    "        return \"No Disease (C2)\", P_C1_given_x, P_C2_given_x\n",
    "\n",
    "# Step 5: Test the classifier with a new test score\n",
    "x_new = 8  # New test score\n",
    "classification, P_C1_given_x, P_C2_given_x = classify(x_new)\n",
    "print(f\"Classification: {classification}\")\n",
    "print(f\"Posterior Probability of Disease (C1): {P_C1_given_x:.4f}\")\n",
    "print(f\"Posterior Probability of No Disease (C2): {P_C2_given_x:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bf292b",
   "metadata": {},
   "source": [
    "## Simple Bayesian classifier\n",
    "\n",
    "A Bayesian classifier  is a probabilistic machine learning model that uses Bayes' Theorem to predict the class of a given input based on the observed features.\n",
    "\n",
    "Below are the salient features of a simple Bayes classifier, including the Bayes decision boundary and Bayes error rate:\n",
    "\n",
    "### Bayes' Theorem\n",
    "The classifier is based on Bayes' Theorem, which calculates the posterior probability of a class given the input features. For two classes $C_1$ and $C_2$,the posterior probability for class $C_i$ given a feature vector $x = (x_1, x_2, ..., x_n)$ is:\n",
    "\n",
    "$$ P(C_i|x) = \\frac{P(x|C_i) \\times P(C_i)}{P(x)} $$\n",
    "- where: \n",
    "    - $P(x|C_i)$: Likelihood of observing x given class $C_i$\n",
    "    - $P(C_i)$: Prior probability of class $C_i$\n",
    "    - $P(x): Marginal probability of x (acts as a normalizing constant).\n",
    "\n",
    "### Decision Rule\n",
    "The classifier assigns the input x to the class with the highest posterior probability.\n",
    "$$ \\text{Assign x to } C_i \\text{ if } P(C_i|x) > P(C_j|x), \\text{ for all } j \\neq \\text{to i} $$\n",
    "\n",
    "### Naive Assumption\n",
    "The classifier assumes that the features $x_1, x_2, ..., x_n$ are conditionally independent given the class. This simplifies the **likelihood calculation**:\n",
    "$$ P(x|C_i) = \\prod^n_{j = 1} P(x_j | C_i)$$\n",
    "\n",
    "This assumption is called \"naive\" because it ignores potential dependencies between features.\n",
    "\n",
    "#### Example: Email Spam detection with two features ($x_1$: free, $x_2$: money)\n",
    "\n",
    "$$P(C_1) = \\frac{3}{5} = 0.6 \\text{ (probability of spam).} $$\n",
    "$$P(C_2) = \\frac{2}{5} = 0.4 \\text{ (probability of not spam).} $$\n",
    "\n",
    "**Step 1: Compute Likelihoods**\n",
    "\n",
    "We compute the likelihoods $P(x_j | C_i)$ for each feature and class.\n",
    "- For $C_1$ (Spam):\n",
    "    - $P(x_1 = 1 | C_1) = \\frac{2}{3} \\quad \\text{(``free'' appears in 2 out of 3 spam emails)}$\n",
    "    - $P(x_1 = 0 | C_1) = \\frac{1}{3}.$\n",
    "    - $P(x_2 = 1 | C_1) = \\frac{2}{3} \\quad \\text{(``money'' appears in 2 out of 3 spam emails).}$\n",
    "    - $P(x_2 = 0 | C_1) = \\frac{1}{3}.$\n",
    "- For $C_2$ (Not Spam):\n",
    "    - $P(x_1 = 1 | C_2) = \\frac{0}{2} = 0 \\quad \\text{(``free'' does not appear in not spam emails).}$\n",
    "    - $P(x_1 = 0 | C_2) = \\frac{2}{2} = 1.$\n",
    "    - $P(x_2 = 1 | C_2) = \\frac{1}{2} \\quad \\text{(``money'' appears in 1 out of 2 not spam emails).}$\n",
    "    - $P(x_2 = 0 | C_2) = \\frac{1}{2}.$\n",
    "\n",
    "**Step 2: Classify a New Email**\n",
    "- For $C_1$ (Spam):\n",
    "$$P(C_1 | x) \\propto P(x_1 = 1 | C_1) \\cdot P(x_2 = 1 | C_1) \\cdot P(C_1)$$\n",
    "$$P(C_1 | x) \\propto \\frac{2}{3} \\times \\frac{2}{3} \\times 0.6 $$\n",
    "$$= \\frac{4}{9} \\times 0.6$$\n",
    "$$= 0.2667$$\n",
    "\n",
    "- For $C_2$ (Not Spam):\n",
    "$$P(C_2 | x) \\propto P(x_1 = 1 | C_2) \\cdot P(x_2 = 1 | C_2) \\cdot P(C_2)$$\n",
    "$$P(C_2 | x) \\propto 0 \\times \\frac{1}{2} \\times 0.4 = 0$$\n",
    "\n",
    "Since $P(C_1 | x) > P(C_2 | x)$, the email is classified as $\\text{spam}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Define the training data\n",
    "# Features: \"free\" (x1), \"money\" (x2)\n",
    "# Class: Spam (C1), Not Spam (C2)\n",
    "training_data = np.array([\n",
    "    [1, 1, 1],  # Email 1: Spam\n",
    "    [1, 0, 1],  # Email 2: Spam\n",
    "    [0, 1, 1],  # Email 3: Spam\n",
    "    [0, 0, 0],  # Email 4: Not Spam\n",
    "    [0, 1, 0]   # Email 5: Not Spam\n",
    "])\n",
    "\n",
    "# Separate features and labels\n",
    "X = training_data[:, :2]  # Features (x1, x2)\n",
    "y = training_data[:, 2]    # Labels (1: Spam, 0: Not Spam)\n",
    "\n",
    "# Step 2: Compute prior probabilities\n",
    "def compute_priors(y):\n",
    "    total_samples = len(y)\n",
    "    p_c1 = np.sum(y == 1) / total_samples  # P(C1)\n",
    "    p_c2 = np.sum(y == 0) / total_samples  # P(C2)\n",
    "    return p_c1, p_c2\n",
    "\n",
    "p_c1, p_c2 = compute_priors(y)\n",
    "print(f\"Prior Probabilities: P(C1) = {p_c1:.2f}, P(C2) = {p_c2:.2f}\")\n",
    "\n",
    "# Step 3: Compute likelihoods\n",
    "def compute_likelihoods(X, y):\n",
    "    # Initialize counts for each feature and class\n",
    "    n_features = X.shape[1]\n",
    "    likelihoods_c1 = np.zeros((n_features, 2))  # For C1 (Spam)\n",
    "    likelihoods_c2 = np.zeros((n_features, 2))  # For C2 (Not Spam)\n",
    "    \n",
    "    # Count occurrences of each feature value for each class\n",
    "    for feature in range(n_features):\n",
    "        for cls in [1, 0]:\n",
    "            if cls == 1:\n",
    "                likelihoods_c1[feature, 1] = np.sum((X[:, feature] == 1) & (y == 1)) / np.sum(y == 1)\n",
    "                likelihoods_c1[feature, 0] = 1 - likelihoods_c1[feature, 1]\n",
    "            else:\n",
    "                likelihoods_c2[feature, 1] = np.sum((X[:, feature] == 1) & (y == 0)) / np.sum(y == 0)\n",
    "                likelihoods_c2[feature, 0] = 1 - likelihoods_c2[feature, 1]\n",
    "    \n",
    "    return likelihoods_c1, likelihoods_c2\n",
    "\n",
    "likelihoods_c1, likelihoods_c2 = compute_likelihoods(X, y)\n",
    "print(\"Likelihoods for C1 (Spam):\")\n",
    "print(likelihoods_c1)\n",
    "print(\"Likelihoods for C2 (Not Spam):\")\n",
    "print(likelihoods_c2)\n",
    "\n",
    "# Step 4: Classify a new email\n",
    "def classify_email(new_email, p_c1, p_c2, likelihoods_c1, likelihoods_c2):\n",
    "    # Compute posterior probabilities\n",
    "    p_x_given_c1 = np.prod([likelihoods_c1[i, new_email[i]] for i in range(len(new_email))])\n",
    "    p_x_given_c2 = np.prod([likelihoods_c2[i, new_email[i]] for i in range(len(new_email))])\n",
    "    \n",
    "    p_c1_given_x = p_x_given_c1 * p_c1\n",
    "    p_c2_given_x = p_x_given_c2 * p_c2\n",
    "    \n",
    "    # Normalize probabilities\n",
    "    p_c1_given_x /= (p_c1_given_x + p_c2_given_x)\n",
    "    p_c2_given_x /= (p_c1_given_x + p_c2_given_x)\n",
    "    \n",
    "    # Decision rule\n",
    "    if p_c1_given_x > p_c2_given_x:\n",
    "        return \"Spam (C1)\", p_c1_given_x, p_c2_given_x\n",
    "    else:\n",
    "        return \"Not Spam (C2)\", p_c1_given_x, p_c2_given_x\n",
    "\n",
    "# New email: \"free\" (1), \"money\" (1)\n",
    "new_email = [1, 1]\n",
    "classification, p_c1_given_x, p_c2_given_x = classify_email(new_email, p_c1, p_c2, likelihoods_c1, likelihoods_c2)\n",
    "print(f\"\\nClassification Result: {classification}\")\n",
    "print(f\"Posterior Probabilities: P(C1 | x) = {p_c1_given_x:.4f}, P(C2 | x) = {p_c2_given_x:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5619149",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier to a medical diagnosis problem\n",
    "Explain why we don’t need to calculate the denominator of Bayes' rule in the Naive Bayes algorithm.\n",
    "\n",
    "Problem: Medical Diagnosis - Scenario:\n",
    "- We want to build a classifier to diagnose a disease based on the following features:\n",
    "    - Fever: Yes (1) or No (0).\n",
    "    - Cough: Yes (1) or No (0).\n",
    "    - Fatigue: Yes (1) or No (0).\n",
    "\n",
    "- The target variable is:\n",
    "    - Diagnosis: Disease (1) or No Disease (0).\n",
    "\n",
    "Dataset:\n",
    "|Fever $(x_1)$\t|Cough $(x_2)$\t|Fatigue $(x_3)$\t|Diagnosis ($y$)  |\n",
    "|-------|-------|-----------|-----------|\n",
    "|1\t|1\t|1\t|1  |\n",
    "|1\t|0\t|1\t|1  |\n",
    "|0\t|1\t|1\t|1  |\n",
    "|0\t|0\t|0\t|0  |\n",
    "|1\t|0\t|0\t|0  |\n",
    "|0\t|1\t|0\t|0  |\n",
    "\n",
    "\n",
    "**Step 1: Train the Naive Bayes Classifier**\n",
    "\n",
    "The Naive Bayes Classifier calculates the posterior probability of each class given the features and assigns the class with the highest probability. The formula for the posterior probability is:\n",
    "\n",
    "$$ P(C_i | x ) = \\frac{P(x | C_i) \\cdot P(C_i)}{P(x)} $$\n",
    "\n",
    "However, in Naive Bayes, we don’t need to calculate the denominator $P(x)$ because:\n",
    "- $P(x)$ is the same for all classes $C_i$.\n",
    "- We are only interested in comparing the relative probabilities of the classes, not their absolute values.\n",
    "- Therefore, we can ignore $P(x)$ and focus on the numerator:\n",
    "\n",
    "$$ P(C_i | x) \\alpha P(x|C_i) \\cdot P(C_i) $$\n",
    "\n",
    "**Step 2: Compute Prior Probabilities**\n",
    "\n",
    "The prior probability $P(C_i)$ is the probability of each class in the training data.\n",
    "- $P(\\text{Disease}) = \\frac{3}{6} = 0.5$\n",
    "- $P(\\text{No Disease}) = \\frac{3}{6} = 0.5$\n",
    "\n",
    "**Step 3: Compute Likelihoods**\n",
    "\n",
    "The likelihood $P(x∣C_i)$ is the probability of observing the features given the class. Under the Naive Bayes assumption, the features are conditionally independent given the class, so:\n",
    "\n",
    "$$ P(x| C_i) = P(x_1 | C_i) \\cdot P(x_2 | C_i) \\cdot P(x_3 | C_i) $$\n",
    "\n",
    "Likelihoods for Disease (C_1):\n",
    "- $P(x_1 | C_1) = \\frac{2}{3}$\n",
    "- $P(x_1 | C_1) = \\frac{2}{3}$\n",
    "- $P(x_1 | C_1) = \\frac{3}{3} = 1$\n",
    "\n",
    "Likelihoods for Disease ($C_2$):\n",
    "- $P(x_1 | C_2) = \\frac{1}{3}$\n",
    "- $P(x_1 | C_2) = \\frac{1}{3}$\n",
    "- $P(x_1 | C_2) = \\frac{0}{3} = 0$\n",
    "\n",
    "**Step 4: Classify a New Sample**\n",
    "\n",
    "Suppose we have a new patient with the following symptoms:\n",
    "- Fever: Yes (1).\n",
    "- Cough: Yes (1).\n",
    "- Fatigue: Yes (1).\n",
    "\n",
    "We want to classify this patient as having the disease or not.\n",
    "- Compute $P(C_1∣ x) or P(Disease∣x)$:\n",
    "$$ P(x| C_1) \\alpha P(x| C_1) \\cdot P(C_1)$$\n",
    "$$ P(x| C_1) =  P(x_1 = 1| C_1) \\cdot P(x_2 = 1| C_1) \\cdot P(x_3 = 1| C_1) $$\n",
    "$$ P(x| C_1) = \\frac{2}{3} \\cdot \\frac{2}{3} \\cdot 1 $$\n",
    "$$ P(x| C_1) = \\frac{4}{9} $$\n",
    "$$ P(C_1∣x) \\alpha \\frac{4}{9} \\cdot 0.5 = \\frac{2}{9} $$\n",
    "\n",
    "- Compute $P(C_2∣ x)$:\n",
    "$$ P(x| C_2) \\alpha P(x| C_2) \\cdot P(C_2)$$\n",
    "$$ P(x| C_2) =  P(x_1 = 1| C_2) \\cdot P(x_2 = 1| C_2) \\cdot P(x_3 = 1| C_2) $$\n",
    "$$ P(x| C_2) = \\frac{1}{3} \\cdot \\frac{1}{3} \\cdot 0 $$\n",
    "$$ P(x| C_2) = 0 $$\n",
    "$$ P(C_2∣x) \\alpha 0 \\cdot 0.5 = 0 $$\n",
    "\n",
    "Decision:\n",
    "- $P(C_1|x) \\alpha \\frac{2}{9}$\n",
    "- $P(C_2|x) \\alpha 0 $\n",
    "   - Since $\\frac{2}{9} > 0$, the patient is classified as having the disease.\n",
    "\n",
    "### **Why We Don’t Need to Calculate the Denominator**\n",
    "- In Naive Bayes, the denominator $P(x)$ is the same for all classes $C_i$. Since we are only interested in comparing the relative probabilities of the classes, we can ignore $P(x)$ and focus on the numerator:\n",
    "$$ P(x| C_1) \\alpha P(x| C_1) \\cdot P(C_1)$$\n",
    "- This simplification makes the computation more efficient without affecting the final classification decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87881cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import numpy as np\n",
    "\n",
    "# Dataset\n",
    "X = np.array([\n",
    "    [1, 1, 1],  # Disease\n",
    "    [1, 0, 1],  # Disease\n",
    "    [0, 1, 1],  # Disease\n",
    "    [0, 0, 0],  # No Disease\n",
    "    [1, 0, 0],  # No Disease\n",
    "    [0, 1, 0]   # No Disease\n",
    "])\n",
    "y = np.array([1, 1, 1, 0, 0, 0])  # 1: Disease, 0: No Disease\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "model = BernoulliNB()\n",
    "model.fit(X, y)\n",
    "\n",
    "# New patient: Fever = 1, Cough = 1, Fatigue = 1\n",
    "new_patient = np.array([[1, 1, 1]])\n",
    "prediction = model.predict(new_patient)\n",
    "print(\"Prediction:\", \"Disease\" if prediction[0] == 1 else \"No Disease\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb3b3c7",
   "metadata": {},
   "source": [
    "##### To extend the Bayesian classifier to handle more features, larger datasets, and continuous features, we need to make a few modifications to the previous implementation. \n",
    "\n",
    "Specifically:\n",
    "\n",
    "Handling More Features:\n",
    "- The classifier can handle any number of features by generalizing the likelihood calculation for each feature.\n",
    "\n",
    "Larger Datasets:\n",
    "- For larger datasets, we can use libraries like scikit-learn to efficiently compute probabilities and handle scalability.\n",
    "\n",
    "Continuous Features:\n",
    "- For continuous features, we assume they follow a probability distribution (e.g., Gaussian) and compute the likelihood using the probability density function (PDF).\n",
    "\n",
    "##### Key Differences from the Previous Implementation\n",
    "Feature Types:\n",
    "- The previous implementation assumed binary features.\n",
    "- This extended version handles both categorical and continuous features.\n",
    "\n",
    "Probability Distributions:\n",
    "- For continuous features, we use the Gaussian (Normal) distribution to model the likelihood.\n",
    "\n",
    "Scalability:\n",
    "- The extended version is designed to handle larger datasets and more features efficiently.\n",
    "\n",
    "Example Usage\n",
    "Dataset\n",
    "Let’s create a synthetic dataset with:\n",
    "- 2 continuous features (e.g., \"age\", \"income\").\n",
    "- 1 categorical feature (e.g., \"education level\").\n",
    "- Binary classes (e.g., \"buy\" or \"not buy\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e78af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def compute_priors(y):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    priors = {cls: count / len(y) for cls, count in zip(classes, counts)}\n",
    "    return priors, classes\n",
    "\n",
    "def is_continuous(feature_values):\n",
    "    return len(np.unique(feature_values)) > 10  # Arbitrary threshold for continuous feature\n",
    "\n",
    "def compute_likelihoods(X, y, classes):\n",
    "    n_features = X.shape[1]\n",
    "    likelihoods = {}\n",
    "    \n",
    "    for feature in range(n_features):\n",
    "        likelihoods[feature] = {}\n",
    "        for cls in classes:\n",
    "            feature_values = X[y == cls, feature]\n",
    "            if is_continuous(feature_values):\n",
    "                mean, std = np.mean(feature_values), np.std(feature_values)\n",
    "                likelihoods[feature][cls] = {\"distribution\": \"gaussian\", \"mean\": mean, \"std\": std}\n",
    "            else:\n",
    "                unique_values, counts = np.unique(feature_values, return_counts=True)\n",
    "                probabilities = counts / np.sum(counts)\n",
    "                likelihoods[feature][cls] = {\"distribution\": \"categorical\", \"values\": unique_values, \"probabilities\": probabilities}\n",
    "    \n",
    "    return likelihoods\n",
    "\n",
    "def fit_bayesian_classifier(X, y):\n",
    "    priors, classes = compute_priors(y)\n",
    "    likelihoods = compute_likelihoods(X, y, classes)\n",
    "    return priors, likelihoods, classes\n",
    "\n",
    "def predict_bayesian_classifier(X, priors, likelihoods, classes):\n",
    "    predictions = []\n",
    "    \n",
    "    for sample in X:\n",
    "        posteriors = {}\n",
    "        for cls in classes:\n",
    "            posterior = priors[cls]\n",
    "            for feature in range(len(sample)):\n",
    "                likelihood_params = likelihoods[feature][cls]\n",
    "                if likelihood_params[\"distribution\"] == \"gaussian\":\n",
    "                    mean, std = likelihood_params[\"mean\"], likelihood_params[\"std\"]\n",
    "                    likelihood = norm.pdf(sample[feature], loc=mean, scale=std)\n",
    "                else:\n",
    "                    value_index = np.where(likelihood_params[\"values\"] == sample[feature])[0]\n",
    "                    likelihood = likelihood_params[\"probabilities\"][value_index[0]] if len(value_index) > 0 else 0\n",
    "                posterior *= likelihood\n",
    "            posteriors[cls] = posterior\n",
    "        predictions.append(max(posteriors, key=posteriors.get))\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "\n",
    "# Synthetic dataset\n",
    "np.random.seed(42)\n",
    "X = np.array([\n",
    "    [25, 50000, 1],  # Age, Income, Education Level\n",
    "    [30, 60000, 2],\n",
    "    [35, 70000, 1],\n",
    "    [40, 80000, 3],\n",
    "    [45, 90000, 2],\n",
    "    [50, 100000, 3]\n",
    "])\n",
    "y = np.array([0, 0, 1, 1, 0, 1])  # 0: Not Buy, 1: Buy\n",
    "\n",
    "# Initialize and fit the classifier\n",
    "classifier = predict_bayesian_classifier()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "# Predict for new samples\n",
    "new_samples = np.array([\n",
    "    [28, 55000, 1],  # New sample 1\n",
    "    [38, 75000, 2]   # New sample 2\n",
    "])\n",
    "predictions = classifier.predict(new_samples)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e43650",
   "metadata": {},
   "source": [
    "##### Explanation of the Extended Implementation\n",
    "Handling Continuous Features:\n",
    "- For continuous features, we assume a Gaussian distribution and compute the likelihood using the probability density function (PDF) of the normal distribution.\n",
    "\n",
    "Handling Categorical Features:\n",
    "- For categorical features, we compute the likelihood as the probability of observing the feature value given the class.\n",
    "\n",
    "Scalability:\n",
    "- The implementation is designed to handle larger datasets and more features efficiently.\n",
    "\n",
    "Flexibility:\n",
    "- The classifier can handle mixed feature types (continuous and categorical) in the same dataset.\n",
    "\n",
    "##### Key Differences from the Previous Implementation\n",
    "Feature Types:\n",
    "- The previous implementation only handled binary features.\n",
    "- The extended version handles both continuous and categorical features.\n",
    "\n",
    "Probability Distributions:\n",
    "- The extended version uses Gaussian distributions for continuous features and categorical distributions for discrete features.\n",
    "\n",
    "Generalization:\n",
    "- The extended version is more general and can be applied to a wider range of datasets.\n",
    "\n",
    "### **Applying a Naive Bayes classifier to a Decision Tree problem**\n",
    "This involves using the Naive Bayes algorithm to solve a classification task that could also be addressed using a Decision Tree. While Decision Trees and Naive Bayes are fundamentally different algorithms, they can both be applied to the same dataset for comparison or specific use cases.\n",
    "\n",
    "Below, I'll demonstrate how to apply a Naive Bayes classifier to a dataset typically used for Decision Tree problems, such as the Iris dataset or a synthetic dataset for binary classification.\n",
    "\n",
    "**Problem: Binary Classification with Synthetic Dataset**\n",
    "\n",
    "Create a synthetic dataset for binary classification and apply the Naive Bayes classifier to it. This dataset could also be used for a Decision Tree problem.\n",
    "\n",
    "Step 1: Create a Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad7d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,  # 1000 samples\n",
    "    n_features=4,    # 4 features\n",
    "    n_classes=2,     # Binary classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf7b5a",
   "metadata": {},
   "source": [
    "Step 2: Train a Naive Bayes Classifier\n",
    "\n",
    "We'll use the Gaussian Naive Bayes classifier, which assumes that the features follow a Gaussian (normal) distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa4ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581d3d92",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "- Accuracy: The model achieves an accuracy of 93.33% on the test set.\n",
    "- Precision and Recall: Both precision and recall are high for both classes, indicating that the model performs well in distinguishing between the two classes.\n",
    "\n",
    "Confusion Matrix:\n",
    "- True Positives (TP): 139\n",
    "- True Negatives (TN): 141\n",
    "- False Positives (FP): 11\n",
    "- False Negatives (FN): 9\n",
    "\n",
    "##### **Comparison with Decision Trees**\n",
    "Key Differences\n",
    "\n",
    "Model Type:\n",
    "- Naive Bayes is a probabilistic model based on Bayes' Theorem.\n",
    "- Decision Trees are non-parametric models that split the data based on feature values.\n",
    "\n",
    "Assumptions:\n",
    "- Naive Bayes assumes conditional independence of features given the class label.\n",
    "- Decision Trees make no such assumptions and can capture complex interactions between features.\n",
    "\n",
    "Interpretability:\n",
    "- Naive Bayes is less interpretable but computationally efficient.\n",
    "- Decision Trees are highly interpretable, as they provide a clear decision path.\n",
    "\n",
    "Performance:\n",
    "- Naive Bayes performs well with small datasets and high-dimensional data.\n",
    "- Decision Trees can handle both small and large datasets but may overfit if not properly regularized.\n",
    "\n",
    "Step 4: Train a Decision Tree for Comparison\n",
    "\n",
    "Let's train a Decision Tree classifier on the same dataset for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6900f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "tree_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_tree = tree_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "print(f\"Decision Tree Accuracy: {accuracy_tree:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nDecision Tree Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tree))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nDecision Tree Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef008c",
   "metadata": {},
   "source": [
    "### **When the target variable is continuous, the problem is a regression task rather than a classification task.**\n",
    "The criterion for determining the optimal feature choice and its threshold is different from classification tasks. The goal is to split the data in a way that minimizes the variance of the target variable within each subset after the split.\n",
    "\n",
    "##### **Criterion for Regression: Variance Reduction**\n",
    "In regression tasks, the most common criterion for determining the optimal feature and its threshold is **variance reduction**. \n",
    "- The idea is to split the data such that the variance of the target variable within each subset is minimized. This is often measured using the Sum of Squared Errors (SSE) or Mean Squared Error (MSE).\n",
    "\n",
    "Steps to Determine the Optimal Feature and Threshold\n",
    "- Calculate the Variance of the Target Variable:\n",
    "    - Compute the variance of the target variable y for the entire dataset.\n",
    "\n",
    "- Evaluate All Possible Splits:\n",
    "    - For each feature, sort its values and consider all possible thresholds to split the data into two subsets.\n",
    "    - For each candidate split, calculate the variance of the target variable in the left and right subsets.\n",
    "\n",
    "Compute the Weighted Variance:\n",
    "- Compute the weighted variance for each split:\n",
    "$$ Weighted Variance = \\frac{n_{left}}{n} \\cdot Var(y_{left}) + \\frac{n_{right}}{n} \\cdot Var(y_{right}) $$\n",
    "- where:\n",
    "    - $n_{left}$ and $n_{right}$ are the number of samples in the left and right subsets, respectively.\n",
    "    - n is the total number of samples.\n",
    "    - $Var(y_{left})$ and $Var(y_{right})$ are the variances of the target variable in the left and right subsets.\n",
    "\n",
    "Choose the Split with the Lowest Weighted Variance:\n",
    "- The optimal split is the one that results in the lowest weighted variance.\n",
    "\n",
    "Repeat for All Features:\n",
    "- Repeat the above steps for all features and choose the feature and threshold that provide the best split.\n",
    "\n",
    "##### Mathematical Formulation\n",
    "The variance reduction for a split S is defined as:\n",
    "$$ Varaiance Reduction = Var(y) - (\\frac{n_{left}}{n} \\cdot Var(y_{left}) + \\frac{n_{right}}{n} \\cdot Var(y_{right})) $$\n",
    "\n",
    "The goal is to maximize the variance reduction, which is equivalent to minimizing the weighted variance.\n",
    "\n",
    "##### Example: Applying Variance Reduction\n",
    "Dataset\n",
    "- Consider a dataset with one feature x and a continuous target variable y:\n",
    "\n",
    "**Step 1: Calculate the Variance of y**\n",
    "- Mean of y: $\\mu = \\frac{2+3+4+5+6}{5} = 5$\n",
    "- Variance of y: $Var(y) = \\frac{(2-4)^2 + (3-4)^2 + (4-4)^2 + (5-4)^2 + (6-4)^2 }{5} = 2$\n",
    "\n",
    "**Step 2: Evaluate All Possible Splits**\n",
    "- Possible thresholds for x: 1.5, 2.5, 3.5, 4.5\n",
    "\n",
    "For each threshold:\n",
    "\n",
    "Threshold = 1.5:\n",
    "- Left subset: $x \\leq 1.5 → y=[2]$\n",
    "- Right subset: $x > 1.5 → y=[3,4,5,6]$\n",
    "- Weighted variance: $\\frac{1}{5} \\cdot 0 + \\frac{4}{5} \\cdot 1.25 = 1$\n",
    "\n",
    "Threshold = 2.5:\n",
    "- Left subset: $x \\leq 2.5 → y=[2, 3]$\n",
    "- Right subset: $x > 2.5 → y=[4,5,6]$\n",
    "- Weighted variance: $\\frac{2}{5} \\cdot 0.25 + \\frac{3}{5} \\cdot 0.6667 = 0.5$\n",
    "\n",
    "Threshold = 3.5:\n",
    "- Left subset: $x \\leq 3.5 → y=[2,3,4]$\n",
    "- Right subset: $x > 3.5 → y=[5,6]$\n",
    "- Weighted variance: $\\frac{3}{5} \\cdot 0.6667 + \\frac{2}{5} \\cdot 0.25 = 0.5$\n",
    "\n",
    "Threshold = 4.5:\n",
    "- Left subset: $x \\leq 4.5 → y=[2,3,4,5]$\n",
    "- Right subset: $x > 4.5 → y=[6]$\n",
    "- Weighted variance: $\\frac{4}{5} \\cdot 1.25 + \\frac{1}{5} \\cdot 0 = 1$\n",
    "\n",
    "**Step 3: Choose the Optimal Split**\n",
    "\n",
    "The splits with thresholds 2.5 and 3.5 result in the lowest weighted variance (0.5).\n",
    "\n",
    "Either of these splits is optimal.\n",
    "\n",
    "##### Explanation of the Code\n",
    "Variance Calculation:\n",
    "- The variance function computes the variance of the target variable y.\n",
    "\n",
    "Finding the Best Split:\n",
    "- The find_best_split function iterates over all unique values of the feature X as potential thresholds.\n",
    "- For each threshold, it calculates the weighted variance of the left and right subsets.\n",
    "- The split with the highest variance reduction is selected as the best split.\n",
    "\n",
    "Result:\n",
    "- The best threshold is 2, and the corresponding variance reduction is 1.0000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49421728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 3, 4, 5, 6])\n",
    "\n",
    "# Function to calculate variance\n",
    "def variance(y):\n",
    "    return np.var(y)\n",
    "\n",
    "# Function to find the best split\n",
    "def find_best_split(X, y):\n",
    "    n = len(y)\n",
    "    best_var_reduction = -np.inf\n",
    "    best_threshold = None\n",
    "    \n",
    "    for threshold in np.unique(X):\n",
    "        left_indices = X <= threshold\n",
    "        right_indices = X > threshold\n",
    "        \n",
    "        y_left = y[left_indices]\n",
    "        y_right = y[right_indices]\n",
    "        \n",
    "        if len(y_left) == 0 or len(y_right) == 0:\n",
    "            continue\n",
    "        \n",
    "        weighted_var = (len(y_left) / n) * variance(y_left) + (len(y_right) / n) * variance(y_right)\n",
    "        var_reduction = variance(y) - weighted_var\n",
    "        \n",
    "        if var_reduction > best_var_reduction:\n",
    "            best_var_reduction = var_reduction\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, best_var_reduction\n",
    "\n",
    "# Find the best split\n",
    "best_threshold, best_var_reduction = find_best_split(X, y)\n",
    "print(f\"Best Threshold: {best_threshold}\")\n",
    "print(f\"Best Variance Reduction: {best_var_reduction:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edbbee1",
   "metadata": {},
   "source": [
    "### **In Naive Bayes method, What is lift**\n",
    "Is a metric used to evaluate the effectiveness of a predictive model. It measures how much better the model performs compared to a random guess or a baseline model. Lift is particularly useful in scenarios like marketing campaigns, where we want to identify the most promising targets based on predicted probabilities.\n",
    "\n",
    "##### **What is Lift?**\n",
    "Lift is defined as the ratio of the target response rate when using the model to the baseline response rate (i.e., the response rate without the model). Mathematically, it is expressed as:\n",
    "$$ \\text{Lift} = \\frac{P(\\text{Target|Method})}{P(\\text{Target})} $$\n",
    "- Where:\n",
    "    - $P(\\text{Target|Method})$:  The probability of the target event occurring given the model's predictions.\n",
    "    - $P(\\text{Target})$: The baseline probability of the target event occurring in the population.\n",
    "\n",
    "Interpretation of Lift\n",
    "- Lift > 1: The model performs better than random guessing.\n",
    "- Lift = 1: The model performs no better than random guessing.\n",
    "- Lift < 1: The model performs worse than random guessing.\n",
    "\n",
    "##### **Steps to Calculate Lift**\n",
    "Train the Naive Bayes Model:\n",
    "- Train the Naive Bayes classifier on the training data.\n",
    "\n",
    "Predict Probabilities:\n",
    "- Use the trained model to predict the probabilities of the target class for each instance in the test data.\n",
    "\n",
    "Sort Instances by Predicted Probability:\n",
    "- Sort the test instances in descending order of their predicted probabilities.\n",
    "\n",
    "Divide into Deciles or Groups:\n",
    "- Divide the sorted instances into groups (e.g., deciles or percentiles).\n",
    "\n",
    "Calculate Lift for Each Group:\n",
    "- For each group, calculate the lift as:\n",
    "$$ \\text{Lift} = \\frac{\\text{Response Rate in Group}}{\\text{Overall Response Rate}} $$\n",
    "\n",
    "##### Example: Calculating Lift in a Marketing Campaign\n",
    "Problem Statement\n",
    "- Suppose we are running a marketing campaign to target customers likely to purchase a product. We have a dataset with the following features:\n",
    "    - Age: Continuous.\n",
    "    - Income: Continuous.\n",
    "    - Purchased: Binary (1 = Purchased, 0 = Did Not Purchase).\n",
    "- We want to calculate the lift of the Naive Bayes model to evaluate its effectiveness.\n",
    "\n",
    "**Step 1: Train the Naive Bayes Model**\n",
    "- We train a Naive Bayes classifier on the dataset to predict the probability of a customer purchasing the product.\n",
    "\n",
    "**Step 2: Predict Probabilities**\n",
    "- For each customer in the test set, the model predicts the probability of purchasing the product.\n",
    "\n",
    "**Step 3: Sort Instances by Predicted Probability**\n",
    "- Sort the test instances in descending order of their predicted probabilities.\n",
    "\n",
    "**Step 4: Divide into Deciles**\n",
    "- Divide the sorted instances into 10 equal groups (deciles).\n",
    "\n",
    "**Step 5: Calculate Lift for Each Decile**\n",
    "- For each decile, calculate the lift as:\n",
    "\n",
    "$$ \\text{Lift} = \\frac{\\text{Response Rate in Decile}}{\\text{Overall Response Rate}} $$\n",
    "\n",
    "Dataset\n",
    "|Customer\t|Predicted Probability\t|Actual Purchase|\n",
    "|-----------|-----------------------|---------------|\n",
    "|1\t|0.95\t|1    |\n",
    "|2\t|0.90\t|1    |\n",
    "|3\t|0.85\t|1    |\n",
    "|4\t|0.80\t|0    |\n",
    "|5\t|0.75\t|1    |\n",
    "|6\t|0.70\t|0    |\n",
    "|7\t|0.65\t|0    |\n",
    "|8\t|0.60\t|0    |\n",
    "|9\t|0.55\t|0    |\n",
    "|10\t|0.50\t|0    |\n",
    "\n",
    "Overall Response Rate\n",
    "- Total purchases: 3.\n",
    "- Total customers: 10.\n",
    "- Overall response rate: $\\frac{3}{10}$ = 0.3\n",
    "\n",
    "Lift for Top Decile\n",
    "- Customers in the top decile: Customers 1, 2, 3.\n",
    "- Purchases in the top decile: 3.\n",
    "- Response rate in the top decile: $\\frac{3}{3}$ = 1\n",
    "- lift: $\\frac{1}{0.3} \\approx 3.33$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9f18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Synthetic dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 2)  # Features: Age, Income\n",
    "y = np.random.randint(0, 2, 100)  # Target: Purchased (0 or 1)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of class 1 (Purchase)\n",
    "\n",
    "# Sort test instances by predicted probability\n",
    "sorted_indices = np.argsort(y_pred_proba)[::-1]\n",
    "y_test_sorted = y_test[sorted_indices]\n",
    "\n",
    "# Divide into deciles\n",
    "n_deciles = 10\n",
    "decile_size = len(y_test_sorted) // n_deciles\n",
    "lift_values = []\n",
    "\n",
    "for i in range(n_deciles):\n",
    "    start = i * decile_size\n",
    "    end = (i + 1) * decile_size\n",
    "    y_decile = y_test_sorted[start:end]\n",
    "    response_rate_decile = np.mean(y_decile)\n",
    "    overall_response_rate = np.mean(y_test)\n",
    "    lift = response_rate_decile / overall_response_rate\n",
    "    lift_values.append(lift)\n",
    "\n",
    "# Print lift values\n",
    "for i, lift in enumerate(lift_values):\n",
    "    print(f\"Decile {i + 1}: Lift = {lift:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6835d8",
   "metadata": {},
   "source": [
    "### **Generative method and Naive-Naive Bayes**\n",
    "A generative method is a type of machine learning model that learns the joint probability distribution P(X,Y) of the input features X and the target labels Y. Once the joint distribution is learned, the model can generate new data samples that resemble the training data. Generative models are used for tasks like:\n",
    "- Data generation (e.g., generating realistic images or text).\n",
    "- Density estimation (e.g., estimating the probability distribution of data).\n",
    "- Classification (e.g., predicting the class label for a given input).\n",
    "\n",
    "Key Characteristics of Generative Models\n",
    "\n",
    "Joint Probability Distribution:\n",
    "- Generative models learn $P(X,Y)$, which can be factored as:\n",
    "$$ P(X,Y) = P(X|Y) \\cdot P(Y)$$\n",
    "- Here, $P(X∣Y)$ is the likelihood of the features given the class, and $P(Y)$ is the prior probability of the class.\n",
    "\n",
    "Data Generation:\n",
    "- Once the model learns $P(X,Y)$, it can generate new samples $(x,y)$ by sampling from this distribution.\n",
    "\n",
    "Examples of Generative Models:\n",
    "- Naive Bayes.\n",
    "- Gaussian Mixture Models (GMM).\n",
    "- Hidden Markov Models (HMM).\n",
    "- Generative Adversarial Networks (GANs).\n",
    "- Variational Autoencoders (VAEs).\n",
    "\n",
    "##### **Naive Bayes as a Generative Model**\n",
    "Naive Bayes is a classic example of a generative model. It assumes that the features X are conditionally independent given the class Y. The joint probability distribution is modeled as:\n",
    "$$ P(X,Y) = P(Y) \\cdot \\Pi_{j=1}^n P(X_j|Y)$$\n",
    "- Where:\n",
    "    - $P(Y)$: Prior probability of the class.\n",
    "    - $P(X_j|Y)$: Likelihood of the j-th feature given the class.\n",
    "\n",
    "Steps in Naive Bayes\n",
    "\n",
    "Learn the Prior $P(Y)$:\n",
    "- Estimate the probability of each class from the training data.\n",
    "\n",
    "Learn the Likelihood $P(X_j∣Y)$:\n",
    "- For each feature $X_j$, estimate its probability distribution given the class.\n",
    "\n",
    "Predict the Posterior $P(Y∣X)$:\n",
    "- Use Bayes' Theorem to compute the posterior probability of the class given the features:\n",
    "$$ P(Y|X) \\alpha P(Y) \\cdot \\Pi_{j=1}^n P(X_j|Y)$$\n",
    "\n",
    "Generate New Samples:\n",
    "- Sample from the learned joint distribution $P(X,Y)$ to generate new data.\n",
    "\n",
    "##### **Naive-Naive Bayes**\n",
    "The term Naive-Naive Bayes is not a standard term in machine learning literature. However, it could refer to a hierarchical or ensemble approach where multiple Naive Bayes models are combined or applied in a layered manner. For example:\n",
    "- Hierarchical Naive Bayes: A two-level Naive Bayes model where the first level predicts a coarse-grained class, and the second level refines the prediction.\n",
    "- Ensemble Naive Bayes: Combining multiple Naive Bayes models (e.g., using bagging or boosting) to improve performance.\n",
    "\n",
    "Alternatively, it might refer to a simplified or \"naive\" version of Naive Bayes where additional assumptions are made to further simplify the model.\n",
    "\n",
    "#### Example: Naive Bayes as a Generative Model\n",
    "\n",
    "build a Naive Bayes classifier to predict whether an email is spam or not spam based on the presence of two keywords: Email Spam detection with two features \n",
    "- $x_1$: free, \n",
    "- $x_2$: money\n",
    "\n",
    "**Step 1: Learn the Prior $P(Y)$**\n",
    "- $P(C_1) = \\frac{3}{6} = 0.5 \\text{ (Spam).} $\n",
    "- $P(C_2) = \\frac{3}{6} = 0.5 \\text{ (Not Spam).} $\n",
    "\n",
    "**Step 2: Compute Likelihoods**\n",
    "\n",
    "We compute the likelihoods $P(x_j | C_i)$ for each feature and class.\n",
    "- For $C_1$ (Spam):\n",
    "    - $P(x_1 = 1 | C_1) = \\frac{2}{3} \\quad \\text{(``free'' appears in 2 out of 3 spam emails)}$\n",
    "    - $P(x_1 = 0 | C_1) = \\frac{1}{3}.$\n",
    "    - $P(x_2 = 1 | C_1) = \\frac{2}{3} \\quad \\text{(``money'' appears in 2 out of 3 spam emails).}$\n",
    "    - $P(x_2 = 0 | C_1) = \\frac{1}{3}.$\n",
    "- For $C_2$ (Not Spam):\n",
    "    - $P(x_1 = 1 | C_2) = \\frac{1}{3} \\quad \\text{(``free'' does not appear in not spam emails).}$\n",
    "    - $P(x_2 = 1 | C_2) = \\frac{1}{3} \\quad \\text{(``money'' appears in 1 out of 2 not spam emails).}$\n",
    "\n",
    "**Step 3: Predict the Posterior probability $P(Y|X)$ - Classify a New Email**\n",
    "- For $C_1$ (Spam):\n",
    "$$P(C_1 | x) \\propto P(x_1 = 1 | C_1) \\cdot P(x_2 = 1 | C_1) \\cdot P(C_1)$$\n",
    "$$P(C_1 | x) \\propto \\frac{2}{3} \\times \\frac{2}{3} \\times 0.5 $$\n",
    "$$= \\frac{4}{9} \\times 0.5$$\n",
    "$$= 0.2222$$\n",
    "\n",
    "- For $C_2$ (Not Spam):\n",
    "$$P(C_2 | x) \\propto P(x_1 = 1 | C_2) \\cdot P(x_2 = 1 | C_2) \\cdot P(C_2)$$\n",
    "$$P(C_2 | x) \\propto \\frac{1}{3} \\times \\frac{1}{3} \\times 0.5 $$\n",
    "$$P(C_2 | x) = \\frac{1}{18}$$\n",
    "\n",
    "Since $P(C_1 | x) = \\frac{2}{9} > P(C_2 | x) = \\frac{1}{18}$, the email is classified as $\\text{spam}$.\n",
    "\n",
    "**Step 4: Generate New Samples**\n",
    "- Sample from $P(Y)$ to generate a class label.\n",
    "- Sample from $P(X_j ∣Y)$ to generate feature values for the sampled class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import numpy as np\n",
    "\n",
    "# Dataset\n",
    "X = np.array([\n",
    "    [1, 1],  # Spam\n",
    "    [1, 0],  # Spam\n",
    "    [0, 1],  # Spam\n",
    "    [0, 0],  # Not Spam\n",
    "    [1, 0],  # Not Spam\n",
    "    [0, 1]   # Not Spam\n",
    "])\n",
    "y = np.array([1, 1, 1, 0, 0, 0])  # 1: Spam, 0: Not Spam\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "model = BernoulliNB()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict probabilities for a new email\n",
    "new_email = np.array([[1, 1]])  # \"free\" = 1, \"money\" = 1\n",
    "predicted_prob = model.predict_proba(new_email)\n",
    "print(\"Predicted Probabilities:\", predicted_prob)\n",
    "\n",
    "# Generate new samples\n",
    "n_samples = 5\n",
    "generated_samples = []\n",
    "for _ in range(n_samples):\n",
    "    y_gen = np.random.choice([0, 1], p=model.class_prior_)  # Sample class\n",
    "    x_gen = [np.random.choice([0, 1], p=model.feature_log_prob_[y_gen, j].exp()) for j in range(X.shape[1])]\n",
    "    generated_samples.append((x_gen, y_gen))\n",
    "\n",
    "print(\"\\nGenerated Samples:\")\n",
    "for sample in generated_samples:\n",
    "    print(f\"Features: {sample[0]}, Class: {sample[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f01e9",
   "metadata": {},
   "source": [
    "### Advantages and Dissadvantages of Naive Bayes Classifier\n",
    "Naive Bayes Classifier is a popular and widely used machine learning algorithm, especially for classification tasks. However, like any algorithm, it has its strengths and weaknesses. Below is a detailed list of the advantages and disadvantages of the Naive Bayes Classifier.\n",
    "\n",
    "#### Advantages of Naive Bayes Classifier\n",
    "Simplicity:\n",
    "- Naive Bayes is easy to understand and implement.\n",
    "- It requires minimal training data to estimate the parameters (prior probabilities and likelihoods).\n",
    "\n",
    "Computational Efficiency:\n",
    "- It is computationally efficient, both in terms of training and prediction.\n",
    "- The algorithm scales well with large datasets and high-dimensional data.\n",
    "\n",
    "Handles High-Dimensional Data:\n",
    "- Naive Bayes performs well with datasets that have a large number of features (e.g., text data with thousands of words).\n",
    "\n",
    "Works Well with Small Datasets:\n",
    "- It can produce reliable results even with limited training data.\n",
    "\n",
    "Robust to Irrelevant Features:\n",
    "- Naive Bayes is relatively robust to irrelevant features because it assumes conditional independence between features.\n",
    "\n",
    "Handles Missing Data:\n",
    "- It can handle missing data by ignoring the missing values during probability estimation.\n",
    "\n",
    "Probabilistic Output:\n",
    "- It provides probabilistic predictions, which can be useful for decision-making (e.g., ranking predictions by confidence).\n",
    "\n",
    "No Need for Feature Scaling:\n",
    "- Naive Bayes does not require feature scaling (e.g., normalization or standardization) because it is based on probability distributions.\n",
    "\n",
    "Works Well for Text Classification:\n",
    "- It is particularly effective for text classification tasks like spam filtering, sentiment analysis, and document categorization.\n",
    "\n",
    "Interpretable:\n",
    "- The model's predictions are based on simple probability calculations, making it interpretable.\n",
    "\n",
    "#### Disadvantages of Naive Bayes Classifier\n",
    "Strong Independence Assumption:\n",
    "- The assumption that features are conditionally independent given the class label is often unrealistic in real-world data.\n",
    "- This can lead to suboptimal performance when features are correlated.\n",
    "\n",
    "Sensitive to Feature Distributions:\n",
    "- Naive Bayes assumes that features follow specific probability distributions (e.g., Gaussian for continuous features).\n",
    "- If the data does not conform to these assumptions, the model's performance may suffer.\n",
    "\n",
    "Zero Probability Problem:\n",
    "- If a feature value does not appear in the training data for a particular class, the likelihood $P(X_j∣Y)$ becomes zero, causing the posterior probability to be zero.\n",
    "- This can be mitigated using techniques like Laplace smoothing.\n",
    "\n",
    "Limited Expressiveness:\n",
    "- Naive Bayes is a simple model and may not capture complex relationships between features and the target variable.\n",
    "\n",
    "Bias from Prior Probabilities:\n",
    "- The model's predictions can be biased if the prior probabilities $P(Y)$ are not estimated accurately.\n",
    "\n",
    "Not Suitable for Regression:\n",
    "- Naive Bayes is designed for classification tasks and cannot be directly applied to regression problems.\n",
    "\n",
    "Difficulty with Continuous Features:\n",
    "- While Naive Bayes can handle continuous features using Gaussian distributions, it may not perform well if the data does not follow a normal distribution.\n",
    "\n",
    "Overfitting with Small Datasets:\n",
    "- Although Naive Bayes works well with small datasets, it can overfit if the dataset is too small or if there are too many features.\n",
    "\n",
    "Dependence on Feature Engineering:\n",
    "- The performance of Naive Bayes can be heavily influenced by the quality of feature engineering (e.g., handling missing values, encoding categorical variables).\n",
    "\n",
    "Limited to Probabilistic Models:\n",
    "- Naive Bayes is limited to probabilistic models and may not be suitable for tasks that require non-probabilistic approaches.\n",
    "\n",
    "#### When to Use Naive Bayes\n",
    "Naive Bayes is a good choice when:\n",
    "- The dataset is small or medium-sized.\n",
    "- The features are conditionally independent or nearly independent.\n",
    "- The problem involves text classification or high-dimensional data.\n",
    "0 Interpretability and computational efficiency are important.\n",
    "\n",
    "#### When Not to Use Naive Bayes\n",
    "Avoid using Naive Bayes when:\n",
    "- The features are highly correlated.\n",
    "- The data does not conform to the assumed probability distributions.\n",
    "- The problem requires capturing complex relationships between features and the target variable.\n",
    "- The task involves regression or non-probabilistic predictions.\n",
    "\n",
    "|Aspect\t|Advantages\t|Disadvantages |\n",
    "|-------|-----------|--------------|\n",
    "|Simplicity\t|Easy to understand and implement.\t|Limited expressiveness; may not capture complex relationships.|\n",
    "|Efficiency\t|Computationally efficient; scales well with large datasets.\t|Sensitive to feature distributions; may not handle non-Gaussian data well.|\n",
    "|High-Dimensional Data\t|Performs well with high-dimensional data (e.g., text).\t|Strong independence assumption; may not work well with correlated features.|\n",
    "|Small Datasets\t|Works well with limited training data.\t|Overfitting risk with very small datasets.|\n",
    "|Robustness\t|Robust to irrelevant features; handles missing data.\t|Zero probability problem; requires Laplace smoothing for unseen feature values.|\n",
    "|Probabilistic Output\t|Provides probabilistic predictions.\t|Not suitable for regression tasks.|\n",
    "|Interpretability\t|Interpretable due to simple probability calculations.\t|Bias from inaccurate prior probabilities.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e7071",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN)\n",
    "K-Nearest Neighbors (KNN) is a non-parametric, instance-based learning algorithm used for both classification and regression tasks. It makes predictions based on the similarity between data points, meaning it relies on a majority vote (for classification) or an average of nearest neighbors (for regression).\n",
    "\n",
    "Definition:\n",
    "The nearest neighbors of a given data point are the K closest training samples determined by a distance metric (usually Euclidean distance).\n",
    "\n",
    "How It Works in KNN:\n",
    "- A new data point is classified based on the majority class of its nearest neighbors.\n",
    "- The number of neighbors (K) is a hyperparameter that determines model behavior.\n",
    "\n",
    "How KNN Algorithm Works:\n",
    "- Choose K (the number of nearest neighbors).\n",
    "- Compute the distance between the query point and all other points in the dataset (commonly using Euclidean distance).\n",
    "- Find the K nearest neighbors based on the computed distances.\n",
    "- Make a prediction:\n",
    "    - For classification: Assign the class that is most frequent among the K neighbors (majority vote).\n",
    "    - For regression: Predict the average (or weighted average) of the target values of K neighbors.\n",
    "\n",
    "Significance in KNN:\n",
    "- Too few neighbors (low K) → High variance (prone to noise).\n",
    "- Too many neighbors (high K) → High bias (over-smooth decision boundaries).\n",
    "\n",
    "### KNN algorithm\n",
    "K-nearest neighbors (KNN) is a powerful, yet easy to understand machine learning algorithm. In principle, this algorithm works by assigning the majority class of the N closest neighbors to the currect data point. As such, absolutely no training is required for the algorithm! All we do is choose K (i.e. the number of neighbors to consider), choose a distance function to calculate proximity and we're good to go. \n",
    "\n",
    "A typical KNN algorithm works as follows:\n",
    "\n",
    "1. Choose K (number of neighbours)\n",
    "2. Choose distance metric, e.g. Euclidean distance: \n",
    "$$\n",
    "d(X^a,X^b) = \\sqrt{(x^{a}_{1}-x^{b}_{1})^2 + (x^{a}_{2}-x^{b}_{2})^2 + \\cdots + (x^{a}_{n}-x^{b}_{n})^2}\n",
    "$$ \n",
    "3. For each data point $X_{test}$ in the testing data do:\n",
    "    - For each data point $X_{train}$ in the training data do\n",
    "        - Calculate the distance $d(X_{train},X_{test})$ between the test point and training observation.\n",
    "    - Find labels of the K closest data points to $X_{test}$\n",
    "    - Assign most frequent (i.e. the mode) class label to $X_{test}$\n",
    "\n",
    "In the context of regression, we would use the mean of the K nearest neighbors instead of the mode of the class labels.\n",
    "\n",
    "### KNN classification using sklearn\n",
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f605ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, log_loss\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier # Importing the kNN Classifier class from sklearn using eucliend distance\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Fit the model\n",
    "n_neighbors = 3 # <--- change this number to play around with how many nearest neighbours to look for.\n",
    "knn = KNeighborsClassifier(n_neighbors) \n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Fit the model 2\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)  # k-NN classifier with k = 3\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79689bac",
   "metadata": {},
   "source": [
    "##### Testing the model: Assess model performance\n",
    "Like before, let's have a look at the log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ee366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the test set \n",
    "y_hat = knn.predict_proba(X_test)\n",
    "# Calculate the loss \n",
    "print(\"The log loss error for our model is: \", log_loss(y_test, y_hat))\n",
    "\n",
    "\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459f9913",
   "metadata": {},
   "source": [
    "##### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_pred, n_class=3)\n",
    "conf_mat\n",
    "\n",
    "# Accuracy\n",
    "accuracy = np.trace(conf_mat)/np.sum(conf_mat)\n",
    "print(\"Correctly identified: {} and total: {}\".format(np.trace(conf_mat), np.sum(conf_mat)))\n",
    "print(\"Accuracy with kNN and k = 3 : {:0.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9081d638",
   "metadata": {},
   "source": [
    "##### Plot the decision boundary\n",
    "\n",
    "We once again visualise the decision boundary of our KNN classifier. Remember to change `i` and `j` to compare different features to one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f31aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0; j = 2\n",
    "knn.fit(X[:, [i, j]], y)\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "x_min, x_max = X[:, i].min(), X[:, i].max()\n",
    "y_min, y_max = X[:, j].min(), X[:, j].max()\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 1000), np.linspace(y_min, y_max, 1000))\n",
    "\n",
    "y_hat = knn.predict(np.concatenate((xx.reshape(-1,1), yy.reshape(-1,1)), axis=1))\n",
    "y_hat = y_hat.reshape(xx.shape)\n",
    "\n",
    "ax1.pcolormesh(xx, yy, y_hat, cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.scatter(X[:, i], X[:, j], c=y, edgecolors='k', cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.set_xlabel('Feature '+str(i))\n",
    "ax1.set_ylabel('Feature '+str(j))\n",
    "ax1.set_xlim(xx.min(), xx.max())\n",
    "ax1.set_ylim(yy.min(), yy.max())\n",
    "ax1.set_xticks(())\n",
    "ax1.set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41921e88",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbour Classification from Scratch\n",
    "\n",
    "##### importing necessary libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e3149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X,y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcbd447",
   "metadata": {},
   "source": [
    "##### Splitting the entire dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e6c876",
   "metadata": {},
   "source": [
    "##### Standardizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd2a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler #normalisng dataset unit variance 1 and o mean \n",
    "\n",
    "SC = StandardScaler()\n",
    "X_train = SC.fit_transform(X_train)\n",
    "X_test = SC.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a40c9bc",
   "metadata": {},
   "source": [
    "##### Pairwise distances : How it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f4323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "pts_1 = np.asarray([[0, 1],[1, -1]])\n",
    "pts_2 = np.asarray([[0, 0], [2, 0]])\n",
    "\n",
    "pairwise_distances(pts_1, pts_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127425b0",
   "metadata": {},
   "source": [
    "##### kNN Classification Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e56f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN_classifier(X_train, y_train, n_class, X_test, k = 3): #by default the value is 3\n",
    "    \n",
    "    n_test_pt = X_test.shape[0]\n",
    "    D = pairwise_distances(X_test, X_train)\n",
    "    y_pred = np.empty(n_test_pt)\n",
    "    \n",
    "    for i in range(n_test_pt):\n",
    "        neighbors = np.argsort(D[i,:])[1:k+1] #selecting row i and all the columns starting from 1 to k+1 to exclude 0\n",
    "        labels_neigh = y_train[neighbors]\n",
    "        count = np.zeros(n_class)\n",
    "        for j in labels_neigh:\n",
    "            count[j] += 1\n",
    "        y_pred[i] = np.argmax(count)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# or\n",
    "def KNN(input_):\n",
    "    col_index = 1\n",
    "    ls = []\n",
    "    k = int(input(\"Please enter your k value: \"))\n",
    "    for i,row in df_.iterrows():\n",
    "        # Calculating Distance\n",
    "        dist = np.sqrt((row[\"Temperature\"]-input_[0])**2 + (row[\"Humidity\"]-input_[1])**2)\n",
    "        # Appending the to empty list\n",
    "        ls.append([i, dist, row[\"Occupancy\"]])\n",
    "        # converting the list into a numpy array\n",
    "    np_ls = np.array(ls)\n",
    "    sorted_np_ls = np_ls[np_ls[:,col_index].argsort()]\n",
    "    top5 = sorted_np_ls[:k,:]\n",
    "    mode_ = stats.mode(top5[:,2])\n",
    "    return top5, mode_\n",
    "\n",
    "KNN([20,18])[1]\n",
    "KNN([20,18])[0]\n",
    "\n",
    "# or\n",
    "def knn_array(df,input_):\n",
    "    ls = []\n",
    "    for i,row in df.iterrows():\n",
    "        dist = np.sqrt((row[\"Temperature\"]-input_[0])**2 + (row[\"Humidity\"]-input_[1])**2)\n",
    "        ls.append([i, dist, row[\"Occupancy\"]])\n",
    "        np_ls = np.array(ls)\n",
    "    return np_ls\n",
    "\n",
    "def sort_array():\n",
    "    col_index = 1\n",
    "    sorted_np_ls = knn_arr[knn_arr[:,col_index].argsort()]\n",
    "    return sorted_np_ls\n",
    "\n",
    "def top_k(arr):\n",
    "    k = int(input(\"Please enter your k value: \"))\n",
    "    top_k = arr[:k,:]\n",
    "    return top_k\n",
    "\n",
    "def mode(arr):\n",
    "    mode_ = stats.mode(arr[:,2])\n",
    "\n",
    "knn_array(df_,[20,18])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11779dc0",
   "metadata": {},
   "source": [
    "##### Prediction using KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c280d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = kNN_classifier(X_train, y_train, n_class = 3, X_test = X_test, k = 3) #K can change\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b35dfa",
   "metadata": {},
   "source": [
    "##### Defining confusion matrix function and Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59af31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred, n_class):\n",
    "    \n",
    "    conf_mat = np.zeros((n_class, n_class),dtype='int')\n",
    "    for i in range(y_true.shape[0]):\n",
    "        conf_mat[int(y_true[i]), int(y_pred[i])] += 1\n",
    "    \n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, n_class=3)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2773b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.trace(cm)/np.sum(cm)\n",
    "\n",
    "print(\"Correctly identified: {} and total: {}\".format(np.trace(cm), np.sum(cm)))\n",
    "print(\"Accuracy with kNN and k = 3 : {:0.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a4fb4",
   "metadata": {},
   "source": [
    "### Example: Occupancy Detection\n",
    "\n",
    "In this practical session, we will design and implement supervised learning method(s) for detecting occupancy of an office room. The dataset we will use is from [Luis M. Candanedo, Veronique Feldheim, \"Accurate occupancy\n",
    "detection of an office room from light, temperature, humidity and CO2 measurements using statistical learning models\", Energy and Buildings, Volume 112, 15 January 2016, Pages 28-39](https://doi.org/10.1016/j.enbuild.2015.11.071)\n",
    "\n",
    "\n",
    "##### Dataset\n",
    "\n",
    "The dataset has a training set of 8143 examples and a test set of 2665 examples. Each example is comprised of features (acquired through sensors from an office room) and the corresponding target value (**Occupancy**). The features (**Temperature**, **Humidty**, **Light**, **CO2**, **Humidity_Ratio**) recorded for each example to predict the state of the office room (**Occupancy**).\n",
    "\n",
    "##### Loading data\n",
    "\n",
    "The following code segment loads both training and testing data from text files `../data/trainingdata.txt` and `../data/testingdata.txt`, respectively. Each row of 2D NumPy arrays `training_data` and `testing_data` respectively refers to an example from the training and testing datasets. The last column of each row refers to the target value, either 0 or 1 respectively representing **Unoccupied** or **Occupied** room."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3189c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "# load training data\n",
    "training_data = np.loadtxt('../data/trainingdata.txt', usecols=(2,3,4,5,6,7), skiprows=1, delimiter=',')\n",
    "x_training = training_data[:, :-1]\n",
    "y_training = training_data[:, -1]\n",
    "\n",
    "# load testing data\n",
    "testing_data = np.loadtxt('../data/testingdata.txt', usecols=(2,3,4,5,6,7), skiprows=1, delimiter=',')\n",
    "x_testing = testing_data[:, :-1]\n",
    "y_testing = testing_data[:, -1]\n",
    "\n",
    "# feature names and their indexes on the 2D NumPy array\n",
    "feature_index_names = {0:'Temperature', 1:'Humidity', 2:'Light', 3:'CO2', 4:'Humidity_Ratio'}\n",
    "\n",
    "# statistics of training and testing datasets\n",
    "print('training data:')\n",
    "print('{:_<24s} = {:d}'.format('number of samples', y_training.shape[0]))\n",
    "print('{:_<24s} = {:d}'.format('number of zeros', np.sum(y_training == 0)))\n",
    "print('{:_<24s} = {:d}'.format('number of ones', np.sum(y_training == 1)))\n",
    "\n",
    "print('testing data:')\n",
    "print('{:_<24s} = {:d}'.format('number of samples', y_testing.shape[0]))\n",
    "print('{:_<24s} = {:d}'.format('number of zeros', np.sum(y_testing == 0)))\n",
    "print('{:_<24s} = {:d}'.format('number of ones', np.sum(y_testing == 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41004b6a",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "Some features may be irrelevant. Let's test KNN performance with different feature sets.\n",
    "\n",
    "Key Insight:\n",
    "- Feature selection removes noise and improves accuracy.\n",
    "\n",
    "##### Scatter plot\n",
    "\n",
    "Scatter plotting is a great tool in order to identify _discriminative_ features. The following function performs scatter plotting of pairwise features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e689dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(x, y, feature_index_names, save_figures=False):\n",
    "    c = ['r', 'b']\n",
    "    m = ['s', 'o']\n",
    "    s = [32, 32]\n",
    "    l = [r'$0$', r'$1$'] \n",
    "    font_size = 22\n",
    "    x_class0 = x[y==0.0, :]\n",
    "    x_class1 = x[y==1.0, :]    \n",
    "    for i in np.arange(len(feature_index_names)-1):\n",
    "        for j in np.arange(i+1, len(feature_index_names)):\n",
    "            figure_name = feature_index_names[i] + ' vs ' + feature_index_names[j]\n",
    "            pl.figure(figure_name)\n",
    "            pl.scatter(x_class0[:, i], x_class0[:, j], c=c[0], marker=m[0], s=s[0], label=l[0])\n",
    "            pl.scatter(x_class1[:, i], x_class1[:, j], c=c[1], marker=m[1], s=s[1], label=l[1])\n",
    "            pl.xlabel(feature_index_names[i], fontsize=font_size)\n",
    "            pl.ylabel(feature_index_names[j], fontsize=font_size)\n",
    "            pl.legend(scatterpoints=1, fontsize=font_size, loc = 'upper right')\n",
    "            pl.title(figure_name, fontsize=font_size)\n",
    "            pl.tight_layout()\n",
    "            if save_figures:\n",
    "                pl.savefig(figure_name + '.png')\n",
    "\n",
    "scatter_plot(x_training, y_training, feature_index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa62c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Select best features\n",
    "selector = SelectKBest(score_func=f_classif, k=1)\n",
    "X_new = selector.fit_transform(X_train, y_train)\n",
    "\n",
    "# Train KNN with selected features\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_new, y_train)\n",
    "print(\"Accuracy with best feature:\", accuracy_score(y_test, knn.predict(selector.transform(X_test))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5a22a3",
   "metadata": {},
   "source": [
    "### The sample correlation coeffient\n",
    "\n",
    "Based on the scatter plots, it may be a good idea to choose **Light** and **CO2** as our discriminative features to predict the state of the office room. But, can we somehow quantify this? How about using _sample correlation coefficient_ between features and target varget values? Let $x_{i}^{(n)}$ denote $i$th feature of example $n$, and $y^{(n)}$ denote the corresponding target value. The _sample correlation coefficient_ between $x_{i}$ and $y$ is defined as follows\n",
    "$$\n",
    "r\\left(x_{i}, y\\right) = \\frac{\\sum\\limits_{n=1}^{N} \\left(x_{i}^{(n)} - \\hat{x}_{i}\\right) \\left(y^{(n)} - \\hat{y}\\right)}{\\sqrt{\\sum\\limits_{n=1}^{N} \\left(x_{i}^{(n)} - \\hat{x}_{i}\\right)^{2} \\sum\\limits_{n=1}^{N} \\left(y^{(n)} - \\hat{y}\\right)^{2}}},\n",
    "$$\n",
    "where $\\hat{x}_{i} = \\frac{1}{N}\\sum\\limits_{n=1}^{N}x_{i}^{(n)}$ is the sample mean of $x_{i}$, and analogously for $\\hat{y}$. \n",
    "\n",
    "The following function `corr_coef` and code segment computes and displays the sample correlation coeffients between features and target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d770767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_coef(x, y):\n",
    "    x_m = (x - np.mean(x))\n",
    "    y_m = (y - np.mean(y))\n",
    "    r = np.sum(x_m * y_m) / np.sqrt(np.sum(x_m**2) * np.sum(y_m**2))\n",
    "    return r\n",
    "\n",
    "y = y_training.copy()\n",
    "for i in np.arange(len(feature_index_names)):\n",
    "    x_i = x_training[:, i]\n",
    "    r = corr_coef(x_i, y)    \n",
    "    print('r(' + feature_index_names[i] + ', Occupancy) = ' + '{:.4f}'.format(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a219b48f",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- observe from the sample correlation coefficients between features and target values, **Light** and **CO2** are highly correlated to the state of the office room. Did you expect this? Based on the sample correlation, can you find out which features are highly correlated? Use the following code segment to compute the sample correlation coefficients between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d28a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(len(feature_index_names)-1):\n",
    "    for j in np.arange(i+1, len(feature_index_names)):\n",
    "        x_i = x_training[:, i]\n",
    "        x_j = x_training[:, j]\n",
    "        r = corr_coef(x_i, x_j)\n",
    "        print('r(' + feature_index_names[i] + ', ' + feature_index_names[j] + ') = ' + '{:.4f}'.format(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b0dc3c",
   "metadata": {},
   "source": [
    "The column indexes of **Light** and **CO2** in `x_training` and `x_testing` arrays are 2 and 3, respectively. The following code segment creates a copy of training and testing datasets comprised of only features **Light** and **CO2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828abffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training_selected = x_training[:,[2, 3]]\n",
    "x_testing_selected = x_testing[:,[2, 3]]\n",
    "print(x_training_selected.shape)\n",
    "print(x_testing_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3126dfc",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbours: Finding and plotting K-Nearest Neighbours of the query point\n",
    "\n",
    "Assume that we have a sample measurement (or query point, or test point) from the office room \n",
    " $\\mathbf{x}^{\\left(q\\right)} = \\left[\\begin{array}{c} x_{1}^{\\left(q\\right)} \\\\ x_{2}^{\\left(q\\right)}\\end{array}\\right] = \\left[\\begin{array}{l} \\text{Light} \\\\ \\text{CO}_{2}\\end{array}\\right] = \\left[\\begin{array}{l} 200.0000 \\\\ 1000.0000 \\end{array}\\right]$, and we want to predict if the room is occupied or unoccupied. If the number of features is less than or equal to 3, then we can either use 2D or 3D scatter plot to observe the proximity of the query point to the examples in our training dataset to make prediction about the room status. The following function and code segment do the scatter plot and shows K-Nearest Neighbours of the query point from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad6756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot_two_features_only(x, y, feature1_name, feature2_name, x_q, K=3, save_figures=False):\n",
    "    c = ['r', 'b', 'g']\n",
    "    m = ['s', 'o', '*']\n",
    "    s = [32, 32, 64]\n",
    "    l = [r'$0$', r'$1$', r'query']\n",
    "    font_size = 22\n",
    "    x_class0 = x[y==0.0, :]\n",
    "    x_class1 = x[y==1.0, :]    \n",
    "    figure_name = feature1_name + ' vs ' + feature2_name + ' Query'\n",
    "    pl.figure(figure_name)\n",
    "    pl.scatter(x_class0[:, 0], x_class0[:, 1], c=c[0], marker=m[0], s=s[0], label=l[0])\n",
    "    pl.scatter(x_class1[:, 0], x_class1[:, 1], c=c[1], marker=m[1], s=s[1], label=l[1])\n",
    "    pl.scatter(x_q[0], x_q[1], c=c[2], marker=m[2], s=s[2], label=l[2])\n",
    "    # find the nearest neighbours\n",
    "    d = np.sqrt(np.sum((x - x_q)**2, axis=1)) # use the Euclidean distance\n",
    "    i = np.argsort(d)\n",
    "    for k in np.arange(K):\n",
    "        temp = np.vstack((x_q, x[i[k],:]))\n",
    "        pl.plot(temp[:,0], temp[:,1], c = c[2], linewidth=2)    \n",
    "    pl.xlabel(feature1_name, fontsize=font_size)\n",
    "    pl.ylabel(feature2_name, fontsize=font_size)\n",
    "    pl.legend(scatterpoints=1, fontsize=font_size, loc = 'upper right')\n",
    "    pl.title(figure_name, fontsize=font_size)\n",
    "    pl.tight_layout()\n",
    "    if save_figures:\n",
    "        pl.savefig(figure_name + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8779bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 11\n",
    "x_q = np.array([200.0, 1000.0])\n",
    "x_q.shape\n",
    "scatter_plot_two_features_only(x_training_selected, y_training, feature_index_names[2], feature_index_names[3], x_q, K=K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b29963",
   "metadata": {},
   "source": [
    "What is the prediction for the query point $\\mathbf{x}^{\\left(q\\right)} = \\left[\\begin{array}{c} x_{1}^{\\left(q\\right)} \\\\ x_{2}^{\\left(q\\right)}\\end{array}\\right] = \\left[\\begin{array}{l} \\text{Light} \\\\ \\text{CO}_{2}\\end{array}\\right] = \\left[\\begin{array}{l} 200.0000 \\\\ 1000.0000 \\end{array}\\right]$ when $K=111$? In order to make prediction for $K=111$, you can use the following function and code segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(x, y, x_q, K=3):\n",
    "    target_labels = np.unique(y) # unique set of target labels\n",
    "    target_labels_counts = np.zeros(len(target_labels)) # keeps counts of target labels\n",
    "    d = np.sqrt(np.sum((x - x_q)**2, axis=1)) # use the Euclidean distance\n",
    "    i = np.argsort(d) # sort distance vector in ascending order\n",
    "    for k in np.arange(len(target_labels)):\n",
    "        target_labels_counts[k] = np.sum(y[i[0:K]]==target_labels[k]) # count the number of each target label in K Nearest Neighbourhood\n",
    "    # apply the majority voting\n",
    "    l = np.argmax(target_labels_counts)\n",
    "    return target_labels[l]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4407285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = KNN(x_training_selected, y_training, x_q, K=111)\n",
    "print('prediction = ' + '{:.0f}'.format(predicted_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c1936",
   "metadata": {},
   "source": [
    "#### Calculating the Training error for different values of K\n",
    "The following code segment computes training error for different values of $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87515cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = y_training.shape[0] # the number of examples in training dataset\n",
    "print('{:>2} {:>16}'.format('K', 'Training Error'))\n",
    "for K in np.arange(1, 34, 2):\n",
    "    y_prediction = np.zeros(N)\n",
    "    for n in np.arange(N):\n",
    "        x_q = x_training_selected[n]\n",
    "        y_prediction[n] = KNN(x_training_selected, y_training, x_q, K)\n",
    "    classification_error = np.sum(y_training != y_prediction) / N\n",
    "    print('{:>2.0f} {:>16.4f}'.format(K, classification_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e3295",
   "metadata": {},
   "source": [
    "Can you compute testing error of the classifier for $K = 111$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c53c428",
   "metadata": {},
   "source": [
    "### Relationship Between KNN and Bayes Classifier\n",
    "The Bayes classifier is the theoretical optimal classifier that assigns a data point to the class with the highest posterior probability, given the feature values:\n",
    "$$ P(Y= c | X = x) = \\frac{P(X = x | Y = c) P(Y = c)}{P(X = x)} $$\n",
    "\n",
    "KNN can be seen as an approximation to the Bayes classifier.\n",
    "- When K=1, KNN assigns a class based on the closest single observation, leading to high variance and possible misclassification.\n",
    "- As K increases, KNN averages over more points, providing a smoother approximation to the Bayes classifier.\n",
    "\n",
    "Impact of the Choice of 𝐾\n",
    "- The choice of K significantly affects the performance of KNN:\n",
    "    - Small K (e.g., 1 or 3)\n",
    "        - Very sensitive to noise (high variance).\n",
    "        - Can lead to overfitting (memorizes the dataset).\n",
    "        - May not generalize well.\n",
    "    - Large K (e.g., 20 or 50)\n",
    "        - Reduces variance and smooths decision boundaries.\n",
    "        - Can lead to underfitting if too large.\n",
    "        - Bias increases as 𝐾 moves towards the majority class in imbalanced datasets.\n",
    "\n",
    "KNN Bias-Variance Tradeoff\n",
    "- Small 𝐾 → Low bias, high variance (fits the data too closely, not generalizable).\n",
    "- Large 𝐾 → High bias, low variance (loses detailed structure, may misclassify complex patterns).\n",
    "- Optimal 𝐾 balances bias and variance, often chosen using cross-validation.\n",
    "\n",
    "### Conditional Probability in K-Nearest Neighbors (KNN)\n",
    "\n",
    "In KNN classification, the probability that a point 𝑥 belongs to class 𝑐 is estimated using the proportion of class 𝑐 points among the 𝐾 nearest neighbors.\n",
    "\n",
    "Mathematically, the conditional probability of a point 𝑥 belonging to class 𝑐 is given by:\n",
    "$$ P(Y = c | X = x) = \\frac{\\text{Number of neighbors in class c}}{K} $$\n",
    "- Where:\n",
    "    - $P(Y=c∣X=x)$ is the estimated probability that 𝑥 belongs to class 𝑐.\n",
    "    - 𝐾 is the number of nearest neighbors.\n",
    "    - The numerator is the number of neighbors within 𝐾 that belong to class 𝑐.\n",
    "\n",
    "Example Calculation\n",
    "- Suppose we have a dataset with two classes:\n",
    "    - Class 0\n",
    "    - Class 1\n",
    "- We use K = 5 (5-nearest neighbors). If among the 5 neighbors:\n",
    "    - 3 belong to Class 1\n",
    "    - 2 belong to Class 0\n",
    "\n",
    "Then the probability estimates are:\n",
    "$$ P(Y = 1| X = x) = \\frac{3}{5} = 0.6 $$\n",
    "$$ P(Y = 0| X = x) = \\frac{2}{5} = 0.4 $$\n",
    "\n",
    "The classifier then assigns the class with the highest probability—in this case, Class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c4017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_classes=2, random_state=42)\n",
    "\n",
    "# Split dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit KNN classifier with K=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Pick a sample point from the test set\n",
    "sample_index = 0\n",
    "sample_point = X_test[sample_index].reshape(1, -1)\n",
    "\n",
    "# Predict class probabilities\n",
    "probabilities = knn.predict_proba(sample_point)\n",
    "\n",
    "print(f\"Estimated Probability of Class 0: {probabilities[0][0]:.4f}\")\n",
    "print(f\"Estimated Probability of Class 1: {probabilities[0][1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962db960",
   "metadata": {},
   "source": [
    "Interpreting the Output\n",
    "- The model calculates the proportion of K nearest neighbors in each class.\n",
    "- The class with the highest probability is assigned to the new data point.\n",
    "- If probabilities are close (e.g., 0.51 vs. 0.49), the decision boundary is unclear, and increasing 𝐾 might help smooth the decision function.\n",
    "\n",
    "## Calcualte Probability of belonging to a class\n",
    "- **Simple Probability**: Proportion of neighbors belonging to each class.\n",
    "- **Weighted Probability**: Proportion of weights (inverse distances) for each class.\n",
    "    - Use weighted probabilities when closer neighbors should have a stronger influence on the classification decision.\n",
    "\n",
    "### Calculate Class Probabilities\n",
    "In K-Nearest Neighbors (KNN) classification, you can calculate the probability of belonging to a class by examining the proportion of the nearest neighbors that belong to each class. This is particularly useful when you want to understand the confidence or uncertainty of the classification decision.\n",
    "\n",
    "Steps to Calculate Class Probabilities\n",
    "- Identify the k nearest neighbors of the test point.\n",
    "- Count the number of neighbors that belong to each class.\n",
    "- Divide the count for each class by k to get the probability.\n",
    "\n",
    "Formula\n",
    "- For a test point x, the probability $P(y = c|x)$ of belonging to class c is:\n",
    "$$ P(y = c|x) = \\frac{\\text{Number of neighbors of class c}}{k}$$\n",
    "\n",
    "### Example\n",
    "Suppose k=5, and the classes of the nearest neighbors are:\n",
    "- Neighbor 1: Class A\n",
    "- Neighbor 2: Class A\n",
    "- Neighbor 3: Class B\n",
    "- Neighbor 4: Class A\n",
    "- Neighbor 5: Class B\n",
    "\n",
    "The probabilities are:\n",
    "- $P(y = A|x) = \\frac{3}{5} = 0.6$\n",
    "- $P(y = B|x) = \\frac{2}{5} = 0.4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc67af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Example: Nearest neighbors' classes\n",
    "neighbor_classes = ['A', 'A', 'B', 'A', 'B']\n",
    "k = len(neighbor_classes)  # Number of neighbors\n",
    "\n",
    "# Count the occurrences of each class\n",
    "class_counts = Counter(neighbor_classes)\n",
    "\n",
    "# Calculate probabilities\n",
    "class_probabilities = {cls: count / k for cls, count in class_counts.items()}\n",
    "\n",
    "print(\"Class Probabilities:\", class_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608817bc",
   "metadata": {},
   "source": [
    "### Calculate Weigted Class Probabilities\n",
    "If you want to incorporate distances into the probability calculation (e.g., weighted KNN), you can use the inverse of the distances as weights. The probability of belonging to a class is then the sum of the weights for that class divided by the total sum of weights.\n",
    "\n",
    "Formula:\n",
    "$$ P(y = c|x) = \\frac{\\sum_{i \\in \\text{neighbors of class c}} \\cdot \\text{weight}_i }{\\sum^k_{i = 1} \\text{weight}_i}$$\n",
    "- where:\n",
    "    - $\\text{weight}_i = \\frac{1}{\\text{distance}_i}$\n",
    "\n",
    "Weighted Probability Example\n",
    "- Suppose k=5, and the distances and classes of the nearest neighbors are:\n",
    "    - Neighbor 1: Distance = 1.0, Class = A\n",
    "    - Neighbor 2: Distance = 2.0, Class = A\n",
    "    - Neighbor 3: Distance = 3.0, Class = B\n",
    "    - Neighbor 4: Distance = 1.5, Class = A\n",
    "    - Neighbor 5: Distance = 2.5, Class = B\n",
    "\n",
    "The weights are:\n",
    "- Class A: $\\frac{1}{1.0} + \\frac{1}{2.0} + \\frac{1}{1.5} = 1 + 0.5 + 0.67 = 2.17$\n",
    "- Class B: $\\frac{1}{3.0} + \\frac{1}{2.5} = 0.33 + 0.4 = 0.73$\n",
    "    - The total weight is $2.17+0.73=2.90$\n",
    "\n",
    "The probabilities are:\n",
    "- $P(y = A|x) = \\frac{2.17}{2.90} \\approx 0.75$\n",
    "- $P(y = B|x) = \\frac{0.73}{2.90} \\approx 0.25$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Distances and classes of nearest neighbors\n",
    "distances = [1.0, 2.0, 3.0, 1.5, 2.5]\n",
    "classes = ['A', 'A', 'B', 'A', 'B']\n",
    "\n",
    "# Calculate weights\n",
    "weights = [1 / dist for dist in distances]\n",
    "\n",
    "# Sum weights for each class\n",
    "class_weights = {'A': 0, 'B': 0}\n",
    "for cls, weight in zip(classes, weights):\n",
    "    class_weights[cls] += weight\n",
    "\n",
    "# Total weight\n",
    "total_weight = sum(class_weights.values())\n",
    "\n",
    "# Calculate probabilities\n",
    "class_probabilities = {cls: weight / total_weight for cls, weight in class_weights.items()}\n",
    "\n",
    "print(\"Weighted Class Probabilities:\", class_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1aad5e",
   "metadata": {},
   "source": [
    "### KNN and the Bias-Variance Trade-off\n",
    "The choice of K (number of neighbors) in K-Nearest Neighbors (KNN) has a direct impact on the bias-variance trade-off:\n",
    "\n",
    "##### What is Overfitting in KNN?\n",
    "Overfitting occurs when a model learns the noise or specific details of the training data, resulting in poor generalization to unseen data. In KNN:\n",
    "- A very small k (e.g., k=1) makes the model highly sensitive to noise and outliers in the training data, leading to overfitting.\n",
    "- A very large k (e.g., k=N, where N is the total number of training samples) makes the model too simplistic, leading to underfitting.\n",
    "\n",
    "|K Value|\tBias|\tVariance|\tOverfitting/Underfitting|\n",
    "|-------|-------|-----------|---------------------------|\n",
    "|Small 𝐾 (e.g., 1, 3, 5)|\tLow|\tHigh|\tOverfitting (captures noise)|\n",
    "|Medium 𝐾 (e.g., 10-20)|\tModerate|\tModerate|\tBalanced trade-off|\n",
    "|Large 𝐾 (e.g., 50, 100, total dataset size)|\tHigh|\tLow|\tUnderfitting (oversmooths decision boundaries)|\n",
    "\n",
    "### How k Controls Overfitting\n",
    "The value of k determines how many neighbors contribute to the prediction:\n",
    "- Small k:\n",
    "    - The model considers only a few nearest neighbors.\n",
    "    - This makes the decision boundary more complex and flexible, capturing fine-grained patterns in the training data.\n",
    "    - However, it also makes the model sensitive to noise and outliers, increasing the risk of overfitting.\n",
    "- Large k:\n",
    "    - The model considers many neighbors, smoothing out the decision boundary.\n",
    "    - This reduces the model's sensitivity to noise and outliers, making it more robust.\n",
    "    - However, it may oversimplify the decision boundary, leading to underfitting.\n",
    "\n",
    "### Effect of 𝐾 on Bias and Variance\n",
    "The choice of k directly affects the bias-variance trade-off:\n",
    "\n",
    "Small 𝐾 (low bias, high variance)\n",
    "- Low Bias: The model fits the training data closely.\n",
    "- High Variance: The model is sensitive to small fluctuations in the training data.\n",
    "- Decision boundary is very flexible.\n",
    "- Fits training data well but is sensitive to noise.\n",
    "- High variance because small changes in data can lead to drastic changes in predictions.\n",
    "\n",
    "Large 𝐾 (high bias, low variance)\n",
    "- High Bias: The model may oversimplify the data, leading to underfitting.\n",
    "- Low Variance: The model is less sensitive to noise and outliers.\n",
    "- Smoother decision boundary, making strong assumptions about the structure of the data.\n",
    "- More stable predictions but less responsive to fine-grained patterns.\n",
    "- High bias because it ignores local details.\n",
    "\n",
    "By selecting an appropriate k, you can balance bias and variance to achieve good generalization.\n",
    "\n",
    "Finding the Optimal 𝐾\n",
    "- Cross-validation is commonly used to find an optimal 𝐾 that balances bias and variance.\n",
    "    - valuate the model's performance (e.g., accuracy, F1-score) for different values of k using cross-validation.\n",
    "- Typically, an elbow method (plotting accuracy vs. 𝐾) helps determine the best 𝐾.\n",
    "    - Plot the error rate (e.g., misclassification rate) against k and choose the k where the error rate stabilizes or starts to increase.\n",
    "\n",
    "Example: Impact of k on Decision Boundaries\n",
    "Consider a binary classification problem with two classes. The decision boundaries for different values of k might look like this:\n",
    "- k = 1: The decision boundary is highly irregular, capturing every detail of the training data (overfitting).\n",
    "- k = 5: The decision boundary is smoother but still captures the general structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c082b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a synthetic dataset (moons for non-linearity)\n",
    "X, y = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Function to plot decision boundaries\n",
    "def plot_knn_decision_boundary(k_values, X_train, y_train):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, k in enumerate(k_values):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        \n",
    "        # Create grid\n",
    "        x_min, x_max = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5\n",
    "        y_min, y_max = X_train[:, 1].min() - 0.5, X_train[:, 1].max() + 0.5\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                             np.linspace(y_min, y_max, 200))\n",
    "        Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        # Plot\n",
    "        plt.subplot(1, len(k_values), i + 1)\n",
    "        plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "        plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k')\n",
    "        plt.title(f'K = {k}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Test different K values\n",
    "plot_knn_decision_boundary([1, 5, 15, 50], X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d2e21c",
   "metadata": {},
   "source": [
    "Expected Observations:\n",
    "- K = 1 → High variance (overfitting)\n",
    "    - The model perfectly memorizes the training data.\n",
    "    - Decision boundary is irregular, capturing noise.\n",
    "- K = 5 → Good balance\n",
    "    - Some smoothness in the boundary, reducing overfitting.\n",
    "- K = 15 → Reduced variance\n",
    "    - Smoother decision boundaries.\n",
    "    - Less sensitivity to small changes.\n",
    "- K = 50 → High bias (underfitting)\n",
    "    - Over-smooths the decision boundary.\n",
    "    - Fails to capture real class structures.\n",
    "\n",
    "### Finding the Optimal 𝐾 Using Cross-Validation\n",
    "To select the best 𝐾, we use cross-validation to measure model performance across different values of 𝐾. The goal is to choose 𝐾 that minimizes test error while avoiding overfitting and underfitting.\n",
    "\n",
    "Finding Optimal 𝐾\n",
    "We will:\n",
    "- Train KNN models with different values of 𝐾.\n",
    "- Use cross-validation to estimate performance.\n",
    "- Plot the error rate vs. 𝐾 to find the best value.\n",
    "\n",
    "Expected Observations\n",
    "- Small 𝐾 (e.g., 1-3) → High variance, unstable performance (overfitting).\n",
    "- Large 𝐾 (e.g., 20-30) → High bias, smooth decision boundary (underfitting).\n",
    "- Optimal 𝐾 is usually in the mid-range (e.g., 5-15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Range of K values to test\n",
    "k_values = range(1, 31)\n",
    "cv_scores = []\n",
    "\n",
    "# Perform cross-validation for each K\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')  # 5-fold cross-validation\n",
    "    cv_scores.append(scores.mean())  # Store average accuracy\n",
    "\n",
    "# Find optimal K\n",
    "optimal_k = k_values[np.argmax(cv_scores)]\n",
    "print(f\"Optimal K: {optimal_k}\")\n",
    "\n",
    "# Plot accuracy vs K\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, cv_scores, marker='o', linestyle='dashed', color='b')\n",
    "plt.xlabel(\"K (Number of Neighbors)\")\n",
    "plt.ylabel(\"Cross-Validation Accuracy\")\n",
    "plt.title(\"Optimal K Selection using Cross-Validation\")\n",
    "plt.axvline(optimal_k, linestyle='--', color='r', label=f'Optimal K = {optimal_k}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd56bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Test different values of k\n",
    "k_values = range(1, 30)\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(k_values, cv_scores, marker='o')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('k vs. Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Optimal k\n",
    "optimal_k = k_values[cv_scores.index(max(cv_scores))]\n",
    "print(\"Optimal k:\", optimal_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a51548",
   "metadata": {},
   "source": [
    "### Effect of Increasing 𝐾 on the Decision Boundary in KNN Classifier\n",
    "The decision boundary in K-Nearest Neighbors (KNN) depends on how the model classifies points based on their nearest neighbors. As the value of 𝐾 increases, the decision boundary undergoes significant changes:\n",
    "\n",
    "1️⃣ Small 𝐾 (e.g., 𝐾=1 or 𝐾=3) → Complex, Highly Flexible Boundaries\n",
    "- Each data point has a strong influence on classification.\n",
    "- The decision boundary is highly irregular and follows the training data very closely.\n",
    "- It captures small details and noise, leading to overfitting.\n",
    "- Predictions are highly sensitive to minor changes in the data.\n",
    "\n",
    "💡 Example:\n",
    "- With 𝐾=1, each test point is classified based on only its nearest neighbor, leading to a jagged, irregular boundary.\n",
    "- If the training data has noise, this noise gets incorporated into the decision boundary.\n",
    "\n",
    "2️⃣ Moderate 𝐾 (e.g., 𝐾=5 to 𝐾=15) → Balanced Boundary\n",
    "- The decision boundary is smoother and less sensitive to individual points.\n",
    "- Classification is based on a group of neighbors, reducing sensitivity to outliers.\n",
    "- It provides a good trade-off between capturing patterns and avoiding noise.\n",
    "\n",
    "💡 Example:\n",
    "- With 𝐾=10, the classifier considers a more generalized decision rule.\n",
    "- The boundary is still flexible but avoids extreme fluctuations.\n",
    "\n",
    "3️⃣ Large 𝐾 (e.g., 𝐾=50 or more) → Overly Smooth Boundaries\n",
    "- The model considers many neighbors, making the decision boundary very smooth.\n",
    "- It starts treating the data as one large region, losing finer distinctions.\n",
    "- This leads to underfitting, where important details in the data structure are ignored.\n",
    "- The classifier behaves more like a global majority vote, losing local patterns.\n",
    "💡 Example:\n",
    "- With 𝐾=100, the model essentially assigns the most common class to all points, leading to an overly simplified decision boundary.\n",
    "\n",
    "##### Visualization of Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a20a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate dataset\n",
    "X, y = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Function to plot decision boundaries\n",
    "def plot_knn_decision_boundary(k_values, X_train, y_train):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, k in enumerate(k_values):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        \n",
    "        # Create grid\n",
    "        x_min, x_max = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5\n",
    "        y_min, y_max = X_train[:, 1].min() - 0.5, X_train[:, 1].max() + 0.5\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                             np.linspace(y_min, y_max, 200))\n",
    "        Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        # Plot\n",
    "        plt.subplot(1, len(k_values), i + 1)\n",
    "        plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "        plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k')\n",
    "        plt.title(f'K = {k}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Test different K values\n",
    "plot_knn_decision_boundary([1, 5, 15, 50], X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7b0401",
   "metadata": {},
   "source": [
    "KNN Classifier and Impact of K\n",
    "- Let's train a KNN classifier and observe how the decision boundary changes with different values of K.\n",
    "\n",
    "Key Takeaways:\n",
    "- Low K (e.g., K=1) → Highly flexible, captures noise (high variance, low bias).\n",
    "- Medium K (e.g., K=5) → Good balance between bias and variance.\n",
    "- High K (e.g., K=15) → Smoother decision boundary, less sensitive to noise (high bias, low variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db69a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_knn_decision_boundary(X, y, k=3):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X, y)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolor='k')\n",
    "    plt.title(f\"KNN Decision Boundary (K={k})\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize for different K values\n",
    "for k in [1, 5, 15]:\n",
    "    plot_knn_decision_boundary(X_train, y_train, k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4683eae5",
   "metadata": {},
   "source": [
    "## Distance Measures | Metrics in KNN\n",
    "Understanding distance metrics is crucial because KNN relies on calculating distances between data points to determine their similarity. Below, I will explain the most commonly used distance metrics, their intuition, use cases\n",
    "\n",
    "##### **Euclidean Distance (Default in KNN)**\n",
    "Definition: Euclidean distance is the straight-line distance between two points in Euclidean space. It is the most common distance metric and works well for continuous numerical data.\n",
    "\n",
    "Significance in KNN:\n",
    "- Measures the straight-line distance.\n",
    "    - Used to measure similarity between data points.\n",
    "- Produces smooth round decision boundaries (good for continuous, well-spaced data).\n",
    "- Sensitive to differences in scale.\n",
    "    - feature normalization is crucial\n",
    "\n",
    "Formula:\n",
    "- For two points $𝑃(𝑥_1, 𝑦_1)$ and 𝑄(𝑥_2, 𝑦_2), the Euclidean distance is:\n",
    "$$ d(P,Q) = \\sqrt{ (x_1 - x_2)^2 + (y_1 - y_2)^2} $$\n",
    "\n",
    "- For two points in N-dimentional:\n",
    "    - $P = p_1, p_2, ..., p_n$\n",
    "    - $Q = q_1, q_2, ..., q_n$\n",
    "$$ d(P,Q) = \\sqrt{\\sum^n_{i =1} (p_i - q_i)^2} $$\n",
    "\n",
    "Intuition:\n",
    "- Euclidean distance measures the \"as-the-crow-flies\" distance between two points. It is sensitive to the magnitude of differences in each dimension.\n",
    "\n",
    "Use Case:\n",
    "- Works well for low-dimensional data.\n",
    "- Suitable for data where all features are on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf0f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: Distance between two points in a 2D space\n",
    "P = np.array([1, 2])\n",
    "Q = np.array([4, 6])\n",
    "\n",
    "# Calculate Euclidean distance\n",
    "euclidean_distance = np.linalg.norm(P - Q)\n",
    "print(\"Euclidean Distance:\", euclidean_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57bcced",
   "metadata": {},
   "source": [
    "##### **Manhattan Distance (L1 Norm | L1 distance)**\n",
    "Definition: Manhattan distance is the sum of the absolute differences between the coordinates of two points. It is also known as \"city block\" distance. It measures the distance between two points by summing the absolute differences of their coordinates. It is called \"Manhattan distance\" because it resembles the path a taxi would take in a grid-like city like Manhattan.\n",
    "- Measures distance by summing absolute differences.\n",
    "- Creates grid-like decision boundaries.\n",
    "- Works well when features have different units or when movements are constrained (e.g., city block movement).\n",
    "\n",
    "Formula:\n",
    "- For two points $P=(p_1, p_2, …,p_n)$ and $Q=(q_1, q_2, …,q_n)$, the Manhattan distance is:\n",
    "$$ d(P,Q) = \\sum^n_{i = 1} |p_i - q_i| $$\n",
    "\n",
    "Intuition:\n",
    "- Manhattan distance measures the distance traveled along axes at right angles, like moving through a grid (e.g., city blocks).\n",
    "\n",
    "Interpretation:\n",
    "- Manhattan distance is robust to outliers because it uses absolute differences.\n",
    "- It is suitable for high-dimensional data and categorical/binary features.\n",
    "- It measures the distance along axes at right angles, like moving through a grid.\n",
    "\n",
    "Use Case:\n",
    "- Useful for high-dimensional data.\n",
    "- Suitable for data with categorical or binary features.\n",
    "\n",
    "Example:\n",
    "- Let P=(1,2) and Q=(4,6). The Manhattan distance is:\n",
    "$$d(P,Q)=∣1−4∣+∣2−6∣=3+4=7$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1fa026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Manhattan distance\n",
    "manhattan_distance = np.sum(np.abs(P - Q))\n",
    "print(\"Manhattan Distance:\", manhattan_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b999ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Points\n",
    "P = np.array([1, 2])\n",
    "Q = np.array([4, 6])\n",
    "\n",
    "# Manhattan distance\n",
    "manhattan_distance = np.sum(np.abs(P - Q))\n",
    "print(\"Manhattan Distance:\", manhattan_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185eaa3d",
   "metadata": {},
   "source": [
    "##### **Minkowski Distance (Generalization of Euclidean and Manhattan)**\n",
    "Definition: Minkowski distance is a generalization of Euclidean and Manhattan distances. It introduces a parameter p that allows it to behave like other distances.\n",
    "\n",
    "Intuition:\n",
    "- If 𝑝=1, it's Manhattan Distance.\n",
    "- If 𝑝=2, it's Euclidean Distance.\n",
    "- For other values of p, it can model different types of distances.\n",
    "\n",
    "Formula:\n",
    "$$ d(P,Q) = (\\sum |p_i - q_i|^p)^{\\frac{1}{p}} $$\n",
    "\n",
    "Use Case:\n",
    "- Flexible for different types of data.\n",
    "- Useful when you want to experiment with different distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6455c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Minkowski distance with p=3\n",
    "p = 3\n",
    "minkowski_distance = np.power(np.sum(np.power(np.abs(P - Q), p)), 1/p)\n",
    "print(\"Minkowski Distance (p=3):\", minkowski_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7b7a0",
   "metadata": {},
   "source": [
    "##### **Chebyshev Distance**\n",
    "Definition: Chebyshev distance is the maximum absolute difference between the coordinates of two points.\n",
    "- Measures only the largest difference in any dimension.\n",
    "- Results in square-shaped decision boundaries.\n",
    "\n",
    "Formula:\n",
    "$$ d(P,Q) = \\max_i |p_i - q_i| $$\n",
    "\n",
    "Intuition:\n",
    "- Chebyshev distance measures the greatest difference in any single dimension. It is like moving like a king in chess (any number of squares in any direction).\n",
    "\n",
    "Use Case:\n",
    "- Useful in games or grid-based systems.\n",
    "- Suitable for data where the maximum difference is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d483d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Chebyshev distance\n",
    "chebyshev_distance = np.max(np.abs(P - Q))\n",
    "print(\"Chebyshev Distance:\", chebyshev_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653dc744",
   "metadata": {},
   "source": [
    "##### **Mahalanobis Distance (Accounts for Correlation Between Features)**\n",
    "Definition: Mahalanobis distance measures the distance between a point and a distribution, taking into account the covariance between variables.\n",
    "- Uses the covariance matrix 𝑆 to measure distances.\n",
    "- Handles correlated features better.\n",
    "- Requires estimating the covariance matrix, making it computationally expensive.\n",
    "\n",
    "Formula:\n",
    "$$ d(P,Q) = \\sqrt{ (p_i - q_i)^T S^{-1} (p - q)} $$\n",
    "- where S is the covariance matrix.\n",
    "\n",
    "Intuition:\n",
    "- Mahalanobis distance accounts for the scale and correlation of the data. It is useful when features are correlated or have different scales.\n",
    "\n",
    "Use Case:\n",
    "- Suitable for multivariate data with correlated features.\n",
    "- Useful in outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bc9c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# Example data\n",
    "P = np.array([1, 2])\n",
    "Q = np.array([4, 6])\n",
    "data = np.array([P, Q])\n",
    "\n",
    "# Calculate covariance matrix and its inverse\n",
    "cov_matrix = np.cov(data, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "\n",
    "# Calculate Mahalanobis distance\n",
    "mahalanobis_distance = mahalanobis(P, Q, inv_cov_matrix)\n",
    "print(\"Mahalanobis Distance:\", mahalanobis_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1880e93e",
   "metadata": {},
   "source": [
    "##### **Cosine Similarity**\n",
    "Definition: Cosine similarity measures the cosine of the angle between two vectors. It is often used to measure similarity in high-dimensional spaces. It is derived from cosine similarity, which measures the orientation (angle) of the vectors rather than their magnitude. Cosine distance is defined as:\n",
    "$$ \\text{Cosine Distance} = 1 - \\text{Cosine Similarity} $$\n",
    "\n",
    "Formula:\n",
    "$$ \\text{Cosine Similarity} = \\frac{P \\cdot Q}{||P|| ||Q||} $$\n",
    "- where:\n",
    "    - $P \\cdot Q$:  is the dot product of P and Q.\n",
    "    - ||P|| ||Q||: are the magnitudes (Euclidean norms) of P and Q.\n",
    "\n",
    "The cosine distance is:\n",
    "$$ \\text{Cosine Distance} = 1 - \\frac{P \\cdot Q}{||P|| ||Q||} $$\n",
    "\n",
    "Intuition:\n",
    "- Cosine similarity focuses on the orientation (angle) of the vectors rather than their magnitude. It is useful for text data or sparse data.\n",
    "\n",
    "Interpretation:\n",
    "- Cosine distance is useful for high-dimensional data, such as text data (e.g., TF-IDF vectors).\n",
    "- It focuses on the orientation of the vectors, making it insensitive to magnitude.\n",
    "- A cosine distance of 0 means the vectors are identical (angle = 0°), while a distance of 1 means they are orthogonal (angle = 90°).\n",
    "\n",
    "Use Case:\n",
    "- Suitable for text data (e.g., TF-IDF vectors).\n",
    "- Useful for high-dimensional sparse data.\n",
    "\n",
    "Key Differences Between Manhattan and Cosine Distance\n",
    "\n",
    "|Metric|\tInterpretation|\tUse Case|\tSensitivity to Magnitude|\n",
    "|------|------------------|---------|---------------------------|\n",
    "|Manhattan|\tSum of absolute differences.|\tHigh-dimensional, categorical/binary data.|\tSensitive.|\n",
    "|Cosine|\tAngle between vectors.|\tText data, high-dimensional sparse data.|\tInsensitive.|\n",
    "\n",
    "##### When to Use Each Distance\n",
    "\n",
    "Manhattan Distance:\n",
    "- Use when the magnitude of differences in each dimension is important.\n",
    "- Suitable for grid-like structures or when outliers should not dominate the distance.\n",
    "\n",
    "Cosine Distance:\n",
    "- Use when the orientation (angle) of the vectors is more important than their magnitude.\n",
    "- Ideal for text data or high-dimensional sparse data.\n",
    "\n",
    "Example:\n",
    "- Let P=(1,2) and Q=(4,6). The cosine distance is calculated as follows:\n",
    "    - Dot product: \n",
    "        - $P⋅Q=(1×4)+(2×6)=4+12=16$\n",
    "    - Magnitudes:\n",
    "        - ||P|| = $\\sqrt{1^2 + 2^2} = \\sqrt{1 + 4} = \\sqrt{5}$\n",
    "        - ||Q|| = $\\sqrt{4^2 + 6^2} = \\sqrt{16 + 36} = \\sqrt{52}$\n",
    "    - Cosine similarity:\n",
    "        - $ \\text{Cosine Similarity} = \\frac{16}{\\sqrt{5} \\times \\sqrt{52}} \\approx \\frac{16}{16.12} \\approx 0.992 $\n",
    "    - Cosine distance:\n",
    "        - Cosine Distance= $1− 0.992 = 0.008$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cos_sim = cosine_similarity([P], [Q])\n",
    "print(\"Cosine Similarity:\", cos_sim[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75ffc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# Points\n",
    "P = np.array([1, 2]).reshape(1, -1)\n",
    "Q = np.array([4, 6]).reshape(1, -1)\n",
    "\n",
    "# Cosine distance\n",
    "cosine_distance = cosine_distances(P, Q)\n",
    "print(\"Cosine Distance:\", cosine_distance[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2d9d44",
   "metadata": {},
   "source": [
    "##### **Hamming Distance**\n",
    "Definition: Hamming distance measures the number of positions at which the corresponding symbols are different. It is used for categorical or binary data.\n",
    "\n",
    "Formula:\n",
    "$$ d(P,Q) = \\sum^n_{i = 1} 1(p_i \\neq q_i) $$\n",
    "- where 1 is the indicator function.\n",
    "\n",
    "Intuition:\n",
    "- Hamming distance counts the number of mismatches between two vectors.\n",
    "\n",
    "Use Case:\n",
    "- Suitable for binary or categorical data.\n",
    "- Useful in error detection and correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61de8f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import hamming\n",
    "\n",
    "# Example binary data\n",
    "P = np.array([1, 0, 1, 1])\n",
    "Q = np.array([0, 0, 1, 0])\n",
    "\n",
    "# Calculate Hamming distance\n",
    "hamming_distance = hamming(P, Q)\n",
    "print(\"Hamming Distance:\", hamming_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797cd0d",
   "metadata": {},
   "source": [
    "|Distance Metric|\tUse Case |\n",
    "|---------------|------------|\n",
    "|Euclidean Distance|\tLow-dimensional, continuous data.|\n",
    "|Manhattan Distance|\tHigh-dimensional, categorical/binary data.|\n",
    "|Minkowski Distance|\tFlexible, customizable distance metric.|\n",
    "|Chebyshev Distance|\tGrid-based systems, maximum difference matters.|\n",
    "|Mahalanobis Distance|\tMultivariate data with correlated features.|\n",
    "|Cosine Similarity|\tText data, high-dimensional sparse data.|\n",
    "|Hamming Distance|\tBinary or categorical data.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d81f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "point1 = np.array([2, 3])\n",
    "point2 = np.array([5, 7])\n",
    "\n",
    "# Euclidean Distance\n",
    "print(\"Euclidean Distance:\", euclidean(point1, point2))\n",
    "\n",
    "# Manhattan Distance\n",
    "print(\"Manhattan Distance:\", cityblock(point1, point2))\n",
    "\n",
    "# Jaccard Distance (for binary feature vectors)\n",
    "binary_vec1 = np.array([1, 0, 1, 0, 1])\n",
    "binary_vec2 = np.array([1, 1, 0, 0, 1])\n",
    "print(\"Jaccard Distance:\", jaccard(binary_vec1, binary_vec2))\n",
    "\n",
    "# Levenshtein Distance (for string similarity)\n",
    "string1 = \"knn\"\n",
    "string2 = \"kmn\"\n",
    "print(\"Levenshtein Distance:\", levenshtein_distance(string1, string2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bd3c02",
   "metadata": {},
   "source": [
    "### Effect of Distance Metrics on the Decision Boundary in KNN\n",
    "The K-Nearest Neighbors (KNN) classifier relies heavily on distance metrics to determine which training samples are the \"nearest neighbors\" of a given point. The choice of distance metric affects the shape and flexibility of the decision boundary.\n",
    "\n",
    "#### Distance Metrics in KNN\n",
    "KNN commonly uses the following distance measures:\n",
    "##### **Euclidean Distance (Default in KNN)**\n",
    "- Measures the straight-line distance.\n",
    "- Produces round decision boundaries (good for continuous, well-spaced data).\n",
    "- Sensitive to differences in scale.\n",
    "$$ d(p,q) = \\sqrt{\\sum (p_i - q_i)^2} $$\n",
    "\n",
    "##### **Manhattan Distance (L1 Norm)**\n",
    "- Measures distance by summing absolute differences.\n",
    "- Creates grid-like decision boundaries.\n",
    "- Works well when features have different units or when movements are constrained (e.g., city block movement).\n",
    "$$ d(p,q) = \\sum |p_i - q_i| $$\n",
    "\n",
    "##### **Minkowski Distance (Generalization of Euclidean and Manhattan)**\n",
    "- If 𝑝=1, it's Manhattan Distance.\n",
    "- If 𝑝=2, it's Euclidean Distance.\n",
    "$$ d(p,q) = (\\sum |p_i - q_i|^p)^{\\frac{1}{p}} $$\n",
    "\n",
    "##### **Chebyshev Distance**\n",
    "- Measures only the largest difference in any dimension.\n",
    "- Results in square-shaped decision boundaries.\n",
    "$$ d(p,q) = \\max |p_i - q_i| $$\n",
    "\n",
    "##### **Mahalanobis Distance (Accounts for Correlation Between Features)**\n",
    "- Uses the covariance matrix 𝑆 to measure distances.\n",
    "- Handles correlated features better.\n",
    "- Requires estimating the covariance matrix, making it computationally expensive.\n",
    "$$ d(p,q) = \\sqrt{ (p_i - q_i)^T S^{-1} (p - q)} $$\n",
    "\n",
    "Observations and Comparisons\n",
    "|Distance Metric|\tDecision Boundary Shape|\tBest for|\n",
    "|---------------|--------------------------|------------|\n",
    "|Euclidean Distance|\tRound decision boundaries (smooth)|\tWell-spaced continuous data|\n",
    "|Manhattan Distance|\tGrid-like decision boundaries|\tCity-block data, different units|\n",
    "|Chebyshev Distance|\tBoxy decision boundaries|\tGrid-based movements (chess, warehouse)|\n",
    "|Mahalanobis Distance|\tAdaptive decision boundary|\tCorrelated features, real-world finance|\n",
    "\n",
    "\n",
    "### Which Distance Metric Should You Use?\n",
    "- Euclidean: Default choice; works well when all features have similar importance.\n",
    "- Manhattan: Preferred when features have different scales or grid-like structures.\n",
    "- Chebyshev: Useful when movement is constrained (e.g., chess, delivery routes).\n",
    "- Mahalanobis: Best when features are correlated, especially in financial data.\n",
    "\n",
    "##### Decision Boundaries for Different Distance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d319c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Function to plot decision boundaries for different distance metrics\n",
    "def plot_decision_boundary(dist_metrics, X_train, y_train):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    for i, metric in enumerate(dist_metrics):\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        # Create a grid\n",
    "        x_min, x_max = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5\n",
    "        y_min, y_max = X_train[:, 1].min() - 0.5, X_train[:, 1].max() + 0.5\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                             np.linspace(y_min, y_max, 200))\n",
    "        Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        # Plot decision boundary\n",
    "        plt.subplot(1, len(dist_metrics), i + 1)\n",
    "        plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "        plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k')\n",
    "        plt.title(f'Metric: {metric}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Test different distance metrics\n",
    "plot_decision_boundary(['euclidean', 'manhattan', 'chebyshev'], X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3027c07",
   "metadata": {},
   "source": [
    "### Combining Functions and how its used for classification\n",
    "Combining functions in the context of K-Nearest Neighbors (KNN) refers to the process of aggregating the predictions or contributions of the k nearest neighbors to make a final classification decision.  They are used  to aggregate the predictions or contributions of multiple components (e.g., neighbors, models, or features) into a final decision. These functions play a critical role in determining how individual contributions are weighted and combined to produce a coherent output. This is a crucial step in KNN because the algorithm relies on the \"wisdom of the crowd\" (i.e., the nearest neighbors) to determine the class of a new data point. Below, I’ll explain how combining functions work, the intuition behind them, and their role in KNN classification.\n",
    "\n",
    "Definition:\n",
    "A combining function determines how the votes of the nearest neighbors are aggregated to make a final decision.\n",
    "\n",
    "What Are Combining Functions?\n",
    "- Combining functions are mathematical or logical operations that aggregate multiple inputs into a single output. In the context of KNN, combining functions are used to:\n",
    "    - Aggregate the class labels or values of the k nearest neighbors.\n",
    "    - Assign weights to neighbors based on their distance or similarity to the test point.\n",
    "    - Produce a final prediction (classification or regression).\n",
    "\n",
    "Types:\n",
    "- Majority voting (for Classification): The most frequent class among neighbors is chosen.\n",
    "    - How It Works:\n",
    "        - Each of the k nearest neighbors \"votes\" for its class label.\n",
    "        - The class with the most votes is assigned to the test point.\n",
    "    - Interpretation:\n",
    "        - Assumes that the most frequent class among the neighbors is the best prediction.\n",
    "        - Works well when the data is balanced and the classes are well-separated.\n",
    "    - Example:\n",
    "        - If k=5 and the neighbors' classes are [A, A, B, A, B], the predicted class is A.\n",
    "- Weighted voting (for Classification): Closer neighbors are given more influence.\n",
    "    - How It Works:\n",
    "        - Each neighbor's vote is weighted by its distance or similarity to the test point.\n",
    "        - Closer neighbors have higher weights.\n",
    "        - The class with the highest total weight is assigned to the test point.\n",
    "    - Interpretation:\n",
    "        - Reflects the intuition that closer neighbors are more similar to the test point and should have a stronger influence on the prediction.\n",
    "        - Useful when the data has varying densities or when some neighbors are significantly closer than others.\n",
    "    - Example:\n",
    "        - If k=5 and the weights for classes A and B are 2.17 and 0.73, respectively, the predicted class is A.\n",
    "- Probability estimation: Predicts probability distribution instead of a strict class label.\n",
    "- Averaging (for Regression)\n",
    "    - How It Works:\n",
    "        - The target values of the k nearest neighbors are averaged to produce the final prediction.\n",
    "    - Interpretation:\n",
    "        - Assumes that the target value of the test point is the average of its neighbors' values.\n",
    "        - Works well when the relationship between features and the target is smooth.\n",
    "    - Example:\n",
    "        - If k=5 and the neighbors' target values are $[3, 5, 4, 6, 5]$, the predicted value is $\\frac{3 + 5 + 4 + 6 + 5}{5} = 4.6$\n",
    "- Weighted Averaging (for Regression)\n",
    "    - How It Works:\n",
    "        - The target values of the k nearest neighbors are weighted by their distance or similarity to the test point.\n",
    "        - Closer neighbors have higher weights.\n",
    "        - The final prediction is the weighted average of the neighbors' target values.\n",
    "    - Interpretation:\n",
    "        - Reflects the intuition that closer neighbors are more similar to the test point and should contribute more to the prediction.\n",
    "        - Useful when the relationship between features and the target is non-linear or when some neighbors are significantly closer than others.\n",
    "    - Example:\n",
    "        - If k=5, the neighbors' target values are $[3, 5, 4, 6, 5]$, and the weights are $[0.5, 0.3, 0.2, 0.4, 0.3]$, the predicted value is: $\\frac{(3 \\times 0.5) + (5 \\times 0.3) + (4 \\times 0.2) + (6 \\times 0.4) + (5 \\times 0.3)}{0.5 + 0.3 + 0.2 + 0.4 + 0.3} = \\frac{1.5 + 1.5 + 0.8 + 2.4 + 1.5}{1.7} = \\frac{7.7}{1.7} \\approx 4.53$\n",
    "\n",
    "Interpretation of Combining Functions\n",
    "- Combining functions determine how the contributions of individual components (e.g., neighbors) are aggregated to make a final decision. The choice of combining function depends on:\n",
    "    - The nature of the problem: Classification vs. regression.\n",
    "    - The data distribution: Balanced vs. imbalanced, linear vs. non-linear.\n",
    "    - The importance of neighbors: Whether closer neighbors should have a stronger influence.\n",
    "\n",
    "|Combining Function|\tUse Case|\tInterpretation|\n",
    "|------------------|------------|-----------------|\n",
    "|Majority Voting|\tClassification|\tThe most frequent class among neighbors is the best prediction.|\n",
    "|Weighted Voting|\tClassification|\tCloser neighbors have a stronger influence on the prediction.|\n",
    "|Averaging|\tRegression|\tThe target value is the average of the neighbors' values.|\n",
    "|Weighted Averaging|\tRegression|\tCloser neighbors contribute more to the predicted value.|\n",
    "\n",
    "Significance in KNN:\n",
    "- Affects classification accuracy.\n",
    "- Weighted voting can reduce misclassification when data points are unevenly distributed.\n",
    "\n",
    "|Combining Function|\tUse Case|\n",
    "|------------------|------------|\n",
    "|Majority Voting|\tSimple, works well for balanced data.|\n",
    "|Weighted Voting|\tAccounts for varying distances; useful for imbalanced or dense data.|\n",
    "|Kernel Smoothing|\tProvides non-linear weighting; useful for complex data distributions.|\n",
    "|Weighted Average|\tUsed in KNN regression to predict continuous values.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority Voting:\n",
    "from collections import Counter\n",
    "\n",
    "# Example: Nearest neighbors' classes\n",
    "neighbor_classes = ['A', 'A', 'B', 'A', 'B']\n",
    "\n",
    "# Majority voting\n",
    "majority_class = Counter(neighbor_classes).most_common(1)[0][0]\n",
    "print(\"Predicted Class (Majority Voting):\", majority_class)\n",
    "\n",
    "# Weighted Voting:\n",
    "# Example: Distances and classes of nearest neighbors\n",
    "distances = [1.0, 2.0, 3.0, 1.5, 2.5]\n",
    "classes = ['A', 'A', 'B', 'A', 'B']\n",
    "\n",
    "# Weighted voting\n",
    "class_weights = {'A': 0, 'B': 0}\n",
    "for dist, cls in zip(distances, classes):\n",
    "    class_weights[cls] += 1 / dist\n",
    "\n",
    "predicted_class = max(class_weights, key=class_weights.get)\n",
    "print(\"Predicted Class (Weighted Voting):\", predicted_class)\n",
    "\n",
    "# Averaging:\n",
    "# Example: Nearest neighbors' target values\n",
    "neighbor_values = [3, 5, 4, 6, 5]\n",
    "\n",
    "# Averaging\n",
    "predicted_value = sum(neighbor_values) / len(neighbor_values)\n",
    "print(\"Predicted Value (Averaging):\", predicted_value)\n",
    "\n",
    "# Weighted Averaging:\n",
    "# Example: Nearest neighbors' target values and weights\n",
    "neighbor_values = [3, 5, 4, 6, 5]\n",
    "weights = [0.5, 0.3, 0.2, 0.4, 0.3]\n",
    "\n",
    "# Weighted averaging\n",
    "weighted_sum = sum(v * w for v, w in zip(neighbor_values, weights))\n",
    "total_weight = sum(weights)\n",
    "predicted_value = weighted_sum / total_weight\n",
    "print(\"Predicted Value (Weighted Averaging):\", predicted_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586b68b4",
   "metadata": {},
   "source": [
    "##### **Majority Voting (Default Combining Function)**\n",
    "How It Works:\n",
    "- For a given test point, KNN identifies the k nearest neighbors in the training data.\n",
    "- Each neighbor \"votes\" for its class label.\n",
    "- The class with the majority of votes is assigned to the test point.\n",
    "\n",
    "Intuition:\n",
    "- The assumption is that the most common class among the nearest neighbors is the best representation of the test point's class.\n",
    "- This works well when the data is balanced and the classes are well-separated.\n",
    "\n",
    "Example:\n",
    "- Suppose k=5, and the classes of the nearest neighbors are: `[Class A, Class A, Class B, Class A, Class B]`.\n",
    "    - Class A has 3 votes.\n",
    "    - Class B has 2 votes.\n",
    "\n",
    "The test point is classified as **Class A**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e8aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from scipy.spatial.distance import euclidean, cityblock, jaccard\n",
    "from Levenshtein import distance as levenshtein_distance  # Install using `pip install python-Levenshtein`\n",
    "\n",
    "# Enable better visualization\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Example: Nearest neighbors' classes\n",
    "neighbor_classes = ['A', 'A', 'B', 'A', 'B']\n",
    "\n",
    "# Majority voting\n",
    "majority_class = Counter(neighbor_classes).most_common(1)[0][0]\n",
    "print(\"Predicted Class (Majority Voting):\", majority_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559bcc8",
   "metadata": {},
   "source": [
    "##### **Explain weighted voting (scoring) or similarity moderated voting (scoring)**\n",
    "Weighted voting (or similarity-modulated voting) is a technique used in K-Nearest Neighbors (KNN) to give more importance to closer neighbors when making predictions. Instead of treating all neighbors equally, weighted voting assigns a weight to each neighbor based on its distance or similarity to the test point. Closer neighbors have higher weights, while farther neighbors have lower weights.\n",
    "\n",
    "key terms:\n",
    "- Weighted Voting: Assigns higher importance to closer neighbors using weights.\n",
    "- Inverse Distance Weighting: Simple and effective for most cases.\n",
    "- Gaussian Kernel Weighting: Provides a smoother weighting scheme, useful for non-linear relationships.\n",
    "- Class Probabilities: Calculated by normalizing the total weights for each class.\n",
    "\n",
    "Intuition:\n",
    "- Closer neighbors are more likely to be similar to the test point, so their votes should carry more importance | have a stronger influence on the prediction..\n",
    "- This is useful when the data has varying densities or when some neighbors are significantly closer than others.\n",
    "\n",
    "How It Works:\n",
    "- Instead of treating all neighbors equally, closer neighbors are given more weight in the voting process.\n",
    "- The weight is typically calculated as: \n",
    "    - the inverse of the distance: weight = $\\frac{1}{\\text{distance}}$.\n",
    "    - or using a kernel function (e.g., Gaussian kernel).\n",
    "- The class with the highest total weight is assigned to the test point.\n",
    "\n",
    "Steps for Weighted Voting\n",
    "- Identify the k nearest neighbors of the test point.\n",
    "- Calculate the weight for each neighbor based on its distance or similarity to the test point.\n",
    "- Sum the weights for each class.\n",
    "- Assign the class with the highest total weight to the test point.\n",
    "- Calculate class probabilities by dividing the total weight for each class by the sum of all weights.\n",
    "\n",
    "Weight Calculation\n",
    "\n",
    "The weight for a neighbor can be calculated in several ways:\n",
    "- **Inverse Distance Weighting**:\n",
    "$$ \\text{weight} = \\frac{1}{\\text{distance}}$$\n",
    "\n",
    "Example:\n",
    "- Suppose k=5, and the distances and classes of the nearest neighbors are:\n",
    "    - Neighbor 1: Distance = 1.0, Class = A\n",
    "    - Neighbor 2: Distance = 2.0, Class = A\n",
    "    - Neighbor 3: Distance = 3.0, Class = B \n",
    "    - Neighbor 4: Distance = 1.5, Class = A\n",
    "    - Neighbor 5: Distance = 2.5, Class = B\n",
    "    \n",
    "\n",
    "Step 1: Calculate Weights\n",
    "- Neighbor 1: $\\frac{1}{1.0}$\n",
    "- Neighbor 2: $\\frac{1}{2.0}$\n",
    "- Neighbor 3: $\\frac{1}{3.0}$\n",
    "- Neighbor 4: $\\frac{1}{1.5}$\n",
    "- Neighbor 5: $\\frac{1}{2.5}$\n",
    "\n",
    "Step 2: Sum Weights for Each Class\n",
    "- The weights are:\n",
    "    - Class A: $\\frac{1}{1.0} + \\frac{1}{2.0} + \\frac{1}{1.5} = 1 + 0.5 + 0.67 = 2.17$\n",
    "    - Class B: $\\frac{1}{3.0} + \\frac{1}{2.5} = 0.33 + 0.4 = 0.73$\n",
    "\n",
    "Step 3: Assign the Class    \n",
    "- The test point is classified as Class A.\n",
    "\n",
    "Step 4: Calculate Class Probabilities\n",
    "- Total weight: $2.17+0.73=2.90$\n",
    "\n",
    "The probabilities are:\n",
    "- $P(y = A|x) = \\frac{2.17}{2.90} \\approx 0.75$\n",
    "- $P(y = B|x) = \\frac{0.73}{2.90} \\approx 0.25$\n",
    "\n",
    "Weighted Voting in KNN\n",
    "- Instead of using majority voting, weighted voting gives closer neighbors more influence.\n",
    "\n",
    "Impact of Weighted Voting:\n",
    "- Nearby points have more influence on the prediction.\n",
    "- Helps when class distribution is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ba68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Distances and classes of nearest neighbors\n",
    "distances = [1.0, 2.0, 3.0, 1.5, 2.5]\n",
    "classes = ['A', 'A', 'B', 'A', 'B']\n",
    "\n",
    "# Weighted voting\n",
    "class_weights = {'A': 0, 'B': 0}\n",
    "for dist, cls in zip(distances, classes):\n",
    "    class_weights[cls] += 1 / dist\n",
    "\n",
    "predicted_class = max(class_weights, key=class_weights.get)\n",
    "print(\"Predicted Class (Weighted Voting):\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1608a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_knn(X_train, y_train, X_test, k=5):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='distance')  # Weighted by distance\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "for k in [1, 5, 15]:\n",
    "    acc = weighted_knn(X_train, y_train, X_test, k)\n",
    "    print(f\"K={k}, Weighted KNN Accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1580f",
   "metadata": {},
   "source": [
    "##### Implement weighted voting and calculate class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a909b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Distances and classes of nearest neighbors\n",
    "distances = [1.0, 2.0, 3.0, 1.5, 2.5]\n",
    "classes = ['A', 'A', 'B', 'A', 'B']\n",
    "\n",
    "# Calculate weights (inverse distance weighting)\n",
    "weights = [1 / dist for dist in distances]\n",
    "\n",
    "# Sum weights for each class\n",
    "class_weights = {'A': 0, 'B': 0}\n",
    "for cls, weight in zip(classes, weights):\n",
    "    class_weights[cls] += weight\n",
    "\n",
    "# Total weight\n",
    "total_weight = sum(class_weights.values())\n",
    "\n",
    "# Calculate class probabilities\n",
    "class_probabilities = {cls: weight / total_weight for cls, weight in class_weights.items()}\n",
    "\n",
    "# Print results\n",
    "print(\"Weights per Class:\", class_weights)\n",
    "print(\"Class Probabilities:\", class_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc49fed",
   "metadata": {},
   "source": [
    "##### **Kernel Smoothing**\n",
    "How It Works:\n",
    "- A kernel function (e.g., Gaussian, Epanechnikov) is used to assign weights to neighbors based on their distance.\n",
    "- The kernel function gives higher weights to closer neighbors and lower weights to farther neighbors.\n",
    "- The class with the highest total kernel-weighted score is assigned to the test point.\n",
    "\n",
    "Intuition:\n",
    "- Kernel smoothing provides a more nuanced way of weighting neighbors, especially when the relationship between distance and similarity is non-linear.\n",
    "\n",
    "Example:\n",
    "- Using a Gaussian kernel:\n",
    "\n",
    "$$ \\text{weight} = \\exp (- \\frac{\\text{distance}^2}{2h^2})$$\n",
    "- where\n",
    "    - h: is the bandwidth parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb861ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Gaussian kernel function\n",
    "def gaussian_kernel(distance, h=1.0):\n",
    "    return np.exp(-(distance**2) / (2 * h**2))\n",
    "\n",
    "# Example: Distances and classes of nearest neighbors\n",
    "distances = [1.0, 2.0, 3.0, 1.5, 2.5]\n",
    "classes = ['A', 'A', 'B', 'A', 'B']\n",
    "\n",
    "# Kernel-weighted voting\n",
    "class_weights = {'A': 0, 'B': 0}\n",
    "for dist, cls in zip(distances, classes):\n",
    "    weight = gaussian_kernel(dist, h=1.0)\n",
    "    class_weights[cls] += weight\n",
    "\n",
    "predicted_class = max(class_weights, key=class_weights.get)\n",
    "print(\"Predicted Class (Kernel Smoothing):\", predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543bdc47",
   "metadata": {},
   "source": [
    "##### Use a Gaussian kernel for weighting and calcualting probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19973207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Gaussian kernel function\n",
    "def gaussian_kernel(distance, h=1.0):\n",
    "    return np.exp(-(distance**2) / (2 * h**2))\n",
    "\n",
    "# Calculate weights (Gaussian kernel weighting)\n",
    "weights = [gaussian_kernel(dist, h=1.0) for dist in distances]\n",
    "\n",
    "# Sum weights for each class\n",
    "class_weights = {'A': 0, 'B': 0}\n",
    "for cls, weight in zip(classes, weights):\n",
    "    class_weights[cls] += weight\n",
    "\n",
    "# Total weight\n",
    "total_weight = sum(class_weights.values())\n",
    "\n",
    "# Calculate class probabilities\n",
    "class_probabilities = {cls: weight / total_weight for cls, weight in class_weights.items()}\n",
    "\n",
    "# Print results\n",
    "print(\"Weights per Class (Gaussian Kernel):\", class_weights)\n",
    "print(\"Class Probabilities (Gaussian Kernel):\", class_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3551170",
   "metadata": {},
   "source": [
    "### Combining Functions for Multi-Class Problems\n",
    "In multi-class classification, the combining function must handle more than two classes. Both majority voting and weighted voting can be extended to multi-class scenarios by:\n",
    "- Counting votes or weights for each class.\n",
    "- Assigning the class with the highest total.\n",
    "\n",
    "### Combining Functions for Regression\n",
    "In KNN regression, instead of voting, the combining function typically computes a weighted average of the target values of the nearest neighbors. For example:\n",
    "$$ \\text{Prediction} = \\frac{\\sum^k_{i =1} \\text{weight}_i \\cdot y_i}{\\sum^k_{i =1} \\text{weight}_i}$$\n",
    "- where \n",
    "    - $y_i$: is the target value of the i-th neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a007486e",
   "metadata": {},
   "source": [
    "### Issues with KNN\n",
    "Nearest-neighbor methods, such as K-Nearest Neighbors (KNN), are simple and intuitive but come with several challenges. Below, I’ll describe the issues related to intelligibility, dimensionality, domain knowledge, and computational efficiency, and then dive deeper into two aspects of intelligibility.\n",
    "\n",
    "##### **Intelligibility (Interpretability)**\n",
    "Intelligibility refers to how easily humans can understand and interpret the model's predictions. While KNN is conceptually simple, it has some limitations in terms of intelligibility:\n",
    "\n",
    "**Aspects of Intelligibility**:\n",
    "\n",
    "Lack of Explicit Model:\n",
    "- KNN is an instance-based (lazy) learning algorithm, meaning it doesn’t build an explicit model during training. Instead, it memorizes the training data and makes predictions based on similarity at inference time.\n",
    "- This makes it difficult to explain why a particular prediction was made, especially in high-dimensional spaces or when k is large.\n",
    "\n",
    "Local vs. Global Understanding:\n",
    "- KNN provides a local explanation for predictions (e.g., \"this point is classified as Class A because its nearest neighbors are mostly Class A\").\n",
    "- However, it doesn’t provide a global understanding of the decision boundaries or feature importance, which can be critical for domain experts.\n",
    "\n",
    "Example:\n",
    "- If KNN classifies a patient as having a disease, it’s hard to explain which features (e.g., age, blood pressure) contributed most to the decision, unlike in models like decision trees or linear regression.\n",
    "\n",
    "**Two Aspects of Intelligibility in Detail**\n",
    "\n",
    "Let’s dive deeper into two key aspects of intelligibility:\n",
    "\n",
    "Lack of Explicit Model\n",
    "- Why It’s a Problem:\n",
    "    - Without an explicit model, it’s hard to summarize the relationship between features and the target variable.\n",
    "    - Stakeholders (e.g., doctors, business leaders) often need clear explanations for predictions, which KNN cannot provide.\n",
    "\n",
    "Mitigation:\n",
    "- Use model-agnostic interpretability techniques like LIME or SHAP to explain individual predictions.\n",
    "- Combine KNN with simpler models (e.g., decision trees) for better interpretability.\n",
    "\n",
    "Local vs. Global Understanding\n",
    "- Why It’s a Problem:\n",
    "    - KNN provides local explanations (e.g., \"this point is classified as Class A because its neighbors are Class A\"), but it doesn’t explain global patterns or feature importance.\n",
    "    - This limits its usefulness in scenarios where understanding the overall behavior of the model is critical.\n",
    "\n",
    "Mitigation:\n",
    "- Use dimensionality reduction techniques (e.g., PCA, t-SNE) to visualize decision boundaries in lower dimensions.\n",
    "- Combine KNN with feature importance methods to identify globally important features.\n",
    "\n",
    "##### **Dimensionality (Curse of Dimensionality)**\n",
    "\n",
    "As the number of features (dimensions) increases, the performance of KNN often degrades due to the curse of dimensionality:\n",
    "- In high-dimensional spaces, distances between points become less meaningful because most points are equally distant from each other.\n",
    "- This makes it difficult for KNN to identify meaningful neighbors, leading to poor predictions.\n",
    "\n",
    "Example:\n",
    "- In a dataset with 100 features, two points may appear close in some dimensions but far in others, making it hard to define a meaningful distance metric.\n",
    "\n",
    "##### **Domain Knowledge**\n",
    "\n",
    "KNN doesn’t inherently incorporate domain knowledge:\n",
    "- It treats all features equally unless a custom distance metric is used.\n",
    "- Domain-specific relationships or constraints (e.g., certain features being more important) must be manually encoded into the distance metric or feature weights.\n",
    "\n",
    "Example:\n",
    "- In a medical diagnosis problem, domain knowledge might suggest that blood pressure is more important than age, but KNN won’t prioritize this unless explicitly told to do so.\n",
    "\n",
    "**How the `curse of dimensionality` could be fixed using `domain knowlegde`**\n",
    "\n",
    "The curse of dimensionality is a significant challenge in machine learning, particularly for algorithms like K-Nearest Neighbors (KNN) that rely on distance metrics. As the number of features (dimensions) increases, the data becomes sparse, and distances between points lose meaning, making it difficult for KNN to identify meaningful neighbors. However, domain knowledge can be leveraged to mitigate this issue.\n",
    "\n",
    "What is the Curse of Dimensionality?\n",
    "- In high-dimensional spaces:\n",
    "    - Data points become increasingly sparse, making it hard to define meaningful neighborhoods.\n",
    "    - Distances between points tend to converge, reducing the discriminative power of distance metrics.\n",
    "    - The volume of the space grows exponentially, requiring exponentially more data to maintain the same density.\n",
    "\n",
    "How Domain Knowledge Can Help\n",
    "- Domain knowledge refers to expertise or understanding of the specific field or problem you’re working on. It can be used to:\n",
    "    - Select relevant features and discard irrelevant or redundant ones.\n",
    "    - Weight features based on their importance.\n",
    "    - Engineer new features that capture meaningful relationships in the data.\n",
    "    - Define custom distance metrics that align with domain-specific relationships.\n",
    "- By incorporating domain knowledge, you can reduce the effective dimensionality of the data, making it more manageable and meaningful for KNN.\n",
    "\n",
    "`Feature Selection`\n",
    "- Domain knowledge can help identify which features are most relevant to the problem, allowing you to discard irrelevant or noisy features.\n",
    "\n",
    "Example:\n",
    "- In a medical diagnosis problem, domain experts might know that features like blood pressure, cholesterol levels, and age are critical, while features like patient ID or zip code are irrelevant.\n",
    "    - By selecting only the relevant features, you reduce the dimensionality and improve the performance of KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f798d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example dataset\n",
    "data = {\n",
    "    'age': [25, 45, 35, 50, 23],\n",
    "    'blood_pressure': [120, 140, 130, 150, 110],\n",
    "    'cholesterol': [200, 240, 220, 260, 180],\n",
    "    'patient_id': [1, 2, 3, 4, 5],\n",
    "    'zip_code': [12345, 23456, 34567, 45678, 56789],\n",
    "    'target': [0, 1, 1, 1, 0]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Use domain knowledge to select relevant features\n",
    "X = df[['age', 'blood_pressure', 'cholesterol']]\n",
    "y = df['target']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcbc8fa",
   "metadata": {},
   "source": [
    "`Feature Weighting`\n",
    "- Domain knowledge can be used to assign weights to features based on their importance. This ensures that more important features have a greater influence on the distance calculation.\n",
    "\n",
    "Example:\n",
    "- In a credit scoring problem, domain experts might know that income and credit history are more important than the number of dependents.\n",
    "- You can assign higher weights to income and credit history when computing distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62bcd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define custom weights based on domain knowledge\n",
    "weights = np.array([0.5, 0.3, 0.2])  # age, blood_pressure, cholesterol\n",
    "\n",
    "# Custom distance function\n",
    "def weighted_euclidean_distance(x1, x2, weights):\n",
    "    return np.sqrt(np.sum(weights * (x1 - x2)**2))\n",
    "\n",
    "# Example points\n",
    "x1 = np.array([25, 120, 200])\n",
    "x2 = np.array([45, 140, 240])\n",
    "\n",
    "# Calculate weighted distance\n",
    "distance = weighted_euclidean_distance(x1, x2, weights)\n",
    "print(\"Weighted Euclidean Distance:\", distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a8c902",
   "metadata": {},
   "source": [
    "`Feature Engineering`\n",
    "- Domain knowledge can guide the creation of new features that capture meaningful relationships in the data, reducing the need for high-dimensional raw features.\n",
    "\n",
    "Example:\n",
    "- In a retail problem, instead of using raw transaction data, domain experts might create features like \"average purchase value\" or \"frequency of purchases.\"\n",
    "- These engineered features are often more informative and reduce dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf25125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Feature engineering\n",
    "df['average_purchase_value'] = df['total_spent'] / df['number_of_purchases']\n",
    "df['purchase_frequency'] = df['number_of_purchases'] / df['days_since_first_purchase']\n",
    "\n",
    "# Use engineered features\n",
    "X = df[['average_purchase_value', 'purchase_frequency']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41169c",
   "metadata": {},
   "source": [
    "`Custom Distance Metrics`\n",
    "- Domain knowledge can be used to define custom distance metrics that align with domain-specific relationships.\n",
    "\n",
    "Example:\n",
    "- In a text classification problem, domain experts might know that certain words are more important than others. A custom distance metric can be designed to reflect this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03259fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Custom distance metric\n",
    "def custom_distance(x1, x2):\n",
    "    # Domain-specific logic\n",
    "    return np.sum(np.abs(x1 - x2))  # Example: Manhattan distance\n",
    "\n",
    "# Compute pairwise distances\n",
    "distances = pairwise_distances(X_train, X_test, metric=custom_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0852929",
   "metadata": {},
   "source": [
    "Summary of Fixes Using Domain Knowledge\n",
    "\n",
    "|Technique|\tDescription|\tExample|\n",
    "|---------|------------|-----------|\n",
    "|Feature Selection|\tSelect only relevant features based on domain knowledge.|\tUse age, blood pressure, and cholesterol for medical diagnosis.|\n",
    "|Feature Weighting|\tAssign higher weights to more important features.|\tWeight income more heavily in credit scoring.|\n",
    "|Feature Engineering|\tCreate new features that capture domain-specific relationships.|\tCompute average purchase value in retail.|\n",
    "|Custom Distance Metrics|\tDefine distance metrics that align with domain-specific logic.|\tUse a custom text similarity metric for text classification.|\n",
    "\n",
    "##### **Computational Efficiency**\n",
    "\n",
    "KNN can be computationally expensive, especially for large datasets:\n",
    "- Training Time: KNN is a lazy learner, so it doesn’t require training. However, it stores the entire training dataset, which can be memory-intensive.\n",
    "- Inference Time: For each test point, KNN must compute distances to all training points, which scales as O(n⋅d), where n is the number of training samples and d is the number of features.\n",
    "- This makes KNN impractical for large datasets or real-time applications.\n",
    "\n",
    "Example:\n",
    "- For a dataset with 1 million samples and 100 features, KNN would need to compute 100 million distances for each test point.\n",
    "\n",
    "|Issue|\tDescription|\tMitigation|\n",
    "|-----|------------|--------------|\n",
    "|Intelligibility|\tHard to explain predictions; lacks global understanding.|\tUse LIME/SHAP, combine with simpler models, or reduce dimensionality.|\n",
    "|Dimensionality|\tCurse of dimensionality reduces meaningfulness of distances.|\tUse dimensionality reduction or feature selection.|\n",
    "|Domain Knowledge|\tDoesn’t incorporate domain-specific knowledge.|\tUse custom distance metrics or feature weighting.|\n",
    "|Computational Efficiency|\tComputationally expensive for large datasets.|\tUse approximate nearest neighbor algorithms (e.g., KD-Trees, Ball Trees).|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe25b8cd",
   "metadata": {},
   "source": [
    "# Clustering Models (e.g., K-Means)\n",
    "What It Means: \n",
    "- Clustering groups similar data points together without predefined labels, often used for segmenting customers or finding patterns.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each cluster represents a natural grouping in the data, with data points in the same cluster sharing similar characteristics.\n",
    "\n",
    "Performance Measures:\n",
    "- Silhouette Score: Measures how well each point fits within its cluster; values closer to 1 indicate better-defined clusters.\n",
    "- Within-Cluster Sum of Squares (WCSS): Measures the compactness of clusters; lower values are better.\n",
    "\n",
    "Lay Explanation: \n",
    "- Clustering is like sorting items into bins based on similarity, helping us identify groups in our data.\n",
    "\n",
    "Use Case: \n",
    "- To group similar observations without predefined labels.\n",
    "\n",
    "Model Types: \n",
    "- K-Means, \n",
    "- Hierarchical Clustering, \n",
    "- DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3cfc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "clusters = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2abff8",
   "metadata": {},
   "source": [
    "###  Primary Idea Behind Clustering\n",
    "Clustering is an unsupervised machine learning technique used to group similar data points together based on some notion of similarity (like distance). The main idea is:\n",
    "- “Group similar items together such that items in the same group (cluster) are more similar to each other than to those in other groups.”\n",
    "\n",
    "This is useful when you don’t have labeled data but want to find patterns or natural groupings in your dataset.\n",
    "\n",
    "### Primary Idea Behind Hierarchical Clustering\n",
    "Hierarchical clustering is a type of clustering that builds a hierarchy or tree (called a dendrogram) of clusters. There are two main approaches:\n",
    "\n",
    "##### **Agglomerative (bottom-up)**:\n",
    "1. Start with each data point as its own cluster.\n",
    "2. Gradually merge the closest clusters until all data points are in a single cluster (or until a stopping criterion is met).\n",
    "\n",
    "##### **Divisive (top-down)**:\n",
    "1. Start with all data points in one cluster.\n",
    "2. Recursively split the cluster into smaller clusters.\n",
    "\n",
    "The core idea: Build a nested tree of clusters where the level of similarity is reflected in how close branches are.\n",
    "\n",
    "It’s especially helpful when:\n",
    "- You want a visual representation of data grouping (dendrogram).\n",
    "- You don’t want to pre-specify the number of clusters (unlike k-means).\n",
    "\n",
    "Intuition\n",
    "- **Clustering** (in general) tries to find structure in data.\n",
    "- **Hierarchical clustering** gives you multi-level grouping—you can decide how many clusters you want after seeing the hierarchy.\n",
    "\n",
    "##### Clustering (K-Means)\n",
    "\n",
    "If there are 3 distinct groups, K-means tries to:\n",
    "- Randomly pick 3 centroids.\n",
    "- Assign each point to the nearest centroid.\n",
    "- Recalculate centroids based on assigned points.\n",
    "- Repeat until things stabilize.\n",
    "\n",
    "You end up with clearly separated blobs or clusters.\n",
    "\n",
    "##### Hierarchical Clustering\n",
    "- Each point = own cluster\n",
    "- Then it starts merging closest pairs of clusters step-by-step, creating a tree\n",
    "     - Level 1: A | B | C | D\n",
    "     - Level 2: (A+B) | (C+D)\n",
    "     - Level 3: (A+B+C+D)\n",
    "- Result is a dendrogram (tree) like:\n",
    "- cut the dendrogram at any height to choose how many clusters you want.\n",
    "\n",
    "Hierarchical clustering is a clustering technique that builds a multilevel hierarchy (or tree) of clusters by either merging smaller clusters or dividing a larger cluster into smaller ones. The result is a dendrogram — a tree-like diagram that shows how clusters are related.\n",
    "\n",
    "📌 Primary Features of Hierarchical Clustering\n",
    "\n",
    "1. Hierarchical Tree Structure (Dendrogram)\n",
    "     - The output is a tree (dendrogram) showing how observations or clusters are merged or split at each level.\n",
    "     - You can cut the tree at any level to get the desired number of clusters.\n",
    "\n",
    "2. Two Main Types\n",
    "\n",
    "🔹 Agglomerative (Bottom-Up)\n",
    "- Starts with each data point as its own cluster\n",
    "- Repeatedly merges the two closest clusters\n",
    "- Most common approach\n",
    "\n",
    "🔹 Divisive (Top-Down)\n",
    "- Starts with all data points in one cluster\n",
    "- Recursively splits clusters into smaller ones\n",
    "\n",
    "3. Distance Metrics\n",
    "\n",
    "Determines how similar or dissimilar observations or clusters are.\n",
    "\n",
    "Common choices:\n",
    "- Euclidean (default)\n",
    "- Manhattan\n",
    "- Cosine\n",
    "- Correlation\n",
    "\n",
    "4. Linkage Criteria\n",
    "\n",
    "Controls how the distance between clusters is computed:\n",
    "- Single Linkage – minimum distance between any two points in different clusters\n",
    "- Complete Linkage – maximum distance between any two points\n",
    "- Average Linkage – average distance between all pairs of points\n",
    "- Ward’s Method – minimizes the increase in total within-cluster variance after merging\n",
    "\n",
    "5. No Need to Pre-specify K\n",
    "     - Unlike K-Means, you don't need to specify the number of clusters upfront.\n",
    "     - You can choose the number of clusters by cutting the dendrogram at a given height.\n",
    "\n",
    "6. Interpretability\n",
    "     - The dendrogram gives a clear visual understanding of the nested structure in data.\n",
    "     - Useful when you want to explore different levels of grouping.\n",
    "\n",
    "When to Use Hierarchical Clustering\n",
    "\n",
    "|Use Case\t|Why It’s Suitable|\n",
    "|---------|-----------------|\n",
    "|Small to medium datasets\t|Computation increases quickly (O(n²))|\n",
    "|You want interpretability\t|Dendrogram gives visual insight|\n",
    "|You don't know how many clusters\t|Can choose K by dendrogram cut|\n",
    "|You expect nested/grouped structure\t|Hierarchy reveals grouping levels|\n",
    "\n",
    "Summary\n",
    "\n",
    "|Feature\t|Description|\n",
    "|---------|-----------|\n",
    "|Clustering type\t|Hierarchical (tree-based), agglomerative or divisive|\n",
    "|Output\t|Dendrogram (tree diagram)|\n",
    "|Distance metric\t|Euclidean (default), others supported|\n",
    "|Linkage methods\t|Single, complete, average, Ward’s|\n",
    "|K needed in advance?\t|❌ No – select K by dendrogram cut|\n",
    "|Interpretability\t|✅ High – shows nested relationships|\n",
    "|Scalability\t|❌ Poor for large datasets (quadratic complexity)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d79f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Create sample data\n",
    "X, _ = make_blobs(n_samples=200, centers=3, cluster_std=1.0, random_state=42)\n",
    "\n",
    "# --- K-Means Clustering ---\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# --- Hierarchical Clustering ---\n",
    "hier = AgglomerativeClustering(n_clusters=3)\n",
    "hier_labels = hier.fit_predict(X)\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Original data\n",
    "axs[0].scatter(X[:, 0], X[:, 1], c='gray')\n",
    "axs[0].set_title(\"Original Data\")\n",
    "\n",
    "# K-means result\n",
    "axs[1].scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis')\n",
    "axs[1].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "               s=200, c='red', marker='X', label='Centroids')\n",
    "axs[1].set_title(\"K-Means Clustering\")\n",
    "axs[1].legend()\n",
    "\n",
    "# Hierarchical Clustering result\n",
    "axs[2].scatter(X[:, 0], X[:, 1], c=hier_labels, cmap='plasma')\n",
    "axs[2].set_title(\"Hierarchical Clustering\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "## Dendrogram for Hierarchical Clustering\n",
    "\n",
    "# Create linkage matrix for dendrogram\n",
    "linked = linkage(X, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(12, 5))\n",
    "dendrogram(linked, truncate_mode='lastp', p=20, leaf_rotation=45., leaf_font_size=12.)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.xlabel(\"Cluster Size\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "X, _ = make_blobs(n_samples=50, centers=3, random_state=42)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "Z = linkage(X, method='ward')  # Try 'single', 'complete', etc.\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(10, 5))\n",
    "dendrogram(Z)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c0d24",
   "metadata": {},
   "source": [
    "### Feature scaling is crucial in KMeans clustering\n",
    "Z-score (Standardization) and Min-Max Scaling (Normalization) helps you make the right decision for your data.\n",
    "\n",
    "#### Why Feature Scaling is Important in KMeans?\n",
    "KMeans is a distance-based algorithm (usually Euclidean distance):\n",
    "$$ \\text{Distance (x,y)} = \\sqrt{(x_1 - y_1)^2 + (x_2 - y^2)^2 + ...}$$\n",
    "\n",
    "So, if features are on different scales, the feature with the largest range dominates the distance metric\n",
    "\n",
    "|Feature\t|Range  |\n",
    "|-----------|-------|\n",
    "|Age\t|18 to 70|\n",
    "|Income ($)\t|10k to 150k|\n",
    "|Spending Score\t|1 to 100 |\n",
    "\n",
    "Without scaling, income will dominate distance calculations.\n",
    "\n",
    "##### **Z-Score Standardization (StandardScaler)**\n",
    "$$ z = \\frac{(x - \\mu)}{\\sigma}$$\n",
    "\n",
    "- Transforms features to have mean = 0 and std = 1\n",
    "- Keeps distribution shape but puts features on comparable scale\n",
    "\n",
    "✅ When to use:\n",
    "- When features are normally distributed\n",
    "- When outliers are not extreme\n",
    "\n",
    "##### **Min-Max Scaling (MinMaxScaler)**\n",
    "$$ x' = \\frac{x - \\text{min}}{\\text{max} - \\text{min}}$$\n",
    "\n",
    "- Rescales values between 0 and 1\n",
    "- Sensitive to outliers\n",
    "\n",
    "✅ When to use:\n",
    "- When you want to preserve distribution and scale into [0, 1]\n",
    "- Often used in neural nets and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8103c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "df = pd.DataFrame({\n",
    "    'Age': [19, 35, 26, 45, 23],\n",
    "    'Income': [15, 75, 45, 100, 32],\n",
    "    'Spending': [39, 40, 77, 20, 6]\n",
    "})\n",
    "\n",
    "# Z-score standardization\n",
    "z_scaled = StandardScaler().fit_transform(df)\n",
    "df_z = pd.DataFrame(z_scaled, columns=df.columns)\n",
    "\n",
    "# Min-max normalization\n",
    "minmax_scaled = MinMaxScaler().fit_transform(df)\n",
    "df_minmax = pd.DataFrame(minmax_scaled, columns=df.columns)\n",
    "\n",
    "print(\"Z-Score Standardization:\\n\", df_z)\n",
    "print(\"\\nMin-Max Normalization:\\n\", df_minmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47346c46",
   "metadata": {},
   "source": [
    "🧠 Interpretation\n",
    "|Age\t|Income\t|Spending|\n",
    "|-------|-------|--------|\n",
    "|Original\t|19\t|15\t|39  |\n",
    "|Z-Score\t|-1.26\t|-1.12\t|-0.02  |\n",
    "|Min-Max\t|0.00\t|0.00\t|0.45   |\n",
    "\n",
    "- Z-score scales relative to mean and variance (e.g., how many std devs a value is from the mean).\n",
    "- Min-max scales based on range.\n",
    "\n",
    "📌 In KMeans: Which to Use?\n",
    "\n",
    "|Situation\t|Recommended Scaling|\n",
    "|-----------|-------------------|\n",
    "|Data has different units/scales\t|✅ Required|\n",
    "|Data is normally distributed\t|Z-score  |\n",
    "|Data has fixed bounds (e.g., 0–1)\t|Min-Max  |\n",
    "|Outliers are present\t|Z-score is safer   |\n",
    "\n",
    "✅ Summary\n",
    "\n",
    "|Scaling Method\t|Preserves Distribution\t|Bounded Output\t|Sensitive to Outliers\t|Centers at 0|\n",
    "|---------------|-----------------------|---------------|-----------------------|------------|\n",
    "|Z-score\t|✅\t|❌\t|❌\t|✅|\n",
    "|Min-Max\t|✅\t|✅\t|✅\t|❌|\n",
    "\n",
    "###  What Is Euclidean Distance?\n",
    "Euclidean distance is the straight-line (\"as-the-crow-flies\") distance between two points in Euclidean space.\n",
    "\n",
    "For two points:\n",
    "- x = $(x_1, x_2,...,x_n)$ \n",
    "- y = $(y_1, y_2,...,y_n)$\n",
    "\n",
    "This is derived from the Pythagorean theorem where the  Euclidean distance d(x,y) is:\n",
    "$$ \\text{Distance (x,y)} = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + ... + (x_n - y_n)^2}$$\n",
    "\n",
    "In KMeans Clustering\n",
    "- Euclidean distance is used to measure similarity between:\n",
    "    - A data point and a cluster centroid\n",
    "    - Centroids are means of all points in a cluster\n",
    "    - KMeans iteratively assigns points to the nearest centroid, based on Euclidean distance.\n",
    "\n",
    "🚨 Why Scaling Matters?\n",
    "- Without scaling, features with large numerical ranges dominate the distance calculation.\n",
    "\n",
    "🔍 Example:\n",
    "\n",
    "|Feature\t|Age (years)\t|Income ($)|\n",
    "|-----------|---------------|----------|\n",
    "|Person 1\t|25\t|30,000  |\n",
    "|Person 2\t|30\t|70,000  |\n",
    "\n",
    "The age difference is 5, but the income difference is 40,000, which would overwhelm the impact of age unless scaled.\n",
    "\n",
    "🔁 In Higher Dimensions (n-D)\n",
    "- Euclidean distance generalizes to any number of features:\n",
    "- Still calculated with square root of the sum of squared differences\n",
    "- In 3D: $ \\text{Distance (x,y)} = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + (x_3 - y_3)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5a05ae",
   "metadata": {},
   "source": [
    "### **What Is K-Means Clustering?**\n",
    "\n",
    "K-Means is an unsupervised learning algorithm used to group data into K clusters, where each data point belongs to the cluster with the nearest centroid (mean of points).\n",
    "\n",
    "### Primary Features of K-Means\n",
    "|Feature\t| Description |\n",
    "|-----------|-------------|\n",
    "|💠 Centroid-Based\t|Clusters are defined around central points (centroids)|\n",
    "|🔁 Iterative Process\t|Repeats until convergence (minimal movement of centroids)|\n",
    "|📏 Distance-Based\t|Commonly uses Euclidean distance to assign points|\n",
    "|🔢 K Must Be Predefined\t|You must specify the number of clusters (K) in advance|\n",
    "|⚡ Efficient & Scalable\t|Works well for large datasets with continuous features|\n",
    "\n",
    "Intuition\n",
    "- Imagine you’re grouping customers into K groups based on spending behavior — you want to find K group centers, then assign each customer to the closest center.\n",
    "\n",
    "### General Approach to K-Means Clustering (Centroid-Based)\n",
    "\n",
    "🎯 Goal:\n",
    "- Partition the data into K clusters, where each cluster is defined by its centroid (mean). \n",
    "- The goal is to minimize the total within-cluster variance, also known as the sum of squared distances between data points and their assigned centroids.\n",
    "\n",
    "✅ Step-by-Step Approach\n",
    "\n",
    "1. Choose the number of clusters (K)\n",
    "    - You decide how many groups you want the data to be divided into.\n",
    "    - This is often based on prior knowledge or using methods like the elbow method.\n",
    "    - Decide how many clusters you want (K).\n",
    "        - 🔹 Example: K = 3 (for three customer segments)\n",
    "\n",
    "2. Initialize K centroids randomly\n",
    "    - Select K data points randomly as the initial centroids.\n",
    "    - Randomly pick K initial centroids from the dataset (or use smart methods like KMeans++ to improve this).\n",
    "    - Each centroid represents the center of a cluster.\n",
    "\n",
    "3. Assign each point to the nearest centroid\n",
    "    - For each data point, compute the Euclidean distance to each centroid.\n",
    "    - Assign the point to the cluster of the closest centroid.\n",
    "\n",
    "4. Recalculate centroids\n",
    "    - For each cluster, calculate the mean of all the points assigned to it.\n",
    "    - Update the centroid position to this mean.\n",
    "\n",
    "5. Repeat steps 3 and 4 until convergence\n",
    "    - Continue assigning points and updating centroids until:\n",
    "        - No change in assignments, or\n",
    "        - Centroids don’t move significantly, or\n",
    "        - A maximum number of iterations is reached.\n",
    "\n",
    "Intuition Behind It\n",
    "- Each cluster is trying to pull similar points toward itself.\n",
    "- Centroids move to the center of mass of their assigned points.\n",
    "- Eventually, a stable configuration (local minimum) is reached.\n",
    "\n",
    "Optional Enhancements\n",
    "|Enhancement\t|Benefit |\n",
    "|---------------|--------|\n",
    "|KMeans++ Init\t|Smarter starting centroids = faster, better convergence|\n",
    "|Scaling Features\t|Prevents domination by large-scale features|\n",
    "|Elbow Method\t|Helps choose the best K|\n",
    "|Silhouette Score\t|Measures how well points fit within clusters|\n",
    "|Cluster Explainability (SHAP)\t|Interprets clusters using supervised models|\n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "Pros:\n",
    "- Simple and fast for large datasets\n",
    "- Easy to interpret\n",
    "- Works well when clusters are spherical and equally sized\n",
    "\n",
    "Cons:\n",
    "- Requires pre-specifying K\n",
    "- Sensitive to initial centroids\n",
    "- Can get stuck in local minima\n",
    "- Not ideal for non-convex or differently sized/density clusters\n",
    "\n",
    "Python example showing how centroids move over iterations in K-Means clustering using matplotlib animation. This will visually demonstrate how K-Means converges.\n",
    "\n",
    "What This Code Does:\n",
    "- Uses make_blobs to create 2D data.\n",
    "- Applies K-Means step-by-step manually.\n",
    "- Animates how centroids move and how points switch clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc26521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Step 1: Generate synthetic data\n",
    "X, _ = make_blobs(n_samples=200, centers=3, cluster_std=1.0, random_state=42)\n",
    "\n",
    "# Step 2: Initialize parameters\n",
    "k = 3\n",
    "np.random.seed(42)\n",
    "initial_centroids = X[np.random.choice(X.shape[0], k, replace=False)]\n",
    "centroids = initial_centroids.copy()\n",
    "colors = ['red', 'green', 'blue']\n",
    "history_centroids = [centroids.copy()]\n",
    "history_labels = []\n",
    "\n",
    "# Helper function to assign clusters\n",
    "def assign_clusters(X, centroids):\n",
    "    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "    return np.argmin(distances, axis=1)\n",
    "\n",
    "# K-means loop\n",
    "for _ in range(10):  # Run for fixed 10 iterations\n",
    "    labels = assign_clusters(X, centroids)\n",
    "    history_labels.append(labels.copy())\n",
    "    \n",
    "    new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n",
    "    if np.allclose(centroids, new_centroids):\n",
    "        break  # Convergence\n",
    "    centroids = new_centroids\n",
    "    history_centroids.append(centroids.copy())\n",
    "\n",
    "# Step 3: Animate the centroid movement\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "def update(frame):\n",
    "    ax.clear()\n",
    "    centroids = history_centroids[frame]\n",
    "    labels = history_labels[frame if frame < len(history_labels) else -1]\n",
    "    \n",
    "    for i in range(k):\n",
    "        points = X[labels == i]\n",
    "        ax.scatter(points[:, 0], points[:, 1], s=30, color=colors[i], label=f'Cluster {i+1}')\n",
    "        ax.scatter(*centroids[i], color='black', s=200, marker='X', edgecolor='white', linewidth=2)\n",
    "    \n",
    "    ax.set_title(f'Iteration {frame + 1}')\n",
    "    ax.legend()\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=len(history_centroids), interval=1000, repeat=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c23409e",
   "metadata": {},
   "source": [
    "### Step-by-Step Walkthrough of the K-Means Logic (Manual Iterations)\n",
    "\n",
    "1. Generate Synthetic Data\n",
    "    - We create a dataset X with 200 points and 3 natural clusters.\n",
    "    - make_blobs is perfect for this kind of demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef728ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = make_blobs(n_samples=200, centers=3, cluster_std=1.0, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087799c",
   "metadata": {},
   "source": [
    "2. Initialize Parameters and Random Centroids\\\n",
    "- k = 3 means we want to find 3 clusters.\n",
    "- We randomly choose 3 data points from X to act as the starting centroids.\n",
    "- These are the \"black X\" markers in the animation’s first frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f41b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "initial_centroids = X[np.random.choice(X.shape[0], k, replace=False)]\n",
    "centroids = initial_centroids.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e7ef6",
   "metadata": {},
   "source": [
    "3. Begin K-Means Iterations\n",
    "- We perform these steps manually, up to 10 times (can stop earlier if convergence happens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fcb3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    labels = assign_clusters(X, centroids)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20dce21",
   "metadata": {},
   "source": [
    "3. Assign Each Point to the Nearest Centroid\n",
    "    - For every point in X, compute the distance to each centroid.\n",
    "    - Assign each point to the nearest centroid.\n",
    "    - This is when colors of the points change in the animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112bb731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters(X, centroids):\n",
    "    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "    return np.argmin(distances, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca73a26b",
   "metadata": {},
   "source": [
    "3. Recompute the Centroids\n",
    "    - For each cluster:\n",
    "        - Take the mean of all points currently assigned to it.\n",
    "        - This new mean becomes the new centroid (center of gravity).\n",
    "    - This is when the X markers jump in the animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f35bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a98296",
   "metadata": {},
   "source": [
    "3. Check for Convergence\n",
    "    - If the centroids don't move (or barely move), the algorithm stops early.\n",
    "    - In the animation, this is the last frame, where points and centroids stabilize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d45037",
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.allclose(centroids, new_centroids):\n",
    "    break  # Converged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc722f0",
   "metadata": {},
   "source": [
    "4. Animation Setup\n",
    "    - Each frame in the animation does the following:\n",
    "        - Points are colored based on cluster assignment.\n",
    "        - Centroids are marked with a large black X.\n",
    "        - You can literally watch the centroids move as the model reassigns and recalculates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb516e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(frame):\n",
    "    ...\n",
    "    # Plot points by cluster label\n",
    "    ax.scatter(points[:, 0], points[:, 1], color=colors[i])\n",
    "    \n",
    "    # Plot centroids as 'X' markers\n",
    "    ax.scatter(*centroids[i], color='black', marker='X', s=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8957b56",
   "metadata": {},
   "source": [
    "##### Manual implementation of the K-Means algorithm in Python, including:\n",
    "🧹 Data setup\n",
    "\n",
    "📏 Manual distance calculation\n",
    "\n",
    "🎯 Assigning clusters\n",
    "\n",
    "♻️ Updating centroids\n",
    "\n",
    "🔁 Iterating\n",
    "\n",
    "📊 Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94715f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create some simple 2D data\n",
    "X = np.array([\n",
    "    [1, 2], [1, 4], [1, 0],\n",
    "    [4, 2], [4, 4], [4, 0]\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=100)\n",
    "plt.title(\"Original Data Points\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Initialize K and Centroids\n",
    "K = 2\n",
    "# Randomly initialize two centroids (here we pick two points directly)\n",
    "centroids = np.array([X[0], X[3]])  # e.g., [1,2] and [4,2]\n",
    "\n",
    "def plot_clusters(X, centroids, labels=None, title=\"\"):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    if labels is None:\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=100)\n",
    "    else:\n",
    "        sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels, palette='Set2', s=100, legend=False)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=200, marker='X', label='Centroids')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_clusters(X, centroids, title=\"Initial Centroids\")\n",
    "\n",
    "# Step 3: Assign Points to Nearest Centroid\n",
    "def assign_clusters(X, centroids):\n",
    "    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "    return np.argmin(distances, axis=1)\n",
    "\n",
    "labels = assign_clusters(X, centroids)\n",
    "plot_clusters(X, centroids, labels, title=\"Assigned Clusters (Iteration 1)\")\n",
    "\n",
    "\n",
    "# Step 4: Update Centroids\n",
    "def update_centroids(X, labels, K):\n",
    "    new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(K)])\n",
    "    return new_centroids\n",
    "\n",
    "new_centroids = update_centroids(X, labels, K)\n",
    "print(\"New Centroids:\\n\", new_centroids)\n",
    "\n",
    "\n",
    "# Step 5: Iterate Until Convergence\n",
    "tolerance = 1e-4\n",
    "max_iters = 10\n",
    "\n",
    "for i in range(max_iters):\n",
    "    labels = assign_clusters(X, centroids)\n",
    "    new_centroids = update_centroids(X, labels, K)\n",
    "    \n",
    "    plot_clusters(X, new_centroids, labels, title=f\"Iteration {i+1}\")\n",
    "    \n",
    "    # Check for convergence\n",
    "    if np.allclose(centroids, new_centroids, atol=tolerance):\n",
    "        print(f\"Converged in {i+1} iterations\")\n",
    "        break\n",
    "    centroids = new_centroids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757748e4",
   "metadata": {},
   "source": [
    "### How to calculate and interpret the centroid of a cluster in KMeans,\n",
    "\n",
    "**What Is a Centroid in KMeans?**\n",
    "\n",
    "In KMeans clustering, a centroid is the center (or average) of all points assigned to a cluster.\n",
    "$$ \\text{Centroid} = \\frac{1}{n} \\sum^n_{i = 1} x_i $$\n",
    "\n",
    "Where:\n",
    "- n = number of points in the cluster\n",
    "- $𝑥_𝑖$ = each point (a vector of features)\n",
    "\n",
    "Intuition:\n",
    "Think of the centroid as the \"prototype customer\" or \"average profile\" of the people in a cluster.\n",
    "\n",
    "For example, if you clustered customers based on Age and Annual Income, the centroid might be:\n",
    "\n",
    "|Feature\t|Cluster 1 Centroid|\n",
    "|-----------|------------------|\n",
    "|Age\t|28.7 years  |\n",
    "|Income ($)\t |45,500   |\n",
    "\n",
    "This represents the average customer profile in Cluster 1.\n",
    "\n",
    "🧪 Example Calculation (Step-by-Step)\n",
    "Suppose we have the following cluster with 3 customers:\n",
    "|Customer \t|Age\t |Income (in $1k)|\n",
    "|-----------|--------|---------------|\n",
    "|A\t|25\t|40|\n",
    "|B\t|30\t|50|\n",
    "|C\t|35\t|45|\n",
    "\n",
    "➤ Step 1: Add Up Each Feature\n",
    "- Age: $25+30+35=90$\n",
    "- Income: $40+50+45=135$\n",
    "\n",
    "➤ Step 2: Divide by Number of Points (3)\n",
    "- Centroid Age: $ \\frac{90}{3} = 30$\n",
    "- Centroid Income: $\\frac{135}{3} = 45$\n",
    "\n",
    "Cluster Centroid:\n",
    "$$\\text{Centroid}=(30,45)$$\n",
    "\n",
    "Interpretation in Clustering\n",
    "- Centroids are not necessarily actual data points.\n",
    "- They summarize the location of the cluster in feature space.\n",
    "- KMeans uses centroids to:\n",
    "    - Reassign points (based on nearest centroid)\n",
    "    - Update centroid after each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ad59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Cluster data\n",
    "data = np.array([[25, 40],\n",
    "                 [30, 50],\n",
    "                 [35, 45]])\n",
    "\n",
    "# Compute centroid\n",
    "centroid = np.mean(data, axis=0)\n",
    "print(\"Centroid:\", centroid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89c73a5",
   "metadata": {},
   "source": [
    "### Interpreting the role of Supervised Learning and intepreting Cluster analysis results\n",
    "Touches on the intersection between unsupervised and supervised learning,\n",
    "\n",
    "Goal of Cluster Analysis\n",
    "- Cluster analysis is an unsupervised learning method:\n",
    "    - It finds natural groupings in data without using labels.\n",
    "    - It’s exploratory and used for understanding structure, segmenting customers, etc.\n",
    "\n",
    "But here's the catch:\n",
    "➡️ Clusters by themselves don't tell us why they exist or what they mean.\n",
    "\n",
    "### Role of Supervised Learning in Interpreting Clusters\n",
    "Once clustering is done, supervised learning helps in interpreting and validating those clusters by:\n",
    "\n",
    "1. Predicting Cluster Membership\n",
    "    - You treat the cluster labels (from K-Means, Hierarchical, etc.) as pseudo-labels (targets) and train a classifier (e.g., Decision Tree, Random Forest, XGBoost) to:\n",
    "        - Understand what features explain cluster membership.\n",
    "        - Determine rules that define each cluster.\n",
    "\n",
    "This turns the problem into supervised classification:\n",
    "- Input: Features (X)  \n",
    "- Target: Cluster label (from unsupervised model)\n",
    "\n",
    "2. Feature Importance\n",
    "    - Supervised models can quantify feature importance:\n",
    "        - Helps interpret which features distinguish clusters.\n",
    "        - Especially useful when clusters are not easily visualized (high-dimensional data).\n",
    "            - Example: A Random Forest might reveal that:\n",
    "            - Cluster A = High income + urban\n",
    "            - Cluster B = Low income + rural\n",
    "\n",
    "3. Naming and Profiling Clusters\n",
    "    - Once features are understood, supervised learning helps:\n",
    "        - Name clusters (“Budget Shoppers”, “Luxury Travelers”, etc.)\n",
    "        - Design strategies: marketing, policy targeting, etc.\n",
    "\n",
    "🧠 Real-World Analogy\n",
    "- Clustering: You group customers into 4 segments based on behavior\n",
    "- Supervised Learning: You now want to know what makes each segment different → use decision trees or XGBoost to model this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbef38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Generate or load data\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=1.2, random_state=42)\n",
    "\n",
    "# Step 2: Cluster using K-Means\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Step 3: Train a supervised model using cluster labels\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X, cluster_labels)\n",
    "\n",
    "# Step 4: Feature importances\n",
    "importances = clf.feature_importances_\n",
    "print(\"Feature importances to explain clusters:\", importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc984362",
   "metadata": {},
   "source": [
    "When is this useful?\n",
    "- Customer segmentation → Understand traits of each group\n",
    "- Fraud detection → Explain why certain transactions fall in an \"anomalous\" cluster\n",
    "- Healthcare → Group patients, then explain what factors drive group differences\n",
    "\n",
    "Bonus Tip:\n",
    "- You can also combine clustering + supervised learning in semi-supervised learning setups or in active learning pipelines — great when you have partial labels.\n",
    "\n",
    "##### full example using customer demographics, and show how to:\n",
    "- Cluster the customers (unsupervised)\n",
    "- Train a supervised model to predict cluster membership\n",
    "- Use SHAP to interpret how features explain each cluster\n",
    "\n",
    "We’ll use a real-world dataset from Mall Customers, which includes:\n",
    "- Age\n",
    "- Annual Income\n",
    "- Spending Score\n",
    "\n",
    "Step 1: Load and Inspect the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99112710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Mall_Customers.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683b894e",
   "metadata": {},
   "source": [
    "We’ll use numeric features: Age, Annual Income, Spending Score.\n",
    "\n",
    "Step 2: Cluster Customers Using KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80336d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Select features\n",
    "X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# KMeans clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to original DataFrame\n",
    "df['Cluster'] = cluster_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248c831",
   "metadata": {},
   "source": [
    "Step 3: Train a Supervised Model to Predict Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea4030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use original features (not scaled) for interpretability\n",
    "X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\n",
    "y = df['Cluster']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f496374a",
   "metadata": {},
   "source": [
    " Step 4: Interpret with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Visualize summary for all classes\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "\n",
    "# You can also drill down to a single cluster:\n",
    "# Explain predictions for one cluster\n",
    "cluster_index = 1\n",
    "shap.summary_plot(shap_values[cluster_index], X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd08059",
   "metadata": {},
   "source": [
    "What Does SHAP Tell You?\n",
    "- Which features (age, income, score) are driving assignment to each cluster\n",
    "- Helps name clusters:\n",
    "    - Cluster 0 = High Income + High Spend\n",
    "    - Cluster 2 = Young + Low Spend\n",
    "- Enables transparent business actions (target marketing, loyalty programs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc628df9",
   "metadata": {},
   "source": [
    "## Cluster Validation metrics\n",
    "- WCSS Inertia\n",
    "- Silhouette Score\n",
    "- Davids-Bouldin Index (DBI)\n",
    "- Calinski–Harabasz Index (CHI)\n",
    "\n",
    "### Tracking WCSS (Within-Cluster Sum of Squares) \n",
    "- is a great way to monitor how the clustering is improving over iterations.\n",
    "\n",
    "### **What Is Inertia in K-Means?**\n",
    "Inertia is the Within-Cluster Sum of Squares (WCSS) — a key internal validation metric that measures how tightly grouped the data points in each cluster are.\n",
    "\n",
    "Definition:\n",
    "Inertia is the sum of squared distances between each data point and the centroid of its assigned cluster.\n",
    "\n",
    "What is WCSS?\n",
    "- WCSS is the sum of squared distances between each point and its assigned cluster centroid. \n",
    "\n",
    "Formula\n",
    "\n",
    "For each cluster:\n",
    "$$ \\text{Inertia} = \\sum^n_{i=1} || x_i - \\mu_{c(i)}||^2 $$\n",
    "Where:\n",
    "- $x_i$ : a data point\n",
    "- $\\mu_{c(i)}$ : the centroid of the cluster assigned to $𝑥_𝑖$\n",
    "- ∥⋅∥: Euclidean norm (distance)\n",
    "- Sum is over all data points\n",
    "\n",
    "It measures how tight the clusters are:\n",
    "- Lower WCSS → Better clustering (points are closer to their centroids).\n",
    "\n",
    "Why Is Inertia Important?\n",
    "|Purpose\t|Explanation |\n",
    "|-----------|------------|\n",
    "|✅ Measures cluster compactness \t|Lower inertia = tighter clusters|\n",
    "|🔍 Helps evaluate K (number of clusters)\t|Useful in the elbow method|\n",
    "|🔧 Optimization metric in K-Means \t|The algorithm minimizes inertia during training |\n",
    "\n",
    "How to Track WCSS in Each Iteration?\n",
    "- Let’s add this logic into the K-Means loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae1ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss_history = []\n",
    "\n",
    "for _ in range(10):\n",
    "    labels = assign_clusters(X, centroids)\n",
    "    history_labels.append(labels.copy())\n",
    "\n",
    "    # Compute WCSS for this iteration\n",
    "    wcss = sum(np.sum((X[labels == i] - centroids[i])**2) for i in range(k))\n",
    "    wcss_history.append(wcss)\n",
    "\n",
    "    new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n",
    "    if np.allclose(centroids, new_centroids):\n",
    "        break\n",
    "    centroids = new_centroids\n",
    "    history_centroids.append(centroids.copy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612beade",
   "metadata": {},
   "source": [
    "Plot WCSS Over Iterations\n",
    "- After running the above loop, you can visualize how clustering improves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74bd2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, len(wcss_history)+1), wcss_history, marker='o', linestyle='--', color='purple')\n",
    "plt.title('WCSS over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('WCSS')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953d6c4",
   "metadata": {},
   "source": [
    "##### When and Why to Use Inertia\n",
    "✅ Use Cases:\n",
    "- Evaluating model quality during unsupervised learning\n",
    "- Choosing the optimal number of clusters (K) using the elbow method\n",
    "- Comparing different clustering results\n",
    "\n",
    "Use it to track performance across different values of K.\n",
    "\n",
    "🚫 Caveats:\n",
    "- Inertia always decreases as K increases (more clusters = smaller groups), so:\n",
    "- It’s not suited to determine the best K alone — combine with silhouette score or Davies-Bouldin index\n",
    "- It’s sensitive to scaling (use z-score or MinMax scaling)\n",
    "\n",
    "##### Python Example: Calculate Inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e431bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Use the same 2D data\n",
    "X = np.array([\n",
    "    [1, 2], [1, 4], [1, 0],\n",
    "    [4, 2], [4, 4], [4, 0]\n",
    "])\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Inertia (WCSS)\n",
    "print(\"Inertia (WCSS):\", kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db171090",
   "metadata": {},
   "source": [
    "### Elbow Method\n",
    "a powerful technique to choose the optimal number of clusters (K) in K-Means.\n",
    "\n",
    "### What Is the Elbow Method?\n",
    "The Elbow Method helps you determine the ideal number of clusters (K) by plotting:\n",
    "$$ \\text{Number of clusters (K) vs. Inertia (WCSS)}$$\n",
    "- Inertia / WCSS = Sum of squared distances from each point to its assigned cluster centroid.\n",
    "\n",
    "The idea is to find the \"elbow\" point — where adding more clusters doesn’t significantly reduce inertia anymore.\n",
    "\n",
    "When plotted, the graph typically shows:\n",
    "- A steep drop in inertia as K increases\n",
    "- A bend or \"elbow\" point\n",
    "- After the elbow, improvements in clustering diminish\n",
    "\n",
    "The Elbow Method involves:\n",
    "- Running K-Means for different values of K (e.g., from 1 to 10).\n",
    "- Calculating WCSS for each value of K.\n",
    "    - Record Inertia (WCSS) for each K\n",
    "- Plotting K vs WCSS.\n",
    "- Finding the “elbow” point — where WCSS starts to flatten — indicating the best trade-off between cluster compactness and model simplicity.\n",
    "- Choose that K value as optimal\n",
    "\n",
    "Why It Works:\n",
    "- For small K, WCSS drops rapidly as clusters better explain the data.\n",
    "    - A lower inertia suggests that points are closer to their centroids.\n",
    "- After a certain K, adding more clusters gives diminishing returns.\n",
    "- The elbow shows where to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69904e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Step 1: Generate synthetic data\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=1.0, random_state=42)\n",
    "\n",
    "# Step 2: Run KMeans for different values of K\n",
    "wcss = []\n",
    "K_range = range(1, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=300, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)  # inertia_ is the WCSS\n",
    "\n",
    "# Step 3: Plot the Elbow Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_range, wcss, 'bo--', linewidth=2, markersize=8)\n",
    "plt.title('Elbow Method to Determine Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('WCSS (Within-Cluster Sum of Squares)')\n",
    "plt.xticks(K_range)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f9434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "K_range = range(1, 10)\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(X)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "plt.plot(K_range, inertias, 'bo-')\n",
    "plt.xlabel('Number of clusters K')\n",
    "plt.ylabel('Inertia (WCSS)')\n",
    "plt.title('Elbow Method')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# The value of K where inertia drops sharply and then levels off. That’s often a good choice for K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c85c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Sample 2D data\n",
    "X = np.array([\n",
    "    [1, 2], [1, 4], [1, 0],\n",
    "    [4, 2], [4, 4], [4, 0]\n",
    "])\n",
    "\n",
    "# Track inertia for different K\n",
    "inertias = []\n",
    "K_range = range(1, 10)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_range, inertias, 'bo-')\n",
    "plt.xlabel('Number of Clusters K')\n",
    "plt.ylabel('Inertia (WCSS)')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624cde4",
   "metadata": {},
   "source": [
    "How to Interpret the Plot:\n",
    "- Look for the \"elbow\" or \"knee\" in the curve — the point where the rate of decrease sharply slows.\n",
    "- That K is a good balance of model simplicity and clustering quality.\n",
    "\n",
    "Example:\n",
    "- If your WCSS looks like:\n",
    "\n",
    "|K\t| WCSS  |\n",
    "|---|-------|\n",
    "|1\t| 1700  |\n",
    "|2\t|900    |\n",
    "|3\t|500    |\n",
    "|4\t|250    |\n",
    "|5\t|230    |\n",
    "|6\t|220    |\n",
    "\n",
    "You might notice a strong bend (elbow) at K = 3 or 4 → those are good candidates.\n",
    "\n",
    "##### When Is the Elbow Point Clear?\n",
    "- A clear bend in the curve makes the elbow obvious.\n",
    "\n",
    "- If the plot is smooth and gradual, consider:\n",
    "    - Silhouette score\n",
    "    - Davies-Bouldin index\n",
    "    - Calinski-Harabasz index\n",
    "    - KneeLocator (to automate elbow detection)\n",
    "\n",
    "### Automate Elbow Detection using KneeLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7253f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcedff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Generate synthetic data\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=1.0, random_state=42)\n",
    "\n",
    "# Step 2: Compute WCSS for K = 1 to 10\n",
    "wcss = []\n",
    "K_range = range(1, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=300, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Step 3: Detect the 'elbow' automatically\n",
    "knee = KneeLocator(K_range, wcss, curve='convex', direction='decreasing')\n",
    "print(f\"Elbow point (optimal K): {knee.elbow}\")\n",
    "\n",
    "# Step 4: Plot with elbow\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_range, wcss, 'bo--')\n",
    "plt.axvline(x=knee.elbow, color='red', linestyle='--', label=f\"Elbow at K={knee.elbow}\")\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('Elbow Method with KneeLocator')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a3acc",
   "metadata": {},
   "source": [
    "This will output the optimal K and visually mark it with a red dashed line on the elbow plot.\n",
    "\n",
    "|Metric\t|Description|\n",
    "|-------|-----------|\n",
    "|Inertia\t|Measures total WCSS (within-cluster sum of squares)|\n",
    "|Goal\t|Minimize it for compact, well-defined clusters|\n",
    "|Use\t|Cluster evaluation, elbow method|\n",
    "|Limit\t|Always decreases with K; not ideal alone|\n",
    "\n",
    "### Silhouette Score Comparison\n",
    "\n",
    "What is Silhouette Score?\n",
    "\n",
    "The Silhouette Score measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation).\n",
    "\n",
    "It answers:\n",
    "- \"How well is each point placed within its cluster?\"\n",
    "\n",
    "Formula\n",
    "For each point i:\n",
    "\n",
    "$$ s(i) = \\frac{b(i) - a(i)}{\\max{a(i), b(i)}}$$\n",
    "- Where:\n",
    "    - a(i) mean distance between i and other points in the same cluster (intra-cluster distance)\n",
    "    - b(i) = mean distance between i and points in the nearest other cluster (nearest-cluster distance.\n",
    "\n",
    "The Silhouette Score measures how well each data point fits within its assigned cluster:\n",
    "- Range: -1 to 1\n",
    "    - +1: Point is well-matched to its own cluster and poorly matched to others.\n",
    "    - 0: Point is on or very close to the boundary between two clusters.\n",
    "    - −1: Point is likely assigned to the wrong cluster.\n",
    "- Closer to 1 → better clustering.\n",
    "- Use it to validate or complement elbow method.\n",
    "\n",
    "How to Use the Silhouette Method to Choose K\n",
    "- Run clustering for a range of K values\n",
    "- Compute average silhouette score for all points\n",
    "- Plot K vs average silhouette score\n",
    "- Select the K with the highest silhouette score.\n",
    "\n",
    "🧪 Code to Compute Silhouette Scores for Multiple K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfff1ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_scores = []\n",
    "\n",
    "# Skip K=1 because silhouette score is undefined for a single cluster\n",
    "K_range_silhouette = range(2, 11)\n",
    "\n",
    "for k in K_range_silhouette:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=300, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "    score = silhouette_score(X, cluster_labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_range_silhouette, silhouette_scores, 'go--')\n",
    "plt.title('Silhouette Score for Different K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xticks(K_range_silhouette)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Best K according to silhouette score\n",
    "best_k_silhouette = K_range_silhouette[np.argmax(silhouette_scores)]\n",
    "print(f\"Best K by Silhouette Score: {best_k_silhouette}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e4484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Simulated data\n",
    "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)\n",
    "\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    score = silhouette_score(X, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"K={k} => Silhouette Score: {score:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_range, silhouette_scores, 'bo-')\n",
    "plt.xlabel('Number of Clusters K')\n",
    "plt.ylabel('Average Silhouette Score')\n",
    "plt.title('Silhouette Method for Optimal K')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fec1c1",
   "metadata": {},
   "source": [
    "Interpretation Tips:\n",
    "\n",
    "|Method\t|What to Look For |\n",
    "|-------|-----------------|\n",
    "|Elbow\t|Look for a knee in WCSS drop |\n",
    "|Silhouette\t |Look for the peak (maximum score) |\n",
    "\n",
    "How to Interpret the Plot\n",
    "- Peak silhouette score = Best K\n",
    "- A drop in the score after the peak means adding more clusters reduces overall quality\n",
    "- High silhouette score (> 0.5) = good clustering\n",
    "- Low score (< 0.25) = poor clustering\n",
    "\n",
    "You may sometimes get slightly different results — use domain knowledge + business context to finalize.\n",
    "\n",
    "|Feature\t|Silhouette Method |\n",
    "|-----------|------------------|\n",
    "|🎯 Purpose\t|Evaluate clustering quality and choose K|\n",
    "|📐 Metric\t|Silhouette Score (from -1 to +1)|\n",
    "|🧠 Best K\t|Value with maximum average silhouette score|\n",
    "|📉 Advantage\t|Balances cohesion and separation|\n",
    "|⚠️ Limitation\t|May not perform well with overlapping clusters|\n",
    "\n",
    "### **Davies–Bouldin Index (DBI)**\n",
    "- a powerful yet lesser-known metric.\n",
    "\n",
    "Cluster Validation Metrics Overview\n",
    "\n",
    "|Metric \t|Range  \t|Goal   \t|Interpretation    |\n",
    "|-----------|-----------|-----------|------------------|\n",
    "|WCSS\t|0 → ∞\t|Minimize\t|Measures total squared distance to centroids|\n",
    "|Silhouette Score\t|-1 → 1|\t|Maximize\t|Measures separation and cohesion|\n",
    "|Davies-Bouldin Index\t|0 → ∞|\tMinimize|\tRatio of intra-cluster distance to inter-cluster distance|\n",
    "\n",
    "### What is the Davies–Bouldin Index (DBI)?\n",
    "The DBI compares similarity between clusters, where similarity is a function of:\n",
    "- Intra-cluster scatter (how tight each cluster is)\n",
    "- Inter-cluster separation (how far clusters are apart)\n",
    "\n",
    "✅ Lower DBI means better clustering (tight, well-separated clusters)\n",
    "\n",
    "Compute WCSS, Silhouette Score, and DBI all together for different K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Create data\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=1.0, random_state=42)\n",
    "\n",
    "# Step 2: Prepare storage\n",
    "K_range = range(2, 11)\n",
    "wcss = []\n",
    "silhouette_scores = []\n",
    "dbi_scores = []\n",
    "\n",
    "# Step 3: Loop through different K values\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=300, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # Metrics\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X, labels))\n",
    "    dbi_scores.append(davies_bouldin_score(X, labels))\n",
    "\n",
    "# Step 4: Plot all three metrics\n",
    "fig, axs = plt.subplots(3, 1, figsize=(8, 12), sharex=True)\n",
    "\n",
    "axs[0].plot(K_range, wcss, 'bo--')\n",
    "axs[0].set_title(\"WCSS (Within-Cluster Sum of Squares)\")\n",
    "axs[0].set_ylabel(\"WCSS\")\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].plot(K_range, silhouette_scores, 'go--')\n",
    "axs[1].set_title(\"Silhouette Score\")\n",
    "axs[1].set_ylabel(\"Silhouette\")\n",
    "axs[1].grid(True)\n",
    "\n",
    "axs[2].plot(K_range, dbi_scores, 'ro--')\n",
    "axs[2].set_title(\"Davies–Bouldin Index\")\n",
    "axs[2].set_ylabel(\"DBI (Lower is Better)\")\n",
    "axs[2].set_xlabel(\"Number of Clusters (K)\")\n",
    "axs[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75361c4b",
   "metadata": {},
   "source": [
    "How to Use These Together:\n",
    "- WCSS tells you how compact clusters are.\n",
    "- Silhouette Score tells you how distinct the clusters are.\n",
    "- DBI tells you how well-separated + compact clusters are (combined signal).\n",
    "\n",
    "|K\t|WCSS\t|Silhouette\t|DBI  |\n",
    "|---|-------|-----------|-----|\n",
    "|2\t|1500\t|0.62\t |0.9  |\n",
    "|3\t|1000\t|0.67\t|0.75  |\n",
    "|4\t|600\t|0.71\t|0.53   |\n",
    "|5\t|520\t|0.65\t|0.60   |\n",
    "\n",
    "✅ Best K: 4, based on peak Silhouette + lowest DBI.\n",
    "\n",
    "### Calinski–Harabasz Index (CHI) \n",
    "is another solid metric for evaluating cluster quality.\n",
    "\n",
    "What is the Calinski–Harabasz Index?\n",
    "- Also called the Variance Ratio Criterion, it evaluates clustering by comparing:\n",
    "    - Between-cluster dispersion (how far clusters are from each other)\n",
    "    - Within-cluster dispersion (how tight points are within a cluster)\n",
    "\n",
    "✅ Higher CHI is better — it means dense, well-separated clusters.\n",
    "\n",
    "|Metric\t|Range\t|Goal  |\tBest Value |\n",
    "|-------|-------|------|---------------|\n",
    "|WCSS\t|0 → ∞\t|Minimize  \t|Smallest  |\n",
    "|Silhouette Score\t|-1 → 1\t|Maximize\t|Closest to 1 |\n",
    "|Davies–Bouldin Index\t|0 → ∞\t|Minimize\t|Smallest |\n",
    "|Calinski–Harabasz\t|0 → ∞\t|Maximize\t|Largest   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c7302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Generate synthetic data\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=1.0, random_state=42)\n",
    "\n",
    "# Step 2: Prepare lists for metrics\n",
    "K_range = range(2, 11)\n",
    "wcss = []\n",
    "silhouette_scores = []\n",
    "dbi_scores = []\n",
    "chi_scores = []\n",
    "\n",
    "# Step 3: Loop and compute all 4 metrics\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=300, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X, labels))\n",
    "    dbi_scores.append(davies_bouldin_score(X, labels))\n",
    "    chi_scores.append(calinski_harabasz_score(X, labels))\n",
    "\n",
    "# Step 4: Plot all metrics\n",
    "fig, axs = plt.subplots(4, 1, figsize=(8, 14), sharex=True)\n",
    "\n",
    "axs[0].plot(K_range, wcss, 'bo--')\n",
    "axs[0].set_title(\"WCSS (Lower is Better)\")\n",
    "axs[0].set_ylabel(\"WCSS\")\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].plot(K_range, silhouette_scores, 'go--')\n",
    "axs[1].set_title(\"Silhouette Score (Higher is Better)\")\n",
    "axs[1].set_ylabel(\"Silhouette\")\n",
    "axs[1].grid(True)\n",
    "\n",
    "axs[2].plot(K_range, dbi_scores, 'ro--')\n",
    "axs[2].set_title(\"Davies–Bouldin Index (Lower is Better)\")\n",
    "axs[2].set_ylabel(\"DBI\")\n",
    "axs[2].grid(True)\n",
    "\n",
    "axs[3].plot(K_range, chi_scores, 'mo--')\n",
    "axs[3].set_title(\"Calinski–Harabasz Index (Higher is Better)\")\n",
    "axs[3].set_ylabel(\"CHI\")\n",
    "axs[3].set_xlabel(\"Number of Clusters (K)\")\n",
    "axs[3].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c44f8e2",
   "metadata": {},
   "source": [
    "Interpretation Strategy\n",
    "\n",
    "Use these together:\n",
    "- 🔍 Look for elbow in WCSS\n",
    "- ✅ Find peak in Silhouette & CHI\n",
    "- 🔻 Look for minimum in DBI\n",
    "\n",
    "If K = 4 gives:\n",
    "- High Silhouette\n",
    "- High CHI\n",
    "- Low DBI\n",
    "→ It’s a great candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f061a",
   "metadata": {},
   "source": [
    "### **What Is the Gap Statistic?**\n",
    "The Gap Statistic compares the inertia (within-cluster variation) of your data’s clustering result to the expected inertia under a null reference distribution (i.e., random/uniformly distributed data).\n",
    "- It tells you how much better your clustering is compared to clustering on random data.\n",
    "\n",
    "It compares the within-cluster dispersion of your actual data to that of a reference dataset with no structure (i.e., data generated uniformly at random).\n",
    "- Idea: If the clustering on your real data is much better (more compact) than clustering on random noise, that's a good K.\n",
    "\n",
    "Why Use the Gap Statistic?\n",
    "- It corrects for the natural tendency of WCSS to decrease as K increases (a flaw in the Elbow Method).\n",
    "- It’s model-agnostic and more robust for choosing K.\n",
    "- It evaluates how much structure exists in the data relative to noise.\n",
    "\n",
    "Formula\n",
    "- For each number of clusters 𝑘:\n",
    "$$ \\text{Gap(k)} = E[\\log (W^*_k)] - \\log(W_k) $$\n",
    "- Where:\n",
    "    - $W_k$ : Within-cluster dispersion for your actual data\n",
    "    - $𝑊^*_𝑘$ : Expected within-cluster dispersion for random (null) data\n",
    "    - E[⋅]: Average over multiple simulations\n",
    "\n",
    "### How the Gap Statistic Works\n",
    "For each value of 𝐾 (number of clusters):\n",
    "1. Run K-Means on your data → compute inertia $𝑊_𝑘$\n",
    "​2. Generate B reference datasets (e.g., random data in the same range)\n",
    "3. Run K-Means on each reference dataset → compute their average inertia $E[W^{ref}_k]$\n",
    "4. Compute the Gap:\n",
    "$$ \\text{Gap(k)} = E_{ref}[\\log (W^{red}_k)] - \\log(W_k) $$\n",
    "5. Choose the smallest K such that:\n",
    "$$ \\text{Gap(k)} \\geq \\text{Gap(k +1)} - s_{k+1}$$\n",
    "- where: \n",
    "    - $s_{k+1}$ is the standard deviation of the log inertia from reference datasets.\n",
    "\n",
    "Interpretation\n",
    "- Higher gap = better clustering compared to random\n",
    "- Choose the smallest K such that:\n",
    "$$ \\text{Gap(k)} \\geq \\text{Gap(k +1)} - s_{k+1}$$\n",
    "\n",
    "Interpreting the Gap\n",
    "- Large Gap(k) → Data is much more structured than random → Good number of clusters\n",
    "- Optimal K is the first K where the gap begins to narrow (similar to elbow logic, but statistically grounded)\n",
    "\n",
    "When to Use Gap Statistic\n",
    "- When elbow or silhouette plots are ambiguous\n",
    "- When you want a more principled, statistically driven method\n",
    "- When cluster quality is crucial for business decisions (e.g., customer segmentation)\n",
    "\n",
    "Python Implementation (Using gap-statistic Package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cebb5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gap-stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3979fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from gap_statistic import OptimalK\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X, _ = make_blobs(n_samples=500, centers=4, random_state=42)\n",
    "\n",
    "# Initialize the Gap Statistic\n",
    "optimalK = OptimalK(parallel_backend='joblib')  # Can also use 'threading'\n",
    "\n",
    "# Compute the optimal K\n",
    "n_clusters = optimalK(X, cluster_array=np.arange(1, 11))\n",
    "print(f\"Optimal number of clusters: {n_clusters}\")\n",
    "\n",
    "# Plot the gap values\n",
    "plt.plot(optimalK.gap_df.n_clusters, optimalK.gap_df.gap_value, marker='o')\n",
    "plt.title('Gap Statistic')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Gap Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a008b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from gap_statistic import OptimalK\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulated data with clear clusters\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)\n",
    "\n",
    "# Compute optimal K using Gap Statistic\n",
    "optimalK = OptimalK(parallel_backend='rust')  # Use 'joblib' if rust fails\n",
    "n_clusters = optimalK(X, cluster_array=np.arange(1, 11))\n",
    "\n",
    "print(f\"Optimal number of clusters by Gap Statistic: {n_clusters}\")\n",
    "\n",
    "# Plot gap values\n",
    "plt.plot(optimalK.gap_df.n_clusters, optimalK.gap_df.gap_value, marker='o')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Gap Statistic')\n",
    "plt.title('Gap Statistic Method for Optimal K')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b0c83",
   "metadata": {},
   "source": [
    "Feature\tDescription\n",
    "|📐 Metric\t|Gap = Difference in log(WCSS) between real and random|\n",
    "|------------|------------------------------------------------------|\n",
    "|🎯 Goal\t|Maximize Gap (find biggest drop from reference)|\n",
    "|🧪 Evaluates\t|Whether real data has stronger clustering than noise|\n",
    "|✅ Usefulness\t|More objective and statistically robust than elbow|\n",
    "|⚠️ Limitation\t|Computationally expensive due to repeated KMeans|\n",
    "\n",
    "\n",
    "Quick Comparison of K-Selection Methods\n",
    "\n",
    "|Method\t|Goal\t|Best K =|\n",
    "|-------|-------|--------|\n",
    "|Elbow\t|Sharp drop in inertia\t|Elbow point|\n",
    "|Silhouette\t|Maximize cluster quality|\tMax score|\n",
    "|Gap Statistic|\tCompare to random null|\tMax gap with tolerance|\n",
    "\n",
    "STEP 1: Create a Customer Demographics Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e32b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import random\n",
    "\n",
    "# Simulated customer demographics: Age, Income, SpendingScore\n",
    "np.random.seed(42)\n",
    "n_samples = 300\n",
    "\n",
    "age = np.random.normal(40, 12, n_samples)\n",
    "income = np.random.normal(60000, 15000, n_samples)\n",
    "score = np.random.normal(50, 20, n_samples)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Age': age,\n",
    "    'Income': income,\n",
    "    'SpendingScore': score\n",
    "})\n",
    "\n",
    "# Preprocess\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b2b22",
   "metadata": {},
   "source": [
    "STEP 2: Define Helper to Compute WCSS\n",
    "\n",
    "This computes the total within-cluster sum of squares (dispersion):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df1a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wcss(X, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    return kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac4818",
   "metadata": {},
   "source": [
    "STEP 3: Generate Reference (Random Uniform) Datasets\n",
    "\n",
    "We simulate B reference datasets and compute WCSS on those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e143c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reference_data(X, n_samples):\n",
    "    mins = np.min(X, axis=0)\n",
    "    maxs = np.max(X, axis=0)\n",
    "    return np.random.uniform(mins, maxs, size=(n_samples, X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29775d0d",
   "metadata": {},
   "source": [
    "STEP 4: Manual Gap Statistic Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498e4294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gap_statistic(X, k_range, B=10):\n",
    "    n_samples = X.shape[0]\n",
    "    gap_values = []\n",
    "    sdk = []\n",
    "\n",
    "    for k in k_range:\n",
    "        # Wk = actual data\n",
    "        wk = compute_wcss(X, k)\n",
    "\n",
    "        # Generate B reference datasets\n",
    "        wk_refs = []\n",
    "        for _ in range(B):\n",
    "            X_ref = generate_reference_data(X, n_samples)\n",
    "            wk_refs.append(compute_wcss(X_ref, k))\n",
    "        \n",
    "        log_wk_refs = np.log(wk_refs)\n",
    "        log_wk = np.log(wk)\n",
    "\n",
    "        # Gap statistic\n",
    "        gap_k = np.mean(log_wk_refs) - log_wk\n",
    "        sk = np.std(log_wk_refs) * np.sqrt(1 + 1/B)\n",
    "\n",
    "        gap_values.append(gap_k)\n",
    "        sdk.append(sk)\n",
    "\n",
    "    return gap_values, sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb5e9d2",
   "metadata": {},
   "source": [
    "STEP 5: Run the Gap Statistic and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81efa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1, 11)\n",
    "gap_vals, sk_vals = gap_statistic(X, k_range, B=10)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_range, gap_vals, marker='o')\n",
    "plt.xlabel(\"Number of Clusters K\")\n",
    "plt.ylabel(\"Gap Value\")\n",
    "plt.title(\"Gap Statistic for Optimal K (Customer Demographics)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86012f4e",
   "metadata": {},
   "source": [
    "STEP 6: Selecting the Best K\n",
    "\n",
    "You can choose the optimal K using the rule:\n",
    "- Choose the smallest K such that\n",
    "- Gap(K) ≥ Gap(K+1) - s_{k+1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915cf2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_k_by_gap(gap_vals, sk_vals):\n",
    "    for k in range(0, len(gap_vals)-1):\n",
    "        if gap_vals[k] >= gap_vals[k+1] - sk_vals[k+1]:\n",
    "            return k + 1  # Since k is zero-indexed\n",
    "    return len(gap_vals)\n",
    "\n",
    "optimal_k = optimal_k_by_gap(gap_vals, sk_vals)\n",
    "print(f\"Optimal number of clusters (by Gap Statistic): {optimal_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3278e087",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "If the optimal K = 4 (say), it means:\n",
    "- Clustering the customer demographics data into 4 groups gives a compact, meaningful grouping.\n",
    "- Going beyond that (e.g., K=5,6) doesn’t significantly reduce WCSS compared to random structure — so extra clusters aren’t meaningful.\n",
    "\n",
    "### **What is Principal Component Analysis (PCA)?**\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional form without losing much information.\n",
    "\n",
    "🧩 Primary Features of PCA\n",
    "\n",
    "|Feature\t|Description  |\n",
    "|-----------|-------------|\n",
    "|✅ Dimensionality reduction \t|Reduces the number of variables (features) while retaining the most variance (information).|\n",
    "|📊 Linear transformation \t|Projects data onto new orthogonal axes (principal components).|\n",
    "|🔢 Principal components (PCs)\t|New axes are linear combinations of the original features.|\n",
    "|🧠 Ordered by variance\t|PC1 captures the most variance, PC2 the second most, etc.|\n",
    "|🔁 Unsupervised\t|No labels are used — similar to clustering in that way.|\n",
    "|🔍 Helps visualization\t|Reduces data to 2D or 3D for visualizing patterns or clusters.|\n",
    "|⚖️ Requires scaling\t|Important to standardize features before applying PCA.|\n",
    "\n",
    "🧠 How PCA Works (Conceptually)\n",
    "\n",
    "Standardize the Data\n",
    "- Scale variables (z-score) to have mean = 0 and std = 1.\n",
    "\n",
    "Compute Covariance Matrix\n",
    "- Measures how features vary with one another.\n",
    "\n",
    "Compute Eigenvectors & Eigenvalues\n",
    "- Eigenvectors define directions (principal components); eigenvalues define importance (variance explained).\n",
    "\n",
    "Project Data\n",
    "- Transform original data onto the selected principal components (usually top 2–3).\n",
    "\n",
    "📌 Example Use Case\n",
    "\n",
    "Suppose you have customer data with:\n",
    "- Age, Income, Spending Score, Education Level, etc.\n",
    "You want to reduce the dimensions to 2 or 3 while keeping the core structure intact — especially useful for clustering or visualization.\n",
    "\n",
    "🔗 How PCA Relates to Cluster Analysis\n",
    "- PCA and clustering complement each other in unsupervised learning.\n",
    "\n",
    "|Relationship\t|Explanation  |\n",
    "|---------------|-------------|\n",
    "|✅ Preprocessing for Clustering\t|PCA reduces dimensionality and noise, helping clustering algorithms like KMeans perform better.|\n",
    "|📉 Deals with multicollinearity\t|Reduces redundancy between variables, making distance-based clustering more effective.|\n",
    "|📊 Visualization\t|PCA makes high-dimensional cluster structure easier to visualize in 2D or 3D plots.|\n",
    "|🔎 Interpretability\t|You can observe clusters forming along PC1 and PC2 axes, revealing structure in otherwise noisy or high-dimensional data.|\n",
    "\n",
    "✅ PCA + Clustering: Typical Workflow\n",
    "1. Start with high-dimensional dataset\n",
    "2. Apply standard scaling (z-score)\n",
    "3. Apply PCA (retain 2 or 3 components)\n",
    "4. Visualize data in PC1-PC2 space\n",
    "5. Apply clustering (e.g., KMeans or Hierarchical)\n",
    "6. Analyze or validate cluster structure\n",
    "\n",
    "Summary\n",
    "|Concept\t|PCA\t|Clustering|\n",
    "|-----------|-------|----------|\n",
    "|Purpose\t|Dimensionality reduction\t|Group similar observations|\n",
    "|Type\t|Unsupervised\t|Unsupervised  |\n",
    "|Output\t|Principal components (new axes)\t|Cluster labels (groups)|\n",
    "|Relationship\t|PCA simplifies data for better clustering & visualization|\n",
    "\n",
    "\n",
    "### How PCA Helps Improve Clustering\n",
    "🧠 Why PCA Helps:\n",
    "- Removes noise and redundancy (like correlated features)\n",
    "- Reduces dimensionality, which:\n",
    "    - Speeds up clustering\n",
    "    - Makes it easier for K-Means to find compact groups\n",
    "    - Improves cluster separation, especially for visualization\n",
    "\n",
    "Example (Using Iris Dataset)\n",
    "\n",
    "- We’ll apply clustering: On the original features\n",
    "- Then on the PCA-reduced features\n",
    "- And compare cluster validity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec74b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Clustering on original features\n",
    "kmeans_original = KMeans(n_clusters=3, random_state=42)\n",
    "labels_orig = kmeans_original.fit_predict(X_scaled)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Clustering on PCA features\n",
    "kmeans_pca = KMeans(n_clusters=3, random_state=42)\n",
    "labels_pca = kmeans_pca.fit_predict(X_pca)\n",
    "\n",
    "# Validation Scores\n",
    "def cluster_metrics(X, labels):\n",
    "    return {\n",
    "        \"Silhouette\": silhouette_score(X, labels),\n",
    "        \"Calinski-Harabasz\": calinski_harabasz_score(X, labels),\n",
    "        \"Davies-Bouldin\": davies_bouldin_score(X, labels)\n",
    "    }\n",
    "\n",
    "metrics_original = cluster_metrics(X_scaled, labels_orig)\n",
    "metrics_pca = cluster_metrics(X_pca, labels_pca)\n",
    "\n",
    "print(\"Original Feature Space:\", metrics_original)\n",
    "print(\"PCA-Reduced Feature Space:\", metrics_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc50b81",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "- Higher Silhouette and Calinski-Harabasz → Better separation\n",
    "- Lower Davies-Bouldin → More compact and distinct clusters\n",
    "\n",
    "In many real-world cases, PCA can enhance the clustering structure especially when features are highly correlated or noisy.\n",
    "\n",
    "### Interpreting PCA Components\n",
    "\n",
    "📌 Understanding PCA Loadings\n",
    "You can check how each original feature contributes to each Principal Component:\n",
    "\n",
    "This tells you how each original feature loads onto each principal component.\n",
    "\n",
    "|Feature\t|PC1\t|PC2  |\n",
    "|----------|---------|------|\n",
    "|sepal length\t|0.36\t|-0.06|\n",
    "|sepal width\t|-0.08\t|0.93 |\n",
    "|petal length\t|0.86\t|0.25 |\n",
    "|petal width\t|0.36\t|0.25 |\n",
    "\n",
    "🔍 Interpretation:\n",
    "- PC1 heavily weights petal length and petal width → captures petal size\n",
    "- PC2 is strongly influenced by sepal width\n",
    "\n",
    "So, cluster separation along PC1 is likely tied to petal dimensions, which is meaningful in the Iris dataset!\n",
    "\n",
    "✅ Summary\n",
    "|Step\t|Purpose|\n",
    "|-------|--------|\n",
    "|PCA\t|Reduce dimensions, simplify data|\n",
    "|K-Means on PCA|\tOften finds better, cleaner clusters|\n",
    "|Validity metrics comparison|\tQuantifies improvement|\n",
    "|Loadings|\tExplain what each PC represents|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab67cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=[\"PC1\", \"PC2\"],\n",
    "    index=iris.feature_names\n",
    ")\n",
    "\n",
    "print(loadings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea1408",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "What It Means: \n",
    "- PCA reduces the number of variables in the data by finding combinations of variables that capture the most information (variance).\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each \"principal component\" explains a percentage of the total variance, helping simplify the data without losing much information.\n",
    "\n",
    "Performance Measures:\n",
    "- Explained Variance Ratio: Shows how much information each principal component holds; higher is better.\n",
    "\n",
    "Lay Explanation: \n",
    "- PCA is like summarizing a book by keeping only the most important points, making data easier to work with without losing key insights.\n",
    "\n",
    "Use Case: \n",
    "- Dimensionality reduction while retaining the most critical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23bf1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87498902",
   "metadata": {},
   "source": [
    "## Principle Complonent Analyis (PCA)\n",
    "In Principal Component Analysis (PCA), the goal is to reduce the number of dimensions while retaining as much variance (information) as possible. \n",
    "\n",
    "There are two primary ways to control the amount of variance retained:\n",
    "\n",
    "✅ 1. Set the Number of Principal Components (n_components)\n",
    "- You can directly specify how many principal components you want to keep — for example:\n",
    "\n",
    "This means PCA will retain the first two components, which usually capture the largest share of variance. \n",
    "- However, you won’t know exactly how much variance is retained unless you calculate or plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f914ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee16843",
   "metadata": {},
   "source": [
    "✅ 2. Set a Variance Threshold (Proportion of Variance Retained)\n",
    "- Instead of fixing the number of components, you can set a target proportion of explained variance you wish to retain — such as 95%:\n",
    "\n",
    "PCA will automatically determine the minimum number of components required to explain at least 95% of the total variance in the dataset. \n",
    "- This is useful for adaptive dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c51aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d9d799",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "|Method\t|How it Works\t|When to Use  |\n",
    "|-------|---------------|-------------|\n",
    "|n_components=int\t|Fix number of components manually\t|When you want a specific number of dimensions|\n",
    "|n_components=float\t|Retain a target % of variance (e.g., 0.95)\t|When you want to preserve a minimum amount of information|\n",
    "\n",
    "\n",
    "Explained Variance Plot (Scree Plot)\n",
    "- You can visualize how variance accumulates across components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pca = PCA().fit(X_scaled)\n",
    "explained_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.plot(range(1, len(explained_var_ratio)+1), explained_var_ratio, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d6a4b7",
   "metadata": {},
   "source": [
    "### Understanding the relationship between the number of features and the number of parameters in a dimensionality reduction method—like Principal Component Analysis (PCA)\n",
    "It is key to both interpreting and designing efficient models.\n",
    "\n",
    "✅ Core Concept\n",
    "- In dimension reduction, we project the original high-dimensional data (with many features) into a lower-dimensional space using a set of transformed axes (components). \n",
    "- The parameters we estimate are tied to how we create this new space.\n",
    "\n",
    "🔁 Relationship Explained\n",
    "- Let’s break it down using PCA as an example:\n",
    "\n",
    "🔹 Suppose:\n",
    "- You have a dataset with n_samples and n_features.\n",
    "- You want to reduce the data to k principal components, where k < n_features.\n",
    "\n",
    "📌 Parameters Estimated in PCA:\n",
    "- Component Loadings (also called eigenvectors or weights):\n",
    "    - PCA learns a weight (loading) for each feature in each component.\n",
    "    - Shape: (n_features × k)\n",
    "- So, PCA estimates n_features × k parameters for the transformation matrix.\n",
    "\n",
    "Explained Variance:\n",
    "- PCA also calculates the eigenvalues (amount of variance each component captures).\n",
    "- You estimate k variance values, but this is small compared to the loadings.\n",
    "\n",
    "🧠 Interpretation:\n",
    "- More Features → More Parameters to Estimate:\n",
    "    - If you start with 100 features and reduce to 5 components, PCA must estimate 100 × 5 = 500 parameters (loadings).\n",
    "- More Components → Closer to Original Representation:\n",
    "    - Retaining more components keeps more information but increases parameter count.\n",
    "- Fewer Components → Simpler Model:\n",
    "    - Reduces parameters and computation, but you may lose some variance.\n",
    "\n",
    "🧮 Quick Formula:\n",
    "- Number of parameters estimated ≈ n_features × k, where k is the number of retained components.\n",
    "\n",
    "📊 Example:\n",
    "\n",
    "If your original dataset has:\n",
    "- 50 features\n",
    "- 2000 samples\n",
    "- You reduce to 10 components\n",
    "\n",
    "Then:\n",
    "- PCA estimates 50 × 10 = 500 parameters for the loadings matrix.\n",
    "\n",
    "📌 Summary Table:\n",
    "|Item\t|Value  |\n",
    "|-------|-------|\n",
    "|Original Features\t|n_features   |\n",
    "|Reduced Components\t| k  |\n",
    "|Parameters Estimated|\tn_features × k  |\n",
    "|Goal\t|Capture max variance with fewer dimensions  |\n",
    "\n",
    "##### walk through a concrete example using simulated data to:\n",
    "1. Generate a dataset with many features\n",
    "2. Apply PCA with different k values\n",
    "3. Calculate and interpret:\n",
    "    - Number of parameters estimated\n",
    "    - Variance retained\n",
    "\n",
    "Step-by-Step PCA Example with Parameter Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3276e58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Simulate data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "n_features = 20\n",
    "\n",
    "X = np.random.rand(n_samples, n_features)\n",
    "X = pd.DataFrame(X, columns=[f\"feature_{i+1}\" for i in range(n_features)])\n",
    "\n",
    "# Step 2: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Fit PCA (retain all components to analyze)\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Step 4: Calculate explained variance and loadings\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Step 5: Plot cumulative variance\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance, marker='o')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
    "plt.title('Cumulative Explained Variance by PCA Components')\n",
    "plt.xlabel('Number of Components (k)')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c54f31",
   "metadata": {},
   "source": [
    "Now Let’s Interpret:\n",
    "\n",
    "✅ How many components needed to retain 95% variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80752cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"Minimum components needed to retain 95% variance: {k_95}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e4cb76",
   "metadata": {},
   "source": [
    "✅ Number of Parameters Estimated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c184ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parameters = n_features * k_95\n",
    "print(f\"Parameters estimated for PCA transformation (loadings): {n_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a88f83",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "- Suppose the output is:\n",
    "    - Minimum components needed to retain 95% variance: 14\n",
    "    - Parameters estimated for PCA transformation (loadings): 280\n",
    "\n",
    "This means:\n",
    "- You can reduce from 20 to 14 components and still retain 95% of the information.\n",
    "- PCA estimates 280 parameters (20 features × 14 components) to form the transformation matrix (the loadings).\n",
    "\n",
    "Projecting the original data onto the top k principal components gives you a reduced-dimension representation of your data — this is one of the main goals of PCA.\n",
    "- Let’s walk through the steps to project your data onto the top k = 14 components (assuming we want to retain 95% variance from the previous example):\n",
    "\n",
    "\n",
    "What this means:\n",
    "- X_pca_df now holds the data projected into a new space with 14 orthogonal axes (principal components).\n",
    "- Each principal component is a linear combination of the original 20 features, designed to maximize variance.\n",
    "\n",
    "This lower-dimensional dataset can now be used for:\n",
    "- Clustering (like K-Means)\n",
    "- Visualization (with 2 or 3 components)\n",
    "- Feeding into supervised learning models\n",
    "\n",
    "✅ Why this is powerful:\n",
    "- Reduces noise and redundancy in your features\n",
    "- Improves model interpretability and training time\n",
    "- Keeps the most informative variation in your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a53937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Standardize the data again (if not already done)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Fit PCA with k=14 components\n",
    "pca_14 = PCA(n_components=14)\n",
    "X_pca_14 = pca_14.fit_transform(X_scaled)\n",
    "\n",
    "# Step 3: Convert to DataFrame for easier viewing\n",
    "X_pca_df = pd.DataFrame(X_pca_14, columns=[f'PC{i+1}' for i in range(14)])\n",
    "\n",
    "# Show the transformed dataset\n",
    "X_pca_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de803b54",
   "metadata": {},
   "source": [
    "### Two major steps in any dimension reduction method\n",
    "\n",
    "Objective:\n",
    "Reduce high-dimensional data into fewer, more meaningful dimensions without losing significant information.\n",
    "\n",
    "We do this in two interconnected steps, where each step feeds into the next.\n",
    "\n",
    "##### **1. Feature Transformation / Projection into a New Space**\n",
    "This step transforms the original high-dimensional features into a new set of dimensions (components), which are often linear combinations of the original features.\n",
    "\n",
    "What Happens:\n",
    "- Transform the original data (features) into a new coordinate system.\n",
    "- These new axes (called principal components in PCA) are:\n",
    "    - Orthogonal (uncorrelated)\n",
    "    - Ordered by how much variance they capture from the original data\n",
    "\n",
    "💡 Why It Matters:\n",
    "- Helps identify the directions in which the data varies the most\n",
    "- Converts correlated features into independent axes\n",
    "- Removes redundancy\n",
    "\n",
    "📊 Mathematically:\n",
    "- Compute the covariance matrix of the standardized data.\n",
    "- Perform eigendecomposition or SVD to extract:\n",
    "    - Eigenvectors (principal axes)\n",
    "    - Eigenvalues (amount of variance captured by each axis)\n",
    "\n",
    "Example: In PCA,\n",
    "- These are the principal components — new axes aligned to directions of maximum variance.\n",
    "    - Find new orthogonal axes (principal components) in the feature space.\n",
    "    - These axes are ordered by how much variance they capture from the data.\n",
    "    - The transformation is linear (e.g., multiplying original data by eigenvectors).\n",
    "- The transformation aims to reorient the data into a more compact and meaningful representation.\n",
    "- Involves computing eigenvectors (directions) and eigenvalues (variance captured) of the data’s covariance matrix.\n",
    "\n",
    "🧠 Goal: Find a new coordinate system that captures the most important patterns in fewer dimensions.\n",
    "\n",
    "✅ Goal: Convert the original features into a new set of variables (called components, axes, or dimensions) that are combinations of the original ones.\n",
    "\n",
    "##### **2. Dimensionality Truncation / Selection of Top-k Components**\n",
    "After transformation, you select the top k components that explain most of the variance in the data, and discard the rest.\n",
    "\n",
    "What Happens:\n",
    "- Select the top k principal components that together capture most of the variance (e.g., 95%).\n",
    "- Project the original data into this reduced space.\n",
    "\n",
    "💡 Why It Matters:\n",
    "- Reduces the number of dimensions while retaining the structure of the original dataset.\n",
    "- Discards noisy or less informative components.\n",
    "\n",
    "📊 Common Methods for Choosing k:\n",
    "- Explained Variance Threshold (e.g., 95%)\n",
    "- Scree Plot / Elbow Method\n",
    "- Eigenvalues > 1 (Kaiser’s rule)\n",
    "\n",
    "Example: In PCA:\n",
    "- This is where you reduce the number of dimensions.\n",
    "    - Evaluate each component’s explained variance (eigenvalues).\n",
    "    - Select enough components to retain a chosen threshold of variance (e.g., 95%).\n",
    "- The number of components k is chosen based on criteria like:\n",
    "    - Cumulative explained variance (e.g., retain 95%)\n",
    "    - Scree plot (elbow method)\n",
    "    - Eigenvalues > 1 rule\n",
    "- You then project the original data onto just these k components.\n",
    "\n",
    "🧠 Goal: Keep the most informative structure of the data while reducing noise and complexity.\n",
    "\n",
    "✅ Goal: Choose the top k components (from all transformed ones) that capture most of the information (variance) while reducing dimensionality.\n",
    "\n",
    "🔁 How They Work Together\n",
    "|Phase\t|Description|\n",
    "|--------|-----------|\n",
    "|Step 1: Transformation\t|Converts the original dataset to a new space with uncorrelated components|\n",
    "|Step 2: Truncation\t|Selects the most informative components to reduce dimensionality|\n",
    "\n",
    "🧠 Analogy: Think of Step 1 as reorienting the camera to get the best angles of a scene. Step 2 is about cropping the picture to only keep the most important view.\n",
    "\n",
    "📌 Summary Table\n",
    "|Step\t|Description |  Purpose    |\n",
    "|-------|------------|-------------|\n",
    "|1. Transform / Feature transformation (e.g., PCA axes)\t|Re-express original data into new components (rotated axes). | Find new informative directions   |\n",
    "|2. Select / Dimension selection (choose top k)\t|Keep only the top components that explain most of the variance.  |  Reduce dimensions, retain meaningful info |\\\n",
    "\n",
    "### Explore the mathematical implementation of PCA using eigenvalues and eigenvectors\n",
    "How PCA reduces dimensions while preserving most of the variance.\n",
    "\n",
    "🔢 PCA: Mathematical Steps Using Eigenvalues & Eigenvectors\n",
    "- Let’s assume we have a dataset X with n samples and d features (dimensions).\n",
    "\n",
    "✅ Step 1: Standardize the Data\n",
    "\n",
    "PCA is sensitive to scale, so we first standardize:\n",
    "$$ Z = \\frac{X - \\mu}{\\sigma} $$\n",
    "- Each feature gets zero mean and unit variance.\n",
    "\n",
    "✅ Step 2: Compute the Covariance Matrix\n",
    "\n",
    "The covariance matrix shows how variables co-vary:\n",
    "$$ \\text{Cov(Z)} = \\frac{1}{n - 1} Z^T Z$$\n",
    "- It will be a d x d symmetric matrix showing how each pair of features relates.\n",
    "\n",
    "✅ Step 3: Calculate Eigenvalues and Eigenvectors\n",
    "\n",
    "Solve:\n",
    "$$ \\text{Cov(Z)} \\cdot v = \\lambda \\cdot v $$\n",
    "- Where:\n",
    "    - v = eigenvector (principal component direction)\n",
    "    - λ = eigenvalue (variance explained by that component)\n",
    "    - The eigenvectors are the new axes (principal components).\n",
    "    - The eigenvalues indicate the importance (variance) of each component.\n",
    "\n",
    "✅ Step 4: Sort and Select Top k Eigenvectors\n",
    "- Sort eigenvectors by their eigenvalues in descending order.\n",
    "- Select top k eigenvectors $V_k \\isin R^{d \\times k}$\n",
    "\n",
    "✅ Step 5: Project the Data\n",
    "- Project the standardized data onto the selected components:\n",
    "$$ Z_{projected} = Z \\cdot V_k $$\n",
    "\n",
    "This gives the data in the new reduced-dimensional space.\n",
    "\n",
    "Example with Code: 2D Dataset Reduced to 1D\n",
    "\n",
    "Interpretation:\n",
    "- The red arrows are the principal axes (eigenvectors).\n",
    "- The length reflects the amount of variance explained (eigenvalue).\n",
    "- If we projected the data onto PC1 (longer arrow), we reduce the data from 2D → 1D, keeping the maximum variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea0e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic 2D data\n",
    "np.random.seed(0)\n",
    "X = np.random.multivariate_normal(mean=[0, 0], cov=[[3, 2.5], [2.5, 3]], size=200)\n",
    "\n",
    "# Step 1: Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 3: Visualize\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_scaled[:,0], X_scaled[:,1], alpha=0.5, label='Original Data')\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    plt.arrow(0, 0, v[0], v[1], width=0.02, color='red', label='Principal Component')\n",
    "plt.title('PCA: Original Data and Principal Components')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True)\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11528c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Eigenvalues (explained variance):\", pca.explained_variance_)\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa33fb",
   "metadata": {},
   "source": [
    "### Interpreting principal components (PCs)\n",
    "Crucial for understanding what each component represents in terms of original features.\n",
    "\n",
    "**How to Interpret Principal Components**\n",
    "After applying PCA, each principal component is a linear combination of the original features:\n",
    "$$ PC_j = w_{1j} x_1 + w_{2j} x_2 + ... + w_{dj} x_d $$\n",
    "- Where:\n",
    "    - $w_{ij}$ is the loading (weight) of feature $𝑥_𝑖$ in principal component 𝑗\n",
    "    - The larger the absolute value of $w_{ij}$ , the more that feature contributes to the component.\n",
    "\n",
    "✅ Step-by-Step Interpretation\n",
    "Look at the Loadings (aka Component Weights)\n",
    "\n",
    "Use pca.components_ from sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe15552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assume you have a PCA model and feature names\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,  # transpose to align features in rows\n",
    "    columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "    index=feature_names  # list of original feature names\n",
    ")\n",
    "\n",
    "print(loadings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ebe3a",
   "metadata": {},
   "source": [
    "Interpret Each PC\n",
    "- Identify the top contributing features (those with highest absolute loadings).\n",
    "- See if those features have thematic similarity (e.g., income-related, age-related).\n",
    "\n",
    "Example Interpretation\n",
    "- Let’s say for customer demographic features:\n",
    "\n",
    "|          | PC1    | PC2    | PC3|\n",
    "|----------|--------|--------|----|\n",
    "|Age      |0.52   |-0.02   |0.13 |\n",
    "|Income   | 0.49  |  0.18  | 0.23|\n",
    "|Spending | 0.51  | -0.45  |-0.09|\n",
    "|Visits   |-0.11  |  0.85  | 0.30|\n",
    "\n",
    "🔹 Interpretation:\n",
    "\n",
    "PC1 (first principal component):\n",
    "- High loadings on Age, Income, Spending\n",
    "- Interpretation: a general economic status dimension\n",
    "\n",
    "PC2:\n",
    "- High positive weight on Visits, negative on Spending\n",
    "- Interpretation: maybe frequency vs amount of spending\n",
    "\n",
    "PC3:\n",
    "- Small or mixed values — might not be as easy to interpret or might capture minor variations\n",
    "\n",
    "Heatmap of Loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec65e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.heatmap(loadings, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Principal Component Loadings\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5f517b",
   "metadata": {},
   "source": [
    "🧠 Key Tips:\n",
    "- Signs of loadings matter for direction but absolute values matter for strength of contribution.\n",
    "- Group features thematically when interpreting PCs.\n",
    "- If PCs are hard to interpret, you might try rotated PCA (e.g., varimax) for better interpretability.\n",
    "\n",
    "###  Characteristics of the First Principal Component\n",
    "The first principal component (PC1) plays a central role in Principal Component Analysis (PCA). Here's a detailed explanation of its characteristics, how it’s derived, and why it’s important:\n",
    "\n",
    "🔷 Characteristics of the First Principal Component (PC1)\n",
    "\n",
    "✅ 1. Direction of Maximum Variance\n",
    "- PC1 captures the largest possible variance in the dataset.\n",
    "- It identifies the single best direction in feature space along which data points are most spread out.\n",
    "\n",
    "$$ PC1 = arg \\max_{||w|| = 1} Var(Xw)$$\n",
    "- Where:\n",
    "    - w s the direction (vector of weights), and X is the standardized data.\n",
    "\n",
    "✅ 2. Linear Combination of Original Features\n",
    "- PC1 is a weighted sum of the original features:\n",
    "$$ PC_1 = w_{1} x_1 + w_{2} x_2 + ... + w_{d} x_d $$\n",
    "- The weights $w_i$ (called loadings) tell you how much each feature contributes to PC1.\n",
    "- Features with higher absolute weights contribute more.\n",
    "\n",
    "✅ 3. Orthogonality\n",
    "- PC1 is orthogonal (statistically uncorrelated) to all subsequent principal components (PC2, PC3...).\n",
    "- This ensures no redundancy between components.\n",
    "\n",
    "✅ 4. Eigenvalue Correspondence\n",
    "- PC1 corresponds to the largest eigenvalue of the covariance matrix.\n",
    "- The associated eigenvector is the direction (loading vector) of PC1.\n",
    "\n",
    "✅ 5. Data Projection and Interpretation\n",
    "- Data projected onto PC1 (i.e., dot product of data with PC1) gives you a 1D representation capturing maximum variance.\n",
    "- Often used for visualization and pattern discovery.\n",
    "\n",
    "✅ 6. Explained Variance\n",
    "- The explained variance ratio for PC1 shows how much of the total variance it captures:\n",
    "$$ \\text{Explained Variance Ratio of PC1} = \\frac{\\lambda_{1}}{\\sum^d_{i = 1} \\lambda_i} $$\n",
    "- Where:\n",
    "    - $\\lambda$ is the eigenvalue corresponding to PC1.\n",
    "\n",
    "A high ratio (e.g., 70%+) indicates that PC1 alone gives a strong summary of the dataset.\n",
    "\n",
    "🧠 Intuition Example:\n",
    "- Suppose you have data on age, income, and spending score.\n",
    "- PC1 might load highly on income and spending, and low on age.\n",
    "- This could mean PC1 represents an \"economic engagement\" dimension.\n",
    "- Individuals with high PC1 scores are higher spenders and earners.\n",
    "\n",
    "##### Computing and interpreting the first principal component (PC1) using a realistic customer demographic dataset\n",
    "use features like:\n",
    "- Age\n",
    "- Income\n",
    "- Spending Score\n",
    "- Visit Frequency\n",
    "\n",
    "These are common in customer segmentation.\n",
    "\n",
    "✅ Step-by-Step: Compute and Interpret PC1\n",
    "🔹 Step 1: Import Libraries and Simulate Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ad38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Simulated customer demographic data\n",
    "data = pd.DataFrame({\n",
    "    'Age': [25, 45, 31, 35, 52, 23, 40, 60, 48, 33],\n",
    "    'Income': [40000, 80000, 52000, 58000, 100000, 39000, 70000, 120000, 85000, 61000],\n",
    "    'SpendingScore': [60, 30, 50, 40, 20, 65, 35, 15, 25, 45],\n",
    "    'VisitFrequency': [4, 2, 3, 3, 1, 5, 2, 1, 1, 3]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc73245",
   "metadata": {},
   "source": [
    "🔹 Step 2: Standardize the Data\n",
    "\n",
    "PCA is sensitive to scale, so we use z-score normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e2e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a13b7",
   "metadata": {},
   "source": [
    "🔹 Step 3: Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63843d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a DataFrame of PCA results\n",
    "pca_df = pd.DataFrame(pca_result, columns=[f'PC{i+1}' for i in range(len(data.columns))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a2578",
   "metadata": {},
   "source": [
    "🔹 Step 4: Interpret PC1 (Loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4974fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loadings (components)\n",
    "loadings = pd.DataFrame(pca.components_.T,\n",
    "                        index=data.columns,\n",
    "                        columns=[f'PC{i+1}' for i in range(len(data.columns))])\n",
    "\n",
    "print(\"Loadings for PC1:\")\n",
    "print(loadings['PC1'].sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0dab00",
   "metadata": {},
   "source": [
    "This tells us how much each original feature contributes to PC1.\n",
    "\n",
    "✅ Example output (interpretation):\n",
    "\n",
    "🔍 Interpretation:\n",
    "- PC1 contrasts Income & Age (positive) with Spending Score & Visit Frequency (negative).\n",
    "- A high PC1 score = older, higher-income customers who spend and visit less.\n",
    "- A low PC1 score = younger customers with lower income who spend more and visit often.\n",
    "\n",
    "🔹 Step 5: Explained Variance\n",
    "- This gives the proportion of total variance captured by PC1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Explained Variance Ratio for PC1:\", pca.explained_variance_ratio_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7ad3e3",
   "metadata": {},
   "source": [
    "🔹 Step 6: Optional – Visualize Customers in PC Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce25d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'])\n",
    "plt.title(\"Customer Distribution in PC1 vs PC2 Space\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.axvline(0, color='gray', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998f59f7",
   "metadata": {},
   "source": [
    "###  What Does It Mean to \"Project a Point onto a Line\"?\n",
    "\n",
    "➤ In simple terms:\n",
    "- When you project a point onto a line, you are dropping a perpendicular from the point to the line and identifying the location on the line where this perpendicular lands.\n",
    "- That new point on the line is the projection.\n",
    "\n",
    "🧠 Why Do We Do This?\n",
    "- Projection answers the question:\n",
    "    - \"If I could represent this point using just this line, where would it fall?\"\n",
    "- In PCA, the line might be a principal component (like PC1), and the original point is a data point in high-dimensional space.\n",
    "\n",
    "🔢 Mathematically Speaking\n",
    "\n",
    "Let’s say:\n",
    "- You have a vector x (your point), And a line through the origin in the direction of a unit vector u.\n",
    "\n",
    "➤ The projection of x onto u is:\n",
    "$$ proj_u(x) = (x \\cdot u) \\cdot u$$\n",
    "- Where:\n",
    "    - $𝑥 \\cdot 𝑢$ is the dot product of x and u,\n",
    "    - This gives the length of the projection in the direction of u,\n",
    "    - Then we multiply by u to get the vector location on the line.\n",
    "\n",
    "📍Geometric Meaning\n",
    "- Imagine a flashlight shining perpendicular to the line u.\n",
    "- The shadow of point x on that line is the projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a865bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([3, 4])      # our point\n",
    "u = np.array([1, 0])      # x-axis unit direction\n",
    "\n",
    "u = u / np.linalg.norm(u)   # ensure unit length\n",
    "proj = np.dot(x, u) * u\n",
    "\n",
    "print(\"Projection:\", proj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927241ea",
   "metadata": {},
   "source": [
    "➡️ This says: If we only cared about the x-axis, then the point [3, 4] becomes [3, 0] when projected.\n",
    "\n",
    "🔷 In PCA: Why is Projection Important?\n",
    "\n",
    "PCA finds the directions (principal components) that explain variance. Then:\n",
    "- Each data point is projected onto these directions,\n",
    "- Those projections become the new features (PC1, PC2, etc.),\n",
    "- This reduces dimensions and retains meaningful variation.\n",
    "\n",
    "So, when you reduce from 10 features to 2, you're saying:\n",
    "- \"I want the 2D projections of all points on the best two lines capturing the most spread.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8589fff6",
   "metadata": {},
   "source": [
    "### Compute loadings for principal components in Principal Component Analysis (PCA)\n",
    "This solves an optimization problem with certain mathematical constraints. These constraints ensure that the solution is unique, meaningful, and preserves the geometry of the data.\n",
    "\n",
    "##### **🔷 What Are Loadings?**\n",
    "- Loadings are the coefficients or weights assigned to each original variable when forming a principal component.\n",
    "- Each principal component is a linear combination of the original variables.\n",
    "$$ PC_1 = w_{1} x_1 + w_{2} x_2 + ... + w_{p} x_p $$\n",
    "- Where \n",
    "    - w = [$w_1, w_2, ..., w_p$] is the loading vector for PC1.\n",
    "\n",
    "##### **🔧 The Optimization Problem in PCA**\n",
    "We want to maximize the variance of the projected data on the principal component, i.e.:\n",
    "$$ \\text{maximize Var (Xw)} = w^T S_w $$\n",
    "- Where:\n",
    "    - X is the standardized data matrix\n",
    "    - S is the covariance matrix of X,\n",
    "    - w is the vector of loadings or weights.\n",
    "\n",
    "##### **✅ The Key Constraint: Unit Length**\n",
    "$$ w^T w = 1 $$\n",
    "\n",
    "The loading vector must be a unit vector (i.e., it has length 1).\n",
    "\n",
    "❓ Why this constraint?\n",
    "- It prevents the trivial solution where you just scale the weights up indefinitely to increase variance.\n",
    "- It ensures that the principal component is a direction, not a magnitude.\n",
    "- Without it, the optimization would be unbounded (you could keep increasing variance by increasing w).\n",
    "\n",
    "🔄 Orthogonality Constraints (for subsequent components)\n",
    "- After computing the first principal component:\n",
    "    - The second component must be orthogonal (perpendicular) to the first:\n",
    "$$ w_1^T w_2 = 0 $$\n",
    "- And this continues for PC3, PC4, etc.\n",
    "- So for each new component $𝑤_𝑘$, the constraint is:\n",
    "$$ w_k^T w_j = 0  \\text{for all j < k}$$\n",
    "\n",
    "This ensures that each component captures new, uncorrelated information.\n",
    "\n",
    "🔑 Summary of Constraints\n",
    "|Constraint Type\t|Mathematical Expression\t|Purpose  |\n",
    "|-------------------|---------------------------|---------|\n",
    "|Unit length  | $𝑤^𝑇 𝑤 = 1$ | Normalize the direction vector|\n",
    "|Orthogonality\t| $ w_k^T w_j = 0  \\text{for all j < k}$| \tEnsure components are uncorrelated|\n",
    "|Maximize variance\t| $\\text{maximize} 𝑤^𝑇 𝑆_𝑤$ |Capture most information in fewer dimensions|\n",
    "\n",
    "💡 Insight\n",
    "These constraints lead to a classic eigenvalue problem. When you solve:\n",
    "$$ Sw = \\lambda w$$\n",
    "- Where:   \n",
    "    - $\\lambda$ : the eigenvalue (variance explained),\n",
    "    - w : the eigenvector (loading vector).\n",
    "\n",
    "##### Step-by-step example in Python to compute the loadings (eigenvectors) for principal components using eigenvalue decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries and Create the Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a small synthetic dataset\n",
    "data = {\n",
    "    'Age': [25, 30, 45, 35, 23],\n",
    "    'Income': [50000, 60000, 80000, 72000, 45000],\n",
    "    'SpendingScore': [60, 70, 45, 55, 65]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# Step 2: Standardize the Data (important for PCA!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "print(\"Standardized Data:\\n\", X_scaled)\n",
    "\n",
    "# Step 3: Compute the Covariance Matrix\n",
    "# PCA is based on the covariance matrix of the data.\n",
    "\n",
    "# Compute the covariance matrix\n",
    "cov_matrix = np.cov(X_scaled.T)  # Transpose to get features as rows\n",
    "print(\"Covariance Matrix:\\n\", cov_matrix)\n",
    "\n",
    "# Step 4: Compute Eigenvalues and Eigenvectors\n",
    "# This gives us the variance explained and the loadings.\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "print(\"Eigenvalues:\\n\", eigenvalues)\n",
    "print(\"Eigenvectors (loadings):\\n\", eigenvectors)\n",
    "\n",
    "# Output\n",
    "\n",
    "Eigenvectors = (columns = PC1, PC2, PC3):\n",
    "[[ 0.58, -0.80, 0.12],\n",
    " [ 0.58,  0.52, 0.63],\n",
    " [ 0.58,  0.27, -0.77]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbb5de",
   "metadata": {},
   "source": [
    "Interpreting the Output\n",
    "- Eigenvalues represent the amount of variance explained by each component.\n",
    "- Eigenvectors are the loadings — the weights for original features that define each principal component.\n",
    "\n",
    "Each column in eigenvectors is a principal component.\n",
    "\n",
    "The first principal component (PC1) is:\n",
    "$$PC1=0.58⋅Age+0.58⋅Income+0.58⋅SpendingScore $$\n",
    "\n",
    "→ This suggests all features contribute equally to PC1.\n",
    "\n",
    "The second component (PC2) contrasts Age (negatively) with Income and Spending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e474f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Project the Data onto Principal Components\n",
    "# Now that we have eigenvectors, we project the data to get PCA-transformed values.\n",
    "\n",
    "# Project the original standardized data onto the principal components\n",
    "X_pca = X_scaled @ eigenvectors\n",
    "\n",
    "# Put into DataFrame for inspection\n",
    "pca_df = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "print(pca_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f70cb4",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA) can be interpreted in two complementary ways\n",
    "\n",
    "##### **Primary Interpretation of PCA (Variance Maximization)**\n",
    "This is the most common interpretation of PCA:\n",
    "- PCA finds new axes (principal components) such that the first captures the most variance in the data, the second captures the most remaining variance orthogonal to the first, and so on.\n",
    "- Each principal component is a linear combination of the original features.\n",
    "- The first principal component (PC1) is the direction in feature space along which the data varies the most.\n",
    "- The components are orthogonal (uncorrelated) to each other.\n",
    "- You reduce dimensions by keeping the top k components that explain most variance.\n",
    "\n",
    "##### **Alternative Interpretation: Projection onto a Low-Dimensional Subspace**\n",
    "This perspective comes from linear algebra and reconstruction error:\n",
    "- PCA finds a lower-dimensional subspace that best approximates the original data by minimizing the reconstruction error (i.e., minimizing the squared distance between original data and its projection).\n",
    "\n",
    "🔍 What does this mean?\n",
    "- Suppose you project high-dimensional data (say 10D) onto a lower-dimensional space (say 2D).\n",
    "- PCA finds the 2D plane that best approximates the original data — where the orthogonal (perpendicular) projection of each data point onto this plane results in minimal loss of information.\n",
    "- This is equivalent to compressing the data and reconstructing it back with the least squared error.\n",
    "\n",
    "In mathematical terms:\n",
    "- PCA minimizes:\n",
    "$$ \\sum || x_i - \\hat{x}_i ||^2 $$\n",
    "- Where:\n",
    "    - $x_i$  is the original point, and  \n",
    "    - $\\hat{x_i}$ is its projection onto the low-dimensional subspace.\n",
    "\n",
    "Comparing the Two Interpretations\n",
    "\n",
    "|Perspective\t|Description\t|Optimization Goal|\n",
    "|---------------|---------------|-----------------|\n",
    "|Variance Maximization\t|Finds directions with most variance\t|Maximize variance captured  |\n",
    "|Projection View (Reconstruction)\t|Finds best low-rank approximation of the data\t|Minimize reconstruction error  |\n",
    "\n",
    "Both interpretations are mathematically equivalent — they lead to the same principal components via eigen decomposition or SVD — but provide different intuitions.\n",
    "\n",
    "🎓 When to Use Which Interpretation?\n",
    "- Use variance-based interpretation when your goal is data exploration, dimensionality reduction, or identifying important directions in the data.\n",
    "- Use projection-based interpretation when you're thinking about compression, reconstruction, or low-rank approximations (e.g., in image compression or latent variable models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3896fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate a simple 2D dataset with correlated features\n",
    "np.random.seed(42)\n",
    "x = np.random.normal(0, 1, 100)\n",
    "y = 2 * x + np.random.normal(0, 0.5, 100)  # Strong linear relationship\n",
    "data = np.vstack((x, y)).T\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Fit PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "data_reconstructed = pca.inverse_transform(data_pca)\n",
    "\n",
    "# Principal component directions\n",
    "components = pca.components_\n",
    "\n",
    "data_mean = np.mean(data_scaled, axis=0)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Original data and principal component directions\n",
    "ax[0].scatter(data_scaled[:, 0], data_scaled[:, 1], alpha=0.5, label='Original Data')\n",
    "for length, vector in zip(pca.explained_variance_, components):\n",
    "    v = vector * 2 * np.sqrt(length)\n",
    "    ax[0].arrow(data_mean[0], data_mean[1], v[0], v[1],\n",
    "                color='red', head_width=0.1, label='Principal Component')\n",
    "ax[0].set_title(\"Original Data with Principal Components\")\n",
    "ax[0].axis('equal')\n",
    "ax[0].legend()\n",
    "\n",
    "# Projection view (reconstruction from first PC only)\n",
    "pca_1d = PCA(n_components=1)\n",
    "data_pca_1d = pca_1d.fit_transform(data_scaled)\n",
    "data_reconstructed_1d = pca_1d.inverse_transform(data_pca_1d)\n",
    "\n",
    "ax[1].scatter(data_scaled[:, 0], data_scaled[:, 1], alpha=0.5, label='Original Data')\n",
    "ax[1].scatter(data_reconstructed_1d[:, 0], data_reconstructed_1d[:, 1], color='red', alpha=0.8, label='Reconstructed (1 PC)')\n",
    "for i in range(len(data_scaled)):\n",
    "    ax[1].plot([data_scaled[i, 0], data_reconstructed_1d[i, 0]],\n",
    "               [data_scaled[i, 1], data_reconstructed_1d[i, 1]], 'gray', linewidth=0.5)\n",
    "ax[1].set_title(\"Projection View (Minimizing Reconstruction Error)\")\n",
    "ax[1].axis('equal')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d339adde",
   "metadata": {},
   "source": [
    "##### Left Plot: Variance Maximization View\n",
    "- The red arrows show the principal components.\n",
    "- The first principal component (longer arrow) captures the most variation in the data — it's aligned with the diagonal trend of the data cloud.\n",
    "- This aligns with the variance maximization perspective.\n",
    "\n",
    "##### Right Plot: Projection & Reconstruction View\n",
    "- Red points are the reconstructions using only the first principal component.\n",
    "- Gray lines connect the original data points to their projections on the first PC.\n",
    "- PCA chooses this line to minimize the total squared distance (reconstruction error).\n",
    "- This aligns with the low-rank approximation view.\n",
    "\n",
    "\n",
    "### 📌 What Is the First Principal Component (PC1)?\n",
    "The first principal component (PC1) is the linear combination of the original features that:\n",
    "- Captures the largest amount of variability (or variance) in the data.\n",
    "- Defines the direction along which the data varies the most.\n",
    "- Is a weighted sum of the original features (based on eigenvectors of the covariance matrix).\n",
    "\n",
    "🔍 What Does \"Information Content\" Mean?\n",
    "In PCA, \"information content\" refers to the amount of total variance in the dataset that is captured by a principal component.\n",
    "\n",
    "So:\n",
    "\n",
    "🔹 The first principal component captures the maximum possible variance that any single axis in the feature space can capture.\n",
    "\n",
    "Mathematically:\n",
    "- Let total variance = sum of variances of all principal components = trace of covariance matrix.\n",
    "- The explained variance ratio of PC1:\n",
    "\n",
    "$$ \\text{ Explained Variance Ratio (PC1)} = \\frac{\\lambda_1}{\\sum^p_{j = 1} \\lambda_j}$$\n",
    "- where:\n",
    "    - $\\lambda$ is the eigenvalue for PC1, and  \n",
    "    - $\\lambda_j$ are eigenvalues for all components.\n",
    "\n",
    "🧠 Interpreting the Information Content\n",
    "- If PC1 explains 80% of the variance, it means the spread of data along this one direction retains 80% of the original data structure.\n",
    "- The features with the highest loadings (coefficients) in PC1 are the most influential in defining this primary direction of variability.\n",
    "- Often, PC1 can reveal:\n",
    "    - Customer segments based on behavior patterns.\n",
    "    - Key driving factors (e.g., income or age) in a population.\n",
    "    - Dominant relationships among correlated features.\n",
    "\n",
    "📊 Example: Customer Demographic Data\n",
    "\n",
    "Suppose PC1 has high loadings for:\n",
    "- income (+0.7),\n",
    "- education level (+0.6), and\n",
    "- low for others.\n",
    "\n",
    "Then:\n",
    "- PC1 is essentially a \"socioeconomic status\" component, and the major variation in customer data is explained by this latent factor.\n",
    "\n",
    "✅ Summary\n",
    "|Aspect|\tDescription|\n",
    "|------|---------------|\n",
    "|PC1|\tDirection of maximum variance in the data|\n",
    "|Information content|\t% of total variance PC1 explains|\n",
    "|Loadings (weights)|\tShow which original features contribute most|\n",
    "|Interpretation|\tTells you what underlying factor drives the largest differences in your data|\n",
    "\n",
    "### Effect of Zero Correlation Between the First and Second Principal Component\n",
    "🔹 What Does It Mean?\n",
    "\n",
    "In PCA, the principal components (PCs) are constructed to be:\n",
    "- Orthogonal to each other (i.e., 90° apart in high-dimensional space).\n",
    "- Therefore, they are uncorrelated with one another.\n",
    "\n",
    "So when we say:\n",
    "- ❝The first and second principal components have zero correlation❞\n",
    "It means:\n",
    "- Changes in the first component do not predict changes in the second.\n",
    "- They capture independent patterns of variation in the data.\n",
    "\n",
    "Mathematically, the covariance between PC1 and PC2 is zero:\n",
    "$$ Cov(PC1, PC2) = 0$$\n",
    "\n",
    "##### 🔍 Why Is This Important?\n",
    "\n",
    "No Redundant Information:\n",
    "- Each principal component contributes unique variance not captured by earlier components.\n",
    "\n",
    "Simplifies Analysis:\n",
    "- You can study PC1, PC2, etc., separately without worrying about multicollinearity.\n",
    "\n",
    "Enables Orthogonal Projection:\n",
    "- Data can be projected onto these components like independent axes of variation.\n",
    "\n",
    "##### 📊 Visual Interpretation\n",
    "Imagine a PCA plot where:\n",
    "- PC1 is the x-axis.\n",
    "- PC2 is the y-axis.\n",
    "\n",
    "If these components are uncorrelated, then:\n",
    "- Data spread in the PC1 direction is independent of spread in the PC2 direction.\n",
    "- The scatterplot appears as a cloud without diagonal structure.\n",
    "\n",
    "🧠 Analogy\n",
    "- Think of a customer segmentation PCA:\n",
    "    - PC1 might represent “wealth-related variation” (income, credit score).\n",
    "    - PC2 might represent “age-related variation” (age, family size).\n",
    "- They capture different, independent traits:\n",
    "    - High-income customers could be young or old — there's no direct relationship between PC1 and PC2.\n",
    "\n",
    "✅ Summary Table\n",
    "|Property|\tImplication|\n",
    "|--------|-------------|\n",
    "|Zero correlation|\tEach PC adds unique, non-overlapping information|\n",
    "|Orthogonality|\tVectors (directions) are 90° apart|\n",
    "|No multicollinearity|\tPCs are uncorrelated; simplifies regression and clustering|\n",
    "|Interpretation|\tHelps isolate underlying dimensions of variation|\n",
    "\n",
    "### ✅ What Does \"Orthogonal\" Mean in PCA?\n",
    "In PCA, the principal components (PCs) are constructed such that:\n",
    "\n",
    "🔹 Each principal component is orthogonal to (i.e., at a 90° angle from) every other component.\n",
    "Mathematically, orthogonality means:\n",
    "\n",
    "$$ PC_i \\cdot PC_j = 0 \\text{ for all } i \\neq j $$\n",
    "\n",
    "This is the dot product = 0, which implies the vectors are perpendicular.\n",
    "\n",
    "### Why Is Orthogonality Important in PCA?\n",
    "\n",
    "|Benefit\t|Explanation |\n",
    "|-----------|------------|\n",
    "|Uncorrelated Components|\tOrthogonality ensures each principal component captures a unique, independent direction of variance in the data. There is no redundancy.|\n",
    "|Simplifies Interpretation|\tBecause there's no overlap, you can analyze one PC without worrying about its relationship to the others.|\n",
    "|Avoids Multicollinearity|\tOrthogonal PCs are ideal inputs for downstream models (e.g., regression), because they’re not collinear.|\n",
    "|Geometric Simplicity|\tOrthogonal axes allow you to decompose the data into independent geometric directions — like projecting a shadow onto a set of perpendicular lines.|\n",
    "\n",
    "🎯 Geometric Intuition\n",
    "\n",
    "Imagine PCA in 2D:\n",
    "- PC1 = line that best fits the spread of data (max variance).\n",
    "- PC2 = next best line that’s perpendicular to PC1 and captures the remaining variance.\n",
    "\n",
    "So if PC1 runs diagonally ↑↗, then PC2 must run ↖←, at 90° to PC1.\n",
    "- ✅ The PCA algorithm forces this perpendicularity when solving for eigenvectors.\n",
    "\n",
    "### 🧠 Mathematical Link\n",
    "- PCA uses eigenvectors of the covariance matrix of the data.\n",
    "- The eigenvectors of a symmetric matrix (like the covariance matrix) are always orthogonal.\n",
    "- So, PCA naturally produces orthogonal components because of how the math works.\n",
    "\n",
    "##### 💡 Big Picture First\n",
    "- PCA (Principal Component Analysis) is built on linear algebra:\n",
    "- It finds eigenvectors and eigenvalues of the covariance matrix of your data.\n",
    "\n",
    "These eigenvectors become the principal components — the directions along which the data varies the most.\n",
    "\n",
    "🔹 What is a Covariance Matrix?\n",
    "- The covariance matrix tells you how much two features vary together.\n",
    "- For a dataset 𝑋 with 𝑛 observations and p features (columns), the covariance matrix $\\sum$ is a 𝑝 $\\times$ matrix where:\n",
    "$$ \\sum_{ij} = Cov(X_i, Y_j) $$\n",
    "\n",
    "So:\n",
    "- The diagonal entries are the variances of each feature.\n",
    "- The off-diagonal entries are the covariances between features.\n",
    "\n",
    "👉 Example:\n",
    "Suppose your features are:\n",
    "- Age\n",
    "- Income\n",
    "- Spending Score\n",
    "\n",
    "🔹 What are Eigenvectors and Eigenvalues?\n",
    "- Given a square matrix (like the covariance matrix), an eigenvector is a vector that doesn’t change direction when the matrix is applied to it — it only gets stretched or squished.\n",
    "$$ A \\cdot v = \\lambda \\cdot v $$\n",
    "- Where:\n",
    "    - A is your covariance matrix.\n",
    "    - 𝑣 is the eigenvector.\n",
    "    - $\\lambda$ is the eigenvalue (a scalar).\n",
    "\n",
    "🔑 Eigenvectors point in the direction of the data's main axes of variation.\n",
    "\n",
    "🔑 Eigenvalues tell you how much variance is along each eigenvector.\n",
    "\n",
    "🔹 How This Applies to PCA\n",
    "\n",
    "Step 1: Standardize the Data\n",
    "- You often start by scaling your features (z-score or min-max).\n",
    "\n",
    "Step 2: Compute Covariance Matrix\n",
    "- Get the relationships between features.\n",
    "\n",
    "Step 3: Compute Eigenvalues & Eigenvectors\n",
    "- These define the new axes (principal components).\n",
    "- Each eigenvector becomes a principal component direction.\n",
    "- Each eigenvalue tells how important that direction is.\n",
    "\n",
    "Imagine you plot 2D data like this:\n",
    "- PC1 is the direction with the maximum spread of data — it aligns with the first eigenvector. \n",
    "- PC2 is perpendicular and captures the next most variance.\n",
    "\n",
    "📊 Practical Implication: Uncorrelated Features\n",
    "- Even if your original features (like income, age, and education) are correlated, the principal components are not.\n",
    "\n",
    "|Original Feature Space|\tPCA-Transformed Space |\n",
    "|Variables may be correlated|\tComponents are orthogonal (uncorrelated)|\n",
    "|Complex relationships|\tClean, independent axes of variation|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5099d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Fake data\n",
    "X = np.array([[2.5, 2.4],\n",
    "              [0.5, 0.7],\n",
    "              [2.2, 2.9],\n",
    "              [1.9, 2.2],\n",
    "              [3.1, 3.0]])\n",
    "\n",
    "# 1. Standardize\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "# 2. Compute covariance matrix\n",
    "cov_matrix = np.cov(X_std.T)\n",
    "\n",
    "# 3. Eigen decomposition\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "print(\"Covariance Matrix:\\n\", cov_matrix)\n",
    "print(\"\\nEigenvalues:\\n\", eigenvalues)\n",
    "print(\"\\nEigenvectors:\\n\", eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce879ce",
   "metadata": {},
   "source": [
    "This tells us:\n",
    "- PC1 = vector [0.707, 0.707] → explains 95% of variance\n",
    "- PC2 = vector [-0.707, 0.707] → explains 5% of variance\n",
    "\n",
    "✅ Summary\n",
    "|Concept|\tMeaning|\n",
    "|-------|----------|\n",
    "|Covariance Matrix|\tMeasures how features vary together|\n",
    "|Eigenvectors|\tPrincipal directions of variance (axes of PCA)|\n",
    "|Eigenvalues|\tMagnitude of variance along each direction|\n",
    "|Orthogonality|\tPCA components are perpendicular (uncorrelated)|\n",
    "|Why it matters|\tEnables dimension reduction while preserving structure|\n",
    "\n",
    "##### Visualize the eigenvectors as arrows over your data to see how PCA rotates the space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Original 2D dataset\n",
    "X = np.array([[2.5, 2.4],\n",
    "              [0.5, 0.7],\n",
    "              [2.2, 2.9],\n",
    "              [1.9, 2.2],\n",
    "              [3.1, 3.0],\n",
    "              [2.3, 2.7],\n",
    "              [2, 1.6],\n",
    "              [1, 1.1],\n",
    "              [1.5, 1.6],\n",
    "              [1.1, 0.9]])\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Step 2: Compute covariance matrix\n",
    "cov_matrix = np.cov(X_std.T)\n",
    "\n",
    "# Step 3: Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_std[:, 0], X_std[:, 1], alpha=0.6)\n",
    "origin = np.mean(X_std, axis=0)  # center of data\n",
    "\n",
    "# Plot eigenvectors\n",
    "for i in range(len(eigenvectors)):\n",
    "    vec = eigenvectors[:, i]\n",
    "    plt.quiver(*origin, vec[0], vec[1], angles='xy', scale_units='xy', scale=1.5, color=['red', 'green'][i], label=f'PC{i+1}')\n",
    "\n",
    "plt.axhline(0, color='gray', linewidth=0.5)\n",
    "plt.axvline(0, color='gray', linewidth=0.5)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.title('Data with Principal Component Directions')\n",
    "plt.xlabel('Standardized Feature 1')\n",
    "plt.ylabel('Standardized Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317f293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Original 2D dataset\n",
    "X = np.array([[2.5, 2.4],\n",
    "              [0.5, 0.7],\n",
    "              [2.2, 2.9],\n",
    "              [1.9, 2.2],\n",
    "              [3.1, 3.0],\n",
    "              [2.3, 2.7],\n",
    "              [2, 1.6],\n",
    "              [1, 1.1],\n",
    "              [1.5, 1.6],\n",
    "              [1.1, 0.9]])\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Step 2: Compute covariance matrix\n",
    "cov_matrix = np.cov(X_std.T)\n",
    "\n",
    "# Step 3: Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_std[:, 0], X_std[:, 1], alpha=0.6)\n",
    "origin = np.mean(X_std, axis=0)  # center of data\n",
    "\n",
    "# Plot eigenvectors\n",
    "for i in range(len(eigenvectors)):\n",
    "    vec = eigenvectors[:, i]\n",
    "    plt.quiver(*origin, vec[0], vec[1], angles='xy', scale_units='xy', scale=1.5, color=['red', 'green'][i], label=f'PC{i+1}')\n",
    "\n",
    "plt.axhline(0, color='gray', linewidth=0.5)\n",
    "plt.axvline(0, color='gray', linewidth=0.5)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.title('Data with Principal Component Directions')\n",
    "plt.xlabel('Standardized Feature 1')\n",
    "plt.ylabel('Standardized Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e1704",
   "metadata": {},
   "source": [
    "✅ Covariance Matrix\n",
    "- The covariance matrix represents the relationships between each pair of features.\n",
    "- In this 2D case, it shows how Feature 1 and Feature 2 vary together.\n",
    "- If the off-diagonal values are large, it suggests strong correlation.\n",
    "\n",
    "✅ Eigenvectors of the Covariance Matrix\n",
    "- The eigenvectors point in the directions of the principal components (PCs).\n",
    "- These are new axes that are orthogonal (perpendicular) and aligned with the directions of maximum variance in the data.\n",
    "\n",
    "✅ What You See in the Plot\n",
    "- Red Arrow (PC1): The first principal component—points in the direction where the data varies the most. It's the major \"trend\" in the data.\n",
    "- Green Arrow (PC2): The second principal component—perpendicular to PC1, capturing the second-most variance.\n",
    "\n",
    "✅ Summary\n",
    "|Term|\tMeaning|\n",
    "|------|----------|\n",
    "|Eigenvector|\tDirection of a principal component (axis of maximum variance)|\n",
    "|Eigenvalue|\tMagnitude of variance captured by its corresponding eigenvector|\n",
    "|PC1|\tCaptures the most variance|\n",
    "|PC2|\tOrthogonal to PC1, captures remaining variance.|\n",
    "\n",
    "##### Compute how much variance each PC explains\n",
    "\n",
    "✅ Explained Variance Ratio (How much variance each principal component captures):\n",
    "- PC1 explains 96.3% of the total variance.\n",
    "- PC2 explains only 3.7% of the total variance.\n",
    "\n",
    "Together, they account for 100% of the variance (since we only have 2 dimensions).\n",
    "\n",
    "📌 Interpretation\n",
    "- Almost all the information in the dataset is captured by the first principal component.\n",
    "- This means the data can effectively be reduced to 1 dimension without much loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d148866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the explained variance ratio\n",
    "explained_variance_ratio = eigenvalues / np.sum(eigenvalues)\n",
    "\n",
    "# Show explained variance of each principal component\n",
    "explained_variance_ratio, np.cumsum(explained_variance_ratio)  # cumulative sum also helpful to decide how many PCs to keep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dc9df1",
   "metadata": {},
   "source": [
    "### Expected Information Content of the Second Principal Component (PC2)\n",
    "When you perform PCA on a dataset with two predictors (let’s say $𝑋_1$ and $𝑋_2$):\n",
    "\n",
    "You will always get two principal components:\n",
    "- PC1 (first principal component): Captures the most variance possible in the data.\n",
    "- PC2 (second principal component): Captures the remaining variance that is uncorrelated with PC1.\n",
    "\n",
    "🔍 Key Properties of PC2\n",
    "\n",
    "Orthogonal to PC1:\n",
    "- PC2 is at a right angle (perpendicular) to PC1.\n",
    "- Ensures no overlap of information between PC1 and PC2 (i.e., zero correlation).\n",
    "\n",
    "Captures Remaining Variance:\n",
    "- If the original two features are highly correlated, PC1 captures most of the variance, and PC2 will contain very little.\n",
    "\n",
    "This is exactly what we saw in your data:\n",
    "- PC1: 96.3% of variance\n",
    "- PC2: 3.7% of variance\n",
    "\n",
    "So, PC2 doesn’t carry much useful information unless the features are not correlated.\n",
    "\n",
    "Usefulness:\n",
    "- If you want a compressed representation (e.g. for visualization or speed), you might drop PC2 entirely.\n",
    "- But if small variations matter (e.g. in anomaly detection or fine classification), PC2 may still be important.\n",
    "\n",
    "📊 Visual Intuition\n",
    "\n",
    "Think of it like this:\n",
    "- If your data is shaped like a long ellipse, PC1 lies along the major axis, PC2 along the minor axis.\n",
    "- PC2 contains the tiny spread or scatter orthogonal to the main trend—it’s like measuring the \"noise\" or \"deviation\" from the main direction.\n",
    "\n",
    "### Relationship Between Number of Principal Components and Number of Features\n",
    "In Principal Component Analysis (PCA), the number of principal components (PCs) you can extract is bounded by the number of original features (variables).\n",
    "\n",
    "✅ General Rule:\n",
    "\n",
    "If your data has:\n",
    "- p features (columns or variables),\n",
    "- Then you can extract at most p principal components.\n",
    "\n",
    "So:\n",
    "- If you have 5 features → at most 5 principal components.\n",
    "- If you have 10 features → at most 10 principal components.\n",
    "\n",
    "Each principal component is a linear combination of all the original features.\n",
    "\n",
    "✅ Why This Limit?\n",
    "\n",
    "Mathematically:\n",
    "- PCA is based on the eigen-decomposition of the covariance matrix.\n",
    "    - A covariance matrix of shape p×p can have at most p eigenvectors and eigenvalues.\n",
    "- Therefore, it can yield at most p principal components.\n",
    "\n",
    "✅ Choosing Fewer Components\n",
    "- Although you can extract all p components, you typically keep only a subset, say k < p, based on how much variance each component explains.\n",
    "\n",
    "This allows you to:\n",
    "- Reduce dimensionality\n",
    "- Remove noise or redundant information\n",
    "- Speed up models\n",
    "- Improve interpretability\n",
    "\n",
    "🧠 Intuition\n",
    "|Term|\tMeaning|\n",
    "|-----|--------|\n",
    "|Original Features (p)|\tRaw variables in your dataset|\n",
    "|Principal Components (≤ p)|\tNew axes that are combinations of the original features, ordered by variance explained|\n",
    "|Retained PCs|\tOften far fewer than p, chosen to capture ~95% of the total variance|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d1f10",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forests\n",
    "What It Means: \n",
    "- Decision trees split data based on conditions, creating branches that lead to a prediction. \n",
    "- Random forests use multiple trees to improve accuracy and reduce overfitting.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each \"branch\" shows how different conditions affect the outcome, \n",
    "- and random forests average the results of many trees for robust predictions.\n",
    "\n",
    "Performance Measures:\n",
    "- Accuracy: Proportion of correctly classified samples.\n",
    "- Gini Index / Entropy: Used to measure the purity of the splits; lower values are better.\n",
    "\n",
    "Lay Explanation: \n",
    "- Decision trees are like flowcharts that guide predictions based on conditions. \n",
    "- Random forests combine many trees to make stronger, more reliable decisions.\n",
    "\n",
    "Use Case: \n",
    "- For classification or regression problems with non-linear relationships and high dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac011c6c",
   "metadata": {},
   "source": [
    "# Survival Analysis (e.g., Cox Proportional Hazards)\n",
    "What It Means: \n",
    "- Survival analysis predicts the time until an event occurs, such as customer churn or equipment failure.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each output shows the likelihood of the event happening over time, considering various risk factors.\n",
    "\n",
    "Performance Measures:\n",
    "- Concordance Index (C-Index): Measures the model’s ability to correctly rank predictions; values closer to 1 indicate better performance.\n",
    "\n",
    "Lay Explanation: \n",
    "Survival analysis is like tracking how long something will last, based on factors that might speed it up or slow it down.\n",
    "\n",
    "Use Case: \n",
    "- For time-to-event data, such as time until a customer churns or equipment fails.\n",
    "\n",
    "Model Types: \n",
    "- Kaplan-Meier estimator, Cox Proportional Hazards Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995ebf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(data, 'time', event_col='event')\n",
    "cph.predict_survival_function(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b4e50",
   "metadata": {},
   "source": [
    "# Time Series Models (e.g., ARIMA)\n",
    "What It Means: \n",
    "- Time series models account for:\n",
    "    - trends, \n",
    "    - seasonality, and \n",
    "    - temporal dependencies in data collected over time, often used for forecasting future values.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each prediction is based on patterns in past data points, accounting for recent trends and cycles.\n",
    "\n",
    "Performance Measures:\n",
    "- Mean Absolute Percentage Error (MAPE): Shows the average prediction error in percentage terms.\n",
    "- Root Mean Squared Error (RMSE): Measures the prediction accuracy; lower values mean better predictions.\n",
    "\n",
    "Lay Explanation: \n",
    "- Time series models are like weather forecasts—they predict future values based on past patterns, like trends and cycles.\n",
    "\n",
    "Use Case: \n",
    "- Forecasting for data with a temporal component (e.g., sales data, stock prices).\n",
    "\n",
    "Model Types: \n",
    "- ARIMA, \n",
    "- SARIMA, \n",
    "- Exponential Smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e901a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "model = ARIMA(time_series_data, order=(1,1,1))\n",
    "model_fit = model.fit()\n",
    "predictions = model_fit.forecast(steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc25f7",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf07849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to compute True Positives, True Negatives, False Positives and False Negatives\n",
    "\n",
    "def true_positive(y_true, y_pred):\n",
    "    tp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 1:\n",
    "            tp += 1\n",
    "    return tp\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    tn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 0:\n",
    "            tn += 1        \n",
    "    return tn\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    fp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 1:\n",
    "            fp += 1       \n",
    "    return fp\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    fn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 0:\n",
    "            fn += 1        \n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea735dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "TP = np.diag(cnf_matrix)\n",
    "TN = cnf_matrix.sum() - (FP + FN + TP)FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy for each class\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf443ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation for table metrics:\n",
    "import sklearn.metrics\n",
    "import mathdef matrix_metrix(real_values,pred_values,beta):\n",
    "CM = confusion_matrix(real_values,pred_values)\n",
    "TN = CM[0][0]\n",
    "FN = CM[1][0] \n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "Population = TN+FN+TP+FP\n",
    "Prevalence = round( (TP+FP) / Population,2)\n",
    "Accuracy   = round( (TP+TN) / Population,4)\n",
    "Precision  = round( TP / (TP+FP),4 )\n",
    "NPV        = round( TN / (TN+FN),4 )\n",
    "FDR        = round( FP / (TP+FP),4 )\n",
    "FOR        = round( FN / (TN+FN),4 ) \n",
    "check_Pos  = Precision + FDR\n",
    "check_Neg  = NPV + FOR\n",
    "Recall     = round( TP / (TP+FN),4 )\n",
    "FPR        = round( FP / (TN+FP),4 )\n",
    "FNR        = round( FN / (TP+FN),4 )\n",
    "TNR        = round( TN / (TN+FP),4 ) \n",
    "check_Pos2 = Recall + FNR\n",
    "check_Neg2 = FPR + TNR\n",
    "LRPos      = round( Recall/FPR,4 ) \n",
    "LRNeg      = round( FNR / TNR ,4 )\n",
    "DOR        = round( LRPos/LRNeg)\n",
    "F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)\n",
    "FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "BM         = Recall+TNR-1\n",
    "MK         = Precision+NPV-1   \n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos','check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','F1','FBeta','MCC','BM','MK'],     \n",
    "                        'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,F1,FBeta,MCC,BM,MK]})   \n",
    "\n",
    "return (mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Implementation\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplotfpr, tpr, thresholds = roc_curve(real_values, prob_values)\n",
    "\n",
    "auc = roc_auc_score(real_values, prob_values)\n",
    "print('AUC: %.3f' % auc)pyplot.plot(fpr, tpr, linestyle='--', label='Roc curve')\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()pyplot.show()\n",
    "\n",
    "# Precision-recall implementation\n",
    "\n",
    "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(real_values,prob_values)pyplot.plot(recall, precision, linestyle='--', label='Precision versus Recall')\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "pyplot.legend()pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba51b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for get many metrics directly from sklearn\n",
    "\n",
    "def sk_metrix(real_values,pred_values,beta):\n",
    "Accuracy = round( sklearn.metrics.accuracy_score(real_values,pred_values) ,4)\n",
    "Precision= round( sklearn.metrics.precision_score(real_values,pred_values),4 )\n",
    "Recall   = round( sklearn.metrics.recall_score(real_values,pred_values),4 )   \n",
    "F1       = round ( sklearn.metrics.f1_score(real_values,pred_values),4)\n",
    "FBeta    = round ( sklearn.metrics.fbeta_score(real_values,pred_values,beta) ,4)\n",
    "MCC      = round ( sklearn.metrics.matthews_corrcoef(real_values,pred_values)  ,4)   \n",
    "Hamming  = round ( sklearn.metrics.hamming_loss(real_values,pred_values) ,4)   \n",
    "Jaccard  = round ( sklearn.metrics.jaccard_score(real_values,pred_values) ,4)   \n",
    "Prec_Avg = round ( sklearn.metrics.average_precision_score(real_values,pred_values) ,4)   \n",
    "Accu_Avg = round ( sklearn.metrics.balanced_accuracy_score(real_values,pred_values) ,4)   \n",
    "\n",
    "mat_met = pd.DataFrame({\n",
    "'Metric': ['Accuracy','Precision','Recall','F1','FBeta','MCC','Hamming','Jaccard','Precision_Avg','Accuracy_Avg'],\n",
    "'Value': [Accuracy,Precision,Recall,F1,FBeta,MCC,Hamming,Jaccard,Prec_Avg,Accu_Avg]})   \n",
    "\n",
    "return (mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics For Multi-class Classification\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate accuracy\n",
    "    -> param y_true: list of true values\n",
    "    -> param y_pred: list of predicted values\n",
    "    -> return: accuracy score\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "# Intitializing variable to store count of correctly predicted classes\n",
    "    correct_predictions = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == yp:\n",
    "            correct_predictions += 1\n",
    "    #returns accuracy\n",
    "    return correct_predictions / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eeb9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of macro-averaged precision\n",
    "\n",
    "def macro_precision(y_true, y_pred):\n",
    "\n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize precision to 0\n",
    "    precision = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(y_true.unique()):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false positive for current class\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute precision for current class\n",
    "        temp_precision = tp / (tp + fp + 1e-6)\n",
    "        # keep adding precision for all classes\n",
    "        precision += temp_precision\n",
    "        \n",
    "    # calculate and return average precision over all classes\n",
    "    precision /= num_classes\n",
    "    \n",
    "    return precision\n",
    "\n",
    "print(f\"Macro-averaged Precision score : {macro_precision(y_test, y_pred) }\")\n",
    "\n",
    "# implement marco-averaged precision using sklearn\n",
    "macro_averaged_precision = metrics.precision_score(y_test, y_pred, average = 'macro')\n",
    "print(f\"Macro-Averaged Precision score using sklearn library : {macro_averaged_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of micro-averaged precision\n",
    "\n",
    "def micro_precision(y_true, y_pred):\n",
    "\n",
    "\n",
    "    # find the number of classes \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in y_true.unique():\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate true positive for current class\n",
    "        # and update overall tp\n",
    "        tp += true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate false positive for current class\n",
    "        # and update overall tp\n",
    "        fp += false_positive(temp_true, temp_pred)\n",
    "        \n",
    "    # calculate and return overall precision\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision\n",
    "\n",
    "print(f\"Micro-averaged Precision score : {micro_precision(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "#  implement mirco-averaged precision using sklearn\n",
    "micro_averaged_precision = metrics.precision_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged Precision score using sklearn library : {micro_averaged_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ed0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of macro-averaged recall\n",
    "\n",
    "def macro_recall(y_true, y_pred):\n",
    "\n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize recall to 0\n",
    "    recall = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(y_true.unique()):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false negative for current class\n",
    "        fn = false_negative(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute recall for current class\n",
    "        temp_recall = tp / (tp + fn + 1e-6)\n",
    "        \n",
    "        # keep adding recall for all classes\n",
    "        recall += temp_recall\n",
    "        \n",
    "    # calculate and return average recall over all classes\n",
    "    recall /= num_classes\n",
    "    \n",
    "    return recall\n",
    "\n",
    "print(f\"Macro-averaged recall score : {macro_recall(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "# implement macro-averaged recall using sklearn\n",
    "\n",
    "macro_averaged_recall = metrics.recall_score(y_test, y_pred, average = 'macro')\n",
    "print(f\"Macro-averaged recall score using sklearn : {macro_averaged_recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd95acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of micro-averaged recall\n",
    "\n",
    "def micro_recall(y_true, y_pred):\n",
    "\n",
    "\n",
    "    # find the number of classes \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in y_true.unique():\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate true positive for current class\n",
    "        # and update overall tp\n",
    "        tp += true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate false negative for current class\n",
    "        # and update overall tp\n",
    "        fn += false_negative(temp_true, temp_pred)\n",
    "        \n",
    "    # calculate and return overall recall\n",
    "    recall = tp / (tp + fn)\n",
    "    return recall\n",
    "\n",
    "print(f\"Micro-averaged recall score : {micro_recall(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "#  implement micro-averaged recall using sklearn\n",
    "\n",
    "micro_averaged_recall = metrics.recall_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged recall score using sklearn library : {micro_averaged_recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d50779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of macro-averaged f1 score\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "\n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize f1 to 0\n",
    "    f1 = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(y_true.unique()):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false negative for current class\n",
    "        fn = false_negative(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false positive for current class\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute recall for current class\n",
    "        temp_recall = tp / (tp + fn + 1e-6)\n",
    "        \n",
    "        # compute precision for current class\n",
    "        temp_precision = tp / (tp + fp + 1e-6)\n",
    "        \n",
    "        \n",
    "        temp_f1 = 2 * temp_precision * temp_recall / (temp_precision + temp_recall + 1e-6)\n",
    "        \n",
    "        # keep adding f1 score for all classes\n",
    "        f1 += temp_f1\n",
    "        \n",
    "    # calculate and return average f1 score over all classes\n",
    "    f1 /= num_classes\n",
    "    \n",
    "    return f1\n",
    "\n",
    "\n",
    "print(f\"Macro-averaged f1 score : {macro_f1(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "# implement macro-averaged F1 score using sklearn\n",
    "\n",
    "macro_averaged_f1 = metrics.f1_score(y_test, y_pred, average = 'macro')\n",
    "print(f\"Macro-Averaged F1 score using sklearn library : {macro_averaged_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of micro-averaged fi score\n",
    "\n",
    "def micro_f1(y_true, y_pred):\n",
    "\n",
    "\n",
    "    #micro-averaged precision score\n",
    "    P = micro_precision(y_true, y_pred)\n",
    "\n",
    "    #micro-averaged recall score\n",
    "    R = micro_recall(y_true, y_pred)\n",
    "\n",
    "    #micro averaged f1 score\n",
    "    f1 = 2*P*R / (P + R)    \n",
    "\n",
    "    return f1\n",
    "\n",
    "print(f\"Micro-averaged recall score : {micro_f1(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "# implement micro-averaged F1 score using sklearn\n",
    "\n",
    "micro_averaged_f1 = metrics.f1_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged F1 score using sklearn library : {micro_averaged_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe51cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC AUCurve Computation\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def roc_auc_score_multiclass(actual_class, pred_class, average = \"macro\"):\n",
    "    \n",
    "    #creating a set of all the unique classes using the actual class list\n",
    "    unique_class = set(actual_class)\n",
    "    roc_auc_dict = {}\n",
    "    for per_class in unique_class:\n",
    "        \n",
    "        #creating a list of all the classes except the current class \n",
    "        other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "        #marking the current class as 1 and all other classes as 0\n",
    "        new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "        new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "\n",
    "        #using the sklearn metrics method to calculate the roc_auc_score\n",
    "        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n",
    "        roc_auc_dict[per_class] = roc_auc\n",
    "\n",
    "    return roc_auc_dict\n",
    "\n",
    "roc_auc_dict = roc_auc_score_multiclass(y_test, y_pred)\n",
    "roc_auc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC implementation: \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from itertools import cycle\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Load the iris data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target# Binarize the output\n",
    "y_bin = label_binarize(y, classes=[0, 1, 2])\n",
    "n_classes = y_bin.shape[1]# We split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size= 0.5, random_state=0)\n",
    "\n",
    "\n",
    "# We define the model as an SVC in OneVsRestClassifier setting.\n",
    "# this means that the model will be used for class 1 vs class 2, \n",
    "# class 2vs class 3 and class 1 vs class 3. \n",
    "# So, we have 3 cases at #the end and within each case, the bias will be varied in order to \n",
    "# Get the ROC curve of the given case - 3 ROC curves as output.\n",
    "\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=0))\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "# Plotting and estimation of FPR, TPR\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "colors = cycle(['blue', 'red', 'green'])\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=1.5, label='ROC curve of class {0} (area = {1:0.2f})' ''.format(i+1, roc_auc[i]))\n",
    "    plt.plot([0, 1], [0, 1], 'k-', lw=1.5)\n",
    "    plt.xlim([-0.05, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic for multi-class data')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69585b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
