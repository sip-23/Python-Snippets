{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb638b74",
   "metadata": {},
   "source": [
    "# Statisitcs and Research methods\n",
    "\n",
    "## Understanding Statistical Models vs. Machine Learning Models\n",
    "\n",
    "It's essential first to understand the distinctions between statistical models and machine learning models, as they serve different purposes, assumptions, and interpretative depth.\n",
    "\n",
    "- Statistical Models: \n",
    "    - These are rooted in traditional statistics and \n",
    "        - focus on relationships between variables through predefined equations. \n",
    "    - Statistical models aim to understand the underlying data-generating process, focusing on hypothesis testing and inference. \n",
    "    - These models often rely on strong assumptions like:\n",
    "        - linearity, \n",
    "        - normality, and \n",
    "        - homoscedasticity \n",
    "        - and are **interpretable**, making it easier to understand the impact of individual variables.\n",
    "\n",
    "- Machine Learning Models: \n",
    "    - These prioritize **predictive** power over interpretability. \n",
    "    - They are designed to automatically learn patterns and relationships within data, often with minimal assumptions. \n",
    "    - Machine learning models can handle complex and high-dimensional data but may lack transparency about how individual features affect the outcome, especially in ‚Äúblack box‚Äù models like neural networks or ensemble methods.\n",
    "\n",
    "\n",
    "## Choosing the Right Statistical Model\n",
    "\n",
    "The type of statistical model you use depends on your data and problem:\n",
    "\n",
    "- Linear Regression: For predicting a **continuous target variable** based on one or more predictors.\n",
    "- Logistic Regression: For predicting a **binary outcomes**, often used in classification problems.\n",
    "- ANOVA (Analysis of Variance): For comparing means across multiple groups.\n",
    "- Time Series Models: For data that‚Äôs ordered by time (e.g., ARIMA, SARIMA).\n",
    "- Survival Analysis: For time-to-event data, such as customer churn timing.\n",
    "- Multivariate Analysis: For understanding interactions across multiple variables (e.g., MANOVA, PCA).\n",
    "\n",
    "## Preprocessing the Data\n",
    "Prepare your data by cleaning and preprocessing it:\n",
    "\n",
    "- Missing Values: Decide whether to impute or drop missing values.\n",
    "- Outliers: Identify and consider handling outliers, especially in regression.\n",
    "- Data Transformation: Transform non-normal variables if required (e.g., using log transformations).\n",
    "- Feature Scaling: For some models, standardizing or normalizing data is essential.\n",
    "\n",
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "EDA is essential to understand: \n",
    "- patterns,\n",
    "    - visualizations\n",
    "- distributions,\n",
    "    - summary statistics\n",
    "- relationships\n",
    "    - correlation matrices\n",
    "    \n",
    "This is to identify relevant features and spot potential issues like multicollinearity.\n",
    "\n",
    "## Building the Statistical Model\n",
    "\n",
    "- **Statsmodels** provides \n",
    "    - coefficients, \n",
    "    - p-values, and \n",
    "    - confidence intervals for each variable, \n",
    "        - enabling hypothesis testing on whether each predictor significantly affects the outcome.\n",
    "\n",
    "## Evaluating Model Performance\n",
    "Regression Metrics: \n",
    "- Use R-squared, \n",
    "- Adjusted R-squared, \n",
    "- RMSE, and \n",
    "- MAE to evaluate regression models.\n",
    "\n",
    "Classification Metrics: \n",
    "- Use confusion matrix, \n",
    "- accuracy, \n",
    "- precision, \n",
    "- recall, and \n",
    "- AUC-ROC.\n",
    "\n",
    "Residual Analysis: \n",
    "- Residual plots help assess assumptions\n",
    "    - homoscedasticity, \n",
    "    - normality of residuals).\n",
    "\n",
    "## Model Interpretation\n",
    "Statistical models are highly interpretable. \n",
    "- In linear regression, each coefficient represents the expected change in the dependent variable for a one-unit change in the predictor, holding all else constant.\n",
    "\n",
    "Confidence Intervals: \n",
    "- Look at 95% CI for each coefficient; if it does not contain zero, it suggests the predictor has a statistically significant effect.\n",
    "\n",
    "P-Values: \n",
    "- A p-value below a threshold (usually 0.05) indicates that the predictor significantly affects the outcome.\n",
    "\n",
    "## Validating Assumptions\n",
    "- Linearity: Check scatter plots of residuals.\n",
    "- Normality of Residuals: Use a Q-Q plot to verify.\n",
    "- No Multicollinearity: Variance inflation factor (VIF) helps detect multicollinearity.\n",
    "- Homoscedasticity: Plot residuals vs. fitted values.\n",
    "\n",
    "## Reporting and Communicating Results\n",
    "Present your findings by focusing on:\n",
    "\n",
    "- Key Coefficients: Explain which predictors significantly affect the outcome.\n",
    "- Model Fit: Interpret R-squared values (e.g., explaining how much variance in the target variable is explained).\n",
    "- Real-World Implications: Describe how insights from the model can impact business decisions.\n",
    "\n",
    "# Approach to statistical modeling\n",
    "\n",
    "Each model type has specific \n",
    "- applications, \n",
    "- strengths, and \n",
    "- limitations, \n",
    "\n",
    "Understand when and how to use them.\n",
    "\n",
    "### Step 1: Define Objectives and Hypotheses\n",
    "\n",
    "Identify the Problem and Objectives: \n",
    "- Clearly define the goal.\n",
    "    - Are you trying to predict, classify, find patterns, or estimate relationships? \n",
    "    - Setting objectives helps in choosing the right model.\n",
    "\n",
    "- Formulate Hypotheses: \n",
    "    - Based on the problem, develop hypotheses. \n",
    "        - For instance, in a sales prediction problem, you may hypothesize that `certain features like advertising spend, time of year, and economic indicators affect sales.`\n",
    "\n",
    "### Step 2: Data Collection and Preprocessing\n",
    "Data Collection: \n",
    "- Gather historical data related to the problem. \n",
    "\n",
    "Data Cleaning: \n",
    "- Handle missing values, remove duplicates, and ensure consistency.\n",
    "\n",
    "Feature Engineering: \n",
    "- Create new features if necessary. \n",
    "- This could involve \n",
    "    - transformations, \n",
    "    - encoding categorical variables, or \n",
    "    - creating interaction terms.\n",
    "\n",
    "Data Splitting: \n",
    "- Split the data into training and testing sets. Typically, an 80-20 or 70-30 split is used.\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "### Why is EDA important?\n",
    "\n",
    "Exploratory Data Analysis (EDA) helps us to understand our data without making any assumptions. EDA is a vital component before we continue with the modelling phase as it provides context and guidance on the course of action to take when developing the appropriate model. It will also assist in interpreting the results correctly. Without doing EDA you will not understand your data fully.\n",
    "\n",
    "\n",
    "### The different types of EDA\n",
    "\n",
    "EDA are generally classified in two ways:\n",
    "\n",
    "    1) Non-graphical or Graphical\n",
    "    2) Univariate or Multivariate\n",
    "    \n",
    "<div align=\"left\" style=\"width: 600px; text-align: left;\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/f860f39251c523eda779dea0140316ccbefdd8e0/eda_map.jpg?raw=True\"\n",
    "     alt=\"EDA Diagram\"\n",
    "     style=\"padding-bottom=0.5em\"\n",
    "     width=600px/>\n",
    "</div>\n",
    "\n",
    "\n",
    "#### Non-graphical EDA\n",
    "Involves calculations of summary/descriptive statistics. \n",
    "\n",
    "#### Graphical EDA\n",
    "This type of analysis will contain data visualisations.\n",
    "\n",
    "#### Univariate Analysis \n",
    "This is performed on one variable at a time as the prefix 'uni' indicates. \n",
    "\n",
    "#### Multivariate Analysis \n",
    "This type of analysis explores the relationship between two or more variables. \n",
    "When only comparing two variables it is known as **bivariate analysis** as indicated by the prefix 'bi'.\n",
    "\n",
    "Read a more detailed explanation <a href=\"https://www.stat.cmu.edu/~hseltman/309/Book/chapter4.pdf\">here</a>.\n",
    "\n",
    "### 1. Basic Analysis\n",
    "\n",
    "For a practical example, we will be looking at the Medical Claims Data. Using these four commands, we will perform a basic analysis:\n",
    "\n",
    "    - df.head()\n",
    "    - df.shape\n",
    "    - df.info()\n",
    "        - feature (variable) is categorical the Dtype is object and if it is a numerical variable the Dtype is an int64 or float64. \n",
    "        - This command also shows us that out of the 1338 none of the features contain any null values.\n",
    "    - df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/regression_sprint/claims_data.csv')\n",
    "\n",
    "# Looking at the top five rows of our data\n",
    "df.head()\n",
    "\n",
    "# shape command shows us that we have x rows of data and y features.\n",
    "df.shape\n",
    "\n",
    "#  confirms our categorical and numerical features.\n",
    "df.info()\n",
    "\n",
    "# Null values for each feature can also be checked by using the following command\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98438ff3",
   "metadata": {},
   "source": [
    "### Univariate Analysis: Non-Graphical\n",
    "\n",
    "The first univariate analysis will be non-graphical. This is where we will be looking at the **descriptive statistics** of each feature. \n",
    "\n",
    "We can get the descriptive statistics of each **numerical feature** by using the following command:\n",
    "\n",
    "    - df.describe()\n",
    "\n",
    "This command will provide the mean, standard deviation and a five number summary of each numerical feature.\n",
    "The five number summary (Minimum, Lower Quartile (Q1) = 25%, Median (Q2) = 50%, Upper Quartile (Q3) = 75%, Maximum) is also used for creating the box plot.\n",
    "\n",
    "Individual statistical measures can also be calculated by using the following commands:\n",
    "\n",
    "    - df.count()\n",
    "    - df.mean()\n",
    "    - df.std()\n",
    "    - df.min()\n",
    "    - df.quantile([0.25, 0.5, 0.75], axis = 0)\n",
    "    - df.median()\n",
    "    - df.max()\n",
    "\n",
    "The three measures for central tendency are the **mode, mean and median**. The command to determine the mode is:\n",
    "\n",
    "    - df.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed976189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "\n",
    "# statistics of a specific feature\n",
    "df.age.describe()\n",
    "df['age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f3953",
   "metadata": {},
   "source": [
    "Additional statistical measures that can be calculated are **kurtosis** and **skew**. \n",
    "\n",
    "Both kurtosis and skew are important statistical terms to be familiar with in data science. Kurtosis is the measure of outliers present in the data. **High kurtosis (>3)** indicates a large number of outliers and **low kurtosis (<3)** a lack of outliers.  Skew will indicate how symmetrical your data is. Below is a table that explains the range of values with regards to skew.\n",
    "\n",
    "\n",
    "|   Skew Value (x)  |       Description of Data      |\n",
    "|:-------------------|:---------------:|\n",
    "| -0.5 < x < 0.5              |Fairly Symmetrical |\n",
    "| -1 < x < -0.5 | Moderate Negative Skew  | \n",
    "| 0.5 < x < 1             | Moderate Positive Skew  | \n",
    "|       x < -1     |High Negative Skew  | \n",
    "|       x > 1  |High Positve Skew | \n",
    "\n",
    "<div align=\"left\" style=\"width: 500px; font-size: 80%; text-align: left; margin: 0 auto\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/f3aeedd2c056ddd233301c7186063618c1041140/regression_analysis_notebook/skew.jpg?raw=True\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: left; padding-bottom=0.5em\"\n",
    "     width=500px/>\n",
    "     For a more detailed explanation on skew and kurtosis read <a href=\"https://codeburst.io/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa\">here</a>.\n",
    "</div>\n",
    "\n",
    "\n",
    "The commands used to determine the skew and kurtosis of data are:\n",
    "\n",
    "    - df.skew()\n",
    "    - df.kurtosis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00270bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()\n",
    "\n",
    "# Closer to 0 implies fairly symmetrical.\n",
    "# Above 0.3 implies  moderately skewed in a positive direction.\n",
    "# Above 1 implies highly skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicates a lack of outliers for all features.\n",
    "df.kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a37bde",
   "metadata": {},
   "source": [
    "### Univariate Analysis: Graphical\n",
    "\n",
    "You can look at the **distribution** of any numerical feature by using the following plots:\n",
    "\n",
    "    - histogram\n",
    "    - density plot\n",
    "    - box plot\n",
    "    - violin plot\n",
    "    \n",
    "For a categorical feature we will use a:\n",
    "\n",
    "    - bar plot\n",
    "\n",
    "#### Histogram and Density Plot\n",
    "\n",
    "For displaying a histogram and density plot we will be using the Matplotlib library and create a list of all numerical features to visualise these features at the same time.\n",
    "\n",
    " both the histogram and density plot display the same information. The density plot can be considered a smoothed version of the histogram and does not depend on the size of bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbdb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['age', 'bmi', 'steps', 'children', 'claim_amount'] # create a list of all numerical features\n",
    "df[features].hist(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[features].plot(kind='density', subplots=True, layout=(3, 2), sharex=False, figsize=(10, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3335937",
   "metadata": {},
   "source": [
    "#### Box Plot and Violin Plot\n",
    "\n",
    "For the Box Plot and Violin Plot, we will use the seaborn library and only select one feature instead of all the numerical features. We can visualise all numerical features simultaneously, but as the range of values for each feature is different, it will not create a useful visualisation. Standardisation or normalisation can be applied to a feature to adjust the range, but we will not apply it in this notebook. Further reading on standardisation and normalisation can be done <a href=\"https://medium.com/@dataakkadian/standardization-vs-normalization-da7a3a308c64\">here</a>.\n",
    "\n",
    "The `bmi` feature will be used.\n",
    "\n",
    "Although both the box plot and violin plot display the distribution of the data, the boxplot provides certain statistics that are useful. \n",
    "\n",
    "The five vertical lines in the boxplot provide the information of the five number summary and the dots on the right hand side of the graph is a display of outliers. The violin plot focuses more on a smoothed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5496c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='bmi', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05719ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x='bmi', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf932ae",
   "metadata": {},
   "source": [
    "#### Bar Plot\n",
    "\n",
    "For the categorical features, we can create a **bar plot** to display the frequency distribution. \n",
    "\n",
    "We'll generate a bar plot of the `children` feature, where each bar represents a unique number of children from the data, and the height represents how many times that number of children occurred. This can be done by using seaborn's `countplot`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'children', data = df, palette=\"hls\")\n",
    "plt.title(\"Distribution of Children\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c575b0",
   "metadata": {},
   "source": [
    "### Multivariate Analysis: Non-Graphical \n",
    "\n",
    "For this analysis, we can **determine the relationship between any two numerical features** by calculating the **correlation coefficient**. \n",
    "- Correlation is a measure of the degree to which two variables change together, if at all. \n",
    "    - If two features have a strong positive correlation, it means that if the value of one feature increases, the value of the other feature also increases. \n",
    "    - There are three different correlation measures:\n",
    "        - Pearson correlation \n",
    "        - Spearman rank correlation\n",
    "        - Kendall correlation\n",
    "\n",
    "For this lesson, we will focus on the **Pearson correlation**. The Pearson correlation measures the linear relationship between features and assumes that the features are normally distributed. Below is a table that explains how to interpret the Pearson correlation measure:\n",
    "\n",
    "\n",
    "|   Pearson Correlation Coefficient (r)  |       Description of Relationship     |\n",
    "|:-------------------|:---------------:|\n",
    "|  r = -1              |Perfect Negative Correlation |\n",
    "| -1 < r < -0.8 | Strong Negative Correlation  | \n",
    "| - 0.8 < r < -0.5             | Moderate Negative Correlation  | \n",
    "|       - 0.5 < r < 0     |Weak Negative Correlation  | \n",
    "|       r = 0  |No Linear Correlation | \n",
    "| 0 < r < 0.5 | Weak Positive Correlation  | \n",
    "| 0.5 < r < 0.8             | Moderate Positive Correlation  | \n",
    "|       0.8 < r < 1     |Strong Positive Correlation  | \n",
    "|       r = 1  |Perfect Positive Correlation | \n",
    "\n",
    "\n",
    "<div align=\"left\" style=\"width: 800px; text-align: left;\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/f3aeedd2c056ddd233301c7186063618c1041140/regression_analysis_notebook/pearson_corr.jpg?raw=True\"\n",
    "     alt=\"Pearson Correlation\"\n",
    "     style=\"padding-bottom=0.5em\"\n",
    "     width=800px/>\n",
    "</div>\n",
    "\n",
    "For a more detailed explanation of correlations, read <a href=\"https://medium.com/fintechexplained/did-you-know-the-importance-of-finding-correlations-in-data-science-1fa3943debc2#:~:text=Correlation%20is%20a%20statistical%20measure,to%20forecast%20our%20target%20variable.&text=It%20means%20that%20when%20the,variable(s)%20also%20increases.\">here</a>.\n",
    "\n",
    "The command we will use to determine the correlation between features is:\n",
    "\n",
    "    - df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a15749",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1483a",
   "metadata": {},
   "source": [
    "### Multivariate Analysis: Graphical\n",
    "\n",
    "For the multivariate graphical analysis the following visualisations will be considered:\n",
    "\n",
    "    - Heatmap\n",
    "    - Scatter Plot\n",
    "    - Pair Plot\n",
    "    - Joint Plot\n",
    "    - Bubble Plot\n",
    "    \n",
    "#### Heatmap\n",
    "\n",
    "The relationship between features can also be displayed graphically using a **heatmap**. The Seaborn library will be used for this basic heatmap visualisation. \n",
    "\n",
    "To see how different heatmap variations can be created, read <a href=\"https://medium.com/@szabo.bibor/how-to-create-a-seaborn-correlation-heatmap-in-python-834c0686b88e\">here</a>.\n",
    "\n",
    "The correlation coefficient value will be displayed on the heatmap using the `vmin` and `vmax` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bcf0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6d815",
   "metadata": {},
   "source": [
    "#### Scatter Plot\n",
    "\n",
    "A Scatter plot is used to visualise the relationship between two different features and is most likely the primary multivariate graphical method. For this exercise, we will create a scatter plot to determine if there is a relationship between `bmi` and `age`. The parameter `hue` is set to the feature `insurance_claim`, colouring the points according to whether or not a claim was submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d5bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='age',y='bmi',hue='insurance_claim', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c43d2",
   "metadata": {},
   "source": [
    "#### Pair Plot\n",
    "\n",
    "A pair plot can be used to visualise the relationships between all the numerical features at the same time. \n",
    "\n",
    "The `hue` is once again set to the feature `insurance_claim` to indicate which data points submitted an insurance claim and which didn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afc8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.pairplot(df, hue=\"insurance_claim\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd213a",
   "metadata": {},
   "source": [
    "#### Joint Plot\n",
    "\n",
    "The joint plot can be used to provide univariate and multivariate analyses at the same time. The central part of the plot will be a scatter plot comparing two different features. The top and right visualisations will display the distribution of each feature as a histogram. \n",
    "\n",
    "For this joint plot, we will once again compare `age` and `bmi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5370b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x = 'age', y = 'bmi', data = df)\n",
    "\n",
    "# including the hue as insurance_claim\n",
    "sns.jointplot(x = 'age', y = 'bmi', data = df, hue='insurance_claim')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd4b7a1",
   "metadata": {},
   "source": [
    "#### Bubble Plots\n",
    "\n",
    "A bubble plot is a variation of a scatter plot. Bubbles vary in size, dependent on another feature in the data. The same applies to the colour of the bubbles; which can be set to vary with the values of another feature. This way, we can visualise up to four dimensions/features at the same time.\n",
    "\n",
    "For this bubble plot, `bmi` and `claim_amount` will be plotted on the x-axis and y-axis, respectively. The colours of the bubbles will vary based on whether the observation is a `smoker` or not, and lastly, the size of the bubbles will vary based on the number of `children` the observation has. We will create this bubble plot by using `seaborn`‚Äôs scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(x=\"bmi\", \n",
    "                y=\"claim_amount\",\n",
    "                size=\"children\",\n",
    "                sizes=(20,100),\n",
    "                alpha=0.8,\n",
    "                hue=\"smoker\",\n",
    "                data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bcc8a9",
   "metadata": {},
   "source": [
    "## Splitting the Data\n",
    "### Two-Way Split\n",
    "\n",
    "When fitting a machine learning model to some data, we ultimately intend to use that model to make predictions/forecasts on real-world data. \n",
    "- Real-world data is unseen - it doesn't exist in the dataset we have at our disposal - so in order to validate our model (check how well it performs), we need to test it on unseen data too.\n",
    "- Gathering unseen data is not as simple as collecting it from outside the window and exposing it to the model: any new data would need to be \n",
    "    - cleaned, \n",
    "    - wrangled and \n",
    "    - annotated just like the data in our dataset.\n",
    "- The next best thing, then, is to simulate some unseen data, which we can do using the existing dataset by splitting it into two sets:\n",
    "    - One for training the model; and\n",
    "    - A second for testing it.\n",
    "   \n",
    "We fit a model using the training data, and then assess its accuracy using the test set.\n",
    "- use 80% of the data for training and \n",
    "    - the training set will contain 80% of the rows, or data points,\n",
    "- keep 20% aside for testing. \n",
    "    - and the remaining 20% of rows will be in the test set.\n",
    "These rows are selected at random, to ensure that the mix of data in the train set is as close as possible to the mix in the test set.\n",
    "\n",
    "### Three-Way Split\n",
    "\n",
    "Many academic works on machine learning talk about splitting the dataset into three distinct parts: \n",
    "- `train`, \n",
    "    - training set is used to fit the model to the observations.\n",
    "- `validation,` and\n",
    "    -  during the model tuning process where hyperparameters are tweaked and decisions on the dataset is made, the validation set is used to test the performance of the model.\n",
    "- `test` sets. \n",
    "    - Once the model designer is satisfied with the performance of the model on the validation set, the previously unseen test set is brought out and used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "#### Caveats for using a validation set\n",
    "\n",
    "On small datasets, it may not be feasible to include a validation set for the following reasons, both of which should be intuitive:\n",
    "\n",
    "- The model may need every possible data point to adequately determine model values;\n",
    "- For small enough test sets, the uncertainty of the test set can be considerably large to the point where different test sets may produce very different results.\n",
    "\n",
    "Clearly, further splitting the training data into training and validation sets would remove precious observations for the training process.\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "In the case that the designer does not desire to use a validation set, or there is simply not enough data, \n",
    "- a technique known as cross validation may be used. \n",
    "A common version of cross validation is known as K-fold cross validation: \n",
    "- during the training process, some proportion of the training data, say 10%, is held back, and effectively used as a validation set while the model parameters are calcuated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import seaborn as sns\n",
    "\n",
    "# Import the split function from sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into the response, y, and features, X\n",
    "y = df['ZAR/USD']\n",
    "X = df.drop('ZAR/USD', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26658ff9",
   "metadata": {},
   "source": [
    "Understand the four parameters to hand to the splitting function.\n",
    "\n",
    "- `X` contains the features on which we will be training the model. In this case: just `exports`;\n",
    "- `y` is the response variable, that which we are trying to predict. In this case: `exchange rate`;\n",
    "- `test_size` is a value between 0 and 1: the proportion of our dataset that we want to be used as test data. Typically 0.2 (20%);\n",
    "- `random_state` is an arbitrary value which, when set, ensures that the _random_ nature in which rows are picked to be in the test set is the same each time the split is carried out. In other words, the rows are picked at random, but we can ensure these random picks are repeatable by using the same value here. This makes it easier to assess model performance across iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02311b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Call the train_test_split function:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd317a",
   "metadata": {},
   "source": [
    "Plotting the data points in each of the training and testing sets in different colours, we should be able to see that we have a similar spread of data in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1299f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.scatter(X_train, y_train, color='green', label='Training')  # plot the training data in green\n",
    "plt.scatter(X_test, y_test, color='darkblue', label='Testing')  # plot the testing data in blue\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d204818a",
   "metadata": {},
   "source": [
    "## Advanced plotting\n",
    "Let's try and create something a little more visually appealing than the two plots above.\n",
    "‚Äã\n",
    "- We'll plot both dependent data series on the same graph;\n",
    "- We'll assign two separate y-axes: one for each series;\n",
    "- We'll display a legend near the top of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc('mathtext', default='regular')\n",
    "# Create blank figure\n",
    "fig = plt.figure()\n",
    "\n",
    "# Split figure to allow two sets of y axes\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot the first line on its axis\n",
    "ax.plot(np.arange(len(df.Y)), df.Y, '-', label = 'ZAR/USD', color='orange')\n",
    "\n",
    "# Create second y axis and plot second line\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(np.arange(len(df.X)), df.X, '-', label = 'Exports (ZAR)')\n",
    "\n",
    "# Add legends for each axis\n",
    "ax.legend(loc=2)\n",
    "ax2.legend(loc=9)\n",
    "\n",
    "ax.grid()\n",
    "\n",
    "# Set labels of axes\n",
    "ax.set_xlabel(\"Months\")\n",
    "ax.set_ylabel(\"ZAR/USD\")\n",
    "ax2.set_ylabel(\"Exports (ZAR, millions)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda9b41d",
   "metadata": {},
   "source": [
    "### Step 3: Select the Type of Statistical Model\n",
    "Statistical models can be broadly categorized as:\n",
    "\n",
    "- **Descriptive Models**: Summarize data patterns.\n",
    "- **Inferential Models**: Help make inferences about the population.\n",
    "- **Predictive Models**: Used to predict future outcomes based on historical data.\n",
    "- **Prescriptive Models**: Suggest actions based on predictions.\n",
    "\n",
    "Let's go through common types of statistical models and their applications.\n",
    "\n",
    "\n",
    "# Regression Analysis\n",
    " \n",
    "Regression Analysis is a statistical method to analyze the relationship between a dependent variable and one or more independent variables.\n",
    "\n",
    "### Three types of regression analysis\n",
    "\n",
    "##### Real-world examples\n",
    "- Simple linear regression\n",
    "    - A real estate agent wants to determine the relationship between the size of a house (in square feet) and its selling price. They can use simple linear regression to predict the selling price of a house based on its size.\n",
    "    \n",
    "-  Multiple Linear Regression / Multivariate Linear Regression\n",
    "    - A car manufacturer wants to predict the fuel efficiency of their vehicles based on various independent variables such as engine size, horsepower, and weight.\n",
    "    \n",
    "- Logistic regression\n",
    "    - A bank wants to predict whether a customer will default on their loan based on their credit score, income, and other factors. By using logistic regression, the bank can estimate the probability of default and take appropriate measures to minimize their risk.\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "Linear Regression is a supervised learning algorithm used to model the relationship between a dependent variable (outcome) and one or more independent variables (predictors).\n",
    "- Predicts the relationship between two variables by assuming they have a straight-line connection. \n",
    "\n",
    "Linear Regression predicts a continuous target variable (e.g., the number of readmissions) by minimizing the residual sum of squares between observed and predicted values.\n",
    "- It finds the best line that minimizes the differences between predicted and actual values.\n",
    "\n",
    "## 1. Simple Linear Regression\n",
    "\n",
    "In a simple linear regression, there is \n",
    "- one independent variable and \n",
    "- one dependent variable. \n",
    "The model estimates the slope and intercept of the line of best fit, which represents the relationship between the variables. \n",
    "- The slope represents the change in the dependent variable for each unit change in the independent variable, while \n",
    "- The intercept represents the predicted value of the dependent variable when the independent variable is zero.\n",
    "\n",
    "What It Means: \n",
    "- Linear regression models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. It assumes a straight-line relationship. \n",
    "- It shows the linear relationship between the independent(predictor) variable i.e. X-axis and the dependent (output) variable i.e. Y-axis, \n",
    "    - called linear regression.\n",
    "\n",
    "- It is employed to establish a link between a dependant variable and a single independent variable. \n",
    "    - A linear equation defines the relationship, with the \n",
    "        - slope and \n",
    "        - intercept \n",
    "    - of the line representing the effect of the independent variable on the dependant variable.\n",
    "        - An independent variable is the variable that is controlled in a scientific experiment to test the effects on the dependent variable.\n",
    "        - A dependent variable is the variable being measured in a scientific experiment.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each coefficient represents how much the dependent variable (outcome) changes when the predictor variable changes by one unit, keeping all else constant.\n",
    "\n",
    "**Assumptions of Linear Regression**\n",
    "\n",
    "Regression is a parametric approach, which means that it makes assumptions about the data\n",
    "\n",
    "For successful regression analysis, it‚Äôs essential to validate the following assumptions.\n",
    "\n",
    "- Linearity (Linear Relationship): The relationship between the predictors and the outcome is linear.\n",
    "    - Plot dependent variable and independent variable(s) and see linear relationship.\n",
    "- Independence of Errors: Residuals (errors) are independent of each other.\n",
    "    - The error terms should not be dependent on one another (like in time-series data wherein the next value is dependent on the previous one). \n",
    "    - There should be no correlation between the residual terms.\n",
    "    - The absence of this phenomenon is known as Autocorrelation.\n",
    "- No or Little Autocorrelation\n",
    "- Normality of Errors: Residuals are normally distributed.\n",
    "    - The mean of residuals should follow a normal distribution with a mean equal to zero or close to zero. \n",
    "    - This is done to check whether the selected line is the line of best fit or not. \n",
    "    - If the error terms are non-normally distributed, suggests that there are a few unusual data points that must be studied closely to make a better model.\n",
    "- Multivariate Normality\n",
    "- No or Little Multicollinearity\n",
    "- Homoscedasticity: Variance of residuals is constant across all levels of predictors.\n",
    "    - The error terms must have constant variance. \n",
    "    - The presence of non-constant variance in the error terms is referred to as Heteroscedasticity. \n",
    "\n",
    "Performance Measures:\n",
    "- R-squared: Indicates the proportion of the variance in the dependent variable explained by the independent variables. \n",
    "    - Values closer to 1 indicate a better fit.\n",
    "- Mean Squared Error (MSE): The average squared difference between observed and predicted values; lower values are better.\n",
    "\n",
    "Lay Explanation: \n",
    "- Think of linear regression like drawing a best-fit line through a scatterplot of data points, aiming to predict outcomes based on relationships in the data.\n",
    "- Finds a relationship between independent and dependent variables by finding a ‚Äúbest-fitted line‚Äù that has minimal distance from all the data points.\n",
    "- The algorithm explains the linear relationship between the dependent(output) variable y and the independent(predictor) variable X using a straight line\n",
    "\n",
    "Use Case: \n",
    "- When there is a linear relationship between the target and predictor variables.\n",
    "\n",
    "### Mathematics or Linear Regression\n",
    "\n",
    "- it is using the least square method finds a linear equation that minimizes the sum of squared residuals (SSR).\n",
    "- Cost Function:\n",
    "\n",
    "$ J(\\theta) = \\frac{1}{2m}\\sum^{m}_{i=1}(h_{\\theta}(x^{(i)})- y^{(i)})^{2}$\n",
    "\n",
    "Model Equation:\n",
    "$ ùë¶=ùõΩ_{0}+ùõΩ_{1}ùë•_{1}+‚Ä¶+ùõΩ_{ùëõ}ùë•_{ùëõ}+ ùúñ $\n",
    "\n",
    "where:\n",
    "- $y$ = dependent variable\n",
    "- $ùõΩ_{0}$ = Y intercept / constant\n",
    "- $ùõΩ_{1}$ = Slope coefficient / intercept\n",
    "- $ùë•_{1}$ = independent variable\n",
    "- $ùúñ $ = error term\n",
    "\n",
    "**What is Cost Function ?**\n",
    "\n",
    "The goal of the linear regression algorithm is to get the best values for $ùõΩ_{0}+ùõΩ_{1}$ to find the **best-fit line**.\n",
    "- is a line that has the least error which means the error between predicted values and actual values should be minimum.\n",
    "\n",
    "A cost function, also referred to as a: \n",
    "- loss function : Used when we refer to the error for a single training example. \n",
    "- objective function : Used to refer to an average of the loss functions over an entire training dataset.\n",
    "It quantifies the difference between predicted and actual values, serving as a metric to evaluate the performance of a model.\n",
    "\n",
    "Objective \n",
    "- is to minimize the cost function, indicating better alignment between predicted and observed outcomes.\n",
    "- Guides the model towards optimal predictions by measuring its accuracy against the training data.\n",
    "\n",
    "AKA - Random Error (Residuals)\n",
    "- the difference between the observed value of the dependent variable($y_{i}$) and the predicted value(predicted) is called the residuals.\n",
    "    - $ùúñ_{i}$ =  $y_{predicted}  ‚Äì  y_{i}$\n",
    "\n",
    "where $ùë¶_{predicted} = ùõΩ_{0}+ùõΩ_{1}ùë•_{1}+‚Ä¶+ùõΩ_{ùëõ}ùë•_{ùëõ}+ ùúñ $\n",
    "\n",
    "**Why to use a Cost function**\n",
    "\n",
    "Cost function helps us reach the optimal solution / work out the optimal values for $ùõΩ_{0}+ùõΩ_{1}$ . \n",
    "- How: It takes both predicted outputs by the model and actual outputs and calculates how much wrong the model was in its prediction.\n",
    "    - It basically measures the discrepancy between the model‚Äôs predictions and the true values it is attempting to predict. \n",
    "    - This variance is depicted as a lone numerical figure, enabling us to measure the model‚Äôs **precision**.\n",
    "- The cost function is the technique of evaluating ‚Äúthe performance of our algorithm/model‚Äù.\n",
    "\n",
    "Classifiers have very high accuracy but one solution (Classifier) is the best because it does not misclassify any point.\n",
    "- Reason why it classifies all the points perfectly is that the:\n",
    "    - line is almost exactly in between the two (n) groups, and not closer to any one of the groups.\n",
    "\n",
    "Explanation of the function of a cost function:\n",
    "\n",
    "- Error calculation: It determines the difference between the predicted outputs (what the model predicts as the answer) and the actual outputs (the true values we possess for the data).\n",
    "- Gives one value: This simplifies comparing the model‚Äôs performance on various datasets or training rounds.\n",
    "- Improving Guides: The objective is to reduce the cost function. \n",
    "    - How: Through modifying the internal parameters of the model such as weights and biases, we can aim to minimize the total error and enhance the accuracy / precision of the model.\n",
    "\n",
    "**Types of Cost function in machine learning**\n",
    "\n",
    "Its use cases depend on whether it is a regression problem or classification problem.\n",
    "- Regression cost Function\n",
    "- Binary Classification cost Functions\n",
    "- Multi-class Classification cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8576db",
   "metadata": {},
   "source": [
    "### Problem Context: Predicting Hospital Readmission Rates\n",
    "The aim to reduce hospital readmission rates. \n",
    "- High readmission rates can strain resources and negatively impact patient outcomes.\n",
    "- The goal is to predict the number of readmissions within 30 days of discharge for a particular condition, such as \n",
    "    - diabetes, based on \n",
    "        - patient demographic, \n",
    "        - clinical data, and \n",
    "        - treatment data.\n",
    "\n",
    "**Step 1. Define the Problem**\n",
    "\n",
    "We want to predict the number of readmissions ($ùëå$) using features ($ùëã$) such as:\n",
    "- Patient age\n",
    "- Length of hospital stay\n",
    "- Severity of condition\n",
    "- Medication adherence rate\n",
    "- Comorbidities (e.g., hypertension, kidney disease)\n",
    "- Number of follow-up visits scheduled\n",
    "\n",
    "**Step 2. Collect and Prepare Data**\n",
    "\n",
    "- Data Collection: Gather historical patient data from the hospital's database.\n",
    "- Understand the \n",
    "    - model description\n",
    "    - causality and \n",
    "    - directionality\n",
    "- Check the data\n",
    "    - categorical data, \n",
    "    - missing data and \n",
    "    - outliers\n",
    "- Data Cleaning: \n",
    "    - Dummy variable takes only the value 0 or 1 to indicate the effect for categorical variables.\n",
    "    - Handle missing values, \n",
    "    - remove duplicates, and \n",
    "    - correct errors.\n",
    "    - Outlier is a data point that differs significantly from other observations. \n",
    "        - use standard deviation method and \n",
    "        - interquartile range (IQR) method.\n",
    "- Feature Engineering: \n",
    "    - Encode categorical variables (e.g., age group), \n",
    "    - scale continuous variables (e.g., length of stay), and \n",
    "    - create interaction terms if necessary.\n",
    "\n",
    "**Step 3. Conduct a Simple Analysis**\n",
    "- Check the **effect** comparing between \n",
    "    - Dependent variable to independent variable and \n",
    "    - Independent variable to independent variable\n",
    "- Check the correlation.\n",
    "    - Use scatter plots\n",
    "- Check Multicollinearity \n",
    "    - This occurs when more than two independent variables are highly correlated. \n",
    "    - Use Variance Inflation Factor (VIF) \n",
    "        - if VIF > 5 there is highly correlated and \n",
    "        - if VIF > 10 there is certainly multicollinearity among the variables.\n",
    "- Interaction Term imply a change in the slope from one value to another value.\n",
    "\n",
    "`Show the relationship between the two variables using a scatter plot.`\n",
    "- We have our Y, our X, and time (months), but we're just trying to model ZAR/USD as a *function* of Exports. \n",
    "    - To see if we can see that there possibly exists a linear relationship between the two variables: Value of Exports and ZAR/USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0754546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['X'], df['Y'])\n",
    "plt.ylabel(\"ZAR/USD\")\n",
    "plt.xlabel(\"Value of Exports (ZAR, millions)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56d3e0",
   "metadata": {},
   "source": [
    "**Step 4. Formulate the Model (From Scratch)**\n",
    "- y in this equation stands for the predicted value, \n",
    "- x means the independent variable and \n",
    "- m & b are the **coefficients** we need to optimize in order to fit the regression line to our data.\n",
    "\n",
    "#### Finding the Best Fit Line\n",
    "Let's say we have estimated some values for $a$ and $b$. We could plug in all of our values of X to find the corresponding values of Y. These *new* values of Y could be compared to the *actual* values of Y to assess the fit of the line. This becomes tedious as the number of data points increases.\n",
    "   \n",
    "Looking at the data, we can make a guess at the values of the slope and intercept of the line. We'll use a rough estimate of the slope as $\\frac{rise}{run} = \\frac{16}{80000} = 0.0002$. For the intercept, we'll just take a guess and call it $-3$.   \n",
    "   \n",
    "Let's plot a line with values of $a = -3$, and $b = 0.0002$:   \n",
    "   \n",
    "First, we will need to generate some values of y using the following formula:\n",
    "   \n",
    "$$\\hat{y}_i = a + bx_i$$   \n",
    "\n",
    "\n",
    "\n",
    "Calculating coefficient of the equation:\n",
    "- To calculate the coefficients we need the formula for \n",
    "\n",
    "Covariance \n",
    "\n",
    "$Cov (X,Y) = \\frac{\\sum (X_{i}- X)(Y_{j} - Y)}{n}$\n",
    "\n",
    "Variance\n",
    "\n",
    "$var(x) = \\frac{\\sum^{n}_{i} (x_i -\\mu)^2}{N}$\n",
    "\n",
    "- To calculate the coefficient m\n",
    "    - m = cov(x, y) / var(x)\n",
    "    - b = mean(y) ‚Äî m * mean(x)\n",
    "\n",
    "**Functions to calculate the Mean, Covariance, and Variance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c74157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean \n",
    "def get_mean(arr):\n",
    "    return np.sum(arr)/len(arr)\n",
    "\n",
    "# variance\n",
    "def get_variance(arr, mean):\n",
    "    return np.sum((arr-mean)**2)\n",
    "\n",
    "# covariance\n",
    "def get_covariance(arr_x, mean_x, arr_y, mean_y):\n",
    "    final_arr = (arr_x - mean_x)*(arr_y - mean_y)\n",
    "    return np.sum(final_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8932f",
   "metadata": {},
   "source": [
    "**Fuction to calculate the coefficients and the Linear Regression Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients \n",
    "# m = cov(x, y) / var(x)\n",
    "# b = y - m*x\n",
    "\n",
    "def get_coefficients(x, y):\n",
    "    x_mean = get_mean(x)\n",
    "    y_mean = get_mean(y)\n",
    "    m = get_covariance(x, x_mean, y, y_mean)/get_variance(x, x_mean)\n",
    "    b = y_mean - x_mean*m\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e179ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression \n",
    "# Train and Test\n",
    "# Train Split 80 % Test Split 20 %\n",
    "def linear_regression(x_train, y_train, x_test, y_test):\n",
    "    prediction = []\n",
    "    m, b = get_coefficients(x_train, y_train)\n",
    "    for x in x_test:\n",
    "        y = m*x + b\n",
    "        prediction.append(y)\n",
    "    \n",
    "    r2 = r2_score(prediction, y_test)\n",
    "    mse = mean_squared_error(prediction, y_test)\n",
    "    print(\"The R2 score of the model is: \", r2)\n",
    "    print(\"The MSE score of the model is: \", mse)\n",
    "    return prediction\n",
    "\n",
    "prediction = linear_regression(x[:80], y[:80], x[80:], y[80:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate values of y from a list of x, \n",
    "# Given parameters a and b\n",
    "\n",
    "def gen_y(x_list, a, b):\n",
    "    y_gen = []\n",
    "    for x_i in x_list:\n",
    "        y_i = a + b*x_i\n",
    "        y_gen.append(y_i)\n",
    "    \n",
    "    return(y_gen)\n",
    "\n",
    "# Generate the values by invoking the 'gen_y' function\n",
    "y_gen = gen_y(df.X, -3, 0.0002)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(df.X, df.Y)  # Plot the original data\n",
    "plt.plot(df.X, y_gen, color='red')  # Plot the line connecting the generated y-values\n",
    "plt.ylabel(\"ZAR/USD\")\n",
    "plt.xlabel(\"Value of Exports (ZAR, millions)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ec0333",
   "metadata": {},
   "source": [
    "**Visualize the regression line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb4088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reg_line(x, y):\n",
    "    # Calculate predictions for x ranging from 1 to 100\n",
    "    prediction = []\n",
    "    m, c = get_coefficients(x, y)\n",
    "    for x0 in range(1,100):\n",
    "        yhat = m*x0 + c\n",
    "        prediction.append(yhat)\n",
    "    \n",
    "    # Scatter plot without regression line\n",
    "    fig = plt.figure(figsize=(20,7))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.scatterplot(x=x, y=y)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Scatter Plot between X and Y')\n",
    "    \n",
    "    # Scatter plot with regression line\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.scatterplot(x=x, y=y, color = 'blue')\n",
    "    sns.lineplot(x = [i for i in range(1, 100)], y = prediction, color='red')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Regression Plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5bc7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression plot form seaborn\n",
    "# regplot is basically the combination of the scatter plot and the line plot\n",
    "sns.regplot(x, y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title(\"Regression Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aeea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reg_line(x, y):\n",
    "    # Calculate predictions for x ranging from 1 to 100\n",
    "    prediction = []\n",
    "    m, c = get_coefficients(x, y)\n",
    "    for x0 in range(1,100):\n",
    "        yhat = m*x0 + c\n",
    "        prediction.append(yhat)\n",
    "    \n",
    "    # Scatter plot without regression line\n",
    "    fig = plt.figure(figsize=(20,7))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.scatterplot(x=x, y=y)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Scatter Plot between X and Y')\n",
    "    \n",
    "    # Scatter plot with regression line\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.scatterplot(x=x, y=y, color = 'blue')\n",
    "    sns.lineplot(x = [i for i in range(1, 100)], y = prediction, color='red')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Regression Plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8247ed70",
   "metadata": {},
   "source": [
    "**Step 4. Formulate the model and Fit the Model (using library)**\n",
    "\n",
    "- Split the Data: Divide data into training and testing sets (e.g., 80% training, 20% testing).\n",
    "- Train the Model: Use a library like sklearn in Python to fit the regression model on the training data.\n",
    "- Evaluate the Model: Check metrics such as $ùëÖ^2$ (explained variance) and RMSE (Root Mean Squared Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Example\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create the dataset\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
    "y = np.array([2, 4, 5, 7, 8, 10, 11, 13, 14, 16])\n",
    "\n",
    "# Create the linear regression model\n",
    "model = LinearRegression().fit(X, y)\n",
    "\n",
    "# Get the slope and intercept of the line\n",
    "slope = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "# Plot the data points and the regression line\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, slope*X + intercept, color='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc4414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Example dataset\n",
    "X = data[['age', 'length_of_stay', 'severity', 'medication_adherence', 'comorbidities']]\n",
    "y = data['readmissions']\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse}, R^2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7183956e",
   "metadata": {},
   "source": [
    "**Let's check the calculted fit of the line** by measuring how far the true y-values of each point are from their corresponding y-value on the line.   \n",
    "   \n",
    "We'll use the equation below to calculate the error of each generated value of y:   \n",
    "   \n",
    "$$e_i = y_i - \\hat{y}_i$$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf61017",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.array(df.Y - y_gen)\n",
    "np.round(errors, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6a59c8",
   "metadata": {},
   "source": [
    "In addition to having some very large errors, we can also see that most of the errors are positive numbers. Ideally, we want our errors to be evenly distributed either side of zero - we want our line to best fit the data, i.e. no bias.\n",
    "   \n",
    "We can measure the overall error of the fit by calculating the **Residual Sum of Squares**:\n",
    "   \n",
    "$$RSS = \\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "The RSS finds the difference between the y-value of each data point and our estimated line (which may be either negative or positive), squares the difference, and then adds all the differences up. In other words, it's the sum of the squares of all the errors we calculated before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb521f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Residual sum of squares:\", (errors ** 2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784bd8a",
   "metadata": {},
   "source": [
    "## Least Squares Method\n",
    "Least Squares is another method that allows us to find the line of best fit while enforcing the constraint of minimising the residuals. More specifically, the **Least Squares Criterion** states that the sum of the squares of the residuals should be minimized, i.e.   \n",
    "$$Q = \\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "The formulae for the intercept, $a$, and the slope, $b$, are determined by minimizing the equation for the sum of the squared prediction errors:   \n",
    "$$Q = \\sum_{i=1}^n(y_i-(a+bx_i))^2$$\n",
    "\n",
    "Optimal values for $a$ and $b$ are found by differentiating $Q$ with respect to $a$ and $b$, setting both equal to 0 and then solving for $a$ and $b$.   \n",
    "   \n",
    "We won't go into the [derivation process](http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_10.pdf) here, but the equations for $a$ and $b$ are:   \n",
    "   \n",
    "$$b = \\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$$   \n",
    "   \n",
    "and:   \n",
    "   \n",
    "$$a = \\bar{y} - b\\bar{x}$$\n",
    "\n",
    "where $\\bar{y}$ and $\\bar{x}$ are the mean values of $y$ and $x$ in our dataset, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add85037",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.X.values\n",
    "Y = df.Y.values\n",
    "\n",
    "# Calculate x bar, y bar\n",
    "x_bar = np.mean(X)\n",
    "y_bar = np.mean(Y)\n",
    "\n",
    "# Calculate slope\n",
    "b = sum( (X-x_bar)*(Y-y_bar) ) / sum( (X-x_bar)**2 )\n",
    "\n",
    "# Calculate intercept\n",
    "a = y_bar - b*x_bar\n",
    "\n",
    "print(\"Slope = \" + str(b))\n",
    "print(\"Intercept = \" + str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function we created earlier:\n",
    "# it generates y-values for given x-values based on parameters a, b\n",
    "y_gen2 = gen_y(df.X, a, b)\n",
    "\n",
    "plt.scatter(df.X, df.Y)\n",
    "plt.plot(df.X, y_gen2, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7483a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors2 = np.array(y_gen2 - df.Y)\n",
    "print(np.round(errors2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce02e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Residual sum of squares:\", (errors2 ** 2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13429034",
   "metadata": {},
   "source": [
    "Here we can see our RSS has improved from ~867 down to ~321.  \n",
    "Furthermore, if we calculate the sum of the errors we find that the value is close to 0.\n",
    "\n",
    "----\n",
    "Intuitively, this should make sense as it is an indication that the sum of the positive errors is equal to the sum of the negative errors. The line fits in the 'middle' of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round off to 11 decimal places\n",
    "np.round(errors2.sum(),11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f3e50",
   "metadata": {},
   "source": [
    "**Regression cost functions: Regression model evaluation metrics**\n",
    "\n",
    "**loss function** is for a single training example. It is also sometimes called an error function. \n",
    "\n",
    "**cost function**, on the other hand, is the average loss over the entire training dataset. \n",
    "\n",
    "**Steps for Loss Functions**\n",
    "1. Define the predictor function f(X), and identify the parameters to find.\n",
    "2. Determine the loss for each training example.\n",
    "3. Derive the expression for the Cost Function, representing the average loss across all examples.\n",
    "4. Compute the gradient of the Cost Function concerning each unknown parameter.\n",
    "5. Select the learning rate and execute the weight update rule for a fixed number of iterations.\n",
    "\n",
    "These steps guide the optimization process, aiding in the determination of optimal model parameters.\n",
    "\n",
    "Regression model we generally use to evaluate the prediction error rates and model performance in regression analysis.\n",
    "\n",
    "- **R-squared (Coefficient of determination)** represents the coefficient of how well the values fit compared to the original values. The value from 0 to 1 interpreted as percentages. The higher the value is, the better the model is.\n",
    "- **MSE (Mean Squared Error)** represents the difference between the original and predicted values extracted by squared the average difference over the data set.\n",
    "- **RMSE (Root Mean Squared Error)** is the error rate by the square root of MSE.\n",
    "- **MAE (Mean absolute error)** represents the difference between the original and predicted values extracted by averaged the absolute difference over the data set.\n",
    "\n",
    "1. Mean Error (ME)\n",
    "- The error for each training data is calculated and then the mean value of all these errors is derived.\n",
    "- Errors can be both negative and positive. So they can cancel each other out during summation giving zero mean error for the model.\n",
    "- Not a recommended cost function but it does lay the foundation for other cost functions of regression models.\n",
    "\n",
    "2. Mean Squared Error (MSE)\n",
    "- known as L2 loss.\n",
    "- Here a square of the difference between the actual and predicted value is calculated to avoid any possibility of negative error(drawback cause).\n",
    "- It is measured as the average of the sum of squared differences between predictions and actual observations.\n",
    "- Since each error is squared, it helps to penalize even small deviations in prediction when compared to MAE. \n",
    "    - But if our dataset has outliers that contribute to larger prediction errors, then squaring this error further will magnify the error many times more and also lead to higher MSE error.\n",
    "    - MSE loss function penalizes the model for making large errors by squaring them. Squaring a large quantity makes it even larger\n",
    "        - it is less robust to outliers\n",
    "        - not to be used if our data is prone to many outliers.\n",
    "- The Significance of R-squared is:\n",
    "    - if $R^2$ = 1 : Best-fit Line\n",
    "    - if $R^2$ = 0.5 : still some errors\n",
    "    - if $R^2$ = 0.05 : not performing well\n",
    "\n",
    "Graphically\n",
    "- It is a positive quadratic function (of the form $ax^2 + bx + c$ where $a > 0$)\n",
    "- A quadratic function only has a global minimum. \n",
    "    - Since there are no local minima, we will never get stuck in one. \n",
    "- Hence, it is always guaranteed that Gradient Descent will converge (if it converges at all) to the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ff6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_MSE(m, b, X, Y, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    for i in range(N):\n",
    "        # Calculate partial derivatives\n",
    "        # -2x(y - (mx + b))\n",
    "        m_deriv += -2*X[i] * (Y[i] - (m*X[i] + b))\n",
    "\n",
    "        # -2(y - (mx + b))\n",
    "        b_deriv += -2*(Y[i] - (m*X[i] + b))\n",
    "\n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfde74f",
   "metadata": {},
   "source": [
    "3. Mean Absolute Error (MAE)\n",
    "- known as L1 Loss.\n",
    "- Absolute Error for each training example is the distance between the predicted and the actual values, irrespective of the sign.\n",
    "    - it is the absolute difference between the actual and predicted values.\n",
    "- Here an absolute difference between the actual and predicted value is calculated to avoid any possibility of negative error.\n",
    "- It is measured as the average of the sum of absolute differences between predictions and actual observations.\n",
    "    - It is robust to outliers thus it will give better results even when our dataset has noise or outliers.\n",
    "    - MAE cost is more robust to outliers as compared to MSE\n",
    "-  The cost is the Mean of these Absolute Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34787f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_MAE(m, b, X, Y, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    for i in range(N):\n",
    "        # Calculate partial derivatives\n",
    "        # -x(y - (mx + b)) / |mx + b|\n",
    "        m_deriv += - X[i] * (Y[i] - (m*X[i] + b)) / abs(Y[i] - (m*X[i] + b))\n",
    "\n",
    "        # -(y - (mx + b)) / |mx + b|\n",
    "        b_deriv += -(Y[i] - (m*X[i] + b)) / abs(Y[i] - (m*X[i] + b))\n",
    "\n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ab3d2",
   "metadata": {},
   "source": [
    "4. Huber Loss\n",
    "\n",
    "- The Huber loss combines the best properties of MSE and MAE.\n",
    "- It is quadratic for smaller errors and is linear otherwise (and similarly for its gradient). \n",
    "- It is identified by its delta parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b94332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_Huber(m, b, X, Y, delta, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    for i in range(N):\n",
    "        # derivative of quadratic for small values and of linear for large values\n",
    "        if abs(Y[i] - m*X[i] - b) <= delta:\n",
    "          m_deriv += -X[i] * (Y[i] - (m*X[i] + b))\n",
    "          b_deriv += - (Y[i] - (m*X[i] + b))\n",
    "        else:\n",
    "          m_deriv += delta * X[i] * ((m*X[i] + b) - Y[i]) / abs((m*X[i] + b) - Y[i])\n",
    "          b_deriv += delta * ((m*X[i] + b) - Y[i]) / abs((m*X[i] + b) - Y[i])\n",
    "    \n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ddd25a",
   "metadata": {},
   "source": [
    "**Step 5: Interpret the Results**\n",
    "\n",
    "Residual Analysis:\n",
    "- Check normal distribution and normality for the residuals.\n",
    "- Homoscedasticity describes a situation in which error term is the same across all values of the independent variables. \n",
    "    - means that the residuals are equal across the regression line.\n",
    "\n",
    "Interpretation of Regression Output\n",
    "- R-Squared : is a statistical measure of fit that indicates how much variation of a dependent variable is explained by the independent variables. \n",
    "    - Higher R-Squared value represents smaller differences between the observed data and fitted values.\n",
    "\n",
    "Hypothesis testing in Linear Regression\n",
    "- Once you have fitted a straight line on the data, you need to ask, \n",
    "    - ‚ÄúIs this straight line a significant fit for the data?‚Äù Or \n",
    "    - ‚ÄúIs the beta coefficient explain the variance in the data plotted?‚Äù \n",
    "- Here comes the idea of hypothesis testing on the beta coefficient:\n",
    "\n",
    "$H_0 : B_1  = 0$\n",
    "    \n",
    "$H_A : B_1  ‚â† 0$\n",
    "\n",
    "Interpret the Regression Equation\n",
    "- The coefficients ($ùõΩ$) indicate the magnitude and direction of the relationship between each predictor and readmissions.\n",
    "    - Example: A coefficient of -0.5 for medication_adherence means that for every 1% increase in medication adherence, readmissions decrease by 0.5.\n",
    "- The intercept ($ùõΩ_0$) represents the expected number of readmissions when all predictors are zero.\n",
    "\n",
    "Assessing the Model Fit\n",
    "- Other parameters to assess a model are:\n",
    "    - t statistic: It is used to determine the p-value and hence, helps in determining whether the coefficient is significant or not\n",
    "    - F statistic: It is used to assess whether the overall model fit is significant or not. \n",
    "        - the higher the value of the F-statistic, the more significant a model turns out to be.\n",
    "\n",
    "**Optimization technique/Strategy**\n",
    "\n",
    "We will use Gradient Descent as an optimization strategy to find the regression line.\n",
    "- Weight Update Rule\n",
    "\n",
    "NB: Perform optimization on the training data and check its performance on a new validation data.\n",
    "\n",
    "**Gradient Descent for Linear Regression**\n",
    "\n",
    "What is gradient descent?\n",
    "- lay man: \n",
    "    - It is a way of checking the ground near you and observe where the land tends to descend.\n",
    "    - It gives an idea in what direction you should take your steps.\n",
    "    - It helps models find the optimal set of parameters by iteratively adjusting them in the opposite direction of the gradient, aiming to find the optimal set of parameters.\n",
    "\n",
    "Mathematical terms:\n",
    "- find out the best parameters ($Œ∏_1$) and ($Œ∏_2$) for our learning algorithm.\n",
    "\n",
    "Cost space is how our algorithm would perform when we choose a particular value for a parameter.\n",
    "\n",
    "Cost Function is a function that measures the performance of a model for any given data. Cost Function quantifies the error between predicted values and expected values and presents it in the form of a single real number.\n",
    "\n",
    "1. Make a hypothesis with initial parameters\n",
    "- Hypothesis: $h_Œ∏(x) = Œ∏_0 + Œ∏_1 x$\n",
    "- Parameters: $Œ∏_o, Œ∏_1$\n",
    "2. Calculate the Cost function\n",
    "- Cost Function: $J(Œ∏_o, Œ∏_1) = \\frac{1}{2m}\\sum^{m}_{i = 1} (h_Œ∏ (x^{(i)}) - y^{i})^2$\n",
    "3. The goal is to reduce the cost function, we modify the parameters by using the Gradient descent algorithm over the given data.\n",
    "- Goal: $minimize_{Œ∏_o, Œ∏_1} J(Œ∏_o, Œ∏_1)$\n",
    "\n",
    "**Gradient descent**\n",
    "\n",
    "- one of the optimization algorithms that optimize the cost function (objective function) to reach the optimal minimal solution.\n",
    "- aims to find the parameters that minimize this discrepancy and improve the model‚Äôs performance.\n",
    "    - Need to reduce the cost function (MSE) for all data points. \n",
    "    - This is done by updating the values of the slope coefficient and the constant coefficient iteratively until we get an optimal solution for the linear function.\n",
    "\n",
    "The algorithm operates by calculating the gradient of the cost function, \n",
    "- which indicates the direction and magnitude of the steepest ascent. \n",
    "\n",
    "However, since the goal is to minimize the cost function, gradient descent moves in the opposite direction of the gradient, \n",
    "- known as the negative gradient direction.\n",
    "\n",
    "Iteratively updating the model‚Äôs parameters in the negative gradient direction, gradient descent gradually converges towards the optimal set of parameters that yields the lowest cost.\n",
    "\n",
    "- Hyperparameter: learning rate, determines the step size taken in each iteration, influencing the speed and stability of convergence.\n",
    "\n",
    "Gradient descent can be applied to:\n",
    "- linear regression, \n",
    "- logistic regression, \n",
    "- neural networks, and \n",
    "- support vector machines.\n",
    "\n",
    "**Definition**: Gradient descent is an iterative optimization algorithm for finding the local minimum of a function.\n",
    "\n",
    "To find the local minimum of a function using gradient descent, we must take steps proportional to the negative of the gradient (move away from the gradient) of the function at the current point.\n",
    "- If we take steps proportional to the positive of the gradient (moving towards the gradient), we will approach a local maximum of the function, and the procedure is called Gradient Ascent.\n",
    "\n",
    "The goal of the gradient descent algorithm is to minimize the given function (say, cost function)\n",
    "- it performs two steps iteratively:\n",
    "1. Compute the gradient (slope), the first-order derivative of the function at that point\n",
    "2. Make a step (move) in the direction opposite to the gradient. The opposite direction of the slope increases from the current point by alpha times the gradient at that point.\n",
    "- number of steps you‚Äôre taking can be considered as the learning rate, and this decides how fast the algorithm converges to the minima.\n",
    "\n",
    "This code creates a function called gradient_descent, which requires the training data, learning rate, and number of iterations as parameters.\n",
    "\n",
    "Steps :\n",
    "1. Sets weights and bias to arbitrary values during initialization.\n",
    "2. Executes a set number of iterations for loops.\n",
    "3. Computes the estimated y values by utilizing the existing weights and bias.\n",
    "4. Calculates the discrepancy between expected and real y values.\n",
    "5. Determines the changes in the cost function based on weights and bias.\n",
    "6. Adjusts the weights and bias by incorporating the gradients and learning rate.\n",
    "7. Outputs the acquired weights and bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad28b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(X, y, learning_rate, num_iters):\n",
    "  \"\"\"\n",
    "  Performs gradient descent to find optimal weights and bias for linear regression.\n",
    "\n",
    "  Args:\n",
    "      X: A numpy array of shape (m, n) representing the training data features.\n",
    "      y: A numpy array of shape (m,) representing the training data target values.\n",
    "      learning_rate: The learning rate to control the step size during updates.\n",
    "      num_iters: The number of iterations to perform gradient descent.\n",
    "\n",
    "  Returns:\n",
    "      A tuple containing the learned weights and bias.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize weights and bias with random values\n",
    "  m, n = X.shape\n",
    "  weights = np.random.rand(n)\n",
    "  bias = 0\n",
    "\n",
    "  # Loop for the number of iterations\n",
    "  for i in range(num_iters):\n",
    "    # Predict y values using current weights and bias\n",
    "    y_predicted = np.dot(X, weights) + bias\n",
    "\n",
    "    # Calculate the error\n",
    "    error = y - y_predicted\n",
    "\n",
    "    # Calculate gradients for weights and bias\n",
    "    weights_gradient = -2/m * np.dot(X.T, error)\n",
    "    bias_gradient = -2/m * np.sum(error)\n",
    "\n",
    "    # Update weights and bias using learning rate\n",
    "    weights -= learning_rate * weights_gradient\n",
    "    bias -= learning_rate * bias_gradient\n",
    "\n",
    "  return weights, bias\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[1, 1], [2, 2], [3, 3]])\n",
    "y = np.array([2, 4, 5])\n",
    "learning_rate = 0.01\n",
    "num_iters = 100\n",
    "\n",
    "weights, bias = gradient_descent(X, y, learning_rate, num_iters)\n",
    "\n",
    "print(\"Learned weights:\", weights)\n",
    "print(\"Learned bias:\", bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af34d913",
   "metadata": {},
   "source": [
    "How Does Gradient Descent Work?\n",
    "1. The algorithm optimizes to minimize the model‚Äôs cost function.\n",
    "2. The cost function measures how well the model fits the training data and defines the difference between the predicted and actual values.\n",
    "3. The cost function‚Äôs gradient is the derivative with respect to the model‚Äôs parameters and points in the direction of the steepest ascent.\n",
    "4. The algorithm starts with an initial set of parameters and updates them in small steps to minimize the cost function.\n",
    "5. In each iteration of the algorithm, it computes the gradient of the cost function with respect to each parameter.\n",
    "6. The gradient tells us the direction of the steepest ascent, and by moving in the opposite direction, we can find the direction of the steepest descent.\n",
    "7. The learning rate controls the step size, which determines how quickly the algorithm moves towards the minimum.\n",
    "8. The process is repeated until the cost function converges to a minimum. Therefore indicating that the model has reached the optimal set of parameters.\n",
    "9. Different variations of gradient descent include batch gradient descent, stochastic gradient descent, and mini-batch gradient descent, each with advantages and limitations.\n",
    "10. Efficient implementation of gradient descent is essential for performing well in machine learning tasks. The choice of the learning rate and the number of iterations can significantly impact the algorithm‚Äôs performance.\n",
    "\n",
    "On the basis of differentiation techniques \n",
    "- Gradient descent requires Calculation of gradient by differentiation of cost function. We can either use first order differentiation or second order differentiation.\n",
    "    - First order Differentiation\n",
    "    - Second order Differentiation.\n",
    "\n",
    "To update B 0 and B 1, we take gradients from the cost function. To find these gradients, we take partial derivatives for $B_0$ and $B_1$.\n",
    "\n",
    "$J = \\frac{1}{n} \\sum^{n}_{i = 1} (ùõΩ_{0}+ùõΩ_{1} . x_i - y_i)^2$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial ùõΩ_{0}} = \\frac{2}{n} \\sum^{n}_{i = 1} (ùõΩ_{0}+ùõΩ_{1} . x_i - y_i)$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial ùõΩ_{1}} = \\frac{2}{n} \\sum^{n}_{i = 1} (ùõΩ_{0}+ùõΩ_{1} . x_i - y_i) . x_i$\n",
    "\n",
    "$ùõΩ_{0} = ùõΩ_{0} - \\alpha . \\frac{2}{n} \\sum^{n}_{i = 1} ( y_{pred} - y_{i}) $\n",
    "\n",
    "$ùõΩ_{1} = ùõΩ_{1} - \\alpha . \\frac{2}{n} \\sum^{n}_{i = 1} ( y_{pred} - y_{i}) . x_i $\n",
    "\n",
    "Where: \n",
    "- The partial derivates are the gradients, and they are used to update the values of $B_0$ and $B_1$. \n",
    "- Alpha is the learning rate.\n",
    "\n",
    "**Types of Gradient Descent**\n",
    "\n",
    "Classified by two methods mainly:\n",
    "- On the basis of data ingestion: choice of gradient descent algorithm depends on the problem at hand and the size of the dataset.\n",
    "\n",
    "**Full Batch Gradient Descent Algorithm**:\n",
    "- Batch gradient descent,\n",
    "    - also known as vanilla gradient descent, \n",
    "- full batch gradient descent algorithms, you use whole data at once to compute the gradient.\n",
    "    - It updates the model‚Äôs parameters using the gradient of the entire training set.\n",
    "- It calculates the average gradient of the cost function for all the training examples and updates the parameters in the opposite direction.\n",
    "    - calculates the error for each example within the training dataset.\n",
    "    - The model is not changed until every training sample has been assessed. \n",
    "        - The entire procedure is referred to as a **cycle and a training epoch**.\n",
    "- Batch gradient descent guarantees convergence to the global minimum but can be computationally expensive and slow for large datasets.\n",
    "    - Batch gradient descent is suitable for small datasets.\n",
    "    - Its computational efficiency, which produces a stable error gradient and a stable convergence.\n",
    "- Drawbacks are that the stable error gradient can sometimes result in a state of convergence that isn‚Äôt the best the model can achieve. \n",
    "    - It also requires the entire training dataset to be in memory and available to the algorithm.\n",
    "\n",
    "Advantages\n",
    "- Fewer model updates mean that this variant of the steepest descent method is more computationally efficient than the stochastic gradient descent method.\n",
    "- Reducing the update frequency provides a more stable error gradient and a more stable convergence for some problems.\n",
    "- Separating forecast error calculations and model updates provides a parallel processing-based algorithm implementation.\n",
    "\n",
    "Disadvantages\n",
    "- A more stable error gradient can cause the model to prematurely converge to a suboptimal set of parameters.\n",
    "- End-of-training epoch updates require the additional complexity of accumulating prediction errors across all training examples.\n",
    "- The batch gradient descent method typically requires the entire training dataset in memory and is implemented for use in the algorithm.\n",
    "- Large datasets can result in very slow model updates or training speeds.\n",
    "- Slow and require more computational power.\n",
    "\n",
    "#### Variants\n",
    "\n",
    "##### Vanilla Gradient Descent, \n",
    "\n",
    "Vanilla means pure / without any adulteration.\n",
    "- simplest form of gradient descent technique\n",
    "    - main feature is that we take small steps in the direction of the minima by taking gradient of the cost function.\n",
    "\n",
    "Pseudocode Vanilla Gradient Descent\n",
    "\n",
    "$ update = learning rate * gradient of parameters$\n",
    "\n",
    "$ parameters = parameters - update$\n",
    "\n",
    "- make an update to the parameters by taking gradient of the parameters. \n",
    "- And multiplying it by a learning rate, which is essentially a constant number suggesting how fast we want to go the minimum. 4\n",
    "**Learning rate** is a hyper-parameter and should be treated with care when choosing its value.\n",
    "\n",
    "##### Gradient Descent with Momentum\n",
    "\n",
    "Tweaks the above algorithm in such a way that we pay heed to the prior step before taking the next step.\n",
    "\n",
    "Pseudocode Gradient Descent with Momentum\n",
    "\n",
    "$ update = learning_rate * gradient$ \n",
    "\n",
    "$ velocity = previous_update * momentum$ \n",
    "\n",
    "$ parameter = parameter + velocity ‚Äì update$ \n",
    "\n",
    "Introduces Velocity, which considers the previous update and a constant which is called momentum.\n",
    "\n",
    "##### ADAGRAD\n",
    "\n",
    "ADAGRAD uses adaptive technique for learning rate updation. In this algorithm, on the basis of how the gradient has been changing for all the previous iterations we try to change the learning rate.\n",
    "\n",
    "Pseudocode ADAGRAD\n",
    "\n",
    "$ grad_component = previous_grad_component + (gradient * gradient)$ \n",
    "\n",
    "$ rate_change = square_root(grad_component) + epsilon$\n",
    "\n",
    "$ adapted_learning_rate = learning_rate * rate_change$\n",
    "\n",
    "$update = adapted_learning_rate * gradient$\n",
    "\n",
    "$parameter = parameter ‚Äì update$\n",
    "\n",
    "where:\n",
    "-  epsilon is a constant which is used to keep rate of change of learning rate in check.\n",
    "\n",
    "##### ADAM\n",
    "\n",
    "ADAM is one more adaptive technique which builds on adagrad and further reduces it downside.\n",
    "- consider this as momentum + ADAGRAD.\n",
    "\n",
    "Pseudocode.\n",
    "\n",
    "$ adapted_gradient = previous_gradient + ((gradient ‚Äì previous_gradient) * (1 ‚Äì beta1))$\n",
    "\n",
    "$ gradient_component = (gradient_change ‚Äì previous_learning_rate)$\n",
    "\n",
    "$ adapted_learning_rate =  previous_learning_rate + (gradient_component * (1 ‚Äì beta2))$\n",
    "\n",
    "$ update = adapted_learning_rate * adapted_gradient$\n",
    "\n",
    "$ parameter = parameter ‚Äì update$\n",
    "\n",
    "where:\n",
    "- beta1 and beta2 are constants to keep changes in gradient and learning rate in check\n",
    "\n",
    "There are also second order differentiation method like **l-BFGS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ec0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDRegressor:\n",
    "    \n",
    "    def __init__(self,learning_rate=0.01,epochs=100):\n",
    "        \n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def fit(self,X_train,y_train):\n",
    "        # init your coefs\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            # update all the coef and the intercept\n",
    "            y_hat = np.dot(X_train,self.coef_) + self.intercept_\n",
    "            #print(\"Shape of y_hat\",y_hat.shape)\n",
    "            intercept_der = -2 * np.mean(y_train - y_hat)\n",
    "            self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "            \n",
    "            coef_der = -2 * np.dot((y_train - y_hat),X_train)/X_train.shape[0]\n",
    "            self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "        \n",
    "        print(self.intercept_,self.coef_)\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c381ad",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent Algorithm**\n",
    "- stochastic you take a sample while computing the gradient.\n",
    "    - It randomly selects a training dataset example, \n",
    "        - changes the parameters for each training sample one at a time for each training example in the dataset.\n",
    "            - The regular updates give us a fairly accurate idea of the rate of improvement. (benefit)\n",
    "    - computes the gradient of the cost function for that example, \n",
    "    - and updates the parameters in the opposite direction.\n",
    "- stochastic gradient descent algorithm is more suitable for large datasets.\n",
    "- It is computationally efficient and can converge faster than batch gradient descent. It can be noisy (produce noisy gradients), cause the error rate to fluctuate rather than gradually go down and may not converge to the global minimum.\n",
    "\n",
    "Advantages\n",
    "- You can instantly see your model‚Äôs performance and improvement rates with frequent updates.\n",
    "- This variant of the steepest descent method is probably the easiest to understand and implement, especially for beginners.\n",
    "- Increasing the frequency of model updates will allow you to learn more about some issues faster.\n",
    "- The noisy update process allows the model to avoid local minima (e.g., premature convergence).\n",
    "- Faster and require less computational power.\n",
    "- Suitable for the larger dataset.\n",
    "\n",
    "Disadvantages\n",
    "- Frequent model updates are more computationally intensive than other steepest descent configurations, and it takes considerable time to train the model with large datasets.\n",
    "- Frequent updates can result in noisy gradient signals. This can result in model parameters and cause errors to fly around (more variance across the training epoch).\n",
    "- A noisy learning process along the error gradient can also make it difficult for the algorithm to commit to the model‚Äôs minimum error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c4540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "clf.fit(X, y)\n",
    "SGDClassifier(max_iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b03ce9",
   "metadata": {},
   "source": [
    "**Mini-batch Gradient Descent**\n",
    "- Mini-batch is a good compromise between the two and is often used in practice.\n",
    "- updates the model‚Äôs parameters using the gradient of a small batch size of the training dataset, known as a mini-batch. \n",
    "- It calculates the average gradient of the cost function for the mini-batch and updates the parameters in the opposite direction.\n",
    "- It is the most commonly used method in practice because combines the ideas of batch gradient descent with SGD.\n",
    "        - strikes a balance between batch gradient descent‚Äôs effectiveness and stochastic gradient descent‚Äôs durability.\n",
    "- It is computationally efficient and less noisy than stochastic gradient descent while still being able to converge to a good solution.\n",
    "- Mini-batch sizes typically range from 50 to 256.\n",
    "\n",
    "Advantages\n",
    "- The model is updated more frequently than the stack gradient descent method, allowing for more robust convergence and avoiding local minima.\n",
    "- Batch updates provide a more computationally efficient process than stochastic gradient descent.\n",
    "- Batch processing allows for both the efficiency of not having all the training data in memory and implementing the algorithm.\n",
    "\n",
    "Disadvantages\n",
    "- Mini-batch requires additional hyperparameters ‚Äúmini-batch size‚Äù to be set for the learning algorithm.\n",
    "- Error information should be accumulated over a mini-batch of training samples, such as batch gradient descent.\n",
    "- it will generate complex functions.\n",
    "\n",
    "Configure Mini-Batch Gradient Descent:\n",
    "\n",
    "- The mini-batch steepest descent method is a variant of the steepest descent method recommended for most applications, intense learning.\n",
    "- Mini-batch sizes, commonly called ‚Äúbatch sizes‚Äù for brevity, are often tailored to some aspect of the computing architecture in which the implementation is running. \n",
    "        - For example, a power of 2 that matches the memory requirements of the GPU or CPU hardware, such as 32, 64, 128, and 256.\n",
    "- The stack size is a slider for the learning process.\n",
    "- Smaller values ‚Äã‚Äãallow the learning process to converge quickly at the expense of noise in the training process. Larger values ‚Äã‚Äãresult in a learning - process that slowly converges to an accurate estimate of the error gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8eaf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBGDRegressor:\n",
    "    \n",
    "    def __init__(self,batch_size,learning_rate=0.01,epochs=100):\n",
    "        \n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def fit(self,X_train,y_train):\n",
    "        # init your coefs\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            for j in range(int(X_train.shape[0]/self.batch_size)):\n",
    "                \n",
    "                idx = random.sample(range(X_train.shape[0]),self.batch_size)\n",
    "                \n",
    "                y_hat = np.dot(X_train[idx],self.coef_) + self.intercept_\n",
    "                #print(\"Shape of y_hat\",y_hat.shape)\n",
    "                intercept_der = -2 * np.mean(y_train[idx] - y_hat)\n",
    "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "\n",
    "                coef_der = -2 * np.dot((y_train[idx] - y_hat),X_train[idx])\n",
    "                self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "        \n",
    "        print(self.intercept_,self.coef_)\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c2ddb9",
   "metadata": {},
   "source": [
    "**Step 6: Use the Model for Decision-Making**\n",
    "\n",
    "Understanding which factors significantly influence readmissions,\n",
    "\n",
    "To do this, you need a systematic approach grounded in exploratory analysis, statistical rigor, and effective communication\n",
    "\n",
    "1. Thinking Approach: Identifying Significant Factors\n",
    "- Define the Business Objective\n",
    "    - Objective: Identify key drivers of hospital readmissions (to improve patient care and optimize resource allocation)\n",
    "    - Questions to Answer:\n",
    "        - What are the strongest predictors of readmissions?\n",
    "        - Which predictors can be influenced through policy or operational changes?\n",
    "        - How much can readmissions be reduced if certain factors are addressed?\n",
    "\n",
    "- Perform Exploratory Data Analysis (EDA)\n",
    "    - Inspect Data Distributions: Use histograms and boxplots to understand the spread of variables.\n",
    "    - Check Relationships:\n",
    "        - Pairwise correlations for numerical variables (e.g., length_of_stay vs. readmissions).\n",
    "        - Grouped summaries for categorical variables (e.g., readmissions across age groups).\n",
    "        - Example Insights:\n",
    "            - Patients with longer stays might have higher readmission risks.\n",
    "            - Non-adherence to medication might strongly correlate with readmissions.\n",
    "\n",
    "- Statistical Hypothesis Testing\n",
    "    - Use statistical tests to confirm relationships:\n",
    "        - T-tests for differences in means (e.g., medication adherence between high and low readmission groups).\n",
    "        - Chi-square tests for independence between categorical variables (e.g., age group vs. readmission rates).\n",
    "\n",
    "Example 1: Statistical Hypothesis Testing for Medication Adherence\n",
    "- Objective: Determine if medication adherence significantly differs between patients who are readmitted and those who are not.\n",
    "- Approach: Two-Sample t-Test\n",
    "- Hypotheses: \n",
    "    - $ùêª_0$ : The mean adherence rate is the same for both groups (readmitted and not readmitted).\n",
    "    - $ùêª_ùëé$ : The mean adherence rate differs between the groups.\n",
    "\n",
    "- Steps:\n",
    "    - Prepare the Data:\n",
    "    - Split patients into two groups: \"Readmitted\" and \"Not Readmitted.\"\n",
    "    - Collect medication adherence rates for each group.\n",
    "\n",
    "- Check Assumptions:\n",
    "    - Normality: Use a Shapiro-Wilk or Kolmogorov-Smirnov test to check if adherence rates are normally distributed.\n",
    "    - Equal Variance: Use Levene‚Äôs test or Bartlett‚Äôs test.\n",
    "\n",
    "- Perform the t-Test:\n",
    "    - If variances are equal, use a standard t-test. If not, use Welch‚Äôs t-test.\n",
    "\n",
    "- Interpret Results: \n",
    "    - If $ùëù < 0.05$, reject $ùêª_0$\n",
    "    - Conclude that adherence rates differ significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21143b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Example data\n",
    "adherence_readmitted = [0.7, 0.65, 0.6, 0.75, 0.8]  # Adherence rates for readmitted\n",
    "adherence_not_readmitted = [0.9, 0.85, 0.88, 0.92, 0.89]  # Adherence rates for not readmitted\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = ttest_ind(adherence_readmitted, adherence_not_readmitted, equal_var=False)\n",
    "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3ddb9d",
   "metadata": {},
   "source": [
    "Example 2: Statistical Hypothesis Testing for Age Group vs. Readmission Rates\n",
    "- Objective: Test if age group (categorical variable) is independent of readmission status.\n",
    "- Approach: Chi-Square Test of Independence\n",
    "- Hypotheses:\n",
    "    - $ùêª_0$ : Age group is independent of readmission status.\n",
    "    - $ùêª_ùëé$ : Age group and readmission status are dependent.\n",
    "\n",
    "- Steps:\n",
    "    - Create a Contingency Table:\n",
    "        - Rows: Age groups (e.g., <40, 40‚Äì60, >60).\n",
    "        - Columns: Readmission status (e.g., Yes, No).\n",
    "\n",
    "- Perform the Chi-Square Test:\n",
    "\n",
    "- Interpret Results:\n",
    "    - If $ ùëù< 0.05$, reject $ùêª_0$‚Äã\n",
    "    - Conclude that age group influences readmission rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aecd0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Contingency table\n",
    "table = np.array([[50, 200], [70, 230], [100, 300]])\n",
    "\n",
    "# Perform Chi-Square Test\n",
    "chi2, p_value, dof, expected = chi2_contingency(table)\n",
    "print(f\"Chi2 Statistic: {chi2}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d538a",
   "metadata": {},
   "source": [
    "Example 3: Statistical Hypothesis Testing for Length of Stay (LOS)\n",
    "- Objective: Compare Average LOS for Readmitted vs. Not Readmitted Patients\n",
    "- Approach: Two-Sample t-Test\n",
    "    - $ùêª_0$ : The mean LOS is the same for readmitted and non-readmitted patients.\n",
    "    - $ùêª_ùëé$ : The mean LOS differs.\n",
    "- Steps:\n",
    "    - Prepare the Data:\n",
    "    - Split patients into two groups: \"Readmitted\" and \"Not Readmitted.\"\n",
    "    - Collect medication Length of stay for each group.\n",
    "\n",
    "- Check Assumptions:\n",
    "    - Normality: Use a Shapiro-Wilk or Kolmogorov-Smirnov test to check if Lengths of stay are normally distributed.\n",
    "    - Equal Variance: Use Levene‚Äôs test or Bartlett‚Äôs test.\n",
    "\n",
    "- Perform the t-Test:\n",
    "    - If variances are equal, use a standard t-test. If not, use Welch‚Äôs t-test.\n",
    "\n",
    "- Interpret Results: \n",
    "    - If $ùëù < 0.05$, reject $ùêª_0$\n",
    "    - Conclude that adherence rates differ significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a39723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe8fb84d",
   "metadata": {},
   "source": [
    "Example 4: Relationship Between LOS and Readmission Rate\n",
    "- Approach: ANOVA (Analysis of Variance)\n",
    "- Objective: Check if LOS groups (<3 days, 3‚Äì7 days, >7 days) have significantly different readmission rates.\n",
    "- Hypotheses: \n",
    "    - $ùêª_0$ : The mean readmission rate is the same across all LOS groups.\n",
    "    - $ùêª_ùëé$ : At least one group differs.\n",
    "- Steps:\n",
    "    - Group the Data:\n",
    "        - Divide LOS into groups.\n",
    "        - Calculate readmission rates for each group.\n",
    "- Perform ANOVA:\n",
    "- Interpret Results:\n",
    "    - If $ùëù < 0.05$\n",
    "    - reject $ùêª_0$\n",
    "    - Conclude that LOS impacts readmission rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab649501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Example data\n",
    "readmission_short = [0.1, 0.12, 0.08, 0.15]  # Readmission rates for <3 days\n",
    "readmission_medium = [0.2, 0.22, 0.25, 0.18]  # Readmission rates for 3‚Äì7 days\n",
    "readmission_long = [0.35, 0.4, 0.38, 0.42]  # Readmission rates for >7 days\n",
    "\n",
    "# Perform ANOVA\n",
    "f_stat, p_value = f_oneway(readmission_short, readmission_medium, readmission_long)\n",
    "print(f\"F-statistic: {f_stat}, P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be03fb0",
   "metadata": {},
   "source": [
    "\n",
    "- Build and Interpret a Regression Model\n",
    "    - Fit the Linear Regression model to identify significant predictors:\n",
    "    - Check p-values of coefficients: Variables with p-values below a chosen threshold (e.g., 0.05) are statistically significant.\n",
    "    - Evaluate effect size: Large coefficients indicate strong influence on the target.\n",
    "    - Test for interaction effects, such as how length_of_stay and severity jointly influence readmissions.\n",
    "\n",
    "- Refine the Model\n",
    "    - Handle multicollinearity: Use Variance Inflation Factor (VIF) to remove or combine highly correlated predictors.\n",
    "    - Validate the model: Perform cross-validation to ensure robustness.\n",
    "\n",
    "This will help the institute to:\n",
    "- Improve medication adherence programs for high-risk patients.\n",
    "- Extend hospital stays for patients with severe conditions if needed.\n",
    "- Schedule follow-up visits more effectively to minimize readmission risks.\n",
    "\n",
    "Example 2: Predicting Readmissions Based on LOS\n",
    "- Approach: Linear Regression\n",
    "- Objective: Use regression to predict readmissions based on LOS and other predictors.\n",
    "\n",
    "##### Linear Regression Helps Solve This Problem\n",
    "- Quantifies Relationships: Identifies and quantifies the factors contributing to readmissions.\n",
    "- Predicts Outcomes: Provides actionable predictions to guide healthcare interventions.\n",
    "- Allocates Resources: Helps prioritize patients who need more attention post-discharge.\n",
    "- Supports Policy Changes: Enables data-driven policy improvements in patient care."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03949464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Example data\n",
    "X = [2, 4, 6, 8, 10]  # LOS\n",
    "y = [0, 1, 0, 1, 1]  # Readmission (0 = No, 1 = Yes)\n",
    "\n",
    "# Add constant for intercept\n",
    "X = sm.add_constant(X)\n",
    "model = sm.Logit(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d99cd9",
   "metadata": {},
   "source": [
    "2. Presenting Findings to Senior Management and Board\n",
    "- Tailor Communication to the Audience\n",
    "    - Senior management: Focus on actionable insights, resource implications, and patient care improvements.\n",
    "    - Board of directors: Emphasize high-level trends, financial impacts, and alignment with strategic goals.\n",
    "\n",
    "- Structure of Presentation\n",
    "    - Introduction\n",
    "        - Start with the context: \"Readmission rates are a critical indicator of hospital performance and patient care quality.\"\n",
    "        - Summarize the objective: \"This study identifies key factors driving readmissions and proposes targeted interventions.\"\n",
    "\n",
    "    - Key Findings\n",
    "        - Use visuals like \n",
    "            - bar charts, \n",
    "            - scatter plots, and \n",
    "            - regression coefficient tables:\n",
    "                - Example: \"Medication adherence has the strongest inverse relationship with readmissions. A 10% increase in adherence reduces readmissions by 5%.\"\n",
    "            - Highlight statistical significance:\n",
    "                - \"Length of stay and severity are significant at p < 0.05, confirming their importance.\"\n",
    "    \n",
    "    - Implications\n",
    "        - Show real-world impact: \"Addressing non-adherence could prevent ~300 readmissions annually, saving $1.2M in costs.\"\n",
    "        - Prioritize recommendations: \"Focus on medication adherence programs, especially for older patients with comorbidities.\"\n",
    "\n",
    "    - Actionable Recommendations\n",
    "        - Immediate Steps:\n",
    "            - Develop a post-discharge follow-up protocol for high-risk groups.\n",
    "            - Launch an adherence monitoring program.\n",
    "        - Future Research:\n",
    "            - Investigate additional factors like social determinants of health.\n",
    "\n",
    "    - Conclusion\n",
    "        - Reinforce value: \"By addressing these factors, we can improve patient outcomes, meet regulatory benchmarks, and reduce financial strain.\"\n",
    "\n",
    "- Tools for Communication\n",
    "    - Visual Dashboards: Create dashboards showing predicted readmissions, trends over time, and \"what-if\" scenarios.\n",
    "    - Executive Summaries: Provide concise summaries with high-impact visuals and key takeaways.\n",
    "    - Financial Impact Models: Quantify cost savings or ROI of proposed interventions.\n",
    "\n",
    "3. Example Insights and Visualizations\n",
    "Insight Example: Medication Adherence\n",
    "    - Insight: \"Medication adherence has a strong negative correlation with readmissions ($ùëÖ=‚àí0.65$)\n",
    "        - A 10% increase in adherence is associated with a 5% reduction in readmissions.\"\n",
    "\n",
    "Visualization:\n",
    "    - A bar chart comparing adherence rates and average readmissions.\n",
    "    - Regression coefficient chart showing the magnitude of influence.\n",
    "\n",
    "Insight Example: Length of Stay\n",
    "    - Insight: \"Patients with hospital stays >7 days are 2x more likely to be readmitted within 30 days.\"\n",
    "\n",
    "Visualization:\n",
    "    - Scatter plot: length_of_stay vs. readmissions.\n",
    "    - Box plot: Readmission rates by length-of-stay categories.\n",
    "\n",
    "4. Implementation Plan\n",
    "Once the board approves, focus on operationalizing findings:\n",
    "\n",
    "- Deploy targeted interventions for high-risk patients.\n",
    "- Set KPIs to monitor the effectiveness of changes.\n",
    "- Continuously refine the model based on new data.\n",
    "\n",
    "##### Set KPIs to monitor the effectiveness of changes\n",
    "\n",
    "**KPI 1: 30-Day Readmission Rate**\n",
    "- Definition: Percentage of patients readmitted to the hospital within 30 days of discharge.\n",
    "- Why Important: This is the primary metric to assess whether interventions are reducing readmissions.\n",
    "- Formula: $Readmission¬†Rate = \\frac{Number¬†of¬†patients¬†readmitted¬†within¬†30¬†days}{Total¬†number¬†of¬†discharged¬†patients} √ó 100$\n",
    "- Target: A reduction in the readmission rate over time indicates success.\n",
    "\n",
    "**KPI 2: Medication Adherence Rate**\n",
    "- Definition: Percentage of patients adhering to their prescribed medications post-discharge.\n",
    "- Why Important: Non-adherence is a leading cause of readmissions. Monitoring this ensures interventions like counseling and follow-ups are effective\n",
    "- Formula: $Medication¬†Adherence¬†Rate = \\frac{Number¬†of¬†patients¬†adhering¬†to¬†medications}{Total¬†number¬†of¬†patients} √ó 100$\n",
    "- Target: An increase in adherence correlates with better outcomes and fewer readmissions.\n",
    "\n",
    "**KPI 3: Follow-Up Appointment Compliance**\n",
    "- Definition: Percentage of discharged patients attending follow-up appointments within the recommended time frame.\n",
    "- Why Important: Follow-up visits can identify issues early and prevent readmissions.\n",
    "- Formula: $Compliance¬†Rate= \\frac{Number¬†of¬†scheduled¬†follow-ups}{Number¬†of¬†attended¬†follow-ups} √ó 100$\n",
    "- Target: High compliance indicates improved patient engagement.\n",
    "\n",
    "**KPI 4: Average Length of Stay (LOS)**\n",
    "- Definition: Average number of days patients spend in the hospital.\n",
    "- Why Important: Shorter stays can indicate efficiency but might increase readmissions if patients are discharged prematurely.\n",
    "- Formula: $LOS= \\frac{Number¬†of¬†discharges}{Total¬†inpatient¬†days}$\n",
    "‚Äã- Target: Maintain an optimal LOS that balances cost and readmission prevention.\n",
    "\n",
    "**KPI 5: Percentage of High-Risk Patients Identified**\n",
    "- Definition: Proportion of discharged patients flagged as high-risk for readmission and targeted for interventions.\n",
    "- Why Important: Monitoring ensures that predictive models and risk stratification tools are working effectively.\n",
    "- Formula:$High-Risk¬†Patients¬†Identified = \\frac{Total¬†number¬†of¬†discharged¬†patients}{Number¬†of¬†flagged¬†high-risk¬†patients} √ó 100$\n",
    "- Target: Increase the identification rate while reducing actual readmissions.\n",
    "\n",
    "##### Presenting KPIs to Stakeholders\n",
    "\n",
    "**Visual Presentation**\n",
    "\n",
    "Use dashboards and visualizations:\n",
    "- Bar charts to compare readmission rates before and after interventions.\n",
    "- Line graphs showing trends over time for medication adherence and follow-up compliance.\n",
    "- Heatmaps for condition-specific readmission trends.\n",
    "\n",
    "Narrative\n",
    "- Highlight success: \"We reduced the 30-day readmission rate from 18% to 12%, saving $500,000 annually.\"\n",
    "- Focus on actionable insights: \"Medication adherence programs have been effective, with a 15% increase in adherence leading to a 5% drop in readmissions.\"\n",
    "\n",
    "Recommendations\n",
    "- Continue monitoring these KPIs for sustained improvements.\n",
    "- Scale successful interventions to other patient groups or hospitals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f51a4",
   "metadata": {},
   "source": [
    "## 2. Multiple Linear Regression:\n",
    "\n",
    "simple linear regression equation is as follows:\n",
    "\n",
    "$$Y = \\beta_{0} + \\beta_{1}X_1$$\n",
    "\n",
    "where:\n",
    "- $\\beta_{0}$ is the intercept, interpreted as the value of $Y$ when $X_1 = 0$;\n",
    "- $\\beta_{1}$ is the coefficient, interpreted as the effect on $Y$ for a one unit increase in $X_1$; and\n",
    "- $X_1$ is the single predictor variable.\n",
    "\n",
    "Extending that idea to multiple linear regression is as simple as adding an $X_{j}$ and corresponding $\\beta_{j}$ for each of the $p$ predictor variables, where $j$ is an element of the set $[1,p]$.\n",
    "   \n",
    "Hence in multiple linear regression, our regression equation becomes:   \n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $Y$ is the reponse variable which depends on the $p$ predictor variables;\n",
    "- $\\beta_0$ is the intercept, interpreted as the value of $Y$ when _all_ predictor variables are equal to zero;\n",
    "- $\\beta_j$ is the average effect on $Y$ of a one unit increase in $X_j$, assuming all other predictors are held fixed.\n",
    "\n",
    "Multiple linear regression is a technique to understand the relationship between a single dependent variable and multiple independent variables.\n",
    "\n",
    "$ ùë¶=ùõΩ_{0}+ùõΩ_{1}ùë•_{1}+‚Ä¶+ùõΩ_{ùëõ}ùë•_{ùëõ}+ ùúñ $\n",
    "\n",
    "What it means:\n",
    "- It is used when two or more independent variables influence the dependant variable. \n",
    "\n",
    "- A linear equation defines the relationship, with the \n",
    "    - coefficients of the independent variables \n",
    "    \n",
    "- representing the effect of each variable on the dependant variable.\n",
    "\n",
    "### Assumptions of Multiple Linear Regression\n",
    "\n",
    "Regression is a parametric approach, which means that it makes assumptions about the data\n",
    "\n",
    "For successful regression analysis, it‚Äôs essential to validate the following assumptions.\n",
    "\n",
    "- Overfitting: When more and more variables are added to a model, the model may become far too complex and usually ends up memorizing all the data points in the training set\n",
    "    - This phenomenon is known as the overfitting of a model. \n",
    "    - This usually leads to high training accuracy and very low test accuracy.\n",
    "- Understanding of linearity and multicollinearity (predictors).\n",
    "    - It is the phenomenon where a model with several independent variables, may have some variables interrelated.\n",
    "- Understanding of independence, homoscedasticity, and normality (residuals).\n",
    "- Feature Selection: With more variables present, selecting the optimal set of predictors from the pool of given features (many of which might be redundant) becomes an important task for building a relevant and better model.\n",
    "\n",
    "We'll be moving through the following sections in order to achieve our objectives:\n",
    "\n",
    "- Investigating our predictor variables:\n",
    "    - Checking for linearity;\n",
    "    - Checking for multicollinearity;\n",
    "- Fitting a model with `statsmodels.OLS`;\n",
    "- Evaluating our fitted model:\n",
    "    - Checking for independence;\n",
    "    - Checking for homoscedasticity;\n",
    "    - Checking for normaility;\n",
    "    - Checking for outliers.\n",
    "\n",
    "### Checking for Linearity\n",
    "\n",
    "The first thing we need to check is the mathematical relationship between each predictor variable and the response variable. == linearity. \n",
    "- A linear relationship means that a change in the response *Y* due to a one-unit change in the predictor $X_j$ is constant, regardless of the value of $X_j$.\n",
    "\n",
    "If we fit a regression model to a dataset that is non-linear, \n",
    "- it will fail to adequately capture the relationship in the data - resulting in a mathematically inappropriate model. \n",
    "\n",
    "To check for linearity, \n",
    "- we can produce scatter plots of each individual predictor against the response variable. \n",
    "- The intuition here is that we are looking for obvious linear relationships.\n",
    "\n",
    "**Result**\n",
    "\n",
    "- State what appears of the variables that have an approximately linear relationship.\n",
    "- State that exhibits no linearity with resonse variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,5, figsize=(14,6),)\n",
    "fig.subplots_adjust(hspace = 0.5, wspace=.2)\n",
    "axs = axs.ravel()\n",
    "\n",
    "for index, column in enumerate(df.columns):\n",
    "    axs[index-1].set_title(\"{} vs. mpg\".format(column),fontsize=16)\n",
    "    axs[index-1].scatter(x=df[column],y=df['mpg'],color='blue',edgecolor='k')\n",
    "    \n",
    "fig.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d75827",
   "metadata": {},
   "source": [
    "### Checking for Multicollinearity\n",
    "\n",
    "- As multicollinearity makes it difficult to find out which variable is contributing towards the prediction of the response variable, it leads one to conclude incorrectly, the effects of a variable on the target variable.\n",
    "- Properly detect and deal with the multicollinearity present in the model, as random removal of any of these correlated variables from the model causes the coefficient values to swing wildly and even change signs.\n",
    "\n",
    "Multicollinearity refers to the presence of strong correlation among two or more of the predictor variables in the dataset. The presence of any correlation among predictors is detrimental to model quality for two reasons:\n",
    "\n",
    "- It tends to increase the standard error;\n",
    "\n",
    "- It becomes difficult to estimate the effect of any one predictor variable on the response variable.\n",
    "\n",
    "We will check for multicollinearity by generating \n",
    "- pairwise scatter plots among predictors\n",
    "- a correlation heatmap.\n",
    "\n",
    "Multicollinearity can be detected using the following methods.\n",
    "\n",
    "- Pairwise Correlations: Checking the pairwise correlations between different pairs of independent variables can throw useful insights into detecting multicollinearity.\n",
    "    - Pairwise correlations may not always be useful as it is possible that just one variable might not be able to completely explain some other variable but some of the variables combined could be ready to do this.  Thus, to check these sorts of relations between variables, one can use VIF:\n",
    "- Variance Inflation Factor (VIF): VIF explains the relationship of one independent variable with all the other independent variables. \n",
    "    - VIF is given by,\n",
    "\n",
    "$ VIF = \\frac{1}{1 - R^2}$\n",
    "\n",
    "where \n",
    "- $i$ refers to the $ith$ variable which is being represented as a linear combination of the rest of the independent variables.\n",
    "\n",
    "Heuristics\n",
    "- if VIF > 10 then the value is high and it should be dropped.\n",
    "- if the VIF=5 then it may be valid but should be inspected first.\n",
    "- if VIF < 5, then it is considered a good VIF value.\n",
    "\n",
    "### Pairwise scatter plots\n",
    "\n",
    "As can be inferred by the name, a pairwise scatter plot simply produces a visual $n \\times n$ matrix, where $n$ is the total number of variables compared, in which each cell represents the relationship between two variables. The diagonal cells of this visual represent the comparison of a variable with itself, and as such are substituted by a representation of the distribution of values taken by the visual.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a4ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the number of visuals created, this codeblock takes about one minute to run.\n",
    "from seaborn import pairplot\n",
    "g = pairplot(df1.drop('mpg', axis='columns'))\n",
    "g.fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb1c52d",
   "metadata": {},
   "source": [
    "### Correlation heatmap\n",
    "\n",
    "Another way we can visually discover linearity between two or more variables within our dataset is through the use of a correlation heatmap. Similar to the pairwise scatter plot we produced above, this visual presents a matrix in which each row represents a distinct variable, with each colum representing the correlation between this variable and another one within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0457f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only compare the predictor variables, and thus drop the target `mpg` column.\n",
    "corr = df1.drop('mpg', axis='columns').corr()\n",
    "\n",
    "from statsmodels.graphics.correlation import plot_corr\n",
    "\n",
    "fig=plot_corr(corr,xnames=corr.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba46285",
   "metadata": {},
   "source": [
    "### Overfitting and Underfitting in Linear Regression\n",
    "\n",
    "When model performs well on training data but not on the test data.\n",
    "\n",
    "**Bias**\n",
    "\n",
    "Bias is a measure to determine how accurate a model‚Äôs predictions are likely to be on future unseen data.\n",
    "- Bias is errors made by training data.\n",
    "    - Complex models, assuming there is enough training data available, can make accurate model predictions. \n",
    "    - Models that are too naive, are very likely to perform badly concerning model predictions.\n",
    "- Linear algorithms have a high bias which makes them fast to learn and easier to understand but in general, are less flexible. \n",
    "    - Implying lower predictive performance on complex problems that fail to meet the expected outcomes.\n",
    "\n",
    "**Variance**\n",
    "\n",
    "Variance is the sensitivity of the model towards training data\n",
    "- it quantifies how much the model will react when input data is changed.\n",
    "    - model shouldn‚Äôt change too much from one training dataset to the next training data \n",
    "        - Whcih means that the algorithm is good at picking out the hidden underlying patterns between the inputs and the output variables.\n",
    "    - model should have lower variance which means that the model doesn‚Äôt change drastically after changing the training data(it is generalizable). \n",
    "        - Having higher variance will make a model change drastically even on a small change in the training dataset.\n",
    "\n",
    "**Bias Variance Tradeoff**\n",
    "\n",
    "A supervised machine learning algorithm seeks to strike a balance between low bias and low variance for increased robustness.\n",
    "\n",
    "The relationship between bias and variance is characterized by an inverse correlation.\n",
    "- Increased bias leads to reduced variance.\n",
    "- Conversely, heightened variance results in diminished bias.\n",
    "Finding an equilibrium between bias and variance is crucial, and algorithms must navigate this trade-off for optimal outcomes.\n",
    "\n",
    "**Overfitting**\n",
    "\n",
    "When a model learns every pattern and noise in the data to such an extent that it affects the performance of the model on the unseen future dataset.\n",
    "- model fits the data so well that it interprets noise as patterns in the data.\n",
    "\n",
    "Caused when a model has low bias and higher variance it ends up memorizing the data.\n",
    "\n",
    "Overfitting causes the model to become specific rather than generic. This usually leads to \n",
    "- high training accuracy and \n",
    "- very low test accuracy.\n",
    "\n",
    "There are several ways to prevent overfitting:\n",
    "- Cross-validation\n",
    "- If the training data is too small to train add more relevant and clean data.\n",
    "- If the training data is too large, do some feature selection and remove unnecessary features.\n",
    "- Regularization\n",
    "\n",
    "**Underfitting**\n",
    "\n",
    "When the model fails to learn from the training dataset and is also not able to generalize the test dataset.\n",
    "\n",
    "Detected by the performance metrics.\n",
    "\n",
    "When a model has high bias and low variance it ends up not generalizing the data and causing underfitting. \n",
    "- It is unable to find the hidden underlying patterns in the data. \n",
    "- This usually leads to low training accuracy and very low test accuracy.\n",
    "\n",
    "Ways to prevent underfitting:\n",
    "- Increase the model complexity\n",
    "- Increase the number of features in the training data\n",
    "- Remove noise from the data.\n",
    "\n",
    "### Fitting the model using `statsmodels.OLS`\n",
    "\n",
    "`sklearn` is limited in terms of metrics and tools available to evaluate the appropriateness of the regression models we fit.\n",
    "-As a means to expland our analysis, we import the `statsmodels` library which has a rich set of statistical tools to help us. \n",
    "\n",
    "##### Generating the regression string\n",
    "\n",
    "Those of you familiar with the R language will know that fitting a machine learning model requires a sort of string of the form:\n",
    "\n",
    "`y ~ X`\n",
    "\n",
    "which is read as follows: \"Regress y on X\". The `statsmodels` library works in a similar way, so we need to generate an appropriate string to feed to the method when we wish to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de013b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48989e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress target variable on all of the predictors.\n",
    "formula_str = df.columns[0]+' ~ '+'+'.join(df.columns[1:]); formula_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing seaborn library for visualizations\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# To plot all the scatterplots in a single plot\n",
    "sns.pairplot(df, x_vars=[ 'TV', ' Newspaper','Radio' ], y_vars = 'Sales', size = 4, kind = 'scatter' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f890be",
   "metadata": {},
   "source": [
    "##### Plotting 3D plot for multiple Linear regression\n",
    "\n",
    "To get a better idea of what a multi-dimensional dataset looks like, we'll generate a 3D scatter plot showing the `mpg` on the _z_-axis (height), with two predictor variables, `cyl` and `disp` on the _x_- and _y_-axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and 3d axes\n",
    "fig = plt.figure(figsize=(8,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# set axis labels\n",
    "ax.set_zlabel('MPG')\n",
    "ax.set_xlabel('No. of Cylinders')\n",
    "ax.set_ylabel('Weight (1000 lbs)')\n",
    "\n",
    "# scatter plot with response variable and 2 predictors\n",
    "ax.scatter(df['cyl'], df['wt'], df['mpg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e9a14",
   "metadata": {},
   "source": [
    "We know that in simple linear regression (2D), any model that we fit to data manifests in the form of a straight line. Extending this idea to 3D, the line becomes a plane - a flat surface which is chosen to minimise the squared vertical distances between each observation (red dots), and the plane, as shown in the figure below from ISLR.\n",
    "\n",
    "<img src=\"https://github.com/Explore-AI/Public-Data/raw/master/3D%20regression%20ISLR.jpg\" alt=\"plane\" style=\"width: 450px\"/>\n",
    "\n",
    "The result of a multivariate linear regression in higher dimensionality is known as a _hyperplane_ - similar to the flat surface in the figure above, but in a _p_-dimensional space, where $p>3$. Unfortunately, humans lack the ability to visualise any number of dimensions greater than three - so we have to be content with the idea that a hyperplane in _p_-dimensional space is effectively like a flat surface in 3-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e91c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot heatmap to find out correlations\n",
    "sns.heamap(df.corr(), cmap = 'YlGnBl', annot = True )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b531842",
   "metadata": {},
   "source": [
    "### Fitting the Multivariate Regression Model\n",
    "\n",
    "In `sklearn`, fitting a multiple linear regression model is much the same as fitting a simple linear regression. This time, of course, our $X$ contains multiple columns, where it only contained one before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb634e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, train_size = 0.7, test_size = 0.3, random_state = 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4ebbe",
   "metadata": {},
   "source": [
    "### Construct and fit the model\n",
    "\n",
    "We now go ahead and fit our model.\n",
    "- use the `ols` or Ordinary Least Squares regression model from the `statsmodels` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923aa12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to get an intercept\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "# Fit the resgression line using 'OLS'\n",
    "lr = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# OR\n",
    "\n",
    "model=sm.ols(formula=formula_str, data=df1)\n",
    "fitted = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a4053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the parameters,i.e. intercept and slope of the regression line obtained\n",
    "lr.params\n",
    "\n",
    "# extract model intercept\n",
    "beta_0 = float(lm.intercept_)\n",
    "\n",
    "# extract model coeffs\n",
    "beta_js = pd.DataFrame(lm.coef_, X.columns, columns=['Coefficient'])\n",
    "beta_js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ba2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a summary operation lists out all different parameters of the regression line fitted\n",
    "print(lr.summary())\n",
    "\n",
    "# OR\n",
    "\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5de94d",
   "metadata": {},
   "source": [
    "few 2-dimensional plots; plotting `wt`, `disp`, `cyl`, and `hp` vs. `mpg`, respectively (top-left to bottom-right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c99e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(9,7))\n",
    "\n",
    "axs[0,0].scatter(df['wt'], df['mpg'])\n",
    "axs[0,0].plot(df['wt'], lm.intercept_ + lm.coef_[4]*df['wt'], color='red')\n",
    "axs[0,0].title.set_text('Weight (wt) vs. mpg')\n",
    "\n",
    "axs[0,1].scatter(df['disp'], df['mpg'])\n",
    "axs[0,1].plot(df['disp'], lm.intercept_ + lm.coef_[1]*df['disp'], color='red')\n",
    "axs[0,1].title.set_text('Engine displacement (disp) vs. mpg')\n",
    "\n",
    "axs[1,0].scatter(df['cyl'], df['mpg'])\n",
    "axs[1,0].plot(df['cyl'], lm.intercept_ + lm.coef_[0]*df['cyl'], color='red')\n",
    "axs[1,0].title.set_text('Number of cylinders (cyl) vs. mpg')\n",
    "\n",
    "axs[1,1].scatter(df['hp'], df['mpg'])\n",
    "axs[1,1].plot(df['hp'], lm.intercept_ + lm.coef_[2]*df['hp'], color='red')\n",
    "axs[1,1].title.set_text('Horsepower (hp) vs. mpg')\n",
    "\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af29b5fd",
   "metadata": {},
   "source": [
    "### Assessing Model Accuracy\n",
    "\n",
    "Let's assess the fit of our multivariate model. For the purpose of a rudimentary comparison, let's measure model accuracy aginst a simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18155fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to X_test\n",
    "X_test_sm = sm.add_constant(X_test)\n",
    "# Predict the y values corresponding to X_test_sm\n",
    "y_pred = lr.predict(X_test_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9e55c",
   "metadata": {},
   "source": [
    "We have included a column *Test RMSE*, which is simply the square root of the *Test MSE*.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "RMSE & = \\sqrt{MSE} \\\\\n",
    "     & = \\sqrt{\\frac{1}{N}\\sum^{N} (\\hat{y_i} - y_i)^{2}}\n",
    "\\end{align}\n",
    "\n",
    "Where $y_i$ are the actual target values for a dataset with $N$ datapoints, and $\\hat{y_i}$ represent our corresponding predictions. RMSE is a more intuitive metric to use than MSE because it is in the same units as the underlying variable being predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec0210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import math\n",
    "\n",
    "# Imporitng libraries\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# dictionary of results\n",
    "results_dict = {'Training MSE':\n",
    "                    {\n",
    "                        \"SLR\": metrics.mean_squared_error(y_train, slr.predict(X_train[['disp']])),\n",
    "                        \"MLR\": metrics.mean_squared_error(y_train, lm.predict(X_train))\n",
    "                    },\n",
    "                'Test MSE':\n",
    "                    {\n",
    "                        \"SLR\": metrics.mean_squared_error(y_test, slr.predict(X_test[['disp']])),\n",
    "                        \"MLR\": metrics.mean_squared_error(y_test, lm.predict(X_test))\n",
    "                    },\n",
    "                'Test RMSE':\n",
    "                    {\n",
    "                        \"SLR\": math.sqrt(metrics.mean_squared_error(y_test, slr.predict(X_test[['disp']]))),\n",
    "                        \"MLR\": math.sqrt(metrics.mean_squared_error(y_test, lm.predict(X_test)))\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d2ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE value\n",
    "print(\"RMSE: \",np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "#R-squared value\n",
    "print(\"R-squared: \",r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c3b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lm = X_train_lm.values.reshape(-1,1)\n",
    "X_test_lm = X_test_lm.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa82d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_lm.shape)\n",
    "print(X_train_lm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ffa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "#Representing LinearRegression as lr (creating LinearRegression object)\n",
    "lr = LinearRegression()\n",
    "#Fit the model using lr.fit()\n",
    "lr.fit(X_train_lm,y_train_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba41d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get intercept\n",
    "print(lr.intercept_)\n",
    "#get slope\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b73f2",
   "metadata": {},
   "source": [
    "### Checking for Independence\n",
    "\n",
    "We have done checks for linearity and multicollinearity, which both referred to the predictor variables. \n",
    "\n",
    "To checking some of the artefacts of the fitted model for three more statistical phenomena which further help us determine its quality.\n",
    "\n",
    "#### Residuals vs. Predictor Variables Plots \n",
    "\n",
    "The first check we do involves plotting the residuals (vertical distances between each data point and the regression hyperplane). \n",
    "- We are looking to confirm the independence assumption here, i.e.: the residuals should be independent. \n",
    "\n",
    "If they are we will see:\n",
    "- Residuals approximately uniformly randomly distributed about the zero x-axes;\n",
    "- Residuals not forming specific clusters.\n",
    "\n",
    "Observing the plots two things should be relatively clear:\n",
    "\n",
    "- Residuals are slightly to skewed to the positive or negative (reaching +5 but only about -3);\n",
    "\n",
    "- check for clustering, \n",
    "    - Check which may present a cluster on the value 6.\n",
    "\n",
    "Conclusion: is the residuals are largely independent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,5, figsize=(14,6),sharey=True)\n",
    "fig.subplots_adjust(hspace = 0.5, wspace=.2)\n",
    "fig.suptitle('Predictor variables vs. model residuals', fontsize=16)\n",
    "axs = axs.ravel()\n",
    "\n",
    "for index, column in enumerate(df.columns):\n",
    "    axs[index-1].set_title(\"{}\".format(column),fontsize=12)\n",
    "    axs[index-1].scatter(x=df[column],y=fitted.resid,color='blue',edgecolor='k')\n",
    "    axs[index-1].grid(True)\n",
    "    xmin = min(df[column])\n",
    "    xmax = max(df[column])\n",
    "    axs[index-1].hlines(y=0,xmin=xmin*0.9,xmax=xmax*1.1,color='red',linestyle='--',lw=3)\n",
    "    if index == 1 or index == 6:\n",
    "        axs[index-1].set_ylabel('Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86f5750",
   "metadata": {},
   "source": [
    "### Checking for Homoscedasticity\n",
    "\n",
    "Check whether the variance of the residuals (the error terms) is constant as the fitted values increase. \n",
    "\n",
    "#### Fitted vs. Residuals\n",
    "\n",
    "Determine this by plotting the magnitude of the fitted values (i.e.: `mpg`) against the residuals. \n",
    "- What we are looking for is the plotted points to approximately form a rectangle.\n",
    "- The magnitude of the residuals should not increase as the fitted values increase (if that is the case, the data will form the shape of a cone on its side).\n",
    "\n",
    "**Observation**\n",
    "- If the variance is constant, we have observed _homoscedasticity_. \n",
    "- If the variance is not constant, we have observed _heteroscedasticity_. \n",
    "\n",
    "Use the same plot to check for outliers: any plotted points that are visibly seperate from the random pattern of the rest of the residuals.\n",
    "\n",
    "**Observation**\n",
    "- Look at data point on particular side of the plot and observe the scatteredness/ density.\n",
    "    - Points towards the right-hand side of the plot tend to be scattered slightly less densely, indicating the presence of heteroscedasticity.\n",
    "    - This violates our assumption of homoscedasticity. \n",
    "- Look at the presesnce of outliers\n",
    "    - The presence of these outliers means that those values are weighted too heavily in the prediction process, disproportionately influencing the model's performance. \n",
    "    - This in turn can lead to the confidence interval for out of sample predictions (unseen data) being unrealistically wide or narrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "p=plt.scatter(x=fitted.fittedvalues,y=fitted.resid,edgecolor='k')\n",
    "xmin = min(fitted.fittedvalues)\n",
    "xmax = max(fitted.fittedvalues)\n",
    "plt.hlines(y=0,xmin=xmin*0.9,xmax=xmax*1.1,color='red',linestyle='--',lw=3)\n",
    "plt.xlabel(\"Fitted values\",fontsize=15)\n",
    "plt.ylabel(\"Residuals\",fontsize=15)\n",
    "plt.title(\"Fitted vs. residuals plot\",fontsize=18)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a82de17",
   "metadata": {},
   "source": [
    "### Checking for Normality\n",
    "\n",
    "To confirm our assumption of normality amongst the residuals. \n",
    "- If the residuals are non-normally distributed, confidence intervals can become too wide or too narrow, \n",
    "    - which leads to difficulty in estimating coefficients based on the minimisation of ordinary least squares.\n",
    "\n",
    "Check for violation of the normality assumption in two different ways:\n",
    "1. Plotting a histogram of the normalised residuals;\n",
    "2. Generating a Q-Q plot of the residuals.\n",
    "\n",
    "#### Histogram of Normalized Residuals\n",
    "\n",
    "Plot a histogram of the residuals to take a look at their distribution. \n",
    "- It is fairly easy to pick up when a distribution looks similar to the classic _bell curve_ shape of the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(fitted.resid_pearson,bins=8,edgecolor='k')\n",
    "plt.ylabel('Count',fontsize=15)\n",
    "plt.xlabel('Normalized residuals',fontsize=15)\n",
    "plt.title(\"Histogram of normalized residuals\",fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9660ce8",
   "metadata": {},
   "source": [
    "#### Q-Q plot of the residuals\n",
    "\n",
    "- A Q-Q plot, A.K.A quantile-quantile plot, attempts to plot the theoretical quantiles of the standard normal distribution against the quantiles of the residuals. \n",
    "- The one-to-one line, indicated in red below, is the ideal line indicating normality. \n",
    "- The closer the plotted points are to the red line, the closer the residual distribution is to the standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5653d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We once again use the statsmodel library to assist us in producing our qqplot visualisation. \n",
    "from statsmodels.graphics.gofplots import qqplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8361da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "fig=qqplot(fitted.resid_pearson,line='45',fit='True')\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.xlabel(\"Theoretical quantiles\",fontsize=15)\n",
    "plt.ylabel(\"Sample quantiles\",fontsize=15)\n",
    "plt.title(\"Q-Q plot of normalized residuals\",fontsize=18)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ee434",
   "metadata": {},
   "source": [
    "### Checking for Outliers in Residuals\n",
    "\n",
    "Check for outliers amongst the residuals.\n",
    "\n",
    "#### Plotting Cook's Distance\n",
    "\n",
    "Cook's distance is a calculation which measures the effect of deleting an observation from the data. \n",
    "- Observations with large Cook's distances should be earmarked for closer examination in the analysis due to their disproportionate impact on the model.\n",
    "\n",
    "**Observation**\n",
    "\n",
    "Check values with much higher Cook's distances than the rest. \n",
    "- A rule of thumb for determining whether a Cook's distance is too large is whether it is greater than four times the mean Cook's distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import OLSInfluence as influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b57b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf=influence(fitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f81425",
   "metadata": {},
   "outputs": [],
   "source": [
    "(c, p) = inf.cooks_distance\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Cook's distance plot for the residuals\",fontsize=16)\n",
    "plt.stem(np.arange(len(c)), c, markerfmt=\",\", use_line_collection=True)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d04294",
   "metadata": {},
   "source": [
    "#### Calculate the mean Cooks Distance\n",
    "\n",
    "Check which observation are 4 X higher the the average\n",
    "\n",
    "Implications: Highly influential in this dataset\n",
    "- warrant closer examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Cook\\'s distance: ', c.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6228e",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression\n",
    "\n",
    "Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).\n",
    "\n",
    "It is used to describe data and to explain the relationship between one dependent binary variable and one or more \n",
    "- nominal, \n",
    "- ordinal, \n",
    "- interval or \n",
    "- ratio-level independent variables.\n",
    "\n",
    "Logistic regression is a statistical model used for binary classification tasks.\n",
    "- The outcome variable is categorical with two possible values (e.g., 1/0, Yes/No, Positive/Negative).\n",
    "- Used to predict the Probabilities for classification problems.\n",
    "\n",
    "It predicts the probability of an event occurring, transforming the linear combination of predictors through a logistic function (sigmoid function) to ensure the predicted probabilities lie between 0 and 1.\n",
    "\n",
    "Model Equation: \n",
    "$ ùëÉ(ùë¶=1)= \\frac{1}{1+ùëí^{‚àí(ùõΩ_{0}+ùõΩ_{1}ùë•_{1}+‚Ä¶+ùõΩ_{ùëõ}ùë•_{ùëõ}})}$\n",
    "\n",
    "**What It Means:** \n",
    "- Logistic regression estimates the probability of a binary outcome (e.g., yes/no, success/failure) based on predictor variables. \n",
    "    - It uses a logistic function to map predictions to probabilities between 0 and 1.\n",
    "\n",
    "- It is a statistical technique for investigating the relationship between a binary dependent variable (outcome) and one or more independent variables (predictors). \n",
    "\n",
    "- The goal of logistic regression is to find the best-fitting model to describe the relationship between the dependent variable and the independent variables and then use that model to predict the outcome variable.\n",
    "\n",
    "**Lay Explanation:**\n",
    "- Logistic regression is like a yes-or-no decision helper. It estimates the chances of an event happening (e.g., a customer buying a product) based on known factors.\n",
    "- It tries to find the best-fitted curve for the data\n",
    "\n",
    "**Why use Logistic Regression rather than Linear Regression?**\n",
    "\n",
    "Outlier Influence:\n",
    "- best fit line in linear regression shifts to fit that point.\n",
    "\n",
    "Predicted outcome out of range:\n",
    "- In linear regression, the predicted values may be out of range.\n",
    "\n",
    "Response Variable:\n",
    "- Linear regression is used when dependent variable is continuous\n",
    "- Logistic Regression is used when our dependent variable is binary.\n",
    "\n",
    "Logistic regression is ideal for this problem because:\n",
    "- Binary Outcome: The target variable is binary: Readmitted (1) or Not Readmitted (0).\n",
    "- Interpretability: It provides coefficients (log odds) that indicate how changes in predictors affect the likelihood of the event (readmission).\n",
    "- Insights: It helps identify the significant factors influencing readmissions.\n",
    "\n",
    "### Outcome Interpretation: \n",
    "- The model outputs probabilities that can be converted to binary outcomes. \n",
    "- Coefficients show how each predictor variable influences the likelihood of the outcome.\n",
    "\n",
    "### Performance Measures:\n",
    "- Accuracy: Proportion of correct predictions.\n",
    "- AUC-ROC: Measures the model's ability to distinguish between classes; values closer to 1 indicate a better model.\n",
    "\n",
    "### Types of Logistic Regression\n",
    "\n",
    "#### Binary Logistic Regression\n",
    "Binary logistic regression is used to predict the probability of a binary outcome, such as \n",
    "- yes or no, \n",
    "- true or false, or \n",
    "- 0 or 1. \n",
    "\n",
    "For example, it could be used to:\n",
    "- predict whether a customer will churn or not, \n",
    "- predict whether a patient has a disease or not, or \n",
    "- predict whether a loan will be repaid or not.\n",
    "\n",
    "#### Multinomial Logistic Regression\n",
    "Multinomial logistic regression is used to predict the probability of one of three or more possible outcomes, such as \n",
    "- the type of product a customer will buy, \n",
    "- the rating a customer will give a product, or \n",
    "- the political party a person will vote for.\n",
    "\n",
    "#### Ordinal Logistic Regression\n",
    "Used to predict the probability of an outcome that falls into a predetermined order, such as \n",
    "- the level of customer satisfaction, \n",
    "- the severity of a disease, or \n",
    "- the stage of cancer.\n",
    "\n",
    "### Differences Between Linear and Logistic Regression\n",
    "\n",
    "The core difference lies in their target predictions.\n",
    "- Linear regression excels at predicting continuous values along a spectrum. \n",
    "    - resulting output would be a specific amount, a continuous value on the amount scale.\n",
    "- Linear regression answers ‚Äúhow much‚Äù questions, providing a specific value on a continuous scale.\n",
    "\n",
    "- Logistic regression deals with categories. \n",
    "    - It doesn‚Äôt predict a specific value but rather the likelihood of something belonging to a particular class.\n",
    "    - output here would be a probability between 0 (not likely spam) and 1 (very likely spam). \n",
    "    - This probability is then used to assign an email to a definitive category (spam or not spam) based on a chosen threshold.\n",
    "- Logistic regression tackles ‚Äúyes or no‚Äù scenarios, giving the probability of something belonging to a certain category.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Objective:\n",
    "- The medical institute, we want to identify the likelihood of patients being readmitted within 30 days of discharge based on patient \n",
    "    - demographics, \n",
    "    - medical history, \n",
    "    - length of stay (LOS), and \n",
    "    - clinical metrics such as blood pressure, \n",
    "    - blood glucose levels, and \n",
    "    - medication adherence.\n",
    "\n",
    "**Key Assumptions of Logistic Regression**\n",
    "\n",
    "Data Specific\n",
    "- Binary Outcome: The dependent variable is binary.\n",
    "    - Logistic regression is designed for binary dependent variables. \n",
    "    - If your outcome has more than two categories, you might need a multinomial logistic regression or other classification techniques.\n",
    "- Independence of Observations: Observations are independent of each other.\n",
    "    -  This means no repeated measurements or clustering within the data.\n",
    "\n",
    "Relationship Between Variables\n",
    "- Linearity of Log-Odds: There is a linear relationship between the log-odds of the outcome and the independent variables.\n",
    "    - Outcome itself has a relationship with log-odds.\n",
    "    - Outcome does not have linear relationship with the independent variables.\n",
    "- No Multicollinearity: Independent variables are not highly correlated.\n",
    "    - Multicollinearity can cause instability in the model and make it difficult to interpret the coefficients.\n",
    "\n",
    "Other\n",
    "- Large Sample Size: Logistic regression performs well with larger datasets.\n",
    "    - To ensure reliable parameter estimates.\n",
    "- Absence of Outliers: outliers can significantly influence the model. \n",
    "    - It‚Äôs important to check for and address any outliers that might distort the results.\n",
    "\n",
    "**Step 1: Define the Problem**\n",
    "- Target Variable: Readmission within 30 days (1 = Yes, 0 = No).\n",
    "- Predictors:\n",
    "    - Patient Demographics: Age, gender, insurance status.\n",
    "    - Clinical Metrics: Blood glucose levels, blood pressure, medication adherence.\n",
    "    - Hospital Metrics: Length of Stay (LOS), number of previous visits.\n",
    "\n",
    "**Step 2: Collect and Prepare Data**\n",
    "- Gather historical patient data and ensure it's clean and consistent.\n",
    "    - Check for Missing Data:\n",
    "    - Impute missing values for predictors like glucose levels using median or mean.\n",
    "    - Standardize Continuous Variables:\n",
    "    - Standardize LOS, glucose levels, and blood pressure for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed07b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    'age': [45, 60, 50, 40, 70],\n",
    "    'los': [3, 7, 4, 2, 10],\n",
    "    'glucose': [150, 200, 180, 140, 220],\n",
    "    'med_adherence': [0.8, 0.6, 0.75, 0.9, 0.5],\n",
    "    'readmitted': [1, 1, 0, 0, 1]\n",
    "})\n",
    "\n",
    "# Features and target\n",
    "X = data[['age', 'los', 'glucose', 'med_adherence']]\n",
    "y = data['readmitted']\n",
    "\n",
    "# Add constant for intercept\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8ef54",
   "metadata": {},
   "source": [
    "**Step 3: Exploratory Data Analysis**\n",
    "- Univariate Analysis: Examine distributions of continuous variables.\n",
    "- Bivariate Analysis: Analyze relationships between predictors and the target variable.\n",
    "- Correlation Matrix: Identify multicollinearity among predictors.\n",
    "\n",
    "**Step 4: Perform Logistic Regression**\n",
    "\n",
    "How logistic regression squeezes the output of linear regression between 0 and 1.\n",
    "\n",
    "Best Fit Equation in Linear Regression\n",
    "\n",
    "$ y = ùõΩ_{0}+ùõΩ_{1}ùë•_{1}$\n",
    "\n",
    "Now we want to take probabilities (P) instead of y.\n",
    "- Issue: the value of (P) will exceed 1 or go below 0 and we know that range of Probability is (0-1)\n",
    "\n",
    "Overcome issue by taking ‚Äúodds‚Äù of P:\n",
    "\n",
    "$ P = ùõΩ_{0}+ùõΩ_{1}ùë•_{1}$\n",
    "$ \\frac{P}{1-P} = ùõΩ_{0}+ùõΩ_{1}ùë•_{1}$\n",
    "\n",
    "Odds can always be positive which means the range will always be ($0,+‚àû $).\n",
    "- Odds are the ratio of the probability of success and probability of failure.\n",
    "\n",
    "Why ‚Äòodds‚Äô?\n",
    "- odds are probably the easiest way to do this.\n",
    "\n",
    "Problem: is that the range is restricted and we don‚Äôt want a restricted range because if we do so then our correlation will decrease.\n",
    "- By restricting the range we are actually decreasing the number of data points and if we decrease our data points, our correlation will decrease.\n",
    "- Making it difficult to model a variable that has a restricted range.\n",
    "\n",
    "Control:\n",
    "- Control this we take the log of odds which has a range from (-‚àû,+‚àû)\n",
    "\n",
    "$ \\log(\\frac{P}{1-P}) = ùõΩ_{0}+ùõΩ_{1}ùë•_{1}$\n",
    "\n",
    "Now we just want a function of P because we want to predict probability not log of odds. To do so we will \n",
    "- multiply by exponent on both sides and then solve for P.\n",
    "\n",
    "$ \\exp[\\log(\\frac{P}{1-P})] = \\exp(ùõΩ_{0}+ùõΩ_{1}ùë•_{1})$\n",
    "\n",
    "$ \\exp^{\\ln[\\frac{P}{1-P})} = \\exp^{(ùõΩ_{0}+ùõΩ_{1}ùë•_{1})} $\n",
    "\n",
    "$ \\frac{P}{1-P} = \\exp^{(ùõΩ_{0}+ùõΩ_{1}ùë•_{1})} $\n",
    "\n",
    "$ p = \\exp^{(ùõΩ_{0}+ùõΩ_{1}ùë•_{1})}  - p\\exp^{(ùõΩ_{0}+ùõΩ_{1}ùë•_{1})}$\n",
    "\n",
    "Now we have sigmoid function.\n",
    "\n",
    "Model Equation: \n",
    "$ ùëÉ(ùë¶=1)= \\frac{1}{1+ùëí^{‚àí(ùõΩ_{0}+ùõΩ_{1}ùë•_{1}+‚Ä¶+ùõΩ_{ùëõ}ùë•_{ùëõ}})}$\n",
    "\n",
    "It squeezes a straight line into an S-curve.\n",
    "\n",
    "### Key properties of the logistic regression equation\n",
    "\n",
    "Expalin the Logistic regression model\n",
    "\n",
    "Sigmoid Function:\n",
    "- uses a special ‚ÄúS‚Äù shaped curve to predict probabilities. It ensures that the predicted probabilities stay between 0 and 1.\n",
    "\n",
    "Straightforward Relationship:\n",
    "- relationship between our inputs and the outcome is like drwing a straight line but a curve is there instead.\n",
    "\n",
    "Coefficients / parameters:\n",
    "- numbers that tell us how much each input affects the outcome in the logistic regression model.\n",
    "- coefficient tells us how much the outcome changes for every one unit increase in predictor variable.\n",
    "\n",
    "Best Guess: \n",
    "- Figure out the best coefficients for the logistic regression model by looking at the data we have and tweaking them until our predictions match the real outcomes as closely as possible.\n",
    "\n",
    "Basic Assumptions:\n",
    "- We assume that our observations are independent, meaning one doesn‚Äôt affect the other. \n",
    "- We assume that there‚Äôs not too much overlap between our predictors (like age and height), \n",
    "- We assume the relationship between our predictors and the outcome is kind of like a straight line.\n",
    "\n",
    "Probabilities, Not Certainties:\n",
    "- Logistic regression gives us probabilities.\n",
    "- Then decide on a cutoff point to make our final decision.\n",
    "\n",
    "Checking Our Work:\n",
    "- We make sure our predictions are good, like \n",
    "    - accuracy, \n",
    "    - precision, \n",
    "    - recall,\n",
    "    - ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "data = pd.read_csv('data.csv') # read data from csv file\n",
    "X = data[['Independent_Var_1', 'Independent_Var_2', 'Independent_Var_3']] # select independent variables\n",
    "Y = data['Dependent_Var'] # select dependent variable\n",
    "\n",
    "# Add a constant to the independent variable set\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = sm.Logit(Y, X).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3daca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train = data[:800]\n",
    "test = data[800:]\n",
    "\n",
    "# Define the independent variables\n",
    "X_train = train[['age', 'gender', 'income']]\n",
    "X_test = test[['age', 'gender', 'income']]\n",
    "\n",
    "# Define the dependent variable\n",
    "y_train = train['buy_product']\n",
    "y_test = test['buy_product']\n",
    "\n",
    "# Fit the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the outcomes for the test data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa885e4",
   "metadata": {},
   "source": [
    "**Step 5: Interpret Coefficients and Evaluate the Model**\n",
    "\n",
    "- Log Odds: Each coefficient represents the change in log odds of readmission for a unit increase in the predictor.\n",
    "- Odds Ratios: Use np.exp(model.params) to convert coefficients to odds ratios.\n",
    "\n",
    "1. Accuracy\n",
    "2. Confusion Matrix\n",
    "3. ROC Curve and AUC\n",
    "\n",
    "**Step 6: Optomisation**\n",
    "\n",
    "### Cost Function in Logistic Regression\n",
    "\n",
    "Linear regression, uses the Mean squared error which was the difference between y_predicted and y_actual\n",
    "- this is derived from the **maximum likelihood estimator**.\n",
    "\n",
    "logistic regression $Yi$ is a non-linear function ($ ≈∂= \\frac{1}‚Äã{1+ e-z}$).\n",
    "- If we use this in the above MSE equation then it will give a non-convex graph with many local minima.\n",
    "\n",
    "Problem: cost function will give results with local minima\n",
    "- End up miss out on our global minima and our error will increase.\n",
    "\n",
    "Solution: derive a different cost function for logistic regression\n",
    "- **log loss** which is also derived from the **maximum likelihood estimation method**.\n",
    "\n",
    "$ Log Loss = \\frac{1}{N} \\sum^{N}_{i = 1} - ( y_i * \\log(Y_i) + (1 - y_i) * log (1 - Y_i))$\n",
    "\n",
    "#### Maximum likelihood estimator\n",
    "\n",
    "#### Math behind this log loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X) > 0.5\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc749e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a4e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y, model.predict(X))\n",
    "auc = roc_auc_score(y, model.predict(X))\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda6f67",
   "metadata": {},
   "source": [
    "**Understanding Factors Significantly Influencing Readmission**\n",
    "\n",
    "1. Use p-values from the logistic regression summary:\n",
    "- Predictors with $ùëù< 0.05$ are statistically significant.\n",
    "2. Assess the odds ratios:\n",
    "- For example, if the odds ratio for LOS is 2.0, each additional day in the hospital doubles the odds of readmission.\n",
    "3. Visualize relationships:\n",
    "- Plot odds ratios for key predictors to present to stakeholders.\n",
    "\n",
    "**Statistical Hypothesis Testing**\n",
    "\n",
    "Example 1: Relationship Between LOS and Readmission\n",
    "- Hypotheses:\n",
    "    - $ùêª_0$: LOS has no effect on readmission.\n",
    "    - $ùêª_ùëé$: LOS has a significant effect on readmission.\n",
    "- Approach: Perform a logistic regression test and check the p-value for LOS.\n",
    "\n",
    "Example 2: Age Group vs. Readmission\n",
    "- Hypotheses:\n",
    "    - $ùêª_0$: Age group is independent of readmission.\n",
    "    - $ùêª_ùëé$: Age group and readmission are dependent.\n",
    "- Approach: Use a Chi-Square test of independence (see previous example).\n",
    "\n",
    "**Actionable Insights**\n",
    "- Highlight key factors significantly influencing readmission (e.g., LOS, medication adherence).\n",
    "- Use odds ratios to explain how much each factor increases or decreases the likelihood of readmission.\n",
    "- Present findings visually (e.g., bar charts for odds ratios, ROC curves for model performance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b8436",
   "metadata": {},
   "source": [
    "### 3. Generalized Linear Models (GLMs)\n",
    "What It Means: \n",
    "- GLMs extend linear regression by allowing different types of data distributions\n",
    "    - Poisson for count data. \n",
    "- It models the mean of the outcome variable based on a link function.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- The coefficients explain how each predictor affects the mean outcome, given the distribution.\n",
    "\n",
    "Performance Measures:\n",
    "- Deviance: Measures how well the model fits compared to a perfect model; lower values are better.\n",
    "\n",
    "Lay Explanation: \n",
    "- GLMs are like flexible versions of linear regression that can handle different data types (like counts or binary data), giving predictions that respect the data‚Äôs nature.\n",
    "\n",
    "Use Case: \n",
    "- Extends linear regression for non-normal distributions (e.g., Poisson regression for count data).\n",
    "\n",
    "Model Types: \n",
    "- Poisson regression, \n",
    "- Binomial regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48888f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "poisson_model = sm.GLM(y_train, X_train, family=sm.families.Poisson()).fit()\n",
    "predictions = poisson_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af5a09e",
   "metadata": {},
   "source": [
    "##### 4. Time Series Models (e.g., ARIMA)\n",
    "What It Means: \n",
    "- Time series models account for:\n",
    "    - trends, \n",
    "    - seasonality, and \n",
    "    - temporal dependencies in data collected over time, often used for forecasting future values.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each prediction is based on patterns in past data points, accounting for recent trends and cycles.\n",
    "\n",
    "Performance Measures:\n",
    "- Mean Absolute Percentage Error (MAPE): Shows the average prediction error in percentage terms.\n",
    "- Root Mean Squared Error (RMSE): Measures the prediction accuracy; lower values mean better predictions.\n",
    "\n",
    "Lay Explanation: \n",
    "- Time series models are like weather forecasts‚Äîthey predict future values based on past patterns, like trends and cycles.\n",
    "\n",
    "Use Case: \n",
    "- Forecasting for data with a temporal component (e.g., sales data, stock prices).\n",
    "\n",
    "Model Types: \n",
    "- ARIMA, \n",
    "- SARIMA, \n",
    "- Exponential Smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce38a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "model = ARIMA(time_series_data, order=(1,1,1))\n",
    "model_fit = model.fit()\n",
    "predictions = model_fit.forecast(steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3ef82",
   "metadata": {},
   "source": [
    "##### 5. Decision Trees and Random Forests\n",
    "What It Means: \n",
    "- Decision trees split data based on conditions, creating branches that lead to a prediction. \n",
    "- Random forests use multiple trees to improve accuracy and reduce overfitting.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each \"branch\" shows how different conditions affect the outcome, \n",
    "- and random forests average the results of many trees for robust predictions.\n",
    "\n",
    "Performance Measures:\n",
    "- Accuracy: Proportion of correctly classified samples.\n",
    "- Gini Index / Entropy: Used to measure the purity of the splits; lower values are better.\n",
    "\n",
    "Lay Explanation: \n",
    "- Decision trees are like flowcharts that guide predictions based on conditions. \n",
    "- Random forests combine many trees to make stronger, more reliable decisions.\n",
    "\n",
    "Use Case: \n",
    "- For classification or regression problems with non-linear relationships and high dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "tree_model = DecisionTreeClassifier()\n",
    "tree_model.fit(X_train, y_train)\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "predictions_tree = tree_model.predict(X_test)\n",
    "predictions_rf = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f382cd",
   "metadata": {},
   "source": [
    "##### 6. Support Vector Machines (SVM)\n",
    "What It Means: \n",
    "- SVMs classify data by finding the best ‚Äúboundary‚Äù (hyperplane) that separates classes with the widest possible margin.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Data points on either side of the boundary belong to different classes, with \"support vectors\" helping to define the boundary.\n",
    "\n",
    "Performance Measures:\n",
    "- Accuracy: Proportion of correct classifications.\n",
    "- Precision and Recall: Used when classes are imbalanced; precision is the correctness of positive predictions, and recall measures coverage.\n",
    "\n",
    "Lay Explanation: \n",
    "- SVMs are like drawing a line to separate different groups, ensuring the groups are as distinct as possible with the help of a few key points.\n",
    "\n",
    "Use Case: \n",
    "- Used for classification and regression in high-dimensional spaces, often for non-linearly separable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe25b8cd",
   "metadata": {},
   "source": [
    "##### 7. Clustering Models (e.g., K-Means)\n",
    "What It Means: \n",
    "- Clustering groups similar data points together without predefined labels, often used for segmenting customers or finding patterns.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each cluster represents a natural grouping in the data, with data points in the same cluster sharing similar characteristics.\n",
    "\n",
    "Performance Measures:\n",
    "- Silhouette Score: Measures how well each point fits within its cluster; values closer to 1 indicate better-defined clusters.\n",
    "- Within-Cluster Sum of Squares (WCSS): Measures the compactness of clusters; lower values are better.\n",
    "\n",
    "Lay Explanation: \n",
    "- Clustering is like sorting items into bins based on similarity, helping us identify groups in our data.\n",
    "\n",
    "Use Case: \n",
    "- To group similar observations without predefined labels.\n",
    "\n",
    "Model Types: \n",
    "- K-Means, \n",
    "- Hierarchical Clustering, \n",
    "- DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3cfc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "clusters = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea1408",
   "metadata": {},
   "source": [
    "##### 8. Principal Component Analysis (PCA)\n",
    "What It Means: \n",
    "- PCA reduces the number of variables in the data by finding combinations of variables that capture the most information (variance).\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each \"principal component\" explains a percentage of the total variance, helping simplify the data without losing much information.\n",
    "\n",
    "Performance Measures:\n",
    "- Explained Variance Ratio: Shows how much information each principal component holds; higher is better.\n",
    "\n",
    "Lay Explanation: \n",
    "- PCA is like summarizing a book by keeping only the most important points, making data easier to work with without losing key insights.\n",
    "\n",
    "Use Case: \n",
    "- Dimensionality reduction while retaining the most critical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23bf1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6d396",
   "metadata": {},
   "source": [
    "##### 9. Bayesian Models\n",
    "What It Means: \n",
    "- Bayesian models incorporate prior knowledge or beliefs with the data to update the probability of outcomes as new evidence is available.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each output is a probability distribution reflecting both prior knowledge and the new data, offering a range of likely outcomes.\n",
    "\n",
    "Performance Measures:\n",
    "- Log-Likelihood: Measures how well the model explains the data; higher values indicate better fit.\n",
    "\n",
    "Lay Explanation: \n",
    "- Bayesian models are like revising a guess based on new evidence‚Äîupdating beliefs as we get more information.\n",
    "\n",
    "Use Case: \n",
    "- To incorporate prior knowledge and quantify uncertainty.\n",
    "\n",
    "Model Types: \n",
    "- Bayesian Linear Regression, \n",
    "- Bayesian Networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6500107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "\n",
    "with pm.Model() as model:\n",
    "    alpha = pm.Normal('alpha', mu=0, sigma=1)\n",
    "    beta = pm.Normal('beta', mu=0, sigma=1, shape=len(X_train.columns))\n",
    "    epsilon = pm.HalfNormal('epsilon', sigma=1)\n",
    "    mu = alpha + pm.math.dot(X_train, beta)\n",
    "    y_pred = pm.Normal('y_pred', mu=mu, sigma=epsilon, observed=y_train)\n",
    "    trace = pm.sample(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac011c6c",
   "metadata": {},
   "source": [
    "##### 10. Survival Analysis (e.g., Cox Proportional Hazards)\n",
    "What It Means: \n",
    "- Survival analysis predicts the time until an event occurs, such as customer churn or equipment failure.\n",
    "\n",
    "Outcome Interpretation: \n",
    "- Each output shows the likelihood of the event happening over time, considering various risk factors.\n",
    "\n",
    "Performance Measures:\n",
    "- Concordance Index (C-Index): Measures the model‚Äôs ability to correctly rank predictions; values closer to 1 indicate better performance.\n",
    "\n",
    "Lay Explanation: \n",
    "Survival analysis is like tracking how long something will last, based on factors that might speed it up or slow it down.\n",
    "\n",
    "Use Case: \n",
    "- For time-to-event data, such as time until a customer churns or equipment fails.\n",
    "\n",
    "Model Types: \n",
    "- Kaplan-Meier estimator, Cox Proportional Hazards Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995ebf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(data, 'time', event_col='event')\n",
    "cph.predict_survival_function(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc25f7",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf07849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to compute True Positives, True Negatives, False Positives and False Negatives\n",
    "\n",
    "def true_positive(y_true, y_pred):\n",
    "    tp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 1:\n",
    "            tp += 1\n",
    "    return tp\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    tn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 0:\n",
    "            tn += 1        \n",
    "    return tn\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    fp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 1:\n",
    "            fp += 1       \n",
    "    return fp\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    fn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 0:\n",
    "            fn += 1        \n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea735dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "TP = np.diag(cnf_matrix)\n",
    "TN = cnf_matrix.sum() - (FP + FN + TP)FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy for each class\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf443ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation for table metrics:\n",
    "import sklearn.metrics\n",
    "import mathdef matrix_metrix(real_values,pred_values,beta):\n",
    "CM = confusion_matrix(real_values,pred_values)\n",
    "TN = CM[0][0]\n",
    "FN = CM[1][0] \n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "Population = TN+FN+TP+FP\n",
    "Prevalence = round( (TP+FP) / Population,2)\n",
    "Accuracy   = round( (TP+TN) / Population,4)\n",
    "Precision  = round( TP / (TP+FP),4 )\n",
    "NPV        = round( TN / (TN+FN),4 )\n",
    "FDR        = round( FP / (TP+FP),4 )\n",
    "FOR        = round( FN / (TN+FN),4 ) \n",
    "check_Pos  = Precision + FDR\n",
    "check_Neg  = NPV + FOR\n",
    "Recall     = round( TP / (TP+FN),4 )\n",
    "FPR        = round( FP / (TN+FP),4 )\n",
    "FNR        = round( FN / (TP+FN),4 )\n",
    "TNR        = round( TN / (TN+FP),4 ) \n",
    "check_Pos2 = Recall + FNR\n",
    "check_Neg2 = FPR + TNR\n",
    "LRPos      = round( Recall/FPR,4 ) \n",
    "LRNeg      = round( FNR / TNR ,4 )\n",
    "DOR        = round( LRPos/LRNeg)\n",
    "F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)\n",
    "FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "BM         = Recall+TNR-1\n",
    "MK         = Precision+NPV-1   \n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos','check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','F1','FBeta','MCC','BM','MK'],     \n",
    "                        'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,F1,FBeta,MCC,BM,MK]})   \n",
    "\n",
    "return (mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Implementation\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplotfpr, tpr, thresholds = roc_curve(real_values, prob_values)\n",
    "\n",
    "auc = roc_auc_score(real_values, prob_values)\n",
    "print('AUC: %.3f' % auc)pyplot.plot(fpr, tpr, linestyle='--', label='Roc curve')\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()pyplot.show()\n",
    "\n",
    "# Precision-recall implementation\n",
    "\n",
    "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(real_values,prob_values)pyplot.plot(recall, precision, linestyle='--', label='Precision versus Recall')\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "pyplot.legend()pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba51b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for get many metrics directly from sklearn\n",
    "\n",
    "def sk_metrix(real_values,pred_values,beta):\n",
    "Accuracy = round( sklearn.metrics.accuracy_score(real_values,pred_values) ,4)\n",
    "Precision= round( sklearn.metrics.precision_score(real_values,pred_values),4 )\n",
    "Recall   = round( sklearn.metrics.recall_score(real_values,pred_values),4 )   \n",
    "F1       = round ( sklearn.metrics.f1_score(real_values,pred_values),4)\n",
    "FBeta    = round ( sklearn.metrics.fbeta_score(real_values,pred_values,beta) ,4)\n",
    "MCC      = round ( sklearn.metrics.matthews_corrcoef(real_values,pred_values)  ,4)   \n",
    "Hamming  = round ( sklearn.metrics.hamming_loss(real_values,pred_values) ,4)   \n",
    "Jaccard  = round ( sklearn.metrics.jaccard_score(real_values,pred_values) ,4)   \n",
    "Prec_Avg = round ( sklearn.metrics.average_precision_score(real_values,pred_values) ,4)   \n",
    "Accu_Avg = round ( sklearn.metrics.balanced_accuracy_score(real_values,pred_values) ,4)   \n",
    "\n",
    "mat_met = pd.DataFrame({\n",
    "'Metric': ['Accuracy','Precision','Recall','F1','FBeta','MCC','Hamming','Jaccard','Precision_Avg','Accuracy_Avg'],\n",
    "'Value': [Accuracy,Precision,Recall,F1,FBeta,MCC,Hamming,Jaccard,Prec_Avg,Accu_Avg]})   \n",
    "\n",
    "return (mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics For Multi-class Classification\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate accuracy\n",
    "    -> param y_true: list of true values\n",
    "    -> param y_pred: list of predicted values\n",
    "    -> return: accuracy score\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "# Intitializing variable to store count of correctly predicted classes\n",
    "    correct_predictions = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == yp:\n",
    "            correct_predictions += 1\n",
    "    #returns accuracy\n",
    "    return correct_predictions / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eeb9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of macro-averaged precision\n",
    "\n",
    "def macro_precision(y_true, y_pred):\n",
    "\n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize precision to 0\n",
    "    precision = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(y_true.unique()):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false positive for current class\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute precision for current class\n",
    "        temp_precision = tp / (tp + fp + 1e-6)\n",
    "        # keep adding precision for all classes\n",
    "        precision += temp_precision\n",
    "        \n",
    "    # calculate and return average precision over all classes\n",
    "    precision /= num_classes\n",
    "    \n",
    "    return precision\n",
    "\n",
    "print(f\"Macro-averaged Precision score : {macro_precision(y_test, y_pred) }\")\n",
    "\n",
    "# implement marco-averaged precision using sklearn\n",
    "macro_averaged_precision = metrics.precision_score(y_test, y_pred, average = 'macro')\n",
    "print(f\"Macro-Averaged Precision score using sklearn library : {macro_averaged_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of micro-averaged precision\n",
    "\n",
    "def micro_precision(y_true, y_pred):\n",
    "\n",
    "\n",
    "    # find the number of classes \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in y_true.unique():\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate true positive for current class\n",
    "        # and update overall tp\n",
    "        tp += true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate false positive for current class\n",
    "        # and update overall tp\n",
    "        fp += false_positive(temp_true, temp_pred)\n",
    "        \n",
    "    # calculate and return overall precision\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision\n",
    "\n",
    "print(f\"Micro-averaged Precision score : {micro_precision(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "#  implement mirco-averaged precision using sklearn\n",
    "micro_averaged_precision = metrics.precision_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged Precision score using sklearn library : {micro_averaged_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ed0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of macro-averaged recall\n",
    "\n",
    "def macro_recall(y_true, y_pred):\n",
    "\n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize recall to 0\n",
    "    recall = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(y_true.unique()):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false negative for current class\n",
    "        fn = false_negative(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute recall for current class\n",
    "        temp_recall = tp / (tp + fn + 1e-6)\n",
    "        \n",
    "        # keep adding recall for all classes\n",
    "        recall += temp_recall\n",
    "        \n",
    "    # calculate and return average recall over all classes\n",
    "    recall /= num_classes\n",
    "    \n",
    "    return recall\n",
    "\n",
    "print(f\"Macro-averaged recall score : {macro_recall(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "# implement macro-averaged recall using sklearn\n",
    "\n",
    "macro_averaged_recall = metrics.recall_score(y_test, y_pred, average = 'macro')\n",
    "print(f\"Macro-averaged recall score using sklearn : {macro_averaged_recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd95acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of micro-averaged recall\n",
    "\n",
    "def micro_recall(y_true, y_pred):\n",
    "\n",
    "\n",
    "    # find the number of classes \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in y_true.unique():\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate true positive for current class\n",
    "        # and update overall tp\n",
    "        tp += true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate false negative for current class\n",
    "        # and update overall tp\n",
    "        fn += false_negative(temp_true, temp_pred)\n",
    "        \n",
    "    # calculate and return overall recall\n",
    "    recall = tp / (tp + fn)\n",
    "    return recall\n",
    "\n",
    "print(f\"Micro-averaged recall score : {micro_recall(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "#  implement micro-averaged recall using sklearn\n",
    "\n",
    "micro_averaged_recall = metrics.recall_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged recall score using sklearn library : {micro_averaged_recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d50779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of macro-averaged f1 score\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "\n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize f1 to 0\n",
    "    f1 = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(y_true.unique()):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false negative for current class\n",
    "        fn = false_negative(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false positive for current class\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute recall for current class\n",
    "        temp_recall = tp / (tp + fn + 1e-6)\n",
    "        \n",
    "        # compute precision for current class\n",
    "        temp_precision = tp / (tp + fp + 1e-6)\n",
    "        \n",
    "        \n",
    "        temp_f1 = 2 * temp_precision * temp_recall / (temp_precision + temp_recall + 1e-6)\n",
    "        \n",
    "        # keep adding f1 score for all classes\n",
    "        f1 += temp_f1\n",
    "        \n",
    "    # calculate and return average f1 score over all classes\n",
    "    f1 /= num_classes\n",
    "    \n",
    "    return f1\n",
    "\n",
    "\n",
    "print(f\"Macro-averaged f1 score : {macro_f1(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "# implement macro-averaged F1 score using sklearn\n",
    "\n",
    "macro_averaged_f1 = metrics.f1_score(y_test, y_pred, average = 'macro')\n",
    "print(f\"Macro-Averaged F1 score using sklearn library : {macro_averaged_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of micro-averaged fi score\n",
    "\n",
    "def micro_f1(y_true, y_pred):\n",
    "\n",
    "\n",
    "    #micro-averaged precision score\n",
    "    P = micro_precision(y_true, y_pred)\n",
    "\n",
    "    #micro-averaged recall score\n",
    "    R = micro_recall(y_true, y_pred)\n",
    "\n",
    "    #micro averaged f1 score\n",
    "    f1 = 2*P*R / (P + R)    \n",
    "\n",
    "    return f1\n",
    "\n",
    "print(f\"Micro-averaged recall score : {micro_f1(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "# implement micro-averaged F1 score using sklearn\n",
    "\n",
    "micro_averaged_f1 = metrics.f1_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged F1 score using sklearn library : {micro_averaged_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe51cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC AUCurve Computation\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def roc_auc_score_multiclass(actual_class, pred_class, average = \"macro\"):\n",
    "    \n",
    "    #creating a set of all the unique classes using the actual class list\n",
    "    unique_class = set(actual_class)\n",
    "    roc_auc_dict = {}\n",
    "    for per_class in unique_class:\n",
    "        \n",
    "        #creating a list of all the classes except the current class \n",
    "        other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "        #marking the current class as 1 and all other classes as 0\n",
    "        new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "        new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "\n",
    "        #using the sklearn metrics method to calculate the roc_auc_score\n",
    "        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n",
    "        roc_auc_dict[per_class] = roc_auc\n",
    "\n",
    "    return roc_auc_dict\n",
    "\n",
    "roc_auc_dict = roc_auc_score_multiclass(y_test, y_pred)\n",
    "roc_auc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC implementation: \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from itertools import cycle\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Load the iris data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target# Binarize the output\n",
    "y_bin = label_binarize(y, classes=[0, 1, 2])\n",
    "n_classes = y_bin.shape[1]# We split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size= 0.5, random_state=0)\n",
    "\n",
    "\n",
    "# We define the model as an SVC in OneVsRestClassifier setting.\n",
    "# this means that the model will be used for class 1 vs class 2, \n",
    "# class 2vs class 3 and class 1 vs class 3. \n",
    "# So, we have 3 cases at #the end and within each case, the bias will be varied in order to \n",
    "# Get the ROC curve of the given case - 3 ROC curves as output.\n",
    "\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=0))\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "# Plotting and estimation of FPR, TPR\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "colors = cycle(['blue', 'red', 'green'])\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=1.5, label='ROC curve of class {0} (area = {1:0.2f})' ''.format(i+1, roc_auc[i]))\n",
    "    plt.plot([0, 1], [0, 1], 'k-', lw=1.5)\n",
    "    plt.xlim([-0.05, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic for multi-class data')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69585b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
